const V=Object.entries,et=Object.fromEntries,st="ENTRIES",L="KEYS",T="VALUES",_="";class D{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case T:return this.value();case L:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],nt=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return R(e,t,s,n,i,1,o,""),n},R=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const v=g!==t[F],z=o[p+F]+ +v,A=o[p+F+1]+1,w=o[m+F]+1,j=o[m+F+1]=Math.min(z,A,w);j<l&&(l=j)}if(l>s)continue t}R(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=O(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,st)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return nt(this._tree,t,s)}get(t){const s=k(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=k(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new D(this,L)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,I(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},k=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return k(e.get(s),t.slice(s.length))},I=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)W(n);else if(s.size===1){const[o,u]=s.entries().next().value;q(n,o,u)}}},W=e=>{if(e.length===0)return;const[t,s]=O(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&q(e.slice(0,-1),n,o)}},q=(e,t,s)=>{if(e.length===0)return;const[n,o]=O(e);n.set(o+t,s),n.delete(o)},O=e=>e[e.length-1],ut=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,M="or",$="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},N=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},P=({score:e},{score:t})=>t-e,lt=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[M]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),N(n.terms,u)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);N(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},ft=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},gt={k:1.2,b:.7,d:.5},mt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:M,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:gt},pt={combineWith:$,prefix:(e,t,s)=>t===s.length-1},Ft={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},_t={...Ft,...U},K=Symbol("*"),yt=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},X=(e,t=M)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=ht[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},S=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){ft(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],v=dt(y,m,e._documentCount,F,p,r),z=n*a*f*v,A=d.get(l);if(A){A.score+=z,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else d.set(l,{score:z,terms:[t],match:{[s]:[c]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:G(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...J.weights,...i},h=e._index.get(t.term),g=S(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);S(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);S(e,t.term,l,F,f,o,u,d,g)}return g},Y=(e,t,s={})=>{if(t===K)return yt(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Y(e,g,a));return X(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(at(i)).map(a=>At(e,a,i));return X(c,i.combineWith)},Q=(e,t,s={})=>{const n=Y(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===K&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(P),o},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Q(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(P),o};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?_t:t.autoVacuum;this._options={...mt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...pt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},B=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},wt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),xt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),Z=(e,t,s={})=>{const n={};return Q(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>B(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>B(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>B(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),V(n).sort(([,o],[,u])=>"max"==="total"?wt(o,u):xt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=ut(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},tt=(e,t,s={})=>{const n=Ct(t,e,{fuzzy:.2,maxFuzzy:3,...s}).map(({suggestion:o})=>o);return e.includes(" ")?n:n.filter(o=>!o.includes(" "))},bt=et(V(JSON.parse("{\"/\":{\"documentCount\":541,\"nextId\":541,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#education-教育\",\"3\":\"1#entrepreneurship-experience-创业经历\",\"4\":\"1#_2022-3-2023-12-数字艺术工作室-b-b-bam-【boomboombam】\",\"5\":\"1#_2022-3-2022-11-overoak-studio\",\"6\":\"1#industry-practice-行业经历\",\"7\":\"1#honours-awards-荣誉奖项\",\"8\":\"1#qualifications-skills-资格与技能\",\"9\":\"1#artist-biography-艺术家自传\",\"10\":\"2\",\"11\":\"3\",\"12\":\"3#项目作品阅读导览\",\"13\":\"3#创业历程和思考\",\"14\":\"3#自媒体运营\",\"15\":\"3#舞美活动与展会项目经历\",\"16\":\"3#人机交互和系统设计系列\",\"17\":\"3#phantasisland-造梦城\",\"18\":\"4\",\"19\":\"4#创意设计工作室\",\"20\":\"4#自媒体创作起步\",\"21\":\"4#变现和转型\",\"22\":\"4#艺术ip运营\",\"23\":\"4#国内nft市场初试ip\",\"24\":\"4#乘风破浪ip合作\",\"25\":\"4#元宇宙ip可能性拓展\",\"26\":\"4#小红书第二届上海社区熟人节\",\"27\":\"4#lifestyle-杂志合作\",\"28\":\"4#_2023广州设计周合作大艺博联名参展\",\"29\":\"4#_2023-真浪数字艺术展\",\"30\":\"4#_2022-2023总结回顾\",\"31\":\"4#平台化的尝试\",\"32\":\"5\",\"33\":\"5#前言\",\"34\":\"5#ip市场现状\",\"35\":\"5#ip传统商业模型和痛点\",\"36\":\"5#nft下的新模式\",\"37\":\"5#案例调研\",\"38\":\"5#当下市场模式和未来商业结构初想\",\"39\":\"5#商业目标和系统地图\",\"40\":\"5#用户旅程图\",\"41\":\"5#商业画布和盈利点\",\"42\":\"5#反思\",\"43\":\"6\",\"44\":\"7\",\"45\":\"7#前言\",\"46\":\"7#序\",\"47\":\"7#起源——乐土之梦\",\"48\":\"7#垃圾游乐场——游乐场历史的关键节点\",\"49\":\"7#演变历程的摘录\",\"50\":\"7#回归的心声和重生的设想\",\"51\":\"7#城市信息\",\"52\":\"7#历史与现实对比\",\"53\":\"7#假设与疑问\",\"54\":\"7#重生三部曲\",\"55\":\"7#诞生\",\"56\":\"7#来自不同身份的诉说\",\"57\":\"7#废弃材料的美妙\",\"58\":\"7#秩序的形成\",\"59\":\"7#围墙\",\"60\":\"7#蓝图设想\",\"61\":\"7#逃离\",\"62\":\"7#船\",\"63\":\"7#起航\",\"64\":\"7#造梦\",\"65\":\"7#抵达\",\"66\":\"7#塔\",\"67\":\"7#穿梭和运送\",\"68\":\"7#飞跃的梦\",\"69\":\"7#塑料城\",\"70\":\"7#乐团\",\"71\":\"7#蒸汽狂欢\",\"72\":\"7#回归\",\"73\":\"7#幕\",\"74\":\"7#结论\",\"75\":\"8\",\"76\":\"8#项目介绍\",\"77\":\"8#为什么做这个项目\",\"78\":\"8#基础背景研究\",\"79\":\"8#适合儿童创造力教育的年龄\",\"80\":\"8#合作机构的资料分析swot\",\"81\":\"8#基于儿童教育理论和技术运用提出的初步思考\",\"82\":\"8#总结\",\"83\":\"8#服务系统\",\"84\":\"8#设计概念\",\"85\":\"8#技术触点和测试\",\"86\":\"8#工作坊现场\",\"87\":\"8#反馈反思\",\"88\":\"8#测试结果亮点\",\"89\":\"8#需要改善的部分\",\"90\":\"8#未来思考\",\"91\":\"9\",\"92\":\"10\",\"93\":\"10#如何克服问题\",\"94\":\"11\",\"95\":\"11#_2024-04-11\",\"96\":\"11#goal-recognition-via-linear-programming\",\"97\":\"11#leveraging-large-language-models-llms-to-support-collaborative-human-ai-online-risk-data-annotation\",\"98\":\"11#snake-story-exploring-game-mechanics-for-mixed-initiative-co-creative-storytelling-games\",\"99\":\"11#apprentice-tutor-builder-a-platform-for-users-to-create-and-personalize-intelligent-tutors\",\"100\":\"11#the-dance-of-logic-and-unpredictability-examining-the-predictability-of-user-behavior-on-visual-analytics-tasks\",\"101\":\"11#generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models-technical-challenges-and-implications-for-monitoring-and-verification\",\"102\":\"11#unraveling-the-dilemma-of-ai-errors-exploring-the-effectiveness-of-human-and-machine-explanations-for-large-language-models\",\"103\":\"11#efficient-semg-based-cross-subject-joint-angle-estimation-via-hierarchical-spiking-attentional-feature-decomposition-network\",\"104\":\"11#interactive-prompt-debugging-with-sequence-salience\",\"105\":\"11#rassar-room-accessibility-and-safety-scanning-in-augmented-reality\",\"106\":\"11#diversity-s-double-edged-sword-analyzing-race-s-effect-on-remote-pair-programming-interactions\",\"107\":\"11#too-good-to-be-true-people-reject-free-gifts-from-robots-because-they-infer-bad-intentions\",\"108\":\"11#sealmates-supporting-communication-in-video-conferencing-using-a-collective-behavior-driven-avatar\",\"109\":\"11#_2024-04-10\",\"110\":\"11#biscuit-scaffolding-llm-generated-code-with-ephemeral-uis-in-computational-notebooks\",\"111\":\"11#interactive-explanation-of-visual-patterns-in-dimensionality-reductions-with-predicate-logic\",\"112\":\"11#building-workflows-for-interactive-human-in-the-loop-automated-experiment-hae-in-stem-eels\",\"113\":\"11#fabricating-paper-circuits-with-subtractive-processing\",\"114\":\"11#we-need-structured-output-towards-user-centered-constraints-on-large-language-model-output\",\"115\":\"11#enhancing-accessibility-in-soft-robotics-exploring-magnet-embedded-paper-based-interactions\",\"116\":\"11#a-transformer-based-model-for-the-prediction-of-human-gaze-behavior-on-videos\",\"117\":\"11#mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites\",\"118\":\"11#gaze-guided-graph-neural-network-for-action-anticipation-conditioned-on-intention\",\"119\":\"11#evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality\",\"120\":\"11#exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation\",\"121\":\"11#how-consistent-are-clinicians-evaluating-the-predictability-of-sepsis-disease-progression-with-dynamics-models\",\"122\":\"11#my-toxic-trait-is-thinking-i-ll-remember-this-gaps-in-the-learner-experience-of-video-tutorials-for-feature-rich-software\",\"123\":\"11#vllms-provide-better-context-for-emotion-understanding-through-common-sense-reasoning\",\"124\":\"11#worddecipher-enhancing-digital-workspace-communication-with-explainable-ai-for-non-native-english-speakers\",\"125\":\"11#untangling-critical-interaction-with-ai-in-students-written-assessment\",\"126\":\"11#childcidblong-longitudinal-child-computer-interaction-database-and-quantitative-analysis-for-child-development\",\"127\":\"11#sara-smart-ai-reading-assistant-for-reading-comprehension\",\"128\":\"11#impact-of-extensions-on-browser-performance-an-empirical-study-on-google-chrome\",\"129\":\"11#a-proposal-for-a-revised-meta-architecture-of-intelligent-tutoring-systems-to-foster-explainability-and-transparency-for-educators\",\"130\":\"11#personality-aware-student-simulation-for-conversational-intelligent-tutoring-systems\",\"131\":\"11#incremental-xai-memorable-understanding-of-ai-with-incremental-explanations\",\"132\":\"11#mathvc-an-llm-simulated-multi-character-virtual-classroom-for-mathematics-education\",\"133\":\"11#culturalteaming-ai-assisted-interactive-red-teaming-for-challenging-llms-lack-of-multicultural-knowledge\",\"134\":\"11#_2024-04-09\",\"135\":\"11#missing-pieces-how-framing-uncertainty-impacts-longitudinal-trust-in-ai-decision-aids-a-gig-driver-case-study\",\"136\":\"11#apprentices-to-research-assistants-advancing-research-with-large-language-models\",\"137\":\"11#actnetformer-transformer-resnet-hybrid-method-for-semi-supervised-action-recognition-in-videos\",\"138\":\"11#multimodal-road-network-generation-based-on-large-language-model\",\"139\":\"11#privacy-preserving-scanpath-comparison-for-pervasive-eye-tracking\",\"140\":\"11#eve-enabling-anyone-to-train-robot-using-augmented-reality\",\"141\":\"11#breathing-new-life-into-existing-visualizations-a-natural-language-driven-manipulation-framework\",\"142\":\"11#cymatics-cup-shape-changing-drinks-by-leveraging-cymatics\",\"143\":\"11#combinational-nonuniform-timeslicing-of-dynamic-networks\",\"144\":\"11#inclusive-practices-for-child-centered-ai-design-and-testing\",\"145\":\"11#_2024-04-08\",\"146\":\"11#clusterradar-an-interactive-web-tool-for-the-multi-method-exploration-of-spatial-clusters-over-time\",\"147\":\"11#with-or-without-permission-site-specific-augmented-reality-for-social-justice-chi-2024-workshop-proceedings\",\"148\":\"11#youth-as-peer-auditors-engaging-teenagers-with-algorithm-auditing-of-machine-learning-applications\",\"149\":\"11#an-empirical-evaluation-for-defining-a-mid-air-gesture-dictionary-for-web-based-interaction\",\"150\":\"11#human-machine-interaction-in-automated-vehicles-reducing-voluntary-driver-intervention\",\"151\":\"11#ferret-ui-grounded-mobile-ui-understanding-with-multimodal-llms\",\"152\":\"11#eye-tracking-on-text-reading-with-visual-enhancements\",\"153\":\"11#interactive-formal-specification-for-mathematical-problems-of-engineers\",\"154\":\"11#unlocking-adaptive-user-experience-with-generative-ai\",\"155\":\"11#re-ranking-news-comments-by-constructiveness-and-curiosity-significantly-increases-perceived-respect-trustworthiness-and-interest\",\"156\":\"11#indexing-analytics-to-instances-how-integrating-a-dashboard-can-support-design-education\",\"157\":\"11#webxr-a-frame-and-networked-aframe-as-a-basis-for-an-open-metaverse-a-conceptual-architecture\",\"158\":\"11#exploiting-preference-elicitation-in-interactive-and-user-centered-algorithmic-recourse-an-initial-exploration\",\"159\":\"11#allowing-humans-to-interactively-guide-machines-where-to-look-does-not-always-improve-a-human-ai-team-s-classification-accuracy\",\"160\":\"11#fair-machine-guidance-to-enhance-fair-decision-making-in-biased-people\",\"161\":\"11#evaluation-of-an-llm-in-identifying-logical-fallacies-a-call-for-rigor-when-adopting-llms-in-hci-research\",\"162\":\"11#_2024-04-07\",\"163\":\"11#chart-what-i-say-exploring-cross-modality-prompt-alignment-in-ai-assisted-chart-authoring\",\"164\":\"11#co-design-accessible-public-robots-insights-from-people-with-mobility-disability-robotic-practitioners-and-their-collaborations\",\"165\":\"11#reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments\",\"166\":\"11#towards-developing-brain-computer-interfaces-for-people-with-multiple-sclerosis\",\"167\":\"11#balancing-information-perception-with-yin-yang-agent-based-information-neutrality-model-for-recommendation-systems\",\"168\":\"11#_2024-04-06\",\"169\":\"11#navigating-the-landscape-of-hint-generation-research-from-the-past-to-the-future\",\"170\":\"11#don-t-step-on-my-toes-resolving-editing-conflicts-in-real-time-collaboration-in-computational-notebooks\",\"171\":\"11#designing-for-complementarity-a-conceptual-framework-to-go-beyond-the-current-paradigm-of-using-xai-in-healthcare\",\"172\":\"11#analyzing-llm-usage-in-an-advanced-computing-class-in-india\",\"173\":\"11#teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion\",\"174\":\"11#a-map-of-exploring-human-interaction-patterns-with-llm-insights-into-collaboration-and-creativity\",\"175\":\"11#language-models-as-critical-thinking-tools-a-case-study-of-philosophers\",\"176\":\"11#majority-voting-of-doctors-improves-appropriateness-of-ai-reliance-in-pathology\",\"177\":\"11#_2024-04-05\",\"178\":\"11#hiv-client-perspectives-on-digital-health-in-malawi\",\"179\":\"11#humanoid-robots-at-work-where-are-we\",\"180\":\"11#social-skill-training-with-large-language-models\",\"181\":\"11#designing-robots-to-help-women\",\"182\":\"11#choreovis-planning-and-assessing-formations-in-dance-choreographies\",\"183\":\"11#hierarchical-neural-additive-models-for-interpretable-demand-forecasts\",\"184\":\"11#voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots\",\"185\":\"11#which-experimental-design-is-better-suited-for-vqa-tasks-eye-tracking-study-on-cognitive-load-performance-and-gaze-allocations\",\"186\":\"11#validation-of-critical-maneuvers-based-on-shared-control\",\"187\":\"11#from-theory-to-comprehension-a-comparative-study-of-differential-privacy-and-k-anonymity\",\"188\":\"11#approximate-umap-allows-for-high-rate-online-visualization-of-high-dimensional-data-streams\",\"189\":\"11#tensions-between-preference-and-performance-designing-for-visual-exploration-of-multi-frequency-medical-network-data\",\"190\":\"11#open-vocabulary-keyword-spotting-through-transfer-learning-from-speech-synthesis\",\"191\":\"11#effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction\",\"192\":\"11#buck-you-designing-easy-to-onboard-blockchain-applications-with-zero-knowledge-login-and-sponsored-transactions-on-sui\",\"193\":\"11#_2024-04-04\",\"194\":\"11#sleepvst-sleep-staging-from-near-infrared-video-signals-using-pre-trained-transformers\",\"195\":\"11#i-did-not-notice-a-comparison-of-immersive-analytics-with-augmented-and-virtual-reality\",\"196\":\"11#learning-social-fairness-preferences-from-non-expert-stakeholder-opinions-in-kidney-placement\",\"197\":\"11#revisiting-categorical-color-perception-in-scatterplots-sequential-diverging-and-categorical-palettes\",\"198\":\"11#fakes-of-varying-shades-how-warning-affects-human-perception-and-engagement-regarding-llm-hallucinations\",\"199\":\"11#explaining-explainability-understanding-concept-activation-vectors\",\"200\":\"11#creator-hearts-investigating-the-impact-positive-signals-from-youtube-creators-in-shaping-comment-section-behavior\",\"201\":\"11#integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work\",\"202\":\"11#agora-elevator-bodily-sensation-study-a-report\",\"203\":\"11#influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr\",\"204\":\"11#exploring-emotions-in-multi-componential-space-using-interactive-vr-games\",\"205\":\"11#nlp4gov-a-comprehensive-library-for-computational-policy-analysis\",\"206\":\"11#towards-collaborative-family-centered-design-for-online-safety-privacy-and-security\",\"207\":\"11#biodegradable-interactive-materials\",\"208\":\"11#_2024-04-03\",\"209\":\"11#writing-with-ai-lowers-psychological-ownership-but-longer-prompts-can-help\",\"210\":\"11#talaria-interactively-optimizing-machine-learning-models-for-efficient-inference\",\"211\":\"11#toward-safe-evolution-of-artificial-intelligence-ai-based-conversational-agents-to-support-adolescent-mental-and-sexual-health-knowledge-discovery\",\"212\":\"11#generative-ai-in-the-wild-prospects-challenges-and-strategies\",\"213\":\"11#asap-interpretable-analysis-and-summarization-of-ai-generated-image-patterns-at-scale\",\"214\":\"11#fragmented-moments-balanced-choices-how-do-people-make-use-of-their-waiting-time\",\"215\":\"11#the-realhumaneval-evaluating-large-language-models-abilities-to-support-programmers\",\"216\":\"11#ai-and-personalized-learning-bridging-the-gap-with-modern-educational-goals\",\"217\":\"11#ieee-vis-workshop-on-visualization-for-climate-action-and-sustainability\",\"218\":\"11#evolving-agents-interactive-simulation-of-dynamic-and-diverse-human-personalities\",\"219\":\"11#unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm\",\"220\":\"11#spatial-summation-of-localized-pressure-for-haptic-sensory-prostheses\",\"221\":\"11#cultural-influence-on-autonomous-vehicles-acceptance\",\"222\":\"11#promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts\",\"223\":\"11#a-neuroergonomics-model-to-evaluating-nuclear-power-plants-operators-performance-under-heat-stress-driven-by-ecg-time-frequency-spectrums-and-fnirs-prefrontal-cortex-network-a-cnn-gat-fusion-model\",\"224\":\"11#a-unified-editing-method-for-co-speech-gesture-generation-via-diffusion-inversion\",\"225\":\"11#_2024-04-02\",\"226\":\"11#from-delays-to-densities-exploring-data-uncertainty-through-speech-text-and-visualization\",\"227\":\"11#a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other\",\"228\":\"11#exploring-how-multiple-levels-of-gpt-generated-programming-hints-support-or-disappoint-novices\",\"229\":\"11#harder-better-faster-stronger-interactive-visualization-for-human-centered-ai-tools\",\"230\":\"11#the-effects-of-group-sanctions-on-participation-and-toxicity-quasi-experimental-evidence-from-the-fediverse\",\"231\":\"11#explainability-in-jupyterlab-and-beyond-interactive-xai-systems-for-integrated-and-collaborative-workflows\",\"232\":\"11#preuve-de-concept-d-un-bot-vocal-dialoguant-en-wolof\",\"233\":\"11#cash-or-non-cash-unveiling-ideators-incentive-preferences-in-crowdsourcing-contests\",\"234\":\"11#fast-and-adaptive-questionnaires-for-voting-advice-applications\",\"235\":\"11#co-speech-gesture-video-generation-via-motion-decoupled-diffusion-model\",\"236\":\"11#that-s-not-good-science-an-argument-for-the-thoughtful-use-of-formative-situations-in-research-through-design\",\"237\":\"11#unmasking-the-nuances-of-loneliness-using-digital-biomarkers-to-understand-social-and-emotional-loneliness-in-college-students\",\"238\":\"11#rethinking-annotator-simulation-realistic-evaluation-of-whole-body-pet-lesion-interactive-segmentation-methods\",\"239\":\"11#generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g\",\"240\":\"11#tell-and-show-combining-multiple-modalities-to-communicate-manipulation-tasks-to-a-robot\",\"241\":\"11#nlp-systems-that-can-t-tell-use-from-mention-censor-counterspeech-but-teaching-the-distinction-helps\",\"242\":\"11#insightlens-discovering-and-exploring-insights-from-conversational-contexts-in-large-language-model-powered-data-analysis\",\"243\":\"11#gen4ds-workshop-on-data-storytelling-in-an-era-of-generative-ai\",\"244\":\"11#collaborative-human-ai-trust-chai-t-a-process-framework-for-active-management-of-trust-in-human-ai-collaboration\",\"245\":\"11#helmsman-of-the-masses-evaluate-the-opinion-leadership-of-large-language-models-in-the-werewolf-game\",\"246\":\"11#leveraging-digital-perceptual-technologies-for-remote-perception-and-analysis-of-human-biomechanical-processes-a-contactless-approach-for-workload-and-joint-force-assessment\",\"247\":\"11#_2024-04-01\",\"248\":\"11#playfutures-imagining-civic-futures-with-ai-and-puppets\",\"249\":\"11#delve-into-earth-s-past-a-visualization-based-exhibit-deployed-across-multiple-museum-contexts\",\"250\":\"11#a-design-space-for-visualization-with-large-scale-item-ratios\",\"251\":\"11#will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms\",\"252\":\"11#a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs\",\"253\":\"11#towards-a-potential-paradigm-shift-in-health-data-collection-and-analysis\",\"254\":\"11#evaluating-privacy-perceptions-experience-and-behavior-of-software-development-teams\",\"255\":\"11#information-plane-analysis-visualization-in-deep-learning-via-transfer-entropy\",\"256\":\"11#image-reconstruction-from-electroencephalography-using-latent-diffusion\",\"257\":\"11#aurora-navigating-ui-tarpits-via-automated-neural-screen-understanding\",\"258\":\"11#llm-attributor-interactive-visual-attribution-for-llm-generation\",\"259\":\"11#chat-modeling-natural-language-based-procedural-modeling-of-biological-structures-without-training\",\"260\":\"11#drag-your-noise-interactive-point-based-editing-via-diffusion-semantic-propagation\",\"261\":\"11#how-can-large-language-models-enable-better-socially-assistive-human-robot-interaction-a-brief-survey\",\"262\":\"11#_2024-03-31\",\"263\":\"11#designing-human-ai-systems-anthropomorphism-and-framing-bias-on-human-ai-collaboration\",\"264\":\"11#my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents\",\"265\":\"11#the-emotional-impact-of-game-duration-a-framework-for-understanding-player-emotions-in-extended-gameplay-sessions\",\"266\":\"11#humane-speech-synthesis-through-zero-shot-emotion-and-disfluency-generation\",\"267\":\"11#_2024-03-30\",\"268\":\"11#contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app\",\"269\":\"11#interactive-multi-robot-flocking-with-gesture-responsiveness-and-musical-accompaniment\",\"270\":\"11#visualizing-routes-with-ai-discovered-street-view-patterns\",\"271\":\"11#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration\",\"272\":\"11#designing-a-user-centric-framework-for-information-quality-ranking-of-large-scale-street-view-images\",\"273\":\"11#on-task-and-in-sync-examining-the-relationship-between-gaze-synchrony-and-self-reported-attention-during-video-lecture-learning\",\"274\":\"11#enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation\",\"275\":\"11#your-co-workers-matter-evaluating-collaborative-capabilities-of-language-models-in-blocks-world\",\"276\":\"11#_2024-03-29\",\"277\":\"11#tools-and-tasks-in-sensemaking-a-visual-accessibility-perspective\",\"278\":\"11#no-risk-no-reward-towards-an-automated-measure-of-psychological-safety-from-online-communication\",\"279\":\"11#circle-back-next-week-the-effect-of-meeting-free-weeks-on-distributed-workers-unstructured-time-and-attention-negotiation\",\"280\":\"11#give-text-a-chance-advocating-for-equal-consideration-for-language-and-visualization\",\"281\":\"11#enhancing-dimension-reduced-scatter-plots-with-class-and-feature-centroids\",\"282\":\"11#entertainment-chatbot-for-the-digital-inclusion-of-elderly-people-without-abstraction-capabilities\",\"283\":\"11#itcma-a-generative-agent-based-on-a-computational-consciousness-structure\",\"284\":\"11#mindarm-mechanized-intelligent-non-invasive-neuro-driven-prosthetic-arm-system\",\"285\":\"11#_2024-03-28\",\"286\":\"11#i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices\",\"287\":\"11#creating-aesthetic-sonifications-on-the-web-with-siren\",\"288\":\"11#leveraging-counterfactual-paths-for-contrastive-explanations-of-pomdp-policies\",\"289\":\"11#collaborative-interactive-evolution-of-art-in-the-latent-space-of-deep-generative-models\",\"290\":\"11#exploring-communication-dynamics-eye-tracking-analysis-in-pair-programming-of-computer-science-education\",\"291\":\"11#llms-as-academic-reading-companions-extending-hci-through-synthetic-personae\",\"292\":\"11#a-theoretical-framework-for-the-design-and-analysis-of-computational-thinking-problems-in-education\",\"293\":\"11#at-the-end-of-the-day-i-am-accountable-gig-workers-self-tracking-for-multi-dimensional-accountability-management\",\"294\":\"11#an-interactive-human-machine-learning-interface-for-collecting-and-learning-from-complex-annotations\",\"295\":\"11#cognidot-vasoactivity-based-cognitive-load-monitoring-with-a-miniature-on-skin-sensor\",\"296\":\"11#algorithmic-ways-of-seeing-using-object-detection-to-facilitate-art-exploration\",\"297\":\"11#exploring-holistic-hmi-design-for-automated-vehicles-insights-from-a-participatory-workshop-to-bridge-in-vehicle-and-external-communication\",\"298\":\"11#real-time-accident-detection-and-physiological-signal-monitoring-to-enhance-motorbike-safety-and-emergency-response\",\"299\":\"11#_2024-03-27\",\"300\":\"11#towards-human-centered-construction-robotics-an-rl-driven-companion-robot-for-contextually-assisting-carpentry-workers\",\"301\":\"11#visualizing-high-dimensional-temporal-data-using-direction-aware-t-sne\",\"302\":\"11#women-are-less-comfortable-expressing-opinions-online-than-men-and-report-heightened-fears-for-safety-surveying-gender-differences-in-experiences-of-online-harms\",\"303\":\"11#should-i-help-a-delivery-robot-cultivating-prosocial-norms-through-observations\",\"304\":\"11#the-correlations-of-scene-complexity-workload-presence-and-cybersickness-in-a-task-based-vr-game\",\"305\":\"11#thelxinoe-recognizing-human-emotions-using-pupillometry-and-machine-learning\",\"306\":\"11#solderlesspcb-reusing-electronic-components-in-pcb-prototyping-through-detachable-3d-printed-housings\",\"307\":\"11#teaching-introductory-hri-uchicago-course-human-robot-interaction-research-and-practice\",\"308\":\"11#an-exploratory-study-on-upper-level-computing-students-use-of-large-language-models-as-tools-in-a-semester-long-project\",\"309\":\"11#aiming-for-relevance\",\"310\":\"12\",\"311\":\"12#_2024-04-11\",\"312\":\"12#leveraging-large-language-models-llms-to-support-collaborative-human-ai-online-risk-data-annotation\",\"313\":\"12#amplegcg-learning-a-universal-and-transferable-generative-model-of-adversarial-suffixes-for-jailbreaking-both-open-and-closed-llms\",\"314\":\"12#oda-observation-driven-agent-for-integrating-llms-and-knowledge-graphs\",\"315\":\"12#medical-mt5-an-open-source-multilingual-text-to-text-llm-for-the-medical-domain\",\"316\":\"12#ultraeval-a-lightweight-platform-for-flexible-and-comprehensive-evaluation-for-llms\",\"317\":\"12#decomposing-label-space-format-and-discrimination-rethinking-how-llms-respond-and-solve-tasks-via-in-context-learning\",\"318\":\"12#wese-weak-exploration-to-strong-exploitation-for-llm-agents\",\"319\":\"12#learning-to-localize-objects-improves-spatial-reasoning-in-visual-llms\",\"320\":\"12#_2024-04-10\",\"321\":\"12#biscuit-scaffolding-llm-generated-code-with-ephemeral-uis-in-computational-notebooks\",\"322\":\"12#learn-from-failure-fine-tuning-llms-with-trial-and-error-data-for-intuitionistic-propositional-logic-proving\",\"323\":\"12#llms-in-biomedicine-a-study-on-clinical-named-entity-recognition\",\"324\":\"12#from-model-centered-to-human-centered-revision-distance-as-a-metric-for-text-evaluation-in-llms-based-applications\",\"325\":\"12#metacheckgpt-a-multi-task-hallucination-detector-using-llm-uncertainty-and-meta-models\",\"326\":\"12#goex-perspectives-and-designs-towards-a-runtime-for-autonomous-llm-applications\",\"327\":\"12#simpler-becomes-harder-do-llms-exhibit-a-coherent-behavior-on-simplified-corpora\",\"328\":\"12#does-mapo-tofu-contain-coffee-probing-llms-for-food-related-cultural-knowledge\",\"329\":\"12#not-all-contexts-are-equal-teaching-llms-credibility-aware-generation\",\"330\":\"12#mathvc-an-llm-simulated-multi-character-virtual-classroom-for-mathematics-education\",\"331\":\"12#culturalteaming-ai-assisted-interactive-red-teaming-for-challenging-llms-lack-of-multicultural-knowledge\",\"332\":\"12#_2024-04-09\",\"333\":\"12#khayyam-challenge-persianmmlu-is-your-llm-truly-wise-to-the-persian-language\",\"334\":\"12#sandwich-attack-multi-language-mixture-adaptive-attack-on-llms\",\"335\":\"12#comparing-two-model-designs-for-clinical-note-generation-is-an-llm-a-useful-evaluator-of-consistency\",\"336\":\"12#pitfalls-of-conversational-llms-on-news-debiasing\",\"337\":\"12#ada-leval-evaluating-long-context-llms-with-length-adaptable-benchmarks\",\"338\":\"12#agentquest-a-modular-benchmark-framework-to-measure-progress-and-improve-llm-agents\",\"339\":\"12#model-generation-from-requirements-with-llms-an-exploratory-study\",\"340\":\"12#llms-reading-comprehension-is-affected-by-parametric-knowledge-and-struggles-with-hypothetical-statements\",\"341\":\"12#a-rag-method-for-source-code-inquiry-tailored-to-long-context-llms\",\"342\":\"12#on-evaluating-the-efficiency-of-source-code-generated-by-llms\",\"343\":\"12#pm4py-llm-a-comprehensive-module-for-implementing-pm-on-llms\",\"344\":\"12#aegis-online-adaptive-ai-content-safety-moderation-with-ensemble-of-llm-experts\",\"345\":\"12#visualwebbench-how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding\",\"346\":\"12#_2024-04-08\",\"347\":\"12#llm-augmented-retrieval-enhancing-retrieval-models-through-language-models-and-doc-level-embedding\",\"348\":\"12#ferret-ui-grounded-mobile-ui-understanding-with-multimodal-llms\",\"349\":\"12#moma-multimodal-llm-adapter-for-fast-personalized-image-generation\",\"350\":\"12#the-fact-selection-problem-in-llm-based-program-repair\",\"351\":\"12#petkaz-at-semeval-2024-task-3-advancing-emotion-classification-with-an-llm-for-emotion-cause-pair-extraction-in-conversations\",\"352\":\"12#petkaz-at-semeval-2024-task-8-can-linguistics-capture-the-specifics-of-llm-generated-text\",\"353\":\"12#llm-reasoners-new-evaluation-library-and-analysis-of-step-by-step-reasoning-with-large-language-models\",\"354\":\"12#evaluation-of-an-llm-in-identifying-logical-fallacies-a-call-for-rigor-when-adopting-llms-in-hci-research\",\"355\":\"12#progressive-alignment-with-vlm-llm-feature-to-augment-defect-classification-for-the-ase-dataset\",\"356\":\"12#enhancing-clinical-efficiency-through-llm-discharge-note-generation-for-cardiac-patients\",\"357\":\"12#_2024-04-07\",\"358\":\"12#clinical-trials-protocol-authoring-using-llms\",\"359\":\"12#adapting-llms-for-efficient-context-processing-through-soft-prompt-compression\",\"360\":\"12#enhancing-llm-based-test-generation-for-hard-to-cover-branches-via-program-analysis\",\"361\":\"12#ai2apps-a-visual-ide-for-building-llm-based-ai-agent-applications\",\"362\":\"12#prompting-multi-modal-tokens-to-enhance-end-to-end-autonomous-driving-imitation-learning-with-llms\",\"363\":\"12#explaining-eda-synthesis-errors-with-llms\",\"364\":\"12#llm-based-multi-agent-systems-for-software-engineering-vision-and-the-road-ahead\",\"365\":\"12#low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language\",\"366\":\"12#squeezeattention-2d-management-of-kv-cache-in-llm-inference-via-layer-wise-optimal-budget\",\"367\":\"12#_2024-04-06\",\"368\":\"12#multicalibration-for-confidence-scoring-in-llms\",\"369\":\"12#on-the-limitations-of-large-language-models-llms-false-attribution\",\"370\":\"12#analyzing-llm-usage-in-an-advanced-computing-class-in-india\",\"371\":\"12#a-map-of-exploring-human-interaction-patterns-with-llm-insights-into-collaboration-and-creativity\",\"372\":\"12#iitk-at-semeval-2024-task-2-exploring-the-capabilities-of-llms-for-safe-biomedical-natural-language-inference-for-clinical-trials\",\"373\":\"12#_2024-04-05\",\"374\":\"12#increased-llm-vulnerabilities-from-fine-tuning-and-quantization\",\"375\":\"12#clickdiffusion-harnessing-llms-for-interactive-precise-image-editing\",\"376\":\"12#koala-key-frame-conditioned-long-video-llm\",\"377\":\"12#chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model\",\"378\":\"12#robust-preference-optimization-with-provable-noise-tolerance-for-llms\",\"379\":\"12#clue-a-clinical-language-understanding-evaluation-for-llms\",\"380\":\"12#voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots\",\"381\":\"12#can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning\",\"382\":\"12#extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction\",\"383\":\"12#_2024-04-04\",\"384\":\"12#cbr-rag-case-based-reasoning-for-retrieval-augmented-generation-in-llms-for-legal-question-answering\",\"385\":\"12#self-in-correct-llms-struggle-with-refining-self-generated-responses\",\"386\":\"12#genqrensemble-zero-shot-llm-ensemble-prompting-for-generative-query-reformulation\",\"387\":\"12#fakes-of-varying-shades-how-warning-affects-human-perception-and-engagement-regarding-llm-hallucinations\",\"388\":\"12#shroom-indelab-at-semeval-2024-task-6-zero-and-few-shot-llm-based-classification-for-hallucination-detection\",\"389\":\"12#training-llms-over-neurally-compressed-text\",\"390\":\"12#unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph\",\"391\":\"12#evaluating-llms-at-detecting-errors-in-llm-responses\",\"392\":\"12#personalized-llm-response-generation-with-parameterized-memory-injection\",\"393\":\"12#minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens\",\"394\":\"12#do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers\",\"395\":\"12#robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased\",\"396\":\"12#towards-standards-compliant-assistive-technology-product-specifications-via-llms\",\"397\":\"12#_2024-04-03\",\"398\":\"12#the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies\",\"399\":\"12#i-design-personalized-llm-interior-designer\",\"400\":\"12#aqua-combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms\",\"401\":\"12#unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm\",\"402\":\"12#improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation\",\"403\":\"12#learn-to-disguise-avoid-refusal-responses-in-llm-s-defense-via-a-multi-agent-attacker-disguiser-game\",\"404\":\"12#utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers\",\"405\":\"12#enhancing-low-resource-llms-classification-with-peft-and-synthetic-data\",\"406\":\"12#_2024-04-02\",\"407\":\"12#constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs\",\"408\":\"12#llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages\",\"409\":\"12#jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks\",\"410\":\"12#topic-based-watermarks-for-llm-generated-text\",\"411\":\"12#advancing-llm-reasoning-generalists-with-preference-trees\",\"412\":\"12#long-context-llms-struggle-with-long-in-context-learning\",\"413\":\"12#multitask-based-evaluation-of-open-source-llm-on-software-vulnerability\",\"414\":\"12#muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving\",\"415\":\"12#self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization\",\"416\":\"12#towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation\",\"417\":\"12#where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation\",\"418\":\"12#great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack\",\"419\":\"12#transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems\",\"420\":\"12#transforming-llms-into-cross-modal-and-cross-lingual-retrievalsystems\",\"421\":\"12#_2024-04-01\",\"422\":\"12#syntactic-robustness-for-llm-based-code-generation\",\"423\":\"12#will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms\",\"424\":\"12#unveiling-divergent-inductive-biases-of-llms-on-temporal-data\",\"425\":\"12#position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms\",\"426\":\"12#a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs\",\"427\":\"12#prompt-prompted-mixture-of-experts-for-efficient-llm-generation\",\"428\":\"12#mapping-the-increasing-use-of-llms-in-scientific-papers\",\"429\":\"12#llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models\",\"430\":\"12#detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms\",\"431\":\"12#do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit\",\"432\":\"12#structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation\",\"433\":\"12#llm-attributor-interactive-visual-attribution-for-llm-generation\",\"434\":\"12#enabling-memory-safety-of-c-programs-using-llms\",\"435\":\"12#can-llms-get-help-from-other-llms-without-revealing-private-information\",\"436\":\"12#efficiently-distilling-llms-for-edge-applications\",\"437\":\"12#exploring-and-evaluating-hallucinations-in-llm-powered-code-generation\",\"438\":\"12#llms-are-good-sign-language-translators\",\"439\":\"12#tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text\",\"440\":\"12#_2024-03-31\",\"441\":\"12#the-larger-the-better-improved-llm-code-generation-via-budget-reallocation\",\"442\":\"12#training-free-semantic-segmentation-via-llm-supervision\",\"443\":\"12#how-much-are-llms-contaminated-a-comprehensive-survey-and-the-llmsanitize-library\",\"444\":\"12#llm-meets-vision-language-models-for-zero-shot-one-class-classification\",\"445\":\"12#face-it-yourselves-an-llm-based-two-stage-strategy-to-localize-configuration-errors-via-logs\",\"446\":\"12#ai-act-and-large-language-models-llms-when-critical-issues-and-privacy-impact-require-human-and-ethical-oversight\",\"447\":\"12#chops-chat-with-customer-profile-systems-for-customer-service-with-llms\",\"448\":\"12#my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents\",\"449\":\"12#divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations\",\"450\":\"12#llms-are-good-action-recognizers\",\"451\":\"12#_2024-03-30\",\"452\":\"12#contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app\",\"453\":\"12#dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms\",\"454\":\"12#numerologic-number-encoding-for-enhanced-llms-numerical-reasoning\",\"455\":\"12#metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks\",\"456\":\"12#quarot-outlier-free-4-bit-inference-in-rotated-llms\",\"457\":\"12#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration\",\"458\":\"12#can-llms-master-math-investigating-large-language-models-on-math-stack-exchange\",\"459\":\"12#augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation\",\"460\":\"12#a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms\",\"461\":\"12#secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits\",\"462\":\"12#deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference\",\"463\":\"12#is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark\",\"464\":\"12#_2024-03-29\",\"465\":\"12#towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference\",\"466\":\"12#can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain\",\"467\":\"12#luq-long-text-uncertainty-quantification-for-llms\",\"468\":\"12#using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations\",\"469\":\"12#accurate-block-quantization-in-llms-with-outliers\",\"470\":\"12#can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning\",\"471\":\"12#enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning\",\"472\":\"12#are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching\",\"473\":\"12#_2024-03-28\",\"474\":\"12#i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices\",\"475\":\"12#llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces\",\"476\":\"12#llms-as-academic-reading-companions-extending-hci-through-synthetic-personae\",\"477\":\"12#enhancing-anomaly-detection-in-financial-markets-with-an-llm-based-multi-agent-framework\",\"478\":\"12#checkpoint-merging-via-bayesian-optimization-in-llm-pretraining\",\"479\":\"12#breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors\",\"480\":\"12#tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios\",\"481\":\"12#generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators\",\"482\":\"12#top-leaderboard-ranking-top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm\",\"483\":\"12#learning-from-correctness-without-prompting-makes-llm-efficient-reasoner\",\"484\":\"12#_2024-03-27\",\"485\":\"12#towards-llm-recsys-alignment-with-textual-id-learning\",\"486\":\"12#physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations\",\"487\":\"12#sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens\",\"488\":\"12#foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms\",\"489\":\"12#rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback\",\"490\":\"12#can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications\",\"491\":\"12#exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges\",\"492\":\"12#llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices\",\"493\":\"12#_2024-03-26\",\"494\":\"12#don-t-trust-verify-grounding-llm-quantitative-reasoning-with-autoformalization\",\"495\":\"12#magis-llm-based-multi-agent-framework-for-github-issue-resolution\",\"496\":\"12#exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications\",\"497\":\"12#verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms\",\"498\":\"12#accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms\",\"499\":\"13\",\"500\":\"14\",\"501\":\"15\",\"502\":\"15#职业-行业思考\",\"503\":\"15#留学思考\",\"504\":\"15#灵感分析\",\"505\":\"16\",\"506\":\"19\",\"507\":\"19#_1-职业发展\",\"508\":\"19#_2-地区与择校\",\"509\":\"20\",\"510\":\"21\",\"511\":\"22\",\"512\":\"23\",\"513\":\"23#原理\",\"514\":\"23#_1-1-技术架构\",\"515\":\"23#_1-2-交互方式\",\"516\":\"23#_1-3-技术挑战\",\"517\":\"23#灵感启发\",\"518\":\"24\",\"519\":\"25\",\"520\":\"26\",\"521\":\"27\",\"522\":\"28\",\"523\":\"29\",\"524\":\"30\",\"525\":\"31\",\"526\":\"32\",\"527\":\"32#概念\",\"528\":\"32#数据驱动设计的应用案例\",\"529\":\"32#数据驱动设计的概念与应用\",\"530\":\"33\",\"531\":\"34\",\"532\":\"35\",\"533\":\"36\",\"534\":\"37\",\"535\":\"38\",\"536\":\"39\",\"537\":\"39#心得体会\",\"538\":\"39#参考理念\",\"539\":\"40\",\"540\":\"41\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[1],\"2\":[2,6],\"3\":[3],\"4\":[4,38],\"5\":[1,7],\"6\":[3,47],\"7\":[3,28],\"8\":[3,25],\"9\":[3,56],\"10\":[1,2],\"11\":[1],\"12\":[1,4],\"13\":[1],\"14\":[1,17],\"15\":[1,20],\"16\":[1,12],\"17\":[1,26],\"18\":[4],\"19\":[1],\"20\":[1,35],\"21\":[1,38],\"22\":[1,29],\"23\":[1,123],\"24\":[1,48],\"25\":[1],\"26\":[1,25],\"27\":[2,13],\"28\":[1,42],\"29\":[2,34],\"30\":[2,31],\"31\":[1,61],\"32\":[1],\"33\":[1,23],\"34\":[1,16],\"35\":[1,9],\"36\":[1,25],\"37\":[1,6],\"38\":[1],\"39\":[1,16],\"40\":[1,51],\"41\":[1],\"42\":[1,53],\"43\":[1,104],\"44\":[1,2],\"45\":[1,27],\"46\":[1,235],\"47\":[2],\"48\":[1,63],\"49\":[1,29],\"50\":[1,42],\"51\":[1,10],\"52\":[1,14],\"53\":[1,19],\"54\":[1],\"55\":[1],\"56\":[1,10],\"57\":[1,21],\"58\":[1,12],\"59\":[1,14],\"60\":[1,5],\"61\":[1],\"62\":[1,5],\"63\":[1,5],\"64\":[1],\"65\":[1,12],\"66\":[1,10],\"67\":[1,8],\"68\":[1,9],\"69\":[1,16],\"70\":[1,12],\"71\":[1,9],\"72\":[1,26],\"73\":[1,31],\"74\":[1,128],\"75\":[2],\"76\":[1,10],\"77\":[1,30],\"78\":[1,53],\"79\":[1,52],\"80\":[1,36],\"81\":[1,113],\"82\":[1,11],\"83\":[1,11],\"84\":[1,8],\"85\":[1],\"86\":[1],\"87\":[1,10],\"88\":[1,6],\"89\":[1,9],\"90\":[1,18],\"91\":[2,15],\"92\":[1,40],\"93\":[1,11],\"94\":[1],\"95\":[3],\"96\":[5,140],\"97\":[14,128],\"98\":[12,124],\"99\":[13,128],\"100\":[15,126],\"101\":[18,133],\"102\":[16,117],\"103\":[15,142],\"104\":[6,129],\"105\":[9,142],\"106\":[13,121],\"107\":[16,139],\"108\":[12,143],\"109\":[3],\"110\":[11,126],\"111\":[11,114],\"112\":[13,165],\"113\":[6,91],\"114\":[13,140],\"115\":[11,57],\"116\":[13,120],\"117\":[10,119],\"118\":[11,129],\"119\":[13,125],\"120\":[16,177],\"121\":[14,102],\"122\":[22,120],\"123\":[11,166],\"124\":[13,148],\"125\":[9,119],\"126\":[11,174],\"127\":[7,116],\"128\":[11,147],\"129\":[17,49],\"130\":[9,129],\"131\":[8,124],\"132\":[11,163],\"133\":[13,195],\"134\":[3],\"135\":[17,152],\"136\":[9,105],\"137\":[12,186],\"138\":[9,106],\"139\":[8,132],\"140\":[9,146],\"141\":[12,115],\"142\":[7,130],\"143\":[6,110],\"144\":[9,87],\"145\":[3],\"146\":[15,153],\"147\":[15,55],\"148\":[13,130],\"149\":[13,134],\"150\":[10,107],\"151\":[8,203],\"152\":[8,106],\"153\":[8,112],\"154\":[7,112],\"155\":[14,151],\"156\":[12,155],\"157\":[15,75],\"158\":[13,142],\"159\":[19,209],\"160\":[10,121],\"161\":[17,117],\"162\":[3],\"163\":[13,120],\"164\":[16,135],\"165\":[16,131],\"166\":[10,147],\"167\":[13,129],\"168\":[3],\"169\":[11,119],\"170\":[16,96],\"171\":[17,140],\"172\":[9,191],\"173\":[10,137],\"174\":[14,140],\"175\":[11,122],\"176\":[10,194],\"177\":[3],\"178\":[8,131],\"179\":[8,128],\"180\":[7,113],\"181\":[5,143],\"182\":[8,140],\"183\":[8,144],\"184\":[10,152],\"185\":[19,95],\"186\":[8,123],\"187\":[13,105],\"188\":[12,125],\"189\":[15,135],\"190\":[10,138],\"191\":[14,140],\"192\":[17,115],\"193\":[2],\"194\":[12,136],\"195\":[14,118],\"196\":[12,152],\"197\":[10,115],\"198\":[14,131],\"199\":[6,147],\"200\":[15,115],\"201\":[16,123],\"202\":[7,75],\"203\":[15,129],\"204\":[10,163],\"205\":[8,128],\"206\":[11,107],\"207\":[3,184],\"208\":[3],\"209\":[11,115],\"210\":[9,135],\"211\":[19,117],\"212\":[9,134],\"213\":[12,198],\"214\":[14,115],\"215\":[10,165],\"216\":[11,133],\"217\":[10,146],\"218\":[10,145],\"219\":[13,151],\"220\":[9,140],\"221\":[6,99],\"222\":[10,151],\"223\":[28,165],\"224\":[12,156],\"225\":[3],\"226\":[12,131],\"227\":[17,141],\"228\":[13,133],\"229\":[11,121],\"230\":[15,107],\"231\":[12,142],\"232\":[10,125],\"233\":[10,124],\"234\":[8,166],\"235\":[10,184],\"236\":[19,102],\"237\":[16,162],\"238\":[13,166],\"239\":[14,132],\"240\":[12,156],\"241\":[16,120],\"242\":[15,142],\"243\":[11,152],\"244\":[15,147],\"245\":[13,146],\"246\":[20,187],\"247\":[3],\"248\":[8,114],\"249\":[14,160],\"250\":[10,110],\"251\":[16,158],\"252\":[14,160],\"253\":[11,163],\"254\":[10,129],\"255\":[10,128],\"256\":[7,133],\"257\":[9,200],\"258\":[7,153],\"259\":[11,141],\"260\":[11,151],\"261\":[15,168],\"262\":[3],\"263\":[10,118],\"264\":[18,108],\"265\":[16,138],\"266\":[10,151],\"267\":[3],\"268\":[21,121],\"269\":[10,178],\"270\":[8,130],\"271\":[10,137],\"272\":[15,155],\"273\":[18,127],\"274\":[11,130],\"275\":[13,140],\"276\":[3],\"277\":[9,88],\"278\":[13,117],\"279\":[18,120],\"280\":[11,77],\"281\":[10,112],\"282\":[12,204],\"283\":[10,140],\"284\":[10,185],\"285\":[3],\"286\":[17,140],\"287\":[8,79],\"288\":[9,108],\"289\":[12,129],\"290\":[13,126],\"291\":[10,139],\"292\":[14,144],\"293\":[18,110],\"294\":[12,89],\"295\":[12,100],\"296\":[11,112],\"297\":[19,128],\"298\":[14,172],\"299\":[3],\"300\":[15,120],\"301\":[10,155],\"302\":[22,170],\"303\":[11,127],\"304\":[15,148],\"305\":[9,126],\"306\":[12,130],\"307\":[12,122],\"308\":[20,178],\"309\":[3,131],\"310\":[1],\"311\":[3],\"312\":[14,128],\"313\":[17,160],\"314\":[10,124],\"315\":[12,168],\"316\":[10,138],\"317\":[16,140],\"318\":[9,161],\"319\":[10,138],\"320\":[3],\"321\":[11,126],\"322\":[16,156],\"323\":[10,134],\"324\":[18,143],\"325\":[12,114],\"326\":[11,195],\"327\":[13,104],\"328\":[12,149],\"329\":[10,154],\"330\":[11,163],\"331\":[13,195],\"332\":[3],\"333\":[13,203],\"334\":[9,193],\"335\":[17,171],\"336\":[7,105],\"337\":[10,164],\"338\":[12,134],\"339\":[9,144],\"340\":[13,166],\"341\":[12,59],\"342\":[10,90],\"343\":[10,59],\"344\":[12,153],\"345\":[14,153],\"346\":[3],\"347\":[11,95],\"348\":[8,203],\"349\":[9,141],\"350\":[9,213],\"351\":[18,84],\"352\":[16,89],\"353\":[15,174],\"354\":[17,117],\"355\":[14,188],\"356\":[11,191],\"357\":[3],\"358\":[6,108],\"359\":[10,190],\"360\":[13,164],\"361\":[11,148],\"362\":[13,157],\"363\":[6,153],\"364\":[13,99],\"365\":[15,151],\"366\":[14,119],\"367\":[3],\"368\":[6,117],\"369\":[10,174],\"370\":[9,191],\"371\":[14,140],\"372\":[19,121],\"373\":[3],\"374\":[8,115],\"375\":[8,115],\"376\":[7,159],\"377\":[9,172],\"378\":[9,157],\"379\":[8,149],\"380\":[10,152],\"381\":[13,144],\"382\":[11,164],\"383\":[2],\"384\":[14,145],\"385\":[10,102],\"386\":[10,143],\"387\":[14,131],\"388\":[17,121],\"389\":[6,174],\"390\":[12,147],\"391\":[8,188],\"392\":[8,110],\"393\":[12,139],\"394\":[16,125],\"395\":[15,158],\"396\":[9,141],\"397\":[3],\"398\":[11,173],\"399\":[6,166],\"400\":[15,143],\"401\":[13,151],\"402\":[13,121],\"403\":[17,156],\"404\":[13,128],\"405\":[10,118],\"406\":[3],\"407\":[16,116],\"408\":[15,158],\"409\":[9,181],\"410\":[7,136],\"411\":[7,194],\"412\":[7,181],\"413\":[10,131],\"414\":[8,145],\"415\":[16,137],\"416\":[12,110],\"417\":[12,178],\"418\":[14,156],\"419\":[9,132],\"420\":[8,132],\"421\":[3],\"422\":[7,142],\"423\":[16,158],\"424\":[9,136],\"425\":[13,145],\"426\":[14,160],\"427\":[9,168],\"428\":[9,195],\"429\":[13,132],\"430\":[12,143],\"431\":[15,116],\"432\":[15,162],\"433\":[7,153],\"434\":[8,176],\"435\":[11,147],\"436\":[6,97],\"437\":[9,173],\"438\":[6,127],\"439\":[18,136],\"440\":[3],\"441\":[11,135],\"442\":[7,148],\"443\":[12,174],\"444\":[11,132],\"445\":[15,150],\"446\":[16,60],\"447\":[10,181],\"448\":[18,108],\"449\":[12,109],\"450\":[5,126],\"451\":[3],\"452\":[21,121],\"453\":[11,175],\"454\":[8,141],\"455\":[14,153],\"456\":[9,123],\"457\":[10,137],\"458\":[11,185],\"459\":[10,140],\"460\":[16,195],\"461\":[11,132],\"462\":[13,178],\"463\":[13,125],\"464\":[3],\"465\":[12,124],\"466\":[13,131],\"467\":[7,176],\"468\":[11,130],\"469\":[7,182],\"470\":[12,180],\"471\":[15,152],\"472\":[17,96],\"473\":[3],\"474\":[17,140],\"475\":[11,164],\"476\":[10,139],\"477\":[13,120],\"478\":[8,127],\"479\":[13,188],\"480\":[12,137],\"481\":[13,118],\"482\":[12,197],\"483\":[9,127],\"484\":[3],\"485\":[8,173],\"486\":[11,158],\"487\":[11,129],\"488\":[11,163],\"489\":[14,134],\"490\":[12,129],\"491\":[15,152],\"492\":[15,147],\"493\":[3],\"494\":[10,157],\"495\":[10,163],\"496\":[15,133],\"497\":[12,140],\"498\":[11,109],\"499\":[1,12],\"500\":[5,12],\"501\":[1],\"502\":[2,34],\"503\":[1,3],\"504\":[1,5],\"505\":[1,4],\"506\":[1,7],\"507\":[2,31],\"508\":[2,57],\"509\":[1,103],\"510\":[1,88],\"511\":[2,44],\"512\":[1,41],\"513\":[1],\"514\":[2,10],\"515\":[3,10],\"516\":[3,45],\"517\":[1,29],\"518\":[2,85],\"519\":[10,275],\"520\":[3,59],\"521\":[1,140],\"522\":[1,91],\"523\":[2,85],\"524\":[2,86],\"525\":[1,110],\"526\":[1],\"527\":[1,49],\"528\":[1,23],\"529\":[1,38],\"530\":[1,77],\"531\":[1,129],\"532\":[2,91],\"533\":[2,120],\"534\":[1,68],\"535\":[2,71],\"536\":[1,12],\"537\":[1,36],\"538\":[1,64],\"539\":[2,118],\"540\":[1,3]},\"averageFieldLength\":[8.635859519408497,111.08959736814853],\"storedFields\":{\"0\":{\"h\":\"Home\"},\"1\":{\"h\":\"\"},\"2\":{\"h\":\"EDUCATION 教育\",\"t\":[\"2015年9月-2019年6月广州美术学院 - 景观艺术设计艺术学学士\",\"2024年9月- 2025年7月香港理工大学 - 创新多媒体娱乐理学硕士\"]},\"3\":{\"h\":\"ENTREPRENEURSHIP EXPERIENCE 创业经历\"},\"4\":{\"h\":\"B.B.BAM 【BoomBoomBAM】\",\"t\":[\"IP运营、制作\",\"雪橇熊NFTCN、雪橇熊x大艺博特别版\",\"芒果TV《乘风破浪3》合作艺术家\",\"IP合作\",\"2022 年 12 月 IP 雪橇熊 x GPLAB 汽车涂鸦绘画\",\"2022年11月 IP雪橇熊参展上海Web3.0加密艺术展——上海喜马拉雅博物馆\",\"留学项目合作\",\"IGO源创有术——- 目前是IGO团队的合作专题导师，工作室服务内容包括UX / HCI / 服务设计 / 产品设计 / 数字媒体 / 游戏设计 / 空间交互等方向。主打专精化教学模式，帮助学生打造专属的个人IP作品集（绝对反对模板化的作品集辅导），帮助学生规划和形成从留学到职业发展的长期路线，目前线上教学为主，其中包括暑期举行的内部大型团队工作坊等。\"]},\"5\":{\"h\":\"\",\"t\":[\"生活美学与创意\",\"空间/布景/婚礼/装置设计\",\"活动/婚礼策划\"]},\"6\":{\"h\":\"INDUSTRY PRACTICE 行业经历\",\"t\":[\"2020 -2022 WOWWedding 高级空间设计师/活动策划师\",\"为几十场活动和公司首场百万元婚礼活动开发创意设计方案\",\"领导并建立健全系统的现场协调流程\",\"拓展客户群，维护潜在客户关系，获得 \\\"销售冠军 \\\"设计师称号\",\"2020 KIND 杂志 设计主管\",\"艺人合作，为杂志担任后期艺术指导\",\"与中国艺人李俊毅合作，为 \\\"Kind Talents \\\"节目进行杂志数字艺术创作\",\"周年庆典活动策划\",\"2019-2020 SilverLove 空间&灯光&宴会设计师\",\"提供设计概念、空间布局规划和销售报价，项目全流程负责总价约 80 万元人民币\",\"与客户、承包商和供应商联系，协调现场事宜\",\"从构思、执行到完成的项目管理\",\"2018 悬亮子工作室 公共艺术设计师&景观设计师\",\"公共艺术设计、设计概念开发\",\"立讯空港小镇项目公共艺术及导览系统设计\",\"济南万科山大路创新项目景观设计\",\"苏州一尚门诚品书店项目装置设计\"]},\"7\":{\"h\":\"HONOURS & AWARDS 荣誉奖项\",\"t\":[\"2023 年 6 月大湾区首届元宇宙数字艺术节元宇宙空间创意类铜奖\",\"2023 年 6 月大湾区首届元宇宙数字艺术节加密艺术类优秀奖\",\"2019 年 11 月由亚洲设计学院奖组委会和亚洲建筑与城市联盟颁发的第17届亚洲设计学院奖文化建筑与空间组优秀奖\",\"2019 年 11 月由全国高等艺术院校建筑与环境艺术设计教学年会组委会颁发的第十六届全国高等院校艺术类建筑与设计专业教学年会银奖\",\"2019年 7 月由羊城设计联盟、广东省高等学校建筑与环境艺术设计学术委员会颁发的 \\\"2019未来展 \\\"广东省第四届高校建筑与环境艺术设计毕业设计 \\\"羊盟奖 \\\"二等奖\",\"2019年 6 月 广州美术学院颁发2019届本科生优秀毕业创作奖\",\"2019 年 6 月 广州美术学院建筑与应用艺术学院颁发2019广州美术学院GAFA SAA荣誉奖\",\"2018 年 1 月 优秀案例，色·块——广州市天河区立德桥下空间微改造方案，由广州市天河区住房和建设水务局颁发\"]},\"8\":{\"h\":\"QUALIFICATIONS & SKILLS 资格与技能\",\"t\":[\"设计软件 PhotoShop, AdobeIllustrator, InDesign, Adobe Photoshop Lightroom, Adobe Premiere, Miro, Figma\",\"绘图/建模软件 CAD、SketchUp、Cinema 4D、Blender\",\"游戏引擎 Unity\",\"编程相关 C#、Arduino IDE（C++）\"]},\"9\":{\"h\":\"ARTIST BIOGRAPHY 艺术家自传\",\"t\":[\"2023 年 5 月，武汉第三届毕业生艺术博览会参展艺术家。\",\"2023 年 5 月，数字艺术家，真浪数字展，与真浪数字艺术、EZVR、RIVTOWER TECHNOLOGY等联合举办。\",\"2023 年 3月，大艺博联合数字艺术家，2023 广州设计周。\",\"2022 年 12 月杂志合作艺术家，个人作品 Phantasisland 被中国时尚杂志《YOURS》12 月刊收录。\",\"2022 年 11 月 FASHION ZOO 全球创作者联盟 IPEDIA 艺术家成员，IPEDIA 是 FASHION ZOO 推出的旨在聚集全球创作者和世界知名 IP 的平台。\",\"2022 年 11 月 在伦敦 Espacio 画廊举办的展览 \\\"乌托邦之跃 \\\"中担任参展艺术家。\",\"2022 年 8 月 应邀参加上海第二届小红书官方社区相亲节，并作为空间负责人参与数字艺术共创。\",\"2022 年 7 月 与艺术家恩利合作，共同制作数字艺术场景，并作为《生活时尚》2022 年 7 月刊的电子封面展示。\",\"2021 年 7 月 参展艺术家及艺术访谈嘉宾，参加 2021 年重庆 O'Kids 国际儿童艺术节，个人作品《Phantasisland》入选 2022 年 7 月 O'Kids 国际儿童艺术节展厅档案。\"]},\"10\":{\"h\":\"\",\"t\":[\" 规划中---\"]},\"11\":{\"h\":\"项目经历\"},\"12\":{\"h\":\"\",\"t\":[\"每个作品系列都是一个独特的IP，代表了我不同阶段的成长与探索，也体现了我对跨学科的理解和从设计师到复合型岗位发展的转变。\"]},\"13\":{\"h\":\"\"},\"14\":{\"h\":\"自媒体运营\",\"t\":[\"B.B.BAM（車间爆炸）：2022年，我创立了数字艺术实验室，致力于将创意与技术相结合，探索数字艺术的无限可能。其中Monsterisland（怪物工厂）是我在自媒体创业阶段构建的IP世界，通过数字藏品（NFT）运营取得了显著的推广效果，获得了众多合作机会，为后续发展奠定了坚实基础。\",\"B.B.BAM (車间爆炸) - 从空间设计到数字艺术\",\"Web3商业形态构想\"]},\"15\":{\"h\":\"舞美活动与展会项目经历\",\"t\":[\"在毕业前的两年里，我积累了丰富的舞美、活动与展会项目经验。这段经历不仅让我突破了后端设计的角色局限，还全方位提升了我的专业能力。\",\"O.O.K Studio 创立与发展：2022年，我创立了O.O.K Studio，专注于舞美活动全案策划与设计执行。创立O.O.K Studio后，我的角色逐步向多岗位能力发展。作为工作室的创始人和负责人，我不仅亲自参与项目的策划与设计执行，还负责团队管理与运营、客户沟通与维护等工作。\",\"活动策划和设计经历思考\"]},\"16\":{\"h\":\"\",\"t\":[\"在人机交互和系统设计领域，我坚信未来各行业都需要复合型人才。通过游戏引擎、硬件装置的运用，我不断提升对技术的理解，突破传统动画形式的局限，探索多元化的展现方式。\",\"The City Of Desires\",\"MOODFLOW\"]},\"17\":{\"h\":\"\",\"t\":[\"这是我本科的个人毕业设计项目和延续。通过乌托邦式的畅想，我探讨如何为孩子们创造一个充满自由与创造的空间。我相信，孩子们拥有巨大的潜力，而这些潜力往往在自由探索与冒险中得以释放。通过结合虚构的叙事与真实的过去，我提出一个设想：如何借助废弃材料等媒介，激发孩子们的创造力与探索精神，让他们在无拘无束的环境中发现自我、实现梦想。\",\"这是一个开放性的命题，旨在引发更多人对孩子们成长环境与教育方式的思考。我期待通过这个项目，激发更多关于孩子创造力教育的讨论与实践，共同探索孩子们的成长路径。最终的答案或许需要更多的阅读者与参与者去解读与实现，而我也将持续关注与推动这一领域的创新与发展。\",\"Phantasisland(造梦城)毕业设计\",\"Phantasisland(造梦城)参加2021年O'Kids国际儿童艺术节\",\"Phantasisland#（AR工作坊）\"]},\"18\":{\"h\":\"B.B.BAM（車间爆炸）的发展\"},\"19\":{\"h\":\"创意设计工作室\"},\"20\":{\"h\":\"自媒体创作起步\",\"t\":[\"2021年，我开始正式接触小红书的内容创作（过去基本都是以浏览为主），作为中国社交媒体平台中比较有审美调性的平台，类比国外的Pinterest和Instagram，当时我还在公司就职，一开始只是将我的一些过往作品分享在平台上。\",\"试发了几个作品，整体数据还不错，不过基本以纯分享形式为主，所以我打算深度分析我的一些过往作品，恰逢当时因为我的毕设收到活动邀请，我决定将我的作品心得也进行分享。\",\" 这一次的数据基本开始打开了小红书的流量池，也是第一次粉丝快速增长期，通过账号数据分析，我的粉丝/赞数据也算不错，不是各种无用的流量笔记（指的是一些单纯获得流量但实际账号无法取得关注的笔记），甚至有一些准备毕业的同学也会询问各种问题。\",\"这时候我也意识到一个新的问题，就是如何通过自媒体变现。\",\"当时我正在公司工作，做的内容和活动类策划、设计相关，在策划案中各种概念图就尤为重要，设计方案的效果图我也是比较熟悉了，加上我这时候发布的内容也多是3d建模的数字作品，所以我想试着把这个风格结合进我的账号内容中，打造成一个3D数字设计、美陈设计、活动策划/设计的工作室。\"]},\"21\":{\"h\":\"变现和转型\",\"t\":[\"这时候我便开始分成两个部分去做内容：\",\"继续发布一些数字作品，强化个人的设计风格。\",\"发布一些自己设计的商业案例（已经施工完成），以及部分设计项目策划和概念。\",\"那段时间发的一些案例都得到比较好的反响，这是第二次粉丝快速增长期，也有一些客户聊到项目合作的事情，项目类型包括空间设计、婚礼设计、美陈设计、时尚杂志商业策划等。次年，也就是2022年年初，我辞职了。当时也是因为小红书平台我拿到了一些资源，于是准备自己创办工作室承接项目。\",\"在2022年，大家非常熟悉的一个概念——“元宇宙”出现了，由于我当时账号定位还未成熟，出现了两波客户群体，一个就是符合最开始想法的设计项目，另一个类型是找我聊IP合作的。\",\"当时因为都是我一个人承包了设计、运营、策划等事项，所以在这个节点上，我需要做一些取舍，以及接下来账号我该如何运营，定位得更加精准一些。\",\"思考后，我决定趁着这波浪潮，去发展数字艺术IP板块，而设计工作室内容转移到新的小红书账号上。\"]},\"22\":{\"h\":\"艺术IP运营\",\"t\":[\"趁着数字藏品、元宇宙这个概念正火热于市场，我正式设立了这个工作室。\",\"“B.B.BAM”是我个人的虚拟艺术实验室，2022年正式诞生了《MonsterIsland》《Forbidden A.R.E》等系列作品。基于构建的叙事世界，通过构建富有想象力的故事世界，融合叙事空间、“人”和“物”，创造出艺术人物和数字概念场景。\",\"名字源于“BoomBoomBAM”，希望在未来的数字时代，人们的灵感能够不断迸发，为世界创造更多充满灵魂的乐土。\",\"我创作的两个主要系列是《MonsterIsland（怪物工厂）》《Forbidden A.R.E（禁地）》，\"]},\"23\":{\"h\":\"国内NFT市场初试IP\",\"t\":[\"创作背景来源于三年前，我的毕业创作Phantasisland（造梦城），通过一个名为“047”的故事拼贴文本结合历史背景加以穿插，进入当下的城市状况做一个新的叙事性创想，“047”文本讲述了2199年世界被垃圾堆满，而一艘“047”的船载着这堆垃圾带孩子逃离世界而来到许多奇幻的岛屿，这堆垃圾在这些岛屿里得到新生，孩子们逃离残酷的现实在这片梦幻乐土中自由创造，最后的结尾留白给人想象，错过船的孩子似乎便无法再进入这个重生的世界里。延续这样的故事背景，我将同样以一个拼贴画和故事文本去推动一个新的创作《MonsterIsland》。\",\"这是《MonsterIsland》故事背景： 2199年间，世界被垃圾堆满，资源消耗过度，物质能量转换出现混乱，据说100多年前出现了一个不为人知的禁地，在真实与虚幻的世界之间交错衍生，一直扰乱了世界的平衡，时空出现裂缝，混沌错乱，虚实在不断变换，动植物也受到污染，细胞开始变异，暗物质和光能量在矛盾体中不断对抗，垃圾堆中一些动植物的遗体早已不复原貌，可谓是“怪物”。虽说世界混沌，但有黑暗就有光明，这些“怪物”同样被当作垃圾运上船体前往某个地方，和那帮孩子去的地方是一样的，一个梦境般的地方，曾经被灾难分开的岛屿成为了孩子们把垃圾变废为宝的魔法屋，而这一趟旅途中，老头们唯一没有带孩子们去的地方就是一个暗藏在水下的秘密基地——这些“怪物”被送往这里，一些研究者也是当年有幸乘上那艘“梦之舟”来到这里的，他们希望研究这些污染的物质来寻求能量混乱的根源，MonsterIsland 就这样形成，这么多年来一直被保密，他们通过不断努力将“怪物”以重新的定义方式呈现出来，生灵无法死而复生，但是基因可以被提取可以进化，光能量注入实验室的各个机器里，“怪物诞生了！”“MonsterIsland Plan启动！”，它们将会是新光明能量的助力者。\",\"雪橇熊（SledBear）是IP故事《MonsterIsland》的首个创作，于2022年3月至5月间以数字IP形式在nftcn平台独家发行。故事中，SledBear的天赋在于运动：“它有熊一样的头，但没有熊的脚，不用眼睛就能判断方向，尾巴是松鼠和浣熊的结合体，天生喜欢拿着雪橇和特制的滑雪板，熊的特征非常明显，所以被命名为雪橇熊（SledBear）。”\",\"当时在平台上发售了一部分数字艺术IP作品，推出赋能计划，平台因为是UGC类型，所有作品都需要自己管理和运营。这时候我集合了艺术创作、运营、策划、市场营销为一体，并且和藏家进行沟通发布理念甚至共创。\",\"在第一个赋能计划中，我发现收藏家对新产品的购买能力不足。经过访谈，发现一些收藏家认为独特版本更新的速度不能跟上购买的步伐。收集会员卡的过程非常冗长，使长期投资变得遥不可及。在调研情况后，我召开了与内部收藏家的一个DAO会议，将积分系统版本升级以及联合线上线下销售方式的推出。\",\"在第二阶段的发布计划中，收藏家现在可以清楚地评估自己当前的积分情况。我加快了会员获取标准，以解决玩家们在购买早期独特版时遇到的缓慢和困难的问题。此外，我还推出了各种促销活动和激励措施，以增强购买欲望。\",\"因为在当时各种规范仍未完善，许多创作者因为难以负担长期运营的成本，以及团队能力不足，都停止发售了，市场也进入了冷却，我也没有完成预计的计划。\",\"不过最后我还是为雪橇熊的一个系列做了一个小小的整理，利用静态图像和声音的尝试，完成了这个作品。\",\"但是，在这个时间节点，我虽然收获了国内NFT不完善体系的“失败”经历，但是也通过各种平台快速的到曝光，获得了芒果TV《乘风破浪3》官方数字藏品的创作邀请，以及小红书官方第二届社区熟人节的数字艺术主理人板块邀请，还有艺术家恩利的杂志数字艺术合作。在这个混乱市场中，我们看到了NFT的不成熟以及监管不完善下的潜在风险，但是也确实让一些内容创作者得到良性的发展机会，可以快速获得曝光被大众认知。\"]},\"24\":{\"h\":\"乘风破浪IP合作\",\"t\":[\"在雪橇熊发售一段时间后，我得到了芒果TV发布的《乘风破浪3》的数字藏品联合发售活动邀请，在芒果TV旗下平台小芒平台进行社区发布和成员投票，最终得到了观众的喜爱，成为节目合作的联合创作者。\",\"左上：乘风破浪3 x 子午莲·浴火\",\"右上：乘风破浪3 x 野蘑菇·采梦\",\"左下：乘风破浪3 x 紫苑花·展翼\",\"右下：乘风破浪3 x 蒲公英·乘风\",\"当时的作品以乘风破浪的姐姐“生日花”作为本次创作的切入主题 借由花语去表达、去描绘、去创造。 花是梦的起点，花映内心，心生梦境， 世界中还有许多未知的答案等待着采花者去追寻和探索 由花的主体去衍生梦的世界，它们在生长，也在创造。\",\"四幅作品的四个主题代表了乘风破浪的心路经历。\",\"浴火中想要突破重生，唯有踏入风暴，坚持自我；\",\"未知的历险之中又往往有许多星光在指引前行， 幻想和美好就在心中，去大胆地享受和创造；\",\"万千风景的途中也该偶尔放慢脚步去自我思考， 领略得失，才能找到最真的自我；\",\"而终点的结果无论如何都不能放弃，等待风来， 重新启航，乘风破浪。\"]},\"25\":{\"h\":\"元宇宙IP可能性拓展\"},\"26\":{\"h\":\"小红书第二届上海社区熟人节\",\"t\":[\"在COVID19期间，小红书官方邀请了第二届上海小红书社区熟人节，作为数字艺术板块的主理人共创作品，本来8月4日-8日在上海百禧公园举行，共有35个创意空间，150多场空间活动，200多位小红书作者，因为COVID19的原因，改到了室内，虽然空间小了，但是在当时大家出行困难的时期，难得可贵的一场活动确实给大家带来许多欢乐，因为很多时候线上办公，透过手机电脑去观看世界的种种，此时的”元宇宙“概念也成了大家连接物理和数字世界的一个钥匙，所以我认为数字化产品的确是非常有价值的，这两年AI、XR、区块链技术的越发成熟，都是在一点点为电影中的元宇宙世界做桥梁，这也是我当时所行的一些新的收获。\",\"下面是当时参加活动的两个作品\"]},\"27\":{\"h\":\"LIFESTYLE 杂志合作\",\"t\":[\"与艺术家En-li恩利共同创作，将恩利的虚拟人与我的数字艺术场景共创融合，并作为LIFESTYLE 2022年7月刊电子封面展示。\",\"这也是当时NFT市场下，大家对于艺术IP这个概念重视起来的体现，每个创作者的特性可以被放大，也可以被融合，甚至再创，同时虚拟人的使用也打破了杂志真人拍摄的传统，全数字化的内容也在向虚拟世界迈进一步。\"]},\"28\":{\"h\":\"2023广州设计周合作大艺博联名参展\",\"t\":[\"大艺博（全称：国际大学生艺术博览会）2012年由广州华艺大艺文化艺术发展有限公司创办于年广州，迄今已成功举办十五届博览会，大艺博是国内最大规模的集中展示美术专业当届毕业生及青年艺术家艺术创作与艺术探索的平台，囊括了国内最具艺术才华、最具市场前景、最具艺术价值的优秀青年艺术家，因其丰富性、广泛性、代表性、前瞻性、权威性，而成为吸引大量艺术机构、艺术品投资人、爱好者、媒体的艺术盛会。\",\"大艺博当时在推出online的app，也找到我进行合作，这时候其实已经进入数字藏品/NFT的市场冷却期了，但是我们还是希望透过一些联动实体艺术和数字艺术的方式来尝试售卖，对于传统艺术机构来说，他们也面临数字商业创新，online、快闪店也是他们策划的一些新商业模式，我也参与了广州设计周活动的一些发布机制讨论。\",\"在现场，我也听到了一些特别有意思的声音。\",\"数字艺术作品比起实体绘画缺乏唯一性，可复制性太强。\",\"虽然我当时的作品都是3D渲染而成的，但是在AI时代许多不太懂的观众会对作品生成的工具产生疑问，AI创作的确有时候已经非常接近软件渲染了。那对于数字艺术家们该如何去选择未来发展的道路能呢。\",\"有部分艺术爱好者、收藏家对于实体产品十分感兴趣。所以也让我思考了实体和数字产品的关系，先后推出市场对于IP的影响是什么，以及这种虚实结合的方式是否可以成为变现方式。\"]},\"29\":{\"h\":\"2023 真浪数字艺术展\",\"t\":[\"当时非常感谢真浪数字艺术和EZVR团队的这次艺术联合活动，也很荣幸和各位艺术家及作品一起在展览见面 ，我们确实敢说这是2023年国内开年最大型的元宇宙数字艺术展，展览不仅仅局限于画作外观观看，点进去还可以观看详情介绍，AIGC、潮流、虚拟服装、虚幻场景等等，同时我们也是使用NFT门票进行售卖希望给观展的用户留有区块链上的数字资产。\",\"当时参加完2023广州设计周的合作后，我其实已经发现NFT当下的惨淡状态国家的规范暂不明确，加上之前产生一些不良效应，很多圈内人已经退出，同时对于大众来说，这仍然是一个陌生的词，但是我们还是集合了不同的团队合作方努力做了一次尝试，希望让大众能够客观认识和看待“元宇宙”、“数字资产”等词，看到AI时代下，数字产品的一些潜在应用场景。\",\"这是我在平台中的一个录制视频，我的IP雪橇熊也参与到了这个梦幻的数字展馆中。 这段期间，基本我都是围绕两个IP故事在创作和发布平台中（下图），同时也参加了一些活动展览，比如上海Web3.0加密艺术展、英国伦敦飞跃乌托邦艺术展、作为视觉艺术家被邀请到成都春熙路的大屏展示。 \"]},\"30\":{\"h\":\"2022-2023总结回顾\",\"t\":[\"2022就像是元宇宙，Web3的元年（虽不是真正的起源时间），一大波热点的推动在上半年带来了许多话题，大家觉得元宇宙很酷很有新鲜感，产生许多爆炸的信息过后AIGC这个词又出来了，大家好像又遗忘了元宇宙这个概念，进入了各种对ai绘画视频等等的探索，进入2023年，艺术文化、科技等等各类话题仍然不断，不管是真正的学习探索还是热度的炒作，大家有共识的一点就是新的互联网时代确实在向我们走来，只是前期经历的质疑和混乱是每个时代都存在的。\",\"就像我参加的真浪数字艺术团队数字艺术馆合作，各个团队辛苦了很久，我也和团队一起探讨目前的行业发展现状以及技术难题，对于未来AIGC时代艺术赋能的探索仍有许多路要走，但是探索者是必须存在的，而这次的各大团队尝试了一个商业模式的探索，通过数字艺术展与权益的结合也许就是一个新的可能（虽然国内NFT混乱的现状可能让许多人还无法相信和重新接受），但也只有大家的尝试才能让行业未来良性发展起来。\",\"因为这短短一年多间，看到许多概念的大火又没落，新技术不断出现，当时的变现模式已经难以继续支撑，所以我需要重新思考未来商业结构，那个时候我也做了一个决定，就是留学读研。\",\"为什么选择留学\"]},\"31\":{\"h\":\"平台化的尝试\",\"t\":[\"我在2022-2023年参加的重要展览和活动。通过在不同地区举办活动，并利用多种媒体，我加强了数字艺术家的个人形象，并获得了IP的曝光和线上合作机会。这是我的基本商业模式，逐步建立品牌形象。未来将会添加更多盈利和推广方法。\",\"这是一个看似理想的模式，不过我还是发现了许多弊端，复盘了可能在当时有着这波NFT与元宇宙的营销热潮，让我利用这个商业模式较好的打开了市场，但是NFT与当下web2的社交媒体其实是有一定隔阂的，web2社交媒体其实没法让用户拥有所有权，而最终上传的作品更多就是一张图片、一个视频，同时web2生态中许多“流量密码”其实都让创作者没法真正去投入到内容当中，抄袭、搬运都对内容创作者造成了很大的伤害又无法受到保护。\",\"区块链技术确实对创作者产权来说是一项很伟大的发明，不过路还很长，不仅仅是技术层面的，对于大众如何去认识这串代码又是一个难题，同时空间计算、AI生成技术等的出现也让未来元宇宙看似更加清晰了，不仅是一段酷炫的动画来向观众展示，不同的穿戴式设备和生成内容的方式，也可能让web3领域有了更多元化的场景。所以，尽管具体的未来我们也难以预测，但保持学习探索，这是我一直坚持的。\",\"那除了和NFT、数字艺术IP相关的项目，我也尝试在当下将工作室转换成一个平台，希望未来大家认识的不仅仅是我，而是可以尽情发挥各种潜力和特色的创作者们。\",\"包括我目前也在进行的留学作品集辅导服务，对于很多转专业的同学来说，这些领域都是非常陌生的，但我也经常告诉同学们，其实在数字艺术领域的这些经历也并非我大学专业技能所传授的，但是，对于创作、策略的思维方式，是我在大学这种开放创作环境下所建立和思考总结的，所以我也希望将这部分思考慢慢传授给大家，就和我经常提到的，留学作品集并不只是一份项目集合的册子，而是一段思考过程，一个可以展示你成长、思考过程、再转换到成果的方式，同时这也是培养你对未来的思考，什么是你感兴趣的，去找到交叉点中最擅长的那部分，我们都是在新时代中不断探索这个交界点。\"]},\"32\":{\"h\":\"Web3商业形态构想\"},\"33\":{\"h\":\"前言\",\"t\":[\"2021年NFT热潮席卷全球，让许多有着一夜暴富的心态的投机者入场炒作，艺术的价值被重新赋予，但是过度金融化的模式把真正的价值掩盖，入局者纷纷退场，很快又陷入了寒冬。我认为，未来的区块链对于艺术、金融等领域的价值必然存在，我们需要在正确的监管下去发挥其最大价值。通过前期创业经历的痛点分析，我提出了一个基于Web3底层逻辑的商业系统，平台核心是提供IP孵化机制，重新链接IP、产品和消费者的关系，构造一个针对Z世代用户的开放式世界，通过全链上模式的经济体系和开放性系统玩法，来改变产品推广、IP孵化的传统逻辑，将消费、社交、游戏融为一体。\"]},\"34\":{\"h\":\"IP市场现状\",\"t\":[\"2020年，中国的授权商品零售市场超过了1000亿元人民币，而在2021年，它仍然保持着积极的增长态势，包括零售销售额、授权IP数量、授权收入以及授权品牌公司数量。所有这些指标都显示出了全面增长。全球范围内，授权商品的零售额超过了3000亿美元。中国的IP授权市场仍然处于新兴阶段。与北美最大的市场（占据58%）相比，中国在2019年的全球授权商品零售销售额份额不到5%。\"]},\"35\":{\"h\":\"IP传统商业模型和痛点\",\"t\":[\" IP产业链条长而复杂，涉及复杂的生产和分销流程。只有少数几个拥有更好时机的主要IP才有可能取得成功。尤其是在过去的十年，成功的新IP很少见。 传统IP创作和运营的模式下，IP的版权与大部分的收益往往归属于头部的中心化机构，强者恒强。\"]},\"36\":{\"h\":\"NFT下的新模式\",\"t\":[\" 2015 年以来，IP 泛娱乐新生态产业链的概念已经出现；2021 年，NFT 将迎来发展热潮，IP 运营逻辑也将发生变化。 传统IP授权模式的授权流程长、标准化程度低。因此，旧IP的NFT化具备一定的门槛，这不仅需要跨行业，还涉及到标准授权、IP价值评估、商业价值计算、内容开发等一系列环节;未来的新数字内容，具足IP与NFT运营(商业化)，在诞生的初期就创新了IP授权模式，链条更短、变现效率更高。\"]},\"37\":{\"h\":\"案例调研\",\"t\":[\"当时我参加的主要是国内的市场，对于海外的web3生态还不够了解，于是我也去调研了一些有关案例来探索web3模式下的商业可能性，以及基于我对CBBE模型的研究和分析，我总结了四个关键的品牌组成部分。\"]},\"38\":{\"h\":\"当下市场模式和未来商业结构初想\"},\"39\":{\"h\":\"商业目标和系统地图\",\"t\":[\"MonsterIsland（怪物工厂）是一个提供IP孵化机制的平台，重新连接知识产权、产品和消费者之间的关系，为Z世代用户构建一个开放的世界。通过全区块链模式经济体系，开放系统玩法来吸引用户。 如图形成三者链接的系统，通过IP的“创造来赚钱”的模式，产品部分帮助用户链接到现实世界，游戏世界帮助用户体验虚拟世界，最终将用户心智绑定到平台。\"]},\"40\":{\"h\":\"用户旅程图\",\"t\":[\"Monster Square (怪兽广场)：这是IP方（知识产权拥有者）发布活动供用户选择的区域。在怪兽广场，你可以参与各种由知名IP方组织的活动，这些活动可能包括挑战、比赛、限时任务等，参与者有机会获得珍稀NFT、特殊奖励或者与IP方互动的机会。\",\"Archipelago Area (群岛区域)：这是一个由用户拥有的地块组成的区域，每个用户都可以在这里建造、创造和发展。在群岛区域，你可以建立属于自己的虚拟世界，与其他玩家互动，体验社交、娱乐和商业活动。\",\"Forge City (锻造城市)：这是一个链接权益NFT（Non-Fungible Token，非同质化代币）与实际产品的桥梁。在锻造城市，你可以使用你的NFT购买真实世界的商品和服务，这些NFT可能代表了特殊的权益、限量版商品或者独特的体验。\",\"Future City (未来城市)：这是一个充满创造机会的区域，有机会成为团队共创的一部分。在未来城市，你可以参与各种项目、会议、共创活动，与其他创作者、开发者、设计师一起构建游戏的未来。这个区域充满了创意、合作和实现梦想的可能性。\"]},\"41\":{\"h\":\"商业画布和盈利点\"},\"42\":{\"h\":\"反思\",\"t\":[\"该商业计划书只是我基于当时市场做的一份初步构建，互联网的Web3生态形成仍然是任重而道远，我有时候在想，仅仅是技术限制了当下的发展吗？我认为也不见得，在早期就被一些可以“一夜暴富”的共识占据，也影响了后期感兴趣的人进场的心态，新用户被认为韭菜，进入了一个死循环。而且如我所做的调研分析，许多品牌想要借此来赋能，发现很多最后还不如新的IP势能来得猛，Web3所提出的去中心化也形成了资本和新个体之间的博弈，强行把Web2的产品套进Web3概念很多时候也不一定行得通。\",\"有人说，“告诉用户怎么用钱包十分简单，难的是告诉用户为什么使用区块链钱包”，或许当有一天不再需要和用户说明区块链是什么，大家自然而然知道这个东西的价值，对待区块链就如当下的网上购物一般成为生活的一部分，那数字资产的价值才能真正凸显。\",\"保持对行业的探索同时，我也时刻关注国家的政策，作为在大湾区工作的人，这里也提供了一片丰富的科技商业土壤，不管未来是不是叫Web3形态的互联网，我相信行业一定会慢慢向阳发展。\",\"相关阅读\",\"大湾区规划周年：粤港澳已成为区块链发展风水宝地_资讯-odaily\",\"粤港澳大湾区区块链产业图谱：世界拼图最全区域|湾区区块链（上）\",\"粤港澳大湾区区块链政策、科研实力全扫描\",\"内容参考\",\"《元宇宙2023：硬件的“大”年》文件\",\"我们是如何做到的：Ben Bridge Jeweler 在 2023 年元宇宙时装周上\",\"Highstreet\"]},\"43\":{\"h\":\"\",\"t\":[\"2021年，我有幸作为特别嘉宾参加了在重庆举办的O'Kids国际儿童艺术节，并在现场对我的作品进行解读和采访。\",\"Y：在您的作品《造梦城》中您描绘了一个巨大的乌托邦城市，您还为这件作品编写了一个故事，讲述了在未来世界生活在废墟上的孩子们的故事，充满了想象力，能分享一下您的创作灵感吗？\",\"我： 最开始引发这个毕设主题的来源是那段垃圾游乐场的历史故事，那时候我们的课题需要寻找一些关于儿童教育的历史文献，偶然发现了这段有趣的历史，这段历史故事也引起我很大的兴趣与共鸣，才开启了这个设想的创作。\",\"其中最开始《047》的故事来自于内心最深处的某些触动而编写成的，我有许多童真的幻想，而这段故事也是重新捕捉我这些幻想而形成的，并没有那么繁杂的约束，但是它是触发内心感性的一面，也是能留住作品里最真诚的一部分的原因。\",\"Y：在您的作品《造梦城》中，您非常细致地描绘了各种细节：有用垃圾为原料生产水果的百果岛，还有不同的废料集中营，为观众展现了一个内容丰富的幻想世界，您是怎样构思作品中的场景的呢？这件作品从对场景的构思到最后的呈现经历了怎样的过程？\",\"我： 作品里分为三部曲，第一部分就像前面说的是一个虚构的小说，我也运用了拼贴的手法去描绘这些抽象虚构的画面，而最终篇我要重新回到当今城市去构造一个乌托邦的时候，我是不能去抛弃前面所讲的这个故事的，它们两者有着直接的联系但并不是需要我去完全还原，而是存在相互交错又不完全重叠的关系。\",\"我在创作过程中有好几个月处于停滞状态，因为我找不到一个切入点去继续深化，最大的原因可能是中间产生了对结果没有把握的质疑心理，但最后我还是选择了极致地去表达作品，我觉得不能以以往常规的思考方式再让我徘徊，基于前面已有的拼贴画面，我需要一个更加具象的图面来展示我的一个乌托邦世界逻辑，所以我用了一副线稿的剖面图来开始形成我的整个故事逻辑，把前面每个点通过一张剖面表达来链接带动，当这个画面构想完成后那么我才根据这个画面来还原整个三维的世界结构，总结整个过程就是由文本拼贴到线稿剖面画面，最后才形成一个有三维立体化的创作场景。\",\"Y：本次展览的主题是“儿童友好城市”，您的作品中也描绘了一座属于儿童的“造梦城”。可以聊聊关于“儿童友好城市”这一概念您是如何理解的呢？在您看来，您的作品与本次展览主题有哪些内在关联性？\",\"我： “儿童友好城市”是一个非常好的主题概念，它可以涵盖很多当下需要我们去思考和关注的地方，从大的层面角度，在现代化城市中，我们需要给予儿童什么，城市公共设计里我们需要去思考哪些方面，一个普通的社会场所里是否在细节上也为孩子去考究考量，更多让孩子参与的活动等等，让他们发自内心去融入到城市生活中。\",\"从精神层面来讲，孩子也需要一些更能激发内心潜能的教育，教育不仅仅是形式上书本上的内容，符合这个社会发展，让孩子未来如何更好地融入社会，拥有一个美好的童年记忆等等也是需要持续关注的话题，这些都是构成一个“儿童友好城市”需要去持续探讨的地方。\",\"一个可以叫做“儿童友好城市”的地方，它的城市发展多元性也不会落后。我的作品虽然并没有直接导向一个目的或者结果，但是通过参加这次展览，我也看到了一些好的导向结果，从很多小朋友的作品中，我也看到了他们内心的无限创造力，就如当时垃圾游乐场上的孩子一样，他们在战争年代的无惧向前的创作精神，当下我们不一定需要那样恶劣的环境，但是精神仍然不可缺失，在他们创作的作品中，我也看到了每个人内心的梦，当他们凝结起来，也就形成了“造梦城”这样的一个精神乌托邦，并且不是唯一，而是会在世界各地出现很多这样的地方。\",\"相关阅读\",\"聊聊建筑转行\",\"毕业逆袭，看我如何挑战传统建筑\",\"O'Kids艺术家采访丨他们眼中的城市、家庭与儿童\",\"爬树与翻墙|Phantasisland(造梦城)\"]},\"44\":{\"h\":\"\",\"t\":[\"---该项目来自我于广州美术学院2019年的个人毕业设计项目\"]},\"45\":{\"h\":\"前言\",\"t\":[\"两端叙事与历史的交错，从2199年的虚构垃圾城市世界开始，追溯到1943年垃圾的诞生以及到冒险游乐场的演变历程，从兴起推广到渐渐消失，孩子们从前那种自由无畏的创造、冒险精神被也渐渐丢失，孩子们的疯狂内心世界谁又得知。\",\"在这样的虚构和背景之下，穿插一个叙事性的设想，提出疑问，孩子们在城市中的新乐土会是一种怎样的状态，而我尝试去构想一片属于孩子们自己的乌托邦天地，承载过去的精神而结合到当下的背景，根据设计的理念去寻找合适的场地。\",\"叙事的推动通过三部曲来进行，从乐土的诞生到逃离而最终是一种创造，通过材料的不同特性提出不同的设想，以废弃材料为媒介作为切入，孩子们可以自己搭建属于自己的天地，可以去走进不同的光怪陆离世界等等，这里的创造不仅仅是一种废料的再造，更是一种内心世界的创造和圆梦，造梦城内外的反差，白天黑夜的不同状态又将是一种父母与孩子的某种内心丝连，造梦城的诞生也许成为城市里的一颗最美星星，属于孩子们的最美星星。\"]},\"46\":{\"h\":\"序\",\"t\":[\"通过一个名为“047”的故事文本，讲述了2199年世界被垃圾堆满，而一艘“047”的船载着这堆垃圾带孩子逃离世界而来到许多奇幻的岛屿，这堆垃圾在这些岛屿里得到新生，孩子们逃离残酷的现实在这片梦幻乐土中自由创造，结尾留白给人想象，错过船的孩子似乎便无法再进入这个重生的世界里。\",\"以下正文——\",\"幕——\",\"“船来了”，孩子们呼喊着。\",\"2199年，人们在满城垃圾的世界里奔波，“三战”发起者竟是这些被人们抛弃丢弃的垃圾品，许多资源被清光，垃圾的大量不规范焚烧引起巨大污染，于是被停止，换来的就是这些垃圾怪的堆积…\",\"“当乘上这艘被称之为“047号”的船，便能去到心中的新世界，上船吧，孩子们。”\",\"这艘船每天会在港口运走这一批垃圾，尽管这看起来像是徒劳无功的，但这个船长仍然孜孜不倦地每天重复着开往港口又离开，奇怪的是，这些垃圾不知道随着船只去哪了，更神奇地，这船经常带回满满的蔬果和各种新鲜稀奇的东西回来给这些被垃圾夺走家园的孩子。\",\"起航——\",\"上船了，又一批孩子搭上这艘“梦之舟”，水中的迷雾很大，让人神志昏迷，许多孩子在摇晃的船上迷糊地睡着了。也不知道过了过久，迷雾退散，而身后令人发恶的城市也消失于视野之中，眼前是一座小岛，整片浓密的树林把岛给包围起来，似乎一种神秘之物汇集在林中。船开进了一个山洞，在一片昏暗之中船突然停下，停在了一束光中。 船长问：“有下船的孩子吗？”\",\"孩子们还沉浸在为这些不可思议的事惊讶的情绪中，\",\"有个五岁的孩子问道：“这是哪里呀？”\",\"“这是百果岛，想停留的可以下船，清理一部分的垃圾后就要重新开船了，前往下一个地方。”\",\"几个满脸胡子的老人突然出现，迎接着这一群他们也不知道来及哪个世纪的孩子，顺手拨下了石壁上的铁渣，一个石洞门突然打开，而船长也示意着拨下按钮，整个铁箱中的垃圾被倾倒入石洞中，孩子们吓坏了，又大片大片地吵了起来。\",\"“天啊，我们城市的垃圾就是来到这里了吗。”\",\"“这样子是不是我们的家是不是就能重新回来了？”\",\"“这里是哪里呀？”\",\"这几个老家伙笑了几下，“走吧孩子们”，这时另一个洞门被打开，他们带着孩子走了进去，穿过壁上点着蜡烛的小道，孩子们被眼前这一景象惊住了，仿佛一个巨型地下工厂般，\",\"不，其实更像一个魔法屋，被挂满各种齿轮，气泡池和搅拌机，整个屋子充斥着各种冒出的蒸汽，屋里的那些孩子开心地奔跑着传递从“魔法机器”中产出的“魔法品”，回望管道道中的东西竟然是刚刚从船中卸下的垃圾，黑暗中让人无法看清，现在灯亮下才发现这是许多食物残留的垃圾，他们被带进各种看不懂的容器中，这里的孩子似乎合作得十分娴熟，甚至经常使用一些奇怪的术语。 一个老家伙突然发话了，“跟你们介绍一下，这里是百果岛的一个地下生产屋，上面还有，这些东西还会通过齿轮再往上运输，我们岛中的孩子每天都很开心，他们喜欢变废为宝这种魔法，这些东西在我们祖先发明的这些机器中混合再处理最终会变成岛中的各种肥料，能长出许多果实和蔬菜，这也是我们叫百果岛的原因。”\",\"“希望你们喜欢这里，和他们一起玩得开心，如果你们想离开那也只能等待下一班船来接送你们了。”\",\"“我想留在这里”，一个孩子马上接上了话。\",\"“可是，其他的孩子会被带去哪里呢。”\",\"“这里呀，每个人精通之处不一样，有人种植果实，那也需要建房子、需要各种材料，我们喜欢用不同材料来尝试做有意思的屋子，还有呀，这里也会整理你们那边世界的各种信息作为记忆呢，当然我们还有小孩子擅长演奏音乐，那可是大自然的音乐...”\",\"孩子们听到这番话，仿佛真的是做梦，他们之前心中对于原来城市的不安都瞬间全无，倒变成一股力量涌在心头。\",\"老人们带着孩子坐上升降梯来到岛上，这一片就如名字般的景象，真的是百果丰盛，这里的树巨大无比，果实和以前城市里看到的一点都不同，奇形怪状，五颜六色，垃圾变成的肥料竟有这么大的魔力，真不可思议。\",\"“我们呀，靠着我们造的船相互联系交流，还有，每年的盛会，你们刚好就要赶上喽。”\",\"“是呀很巧，今晚就要开始了。”\",\"“哇我想永远留下来！”\",\"“对了，那艘船其他垃圾都会运到不同的岛上吗”\",\"“是的没错，垃圾送上船后，垃圾就会被自动归类，分配到每个需要这种垃圾的岛上，有时我们还会把一些我们制造的东西带给船长，让他捎给你们，我们需要你们，你们还有无限的潜能去发掘、去创造…”\",\"“以前就是一个岛，后来一场大灾难把我们分开了，唉，也没办法…”这群孩子还没听完就已经兴奋地跑到许多地方去，对于他们来说，这是一个无比新奇的世界，或许对于还在废都里的大人也是，但是船长不会带他们过来的，因为他害怕，害怕有一天这些岛也会成为一个垃圾堆无法挽回。\",\"盛会——\",\"玩到天黑了，老人带着孩子乘着自己建造的船来到另外一个岛中，在这样宁静的夜晚，这是竟是灯火通明，这就是老人所说盛会，每个岛会提供自己的一些材料去重新搭建一些盛会的场景，这里遍布着奇奇怪怪的房子，有糖果做的屋子，有黏土捏的屋子，有红色铜块覆盖的小山包，还有水底下的大苹果，挂在苍天大树上的鲸鱼，孩子们的欢声笑语遍布这些奇怪形态的东西，新的孩子们也很快跑到人群中与他们玩耍起来。\",\"这真的真的像是一场梦，似乎047把孩子带到一片属于他们未来的重生净土，究竟是一个起点还是一个终点，城市的这场垃圾战争究竟是摧毁了孩子还是给孩子带来重生，或许也只有孩子们知道。\",\"轮回——\",\"“047”早已带着那些没有下船的孩子继续前往其他小岛清理各个部分的垃圾，一班又一班的孩子看着各个新奇的岛走上他们不同的道路，每天载着孩子的船穿梭于这片迷雾中的水域，欢声笑语让这冷寂之地又有了新生。\",\"“你们真的不下船吗，孩子”，047号的船长问，“没有一个岛是你们想去探索的吗？”\",\"“我们害怕，不敢下去…”\",\"“那好吧…你们自己做的选择，我就带你们回港口吧。”\",\"船长带着这些从不下船的孩子重新穿过了这片迷雾，又看到了早上离开的那个“垃圾城”，\",\"孩子们道别船长后，累到直接睡倒在码头地上。\",\"第二天黎明，船笛声再响起，孩子们清醒了，又看到了“047”，回望身后丝毫没有容身之地的城市。\",\"“要不我们让船长再带我们再去一次岛屿吧，想想这里比那里都恐怖。”\",\"“我也后悔了…想下船去看看。”\",\"“船长船长，我们要上船！”\",\"船长停下船，说：“你们小孩子要去哪啊，我还要去倒垃圾！这不是你们该来的地方。”\",\"“啊…昨天你不是带着我们去了好多岛吗，还有其他还多小孩呢。”\",\"“对呀。”\",\"“对呀…”\",\"“什么昨天，我今天才来到这个城市，别妨碍我工作了，我清理完垃圾就要走了，你们赶紧回家吧。”\",\"“047”装载完了垃圾，重新起航，又鸣起它的船笛，在水天线中渐渐消失了。太阳升了起来，可是却一点也不让人觉得温暖。\"]},\"47\":{\"h\":\"起源——乐土之梦\"},\"48\":{\"h\":\"\",\"t\":[\"丹麦景观设计师Carl Theodor Sorenson（卡尔·西奥多· 索伦森）注意到，孩子们喜欢到处玩，但是在他建造的游乐场里。1931年，他想象出“一个儿童可以创造和塑造，梦想和想象成为现实的垃圾操场”。为什么不给这个城市的孩子和国内的孩子一样的游戏机会呢？他最初的想法开始了冒险游乐场运动。\",\"第一个冒险游乐场于1943年在第二次世界大战期间在丹麦Emdrup开放。1946年，赫特伍德的艾伦夫人从英格兰访问了Emdrup，并对“垃圾游乐场”印象深刻。她把这个想法带到了伦敦。这些“垃圾游乐场”被称为“冒险游乐场”。 \",\"丹麦哥本哈根Emdrup的第一个冒险乐园被称为“Skrammellegepladsen”。丹麦语“Skrammel”意为垃圾，可重复使用的垃圾等，“Legepladsen”意为游乐场。值得注意的是，“Skrammel”一词在丹麦语中具有积极的内涵，而“垃圾”一词在英语中具有更多的负面价值。多年来，Emdrup还使用了“建筑”游乐场这个词。\",\"John Bertelsen，Emdrup的第一位主持人，也创造了“skrammologi”或“junkology”这个词来指代垃圾游戏的哲学和理论，\",\"英国最初的冒险游乐场被称为“垃圾游乐场”或“废物游乐场”。1953年，赫特伍德的艾伦夫人和乔治佩普勒爵士在午餐时间会面，讨论并同意垃圾游乐场的新名词。艾伦夫人觉得垃圾游乐场值得一个更 好的名字，并达到“冒险游乐场”一词。\",\" 诺丁山冒险游乐场——大约20世纪60年代\",\"左图艾伦夫人，右图左一索伦森\"]},\"49\":{\"h\":\"演变历程的摘录\",\"t\":[\"破坏和犯罪之间的区别并不明显，许多孩子变得不守规矩和反社会\",\"Emdrup 的成功在于它就\\\"破坏和犯罪\\\"之间的这条界线进行了谈判\",\"战争唤醒了人们对孩子内部混乱的担忧，孩子们在瓦砾中玩耍成为重建的有趄赴力隐喻\",\"艾伦夫人:“骨折比精神破碎更好”\",\"从垃圾游乐场到冒险游乐场的进化，又到最后渐渐衰落\",\"由于所谓的安全问题，许多地方当局以此为借口撤回对冒险游乐场的支持\",\"卡姆登委员会拆除了它的冒险游乐场，取而代之的是带有固定设备的课后俱乐部\",\"设计师和设备制造商偷走了设计版本的冒险游乐设备\",\"20 世纪 70 年代，英国有 近 500 个冒险乐园。今天，不到 150 个\"]},\"50\":{\"h\":\"回归的心声和重生的设想\",\"t\":[\"游戏工作者Sherriff将自己的生命奉献给了垃圾游戏的哲学。他说，“孩子们需要能够承担风险，他们天生就会长大，我们知道这对他们有好处，”。Wendy Russell是格洛斯特大学的一名讲师，他刚刚完成了一个收集布里斯托尔和格洛斯特冒险游乐场记忆的项目，他表示，城市对年轻人越来越敌视。“儿童在公共领域不合适，他们不欢迎在街上。这些曾经贫困的地区现在已经变质，优先级也不同。对于非中产阶级的孩子们有一种谨慎态度“。\",\"在索伦森生命的后期， 写了关于垃圾游乐场的文章：他们可以梦想，想象，让梦想和想象成为现实，无论如何，这是一个现实，孩子的心灵是完全满足的……很明显，孩子们在这里茁壮成长，感觉良好，他们展开，他们生活。”在所有的事情中，我意识到，垃圾操场是最丑的，然而，对我来说，它是我的作品中最美好的。垃圾游乐场和冒险游乐场渐渐消失，精神却仍被延续...让我们回到当下，试着可以寻求一个精神复生的乌托邦，在繁华的都市中心、摩天大楼城市花园之下，尝试着一种逃离式的重生。\"]},\"51\":{\"h\":\"城市信息\",\"t\":[\"繁华的广州都市里，珠江新城就是城市的CBD，海心沙与广州塔之间的水域多被作为珠江夜游经过的路线，也是水上巴士的航线之一，海心沙码头与广州塔码头可通过船来过渡。这里有遍地的摩天大楼和及高度的现代规划，有曾经风光的亚运场地，有城市的中心高层地标，城市繁华尽收眼下。\"]},\"52\":{\"h\":\"历史与现实对比\",\"t\":[\"对于场地，是一种逃离式的选择。海心沙与广州塔之间的水域多被作为珠江夜游经过的路线，也是水上巴士的航线之一，海心沙码头与广州塔码头可通过船来过渡。这里没有横穿的天桥、没有地下隧道，在繁荣的地段、密集高耸的天际线中，这片水域更像是一个被孤立、难以随意踏上的片区这是一片只有船才能走上的地方，高度规划的城市花园都是成人世界所赋予的，这片水域是否可以成为孩子们自由的乌托邦世界。\"]},\"53\":{\"h\":\"假设与疑问\",\"t\":[\"通过虚幻的小说和真实的历史，回归到当下的场地，提出一个假想去回应，孩子们暂离繁华都市，一个可变式的围墙包围着这片乐土，大人无法抵达。白天，江边的大人看到空白围墙的阻挡，墙内是属于孩子们自由的岛屿，一个光怪陆离的世界，晚上，围墙被降下，犹如绽放江面的灿烂之花，这是一份孩子们的力量结晶，外部与内部、隔绝和绽放的反差都代表着一种强大精神力量的回归，自由无畏、冒险创造的重夺。\"]},\"54\":{\"h\":\"重生三部曲\"},\"55\":{\"h\":\"诞生\"},\"56\":{\"h\":\"来自不同身份的诉说\",\"t\":[\"一群不同职业身份的人，他们诉说着自己的心声，工程师叹息着建筑的拆迁，工人麻木了机械化的工厂造件，厨师为了最美好的一道菜舍弃许多少有瑕疵的食材，音乐家上街追寻自然的声音等等，他们看到这些材料不断被浪费、不断地成为机械化的程序，感叹着那份美好的丢失。\"]},\"57\":{\"h\":\"废弃材料的美妙\",\"t\":[\"食材的美妙香味和色彩足以构成美妙的画面，橡胶一直是成为人们飞天和入海的某种载体伴随，塑料造就着孩子们童年的玩具梦，然而却被工程化的市场牵引，金属独特的声响已经可以埋下音乐的种子，建筑材料藏着故事的沉淀、历史的反思，纸材同样蕴藏着记忆，而又可以成为空间的载体…他们不忍心看着废弃材料的丢弃，将他们收集在城市的某个地方，成立了六种集中营去收集着废弃的食物、金属、塑料、纸材、建筑材料、橡胶。他们希望看到这些废物的重塑，他们想到了充满梦想和创造的孩子们，而同样需要一个地方去创造去圆梦。\"]},\"58\":{\"h\":\"秩序的形成\",\"t\":[\"于是，造梦城开始诞生，两座塔在江面上形成，六个灯塔散布在四周，不同的平台在它们之间过度连接，扭曲的岩石层穿插在塔和平台之间，中间打通着一片船经过的水域，一个齿轮运转的机械缆车系统贯穿着两边的土地，最后是孩子们的创造天地，沉入水底的橡胶筒和岩石层，高层的塑料城的演化…\"]},\"59\":{\"h\":\"围墙\",\"t\":[\"大人们无法进入到这个地方，他们只能在江边观望着这里，这是属于一片属于孩子们的自由世界，而一片可变的。\",\"围墙包裹着这片乐土，在白天，它产生着这种隔离，而晚上，它将被打开，降下，这片孩子们的造梦天地绽放在江面上，灯火灿烂，这是一朵存在又似乎不属于这个城市的精神之花。\"]},\"60\":{\"h\":\"蓝图设想\",\"t\":[\"通过剖面的关系去设想造梦城的运作，孩子们的疯狂世界的展现。\",\"欢庆之日，造梦城诞生了。 \"]},\"61\":{\"h\":\"逃离\"},\"62\":{\"h\":\"船\",\"t\":[\"在这个城市最中心繁华的地方，孩子们来到码头等待着这六艘船的到来，灯塔成为码头的一部分，孩子们可以登上塔顶去去观望。\"]},\"63\":{\"h\":\"起航\",\"t\":[\"船来了，他们将跟着着六艘船去到城市的各个废料集中营里，去走入这些被遗弃物品的世界，暂别这个繁闹的都市中心。\"]},\"64\":{\"h\":\"造梦\"},\"65\":{\"h\":\"抵达\",\"t\":[\"踏上了寻找之路后，他们带着废料来到这个城市中心的水面上重生。进入造梦城后，船把孩子们下船了，而船开始运送废料到各个储存罐中，在水下传输系统和运输塔的传送下，废料被传送到上层的不同地方，孩子们在这里的岩石大地上奔跑欢呼，他们坐上了机械缆车穿梭，往灯塔上攀爬着，来到了各个不同的平台。\"]},\"66\":{\"h\":\"塔\",\"t\":[\"跨过这座石桥，这里有一个通天塔，它被许多怪异的东西附着拼撞组合，这里没有平台没有停止的地方，唯有一直往上攀爬，到最高的塔顶去，这也是造梦城的最顶端，突破了洞口，孩子们站在平台遥望远方的城市。\"]},\"67\":{\"h\":\"穿梭和运送\",\"t\":[\"孩子们爬上了塔，乘坐着机械滑轮车，甚至疯狂得去攀附陡峭的岩石去登高，来到了这片造梦天地，运输塔往上运输这批废料，当塔门被打开，孩子们驾驶着飞行的运输小车来帮忙装载和运输这些废料到他们想去的地方。\"]},\"68\":{\"h\":\"飞跃的梦\",\"t\":[\"废弃的橡胶经过再造成为孩子的充气服，他们将会被橡胶绳绑上一个动力装置，跳入这个可以沉入水底的橡胶筒中，筒壁还附着着许多奇异的橡胶球让他们借力和缓冲，他们尝试着水底的探索，而另一部分被制成气球，他们坐在装配过的橡胶圈随着气球一起升上天空，他们可以尝试去操控达到不同的体验。\"]},\"69\":{\"h\":\"塑料城\",\"t\":[\"搭建 这上方一片巨大的塑料城都是孩子们的建造空间，孩子们操控着机器，废弃的塑料又被合成形状大小不一的塑料片，他们把它们带到塑料城中，去再次构筑这片天地，在飞行器的协助下，他们尝试着钻孔、拼接，他们可以疯狂地在墙上挖洞，建造新的塑料楼梯等等…\",\"而纸同样成为他们在这片塑料城的一种搭建材料，废弃的纸堆在机器中可以使用不同的试液去处理，或凝固成形或柔软成片，也可能是纸张残存记忆的巨幅拼图碎片。\"]},\"70\":{\"h\":\"乐团\",\"t\":[\"来自自然的声音或许为这片乐土再添生机，巨大的金属扩音器底下是孩子们心中音乐种子的萌发，孩子们使用机器寻找着废弃金属里的美妙声音，构造属于自己的敲击乐器，他们将会使用一个塑料底座将这些“新乐器”固定之上，站在这些巨大的扩音器内，一场疯狂的“金属乐”演出就此开始。\"]},\"71\":{\"h\":\"蒸汽狂欢\",\"t\":[\"这些废弃蔬果的独特颜色和香味被机器所提取，孩子们就是这个实验室的主导者，他们重新调配着这些色彩，他们可以为他们的自由天地喷上颜色，香味被合成器处理和调配，孩子们在疯狂创造这一场香味盛宴，它们是自然的味道，是那样的真诚和温暖。\"]},\"72\":{\"h\":\"回归\",\"t\":[\"这里还藏着一个秘密之地，这是一片水下的岩石世界，记得橡胶筒水底的孩子吗，当他们身上的装置被触发，绳子解开，他们被冲刺进入到缓冲室，从而来到这片秘密天地，而这一个运输塔也不只是运输着废料，当上层的塔门打开，废料达到之时，孩子们可以跟随着这个上下循环的运输塔来到水下的世界，同时一些再造后的残渣也可以被一起向下运输，最后它们也被带走离开被城市的处理厂消化，这一个水下的世界是一种对原始垃圾游乐场的回归，六种材料中只有建筑材料被运输到水底，它们身上是许多城市故事的残留，某种历史的深藏，在这片水下岩石层中，孩子们似乎又回到了20世纪3、40年代的场景，面对建筑的废墟，看到了自由和无畏，孩子们凝视着这些废弃的建筑碎片，又看到了重造的希望，构筑着他们心中的小小世界。\"]},\"73\":{\"h\":\"幕\",\"t\":[\"白天的围墙隔离着造梦城，放映机偶尔闪过江边父母的神情写照，孩子们虽然在逃离，但或许一种逃离背后也藏着这样的思考，当孩子真的完全独立地去创造和冒险，父母会是紧张，会是兴奋，又或是惊讶，这面“明镜”像一部即时默片，或许在某个时刻认出父母地半张面孔，孩子又会怎么想呢，夜晚，造梦城灯光闪亮，围墙被收缩到水下，一朵绽放江面的花在绽放，它绽放的不仅仅是与高楼媲美的靓丽，更是孩子们内心精神世界的疯狂、无畏、巨大力量的释放。而这一次，周围的人看到的是孩子们的身影，他们的创造和疯狂自成影片，他们就是这部电影的主导者。\",\"一天的疯狂又落下帷幕，太阳将要升起，天空无比温暖，造梦城又再次升起了围墙，新的一天开始了。\"]},\"74\":{\"h\":\"结论\",\"t\":[\"本设计基于历史背景下的经历和垃圾（冒险）游乐场的回归抗议问题的现象存在，以两段叙事文本结合做出一个乌托邦式的畅想来进行讨论，许多问题无法解决，一个美好的设想更多是为了提醒和告诉人们孩子们还有许多巨大的潜力或许尚未发现，文中提到的垃圾游乐场在渐渐消失，或许是时代所变让他难以生存，但是同时孩子们被越加过度保护，城市中被束缚的自由越多，而欧洲出现的街头抗议也是表明孩子们在冒险游乐场（演变进化的名字）中更加得到在城市街头中从未得到的自由，还有那种平等相处能让他们忘却出身背景的氛围，对于垃圾游乐场的“垃圾”一词，本身是对于自然的一种态度，孩子们更需要接触自然的产物，甚至被淘汰的东西，他们的内心对于这样一些东西或许和成人心中所感并不相同，我们也很难真正走入孩子的心中，但这些历史也表明这样一个地方就像是孩子们的精神乌托邦，以至于他们能在这里被释放内心，小身体中的巨大力量被挖掘。\",\"我通过虚幻的构造和真实的过去去提出一个设想，但是这种畅想也是一种对孩子们内心世界的回应，同时借由废弃材料的媒介去发挥，引出问题去思考，如果我们能借由某种手段去帮助孩子们又能让他们更自由去创造、探索、冒险，这样的世界里会产生怎么样的反应，而我也通过一个美好的假想去回应这个结果，在这个杂糅的世界中，孩子们得到自由、体验到前所未有的冒险之旅，没有大人的看管，内心的力量被放大，他们又重回那种自由无畏的精神状态，而废弃材料的利用又经由孩子之手得到创造，帮助孩子们圆梦。创造可以是物质性的，亦可以是精神性的，冒险、探索未知事物某种角度也可以说一种创造，一种属于孩子们才能读懂的创造。\",\"最后，这是一个开放性的命题，也许最终答案应该由更多的阅读者去解读。\",\"相关阅读\",\"聊聊建筑转行\",\"毕业逆袭，看我如何挑战传统建筑\",\"O'Kids艺术家采访丨他们眼中的城市、家庭与儿童\",\"爬树与翻墙|Phantasisland(造梦城)\",\"参考文献：\",\"[1] 游戏史[The Latest - Design Museum Everywhere]()\",\"[2] 罗伯特·戴盾: [Welcome to the Home of British Adventure Play: An internet resource for those interested in and passionate about adventure play]()\",\"[3] CJ Lim+ED liu 《SHORT STORIES:london in two-and-a-half dimensions》\",\"[4] UCL《EMPOWERING THE LERACY GENERMTON Z -UNIT 22-》 Simultaneously published inthe USA and Canada by Routledge 711 Third Avenue. New York, NY 10017\",\"[5] 游戏及游乐场百科全书：[Fetching Title#e9t8]()\"]},\"75\":{\"h\":\"Phantasisland#\"},\"76\":{\"h\":\"项目介绍\",\"t\":[\"作为本科毕业项目 \\\"Phantasisland \\\"的延续项目，通过在广州华德福学校和重庆国际儿童艺术节的学术交流，我对儿童创造力教育进行了总结和思考，最终利用AR技术将游戏与教育相结合。我与当地艺术机构开展工作坊，完成项目测试，并提出了未来艺术与科技融合教育的可能性。\"]},\"77\":{\"h\":\"为什么做这个项目\",\"t\":[\"这几年我通过这个IP获得了许多交流机会，从广州的华德福学校，到重庆参加国际儿童艺术节，我也认识到中国儿童教育的发展，很多教育组织都在迭代教育理念，不再是类比高考的提前填鸭式教育，而是向更全面的能力发展，去挖掘潜力。\",\"这几年，STEM教育里面引入中国，甚至现在还加入了STEAM的概念，A是art，艺术，过往很多儿童艺术教育还是停留在传统绘画，当然这也是一个培养的方式，但是，当下时代的快速发展个更迭，儿童教育也需要面向时代未来，很多儿童机构已经开设了少儿编程，儿童机器人设计等课程，也看到STEAM教育越来越被重视。\",\"在几年前的毕业展期间，我收到一些教育机构的交流邀请，当时我对于儿童教育的认知还并不完善，这几年经过更多学习交流，我也得到新的启发，重新去思考传统艺术教育的痛点和未来的机会点，于是也联合机构开展了这个工作坊，探索科技艺术如何赋能儿童教育。\"]},\"78\":{\"h\":\"基础背景研究\",\"t\":[\" 美国弗吉尼亚理工大学教授雅克曼认为，最初的STEM教育只关注项目本身（做什么和怎么做），而忽略了对人本身和背景的关注（谁来做，为什幺做）。知识的广度和深度仍存在一定的局限性，教学过程中缺乏趣味性、情境性和艺术性。因此，她将艺术（Arts）和STEM有机地融合在一起，并于2006年提出了STEAM教育的概念。2014年前后，STEAM教育理念传入我国，掀起了我国STEAM教育的热潮。\",\"*2014年一月《北京市教育委员会关于在义务教育2014年1月，阶段推行中小学生课外活动计划的通知》出台，明确要求在课后服务中引入体育、艺术、科技类项目科学素养类产品进校服务应运而生\",\"2015年6月，《国务院关于大力推进大众创业7创新若干政策措施的意见》发布，创客款有、3D打印火热，随后，人工智能、Python筹又成为热名 。\",\"*2015年起，在线教育开始快还发展，软件偏程随之兴起，创业者涌入，资本加持。\",\"*2019年左右，人工智能、Python热度持续。\",\"国内硬件编程类机构崛起，如光必选、Makeblock等。\",\"STEAM教育的最新发展和艺术的融入已不再局限于绘画、音乐等领域。儿童教育的多样化发展，结合新技术带来的有利条件，引起了社会的极大关注。\",\"我当时的毕业项目\\\"Phantasisland \\\"探索的正是儿童的创造力培养问题，所以我也希望可以在此方向去深入探索，那么多少岁的儿童适合创造力教育呢？\"]},\"79\":{\"h\":\"适合儿童创造力教育的年龄\",\"t\":[\"让·皮亚杰（Jean Piaget，1896年8月9日-1980年9月16日），瑞士人，近代最有名的儿童心理学家。他的认知发展理论成为了这个学科的典范，一生留给后人60多本专著、500多篇论文，他曾到过许多国家讲学，获得几十个名誉博士、荣誉教授和荣誉科学院士的称号。\",\"让·皮亚杰提出的四个阶段中的第二个阶段,用于描述婴儿、儿童和青少年的认知发展 。\",\"4至7岁的术前儿童进入直觉思维子阶段。原始推理始于儿童的思维过程从符号思维转变为直觉思维。子项可以将对象组织到基元集合中,但不能以一致的方式进行。由于它们一次只能聚焦或居中对象的一个属性,因此它们的组织可能一次按形状,另一次按颜色进行。这被称为中心化,一旦孩子们能够超越这种有限的思维,他们就可以进入分类和保护的能力,这在具体的操作阶段变得明显\",\"皮亚杰更多地描述了术前阶段的局限性和他们无法完成的脑力任务。它们包括无法分心、保存、理解对象可以组织成逻辑顺序以及执行包含任务。他进行了许多关于数量、长度、质量、重量、体积和数量守恒的实验。守恒是理解即使外观发生变化,事物的数量也保持不变。操作前的孩子专注于对象现在的显示方式,无法跟踪转换。他们也无法理解逆向变换会将材料恢复到原始状态。这些能力是在随后的具体操作阶段发展起来的。\",\"参考来源：PREOPERATIONAL STAGE\",\"结合让·皮亚杰的研究，4-7岁也对应中国学龄前儿童的年龄，所以我也将年龄群体对准在这个阶段的儿童。\"]},\"80\":{\"h\":\"合作机构的资料分析SWOT\",\"t\":[\" 在我毕业展期间，机构的负责人看到我的作品感兴趣，我们也认识了，经过一些了解我们也达成共识希望做一次工作坊实验来赋能儿童教育。\",\"该机构优先重视传统的艺术课程，但同时强调早期培养儿童的创造力和抽象思维。他们为艺术打下坚实的基础，并不断推出新的特别课程，以激发儿童对艺术的兴趣。\",\"在2021年，我有幸参加了在重庆举行的O'Kids国际儿童艺术节，并与主办方倪昆老师（艺术家，O’Kids国际儿童艺术节创始人）进行了采访。倪昆老师谈到，创新\\\"意味着采用多样化的方法，专注于创造力、合作以及新兴技术，比如数字化在文化和艺术领域的运用。创新教育面临着广泛的挑战，因为它涉及到社会基本层面。艺术与城市的关系正在增长，受全球创意产业经验的影响。未来的智慧城市依赖于互联网、技术和大数据，强调创造力。这些创新世界将塑造我们的未来。 2021年O'Kids国际儿童艺术节\",\"通过访谈和调研我也得到了启发，我往下搜集了一些儿童创造力教育相关的资料，以及科技产品对儿童的影响。\"]},\"81\":{\"h\":\"基于儿童教育理论和技术运用提出的初步思考\",\"t\":[\"形状对认知发展的影响： 学习形状可以帮助儿童发展空间认知和思维能力。通过认识和区分不同的形状，儿童可以增强他们的空间感知、空间关系和空间想象力，从而提高解决问题和推理的能力。\",\"形状在儿童生活中的实际应用： 了解不同的形状可以帮助儿童更好地理解日常生活中的物体和环境。儿童可以利用形状来识别和描述物体，如区分圆形饼干和方形巧克力，评估物体的稳定性和适用性等。此外，形状还与许多其他学科和技能密切相关，如几何、建筑设计、绘画等。\",\"VR技术的潜在危险性： \\\"VR对孩子安全吗？这个问题一直被头显制造商在产品上张贴警告所传播： 三星 Gear VR：13 岁以下儿童不宜使用 Gear VR。谷歌 Daydream View： 13 岁以下儿童不宜使用 Daydream View。索尼 PlayStation VR：该 VR 头显不适合 12 岁以下儿童使用。 HTC Vive：HTC 没有明确说明使用年龄，但建议年幼儿童不要使用该产品。Oculus Quest： 14 岁以下儿童不宜使用 Quest。\",\"逃避现实和分散注意力： 有人担心，像《玩家一号》（Ready Player One）中的虚拟体验可能会如此吸引人，以至于用户更喜欢虚拟世界而不是现实世界。电视、电影和漫画书的出现也引发了类似的担忧。许多家长（61%）担心 VR 会造成社交孤立。要解决这个问题，家长必须积极管理孩子接触 VR 的时间和屏幕时间。VR 的身临其境性为已经在努力限制孩子屏幕时间的家长们增加了新的挑战（Rich, Bickham, & Shrier, 2015）。\",\"感官和视觉效果： 在虚拟现实（VR）中，计算机生成的图像显示在屏幕上，要求用户将视线集中在一个固定点上。然而，在呈现三维图像时，眼睛需要像观看不同距离的物体一样进行调整。这可能会导致眼睛疲劳和头痛，尤其是儿童，他们正在发育的大脑可能更容易受到这些影响。\",\"AR技术探究： 研究表明，由于 3D 物体的质量，儿童认为 AR 应用程序具有魔力，将其视为独特的玩具（Billinghurst, Kato, & Poupyrev, 2001; Bujak 等人, 2013）。\",\"创建增强型玩具环境并不仅仅是在传统玩具中加入一些技术。而是要为儿童设计和实施一个互动系统，既要考虑到传统玩具的无技术性，又要考虑到新接入的虚拟环境的新颖互动性，将二者结合起来，设计出一个高要求的混合现实玩具，用于娱乐和教育。--史蒂夫-辛斯克、马克-朗海因里希、马蒂亚斯-兰佩\",\"现实世界与虚拟世界的融合，传统游戏与交互式电脑游戏的结合，创造了一种引人入胜的共生关系。根据 Stapleton 等人的观点，混合现实游戏环境中的最佳体验源于物理活动、虚拟内容、故事讲述和用户想象力的融合。\"]},\"82\":{\"h\":\"总结\",\"t\":[\"虚拟现实和儿童的研究是有限的，尤其是在非临床的情况下。还需要进行更多的研究来确认潜在的风险。现有的研究表明，应该营造一个开放的环境，让增强现实内容能够提升儿童的学习和创造力，而不是完全让他们沉浸在内容中。\",\"所以我基于此提出了一个利用抽象与具象，平面和立体相互转换的概念，利用传统儿童教育的美术适用性和比较可控的AR技术来提出一个培养儿童创造力的基础方案。\"]},\"83\":{\"h\":\"服务系统\",\"t\":[\" 基于初步的思考，我提出了一个服务策略框架，涉及的利益相关者主要是平台方，学校或者其他教育机构，学生，以及学校里的人。平台是负责提供教育游戏工具的一方，在教育场所来指导儿童进行教育游戏，该机制既有一定的引导性，又并不给儿童太多规则上的限制。\"]},\"84\":{\"h\":\"设计概念\",\"t\":[\"这款教育游戏通过一个概念地图探索儿童与玩耍指导者之间多样的角色和互动。它强调多重角色扮演身份，允许儿童在不同的角色中创造和探索。增强现实（AR）技术增强了他们的体验，他们将成为创造新故事的主角。\"]},\"85\":{\"h\":\"技术触点和测试\"},\"86\":{\"h\":\"工作坊现场\"},\"87\":{\"h\":\"反馈反思\",\"t\":[\"现场工作人员：当孩子们看到AR模型并听到音效时，他们会变得非常快乐和惊叹。他们经常会伸手去触摸现实世界的环境。\",\"孩子们：我想要下载这个应用。我更喜欢看这些有趣的形状，而不是在平板上玩游戏。它们能变成我想要的形状吗？\"]},\"88\":{\"h\":\"测试结果亮点\",\"t\":[\"整个项目最显著的观察是，孩子们对增强现实内容的兴奋程度比我预期的更高。他们独特的想象力在之前的手工拼贴中就已经表现出来。接下来的考虑是如何通过增强现实来增加乐趣，并通过未来产品来补充教育游戏。\"]},\"89\":{\"h\":\"需要改善的部分\",\"t\":[\"该年龄段的孩子非常难控制，在一定的时间过后他们便会产生厌烦心理，游戏规则容易被打破，因此在后期做AR相关的儿童产品时候，需要更加简化游戏部分，可以着力提升AR技术的表现力，如从视觉、听觉方面出发去改进。\"]},\"90\":{\"h\":\"未来思考\",\"t\":[\"根据反思，孩子们在某些游戏步骤中很难管理，因为现有技术和原型测试的限制。未来如果可以结合AR眼镜加入这个项目，这将使AR教育游戏更加具有体验感，让每个参与项目的孩子都能更直观地体验到AR世界，并更积极地参与教育游戏！\",\"我希望能够将AR眼镜与APP相匹配，增加儿童与计算机的互动内容，并将AR和创意发展的教育系统打造成更完整的游戏。为此，我设计了一个主要场景的框架，以建立更新的概念。\",\"该框架仍然是很初级的阶段，在当下空间计算概念提出后，我也希望去构建更多产品的可能性来赋能儿童教育领域，后续也欢迎大家来找我交流探讨。\"]},\"91\":{\"h\":\"舞美活动全案策划 & 设计执行\",\"t\":[\"舞美、活动、展会、婚礼设计，这些是我在毕业的前两年做的一些项目案例，以及在辞职后作为个人设计师创业的项目经历，选择这一行业的主要原因是不仅仅停留于后端设计的角色，我可以学到更多软技能，学会独立和客户沟通、汇报，和团队协同，学会如何去面对各种紧急事件，因为这个行业，我的角色也从一个设计师的角色开始向其他岗位能力发展。\"]},\"92\":{\"h\":\"收获\",\"t\":[\"与客户的沟通，让我挑战到设计师以外的角色，从销售、服务层面去看待设计\",\"设计成本的控制如何利用更低的成本也可以做出好的现场效果，以及如何控制在客户预算内进行设计；\",\"落地材料的选择，从建筑材料到服装面料，以及任何生活中的日常材料都可以应用到空间置景中；\",\"设计结构问题，比如场地的尺寸限制，安全、消防等等，能否安全落地需要不断和执行人员、供应商沟通确认，比例模型制作到最终落地。\",\"每一次活动都需要和许多不同的人接触，客户、供应商以及公司内部团队协调等等，每一个部分的内容都各不相同沟通失误也难免会出现，不断提升沟通能力就是最好的方式，遇到沟通矛盾只能冷静处理再一步步解决。活动现场总是会有意外，也可能出现许多前期无法预判的事情，唯一能做的就是遇到问题迅速和团队沟通解决问题\",\"相对于本科的学习，我在这段期间得到更多实践落地的经验，不只是停留在电脑绘图的概念上，空间结构、材料都是我经常要考虑的，以及怎么配合和指导团队完成项目，通常这些项目时间都不长，一般3-6个月，所以也锻炼我快速从概念到执行的能力。这一点在留学申请中也应用到，比如我的项目MOODFLOW，利用我空间背景的优势，结合交互技术形成一个社会创新设计。\"]},\"93\":{\"h\":\"\",\"t\":[\"执行层面：现场就是最好的老师 沟通层面：多站在其他角色的立场思考设计怎么打动客户\",\"这几年对我最有价值的也是设计方面以外的东西，让我很快地在职场成长，与人的沟通和处理应急问题的能力，带着不同的角色去思考问题，不局限于线性的思维方式，这也是我拥有跨学科能力的一个基础。\"]},\"94\":{\"h\":\"HCI\"},\"95\":{\"h\":\"2024-04-11\"},\"96\":{\"h\":\"Goal Recognition via Linear Programming\",\"t\":[\"Authors: Felipe Meneguzzi, Luísa R. de A. Santos, Ramon Fraga Pereira, André G. Pereira\",\"Link: http://arxiv.org/abs/2404.07934v1\",\"Abstract: Goal Recognition is the task by which an observer aims to discern the goals that correspond to plans that comply with the perceived behavior of subject agents given as a sequence of observations. Research on Goal Recognition as Planning encompasses reasoning about the model of a planning task, the observations, and the goals using planning techniques, resulting in very efficient recognition approaches. In this article, we design novel recognition approaches that rely on the Operator-Counting framework, proposing new constraints, and analyze their constraints' properties both theoretically and empirically. The Operator-Counting framework is a technique that efficiently computes heuristic estimates of cost-to-goal using Integer/Linear Programming (IP/LP). In the realm of theory, we prove that the new constraints provide lower bounds on the cost of plans that comply with observations. We also provide an extensive empirical evaluation to assess how the new constraints improve the quality of the solution, and we found that they are especially informed in deciding which goals are unlikely to be part of the solution. Our novel recognition approaches have two pivotal advantages: first, they employ new IP/LP constraints for efficiently recognizing goals; second, we show how the new IP/LP constraints can improve the recognition of goals under both partial and noisy observability.\"]},\"97\":{\"h\":\"Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation\",\"t\":[\"Authors: Jinkyung Park, Pamela Wisniewski, Vivek Singh\",\"Link: http://arxiv.org/abs/2404.07926v1\",\"Abstract: In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized. Therefore, we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI community's methods for working with data.\"]},\"98\":{\"h\":\"Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games\",\"t\":[\"Authors: Daijin Yang, Erica Kleinman, Giovanni Maria Troiano, Elina Tochilnikova, Casper Harteveld\",\"Link: http://arxiv.org/abs/2404.07901v1\",\"Abstract: Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.\"]},\"99\":{\"h\":\"Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors\",\"t\":[\"Authors: Glen Smith, Adit Gupta, Christopher MacLellan\",\"Link: http://arxiv.org/abs/2404.07883v1\",\"Abstract: Intelligent tutoring systems (ITS) are effective for improving students' learning outcomes. However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization. We present the Apprentice Tutor Builder (ATB) , a platform that simplifies tutor creation and personalization. Instructors can utilize ATB's drag-and-drop tool to build tutor interfaces. Instructors can then interactively train the tutors' underlying AI agent to produce expert models that can solve problems. Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels. We conducted a user study with 14 instructors to evaluate the effectiveness of ATB's design with end users. We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features. With these insights, we identified a set of design recommendations for our platform and others that utilize interactive AI agents for tutor creation and customization.\"]},\"100\":{\"h\":\"The Dance of Logic and Unpredictability: Examining the Predictability of User Behavior on Visual Analytics Tasks\",\"t\":[\"Authors: Alvitta Ottley\",\"Link: http://arxiv.org/abs/2404.07865v1\",\"Abstract: The quest to develop intelligent visual analytics (VA) systems capable of collaborating and naturally interacting with humans presents a multifaceted and intriguing challenge. VA systems designed for collaboration must adeptly navigate a complex landscape filled with the subtleties and unpredictabilities that characterize human behavior. However, it is noteworthy that scenarios exist where human behavior manifests predictably. These scenarios typically involve routine actions or present a limited range of choices. This paper delves into the predictability of user behavior in the context of visual analytics tasks. It offers an evidence-based discussion on the circumstances under which predicting user behavior is feasible and those where it proves challenging. We conclude with a forward-looking discussion of the future work necessary to cultivate more synergistic and efficient partnerships between humans and the VA system. This exploration is not just about understanding our current capabilities and limitations in mirroring human behavior but also about envisioning and paving the way for a future where human-machine interaction is more intuitive and productive.\"]},\"101\":{\"h\":\"Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification\",\"t\":[\"Authors: Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann\",\"Link: http://arxiv.org/abs/2404.07754v1\",\"Abstract: Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.\"]},\"102\":{\"h\":\"Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models\",\"t\":[\"Authors: Marvin Pafla, Kate Larson, Mark Hancock\",\"Link: http://arxiv.org/abs/2404.07725v1\",\"Abstract: The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N=40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N=136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.\"]},\"103\":{\"h\":\"Efficient sEMG-based Cross-Subject Joint Angle Estimation via Hierarchical Spiking Attentional Feature Decomposition Network\",\"t\":[\"Authors: Xin Zhou, Chuang Lin, Can Wang, Xiaojiang Peng\",\"Link: http://arxiv.org/abs/2404.07517v1\",\"Abstract: Surface electromyography (sEMG) has demonstrated significant potential in simultaneous and proportional control (SPC). However, existing algorithms for predicting joint angles based on sEMG often suffer from high inference costs or are limited to specific subjects rather than cross-subject scenarios. To address these challenges, we introduced a hierarchical Spiking Attentional Feature Decomposition Network (SAFE-Net). This network initially compresses sEMG signals into neural spiking forms using a Spiking Sparse Attention Encoder (SSAE). Subsequently, the compressed features are decomposed into kinematic and biological features through a Spiking Attentional Feature Decomposition (SAFD) module. Finally, the kinematic and biological features are used to predict joint angles and identify subject identities, respectively. Our validation on two datasets (SIAT-DB1 and SIAT-DB2) and comparison with two existing methods, Informer and Spikformer, demonstrate that SSAE achieves significant power consumption savings of 39.1% and 37.5% respectively over them in terms of inference costs. Furthermore, SAFE-Net surpasses Informer and Spikformer in recognition accuracy on both datasets. This study underscores the potential of SAFE-Net to advance the field of SPC in lower limb rehabilitation exoskeleton robots.\"]},\"104\":{\"h\":\"Interactive Prompt Debugging with Sequence Salience\",\"t\":[\"Authors: Ian Tenney, Ryan Mullins, Bin Du, Shree Pandya, Minsuk Kahng, Lucas Dixon\",\"Link: http://arxiv.org/abs/2404.07498v1\",\"Abstract: We present Sequence Salience, a visual tool for interactive prompt debugging with input salience methods. Sequence Salience builds on widely used salience methods for text classification and single-token prediction, and extends this to a system tailored for debugging complex LLM prompts. Our system is well-suited for long texts, and expands on previous work by 1) providing controllable aggregation of token-level salience to the word, sentence, or paragraph level, making salience over long inputs tractable; and 2) supporting rapid iteration where practitioners can act on salience results, refine prompts, and run salience on the new output. We include case studies showing how Sequence Salience can help practitioners work with several complex prompting strategies, including few-shot, chain-of-thought, and constitutional principles. Sequence Salience is built on the Learning Interpretability Tool, an open-source platform for ML model visualizations, and code, notebooks, and tutorials are available at http://goo.gle/sequence-salience.\"]},\"105\":{\"h\":\"RASSAR: Room Accessibility and Safety Scanning in Augmented Reality\",\"t\":[\"Authors: Xia Su, Han Zhang, Kaiming Cheng, Jaewook Lee, Qiaochu Liu, Wyatt Olson, Jon Froehlich\",\"Link: http://arxiv.org/abs/2404.07479v1\",\"Abstract: The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, augmented reality (AR), and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR's extensibility, and key application scenarios.\"]},\"106\":{\"h\":\"Diversity's Double-Edged Sword: Analyzing Race's Effect on Remote Pair Programming Interactions\",\"t\":[\"Authors: Shandler A. Mason, Sandeep Kaur Kuttal\",\"Link: http://arxiv.org/abs/2404.07427v1\",\"Abstract: Remote pair programming is widely used in software development, but no research has examined how race affects these interactions. We embarked on this study due to the historical under representation of Black developers in the tech industry, with White developers comprising the majority. Our study involved 24 experienced developers, forming 12 gender-balanced same- and mixed-race pairs. Pairs collaborated on a programming task using the think-aloud method, followed by individual retrospective interviews. Our findings revealed elevated productivity scores for mixed-race pairs, with no differences in code quality between same- and mixed-race pairs. Mixed-race pairs excelled in task distribution, shared decision-making, and role-exchange but encountered communication challenges, discomfort, and anxiety, shedding light on the complexity of diversity dynamics. Our study emphasizes race's impact on remote pair programming and underscores the need for diverse tools and methods to address racial disparities for collaboration.\"]},\"107\":{\"h\":\"Too good to be true: People reject free gifts from robots because they infer bad intentions\",\"t\":[\"Authors: Benjamin Lebrun, Andrew Vonasch, Christoph Bartneck\",\"Link: http://arxiv.org/abs/2404.07409v1\",\"Abstract: A recent psychology study found that people sometimes reject overly generous offers from people because they imagine hidden ''phantom costs'' must be part of the transaction. Phantom costs occur when a person seems overly generous for no apparent reason. This study aims to explore whether people can imagine phantom costs when interacting with a robot. To this end, screen or physically embodied agents (human or robot) offered to people either a cookie or a cookie + $2. Participants were then asked to make a choice whether they would accept or decline the offer. Results showed that people did perceive phantom costs in the offer + $2 conditions when interacting with a human, but also with a robot, across both embodiment levels, leading to the characteristic behavioral effect that offering more money made people less likely to accept the offer. While people were more likely to accept offers from a robot than from a human, people more often accepted offers from humans when they were physically compared to screen embodied but were equally likely to accept the offer from a robot whether it was screen or physically embodied. This suggests that people can treat robots (and humans) as social agents with hidden intentions and knowledge, and that this influences their behavior toward them. This provides not only new insights on how people make decisions when interacting with a robot but also how robot embodiment impacts HRI research.\"]},\"108\":{\"h\":\"SealMates: Supporting Communication in Video Conferencing using a Collective Behavior-Driven Avatar\",\"t\":[\"Authors: Mark Armstrong, Chi-Lan Yang, Kinga Skiers, Mengzhen Lim, Tamil Selvan Gunasekaran, Ziyue Wang, Takuji Narumi, Kouta Minamizawa, Yun Suen Pai\",\"Link: http://arxiv.org/abs/2404.07403v1\",\"Abstract: The limited nonverbal cues and spatially distributed nature of remote communication make it challenging for unacquainted members to be expressive during social interactions over video conferencing. Though it enables seeing others' facial expressions, the visual feedback can instead lead to unexpected self-focus, resulting in users missing cues for others to engage in the conversation equally. To support expressive communication and equal participation among unacquainted counterparts, we propose SealMates, a behavior-driven avatar in which the avatar infers the engagement level of the group based on collective gaze and speech patterns and then moves across interlocutors' windows in the video conferencing. By conducting a controlled experiment with 15 groups of triads, we found the avatar's movement encouraged people to experience more self-disclosure and made them perceive everyone was equally engaged in the conversation than when there was no behavior-driven avatar. We discuss how a behavior-driven avatar influences distributed members' perceptions and the implications of avatar-mediated communication for future platforms.\"]},\"109\":{\"h\":\"2024-04-10\"},\"110\":{\"h\":\"BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks\",\"t\":[\"Authors: Ruijia Cheng, Titus Barik, Alan Leung, Fred Hohman, Jeffrey Nichols\",\"Link: http://arxiv.org/abs/2404.07387v1\",\"Abstract: Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs). However, they encounter difficulties in understanding and working with code produced by LLMs. To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user prompts and code generation. We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code. Through 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas. We discuss the implications of our findings for UI-centric interactive paradigm in code generation LLMs.\"]},\"111\":{\"h\":\"Interactive Explanation of Visual Patterns in Dimensionality Reductions with Predicate Logic\",\"t\":[\"Authors: Brian Montambault, Gabriel Appleby, Jen Rogers, Camelia D. Brumar, Mingwei Li, Remco Chang\",\"Link: http://arxiv.org/abs/2404.07386v1\",\"Abstract: Dimensionality reduction techniques are widely used for visualizing high-dimensional data. However, support for interpreting patterns of dimension reduction results in the context of the original data space is often insufficient. Consequently, users may struggle to extract insights from the projections. In this paper, we introduce DimBridge, a visual analytics tool that allows users to interact with visual patterns in a projection and retrieve corresponding data patterns. DimBridge supports several interactions, allowing users to perform various analyses, from contrasting multiple clusters to explaining complex latent structures. Leveraging first-order predicate logic, DimBridge identifies subspaces in the original dimensions relevant to a queried pattern and provides an interface for users to visualize and interact with them. We demonstrate how DimBridge can help users overcome the challenges associated with interpreting visual patterns in projections.\"]},\"112\":{\"h\":\"Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS\",\"t\":[\"Authors: Utkarsh Pratiush, Kevin M. Roccapriore, Yongtao Liu, Gerd Duscher, Maxim Ziatdinov, Sergei V. Kalinin\",\"Link: http://arxiv.org/abs/2404.07381v1\",\"Abstract: Exploring the structural, chemical, and physical properties of matter on the nano- and atomic scales has become possible with the recent advances in aberration-corrected electron energy-loss spectroscopy (EELS) in scanning transmission electron microscopy (STEM). However, the current paradigm of STEM-EELS relies on the classical rectangular grid sampling, in which all surface regions are assumed to be of equal a priori interest. This is typically not the case for real-world scenarios, where phenomena of interest are concentrated in a small number of spatial locations. One of foundational problems is the discovery of nanometer- or atomic scale structures having specific signatures in EELS spectra. Here we systematically explore the hyperparameters controlling deep kernel learning (DKL) discovery workflows for STEM-EELS and identify the role of the local structural descriptors and acquisition functions on the experiment progression. In agreement with actual experiment, we observe that for certain parameter combinations the experiment path can be trapped in the local minima. We demonstrate the approaches for monitoring automated experiment in the real and feature space of the system and monitor knowledge acquisition of the DKL model. Based on these, we construct intervention strategies, thus defining human-in the loop automated experiment (hAE). This approach can be further extended to other techniques including 4D STEM and other forms of spectroscopic imaging.\"]},\"113\":{\"h\":\"Fabricating Paper Circuits with Subtractive Processing\",\"t\":[\"Authors: Ruhan Yang, Krithik Ranjan, Ellen Yi-Luen Do\",\"Link: http://arxiv.org/abs/2404.07364v1\",\"Abstract: This paper introduces a new method of paper circuit fabrication that overcomes design barriers and increases flexibility in circuit design. Conventional circuit boards rely on thin traces, which limits the complexity and accuracy when applied to paper circuits. To address this issue, we propose a method that uses large conductive zones in paper circuits and performs subtractive processing during their fabrication. This approach eliminates design barriers and allows for more flexibility in circuit design. We introduce PaperCAD, a software tool that simplifies the design process by converting traditional circuit design to paper circuit design. We demonstrate our technique by creating two paper circuit boards. Our approach has the potential to promote the development of new applications for paper circuits.\"]},\"114\":{\"h\":\"\\\"We Need Structured Output\\\": Towards User-centered Constraints on Large Language Model Output\",\"t\":[\"Authors: Michael Xieyang Liu, Frederick Liu, Alexander J. Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, Carrie J. Cai\",\"Link: http://arxiv.org/abs/2404.07362v1\",\"Abstract: Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.\"]},\"115\":{\"h\":\"Enhancing Accessibility in Soft Robotics: Exploring Magnet-Embedded Paper-Based Interactions\",\"t\":[\"Authors: Ruhan Yang, Ellen Yi-Luen Do\",\"Link: http://arxiv.org/abs/2404.07360v1\",\"Abstract: This paper explores the implementation of embedded magnets to enhance paper-based interactions. The integration of magnets in paper-based interactions simplifies the fabrication process, making it more accessible for building soft robotics systems. We discuss various interaction patterns achievable through this approach and highlight their potential applications.\"]},\"116\":{\"h\":\"A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos\",\"t\":[\"Authors: Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang\",\"Link: http://arxiv.org/abs/2404.07351v1\",\"Abstract: Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.\"]},\"117\":{\"h\":\"Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites\",\"t\":[\"Authors: Mariza Dima, Damon Daylamani-Zad, Vangelis Lympouridis\",\"Link: http://arxiv.org/abs/2404.07348v1\",\"Abstract: In this paper we introduce two world-first Mixed Reality (MR) experiences that fuse smart AR glasses and live theatre and take place in a heritage site with the purpose to reveal the site's hidden and difficult histories about slavery. We term these unique general audience experiences Mixed Reality Heritage Performances (MRHP). Along with the development of our initial two performances we designed and developed a tool and guidelines that can help heritage organisations with their decolonising process by critically engaging the public with under-represented voices and viewpoints of troubled European and colonial narratives. The evaluations showed the embodied and affective potential of MRHP to attract and educate heritage audiences visitors. Insights of the design process are being formulated into an extensive design toolkit that aims to support experience design, theatre and heritage professionals to collaboratively carry out similar projects.\"]},\"118\":{\"h\":\"Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention\",\"t\":[\"Authors: Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang\",\"Link: http://arxiv.org/abs/2404.07347v1\",\"Abstract: Humans utilize their gaze to concentrate on essential information while perceiving and interpreting intentions in videos. Incorporating human gaze into computational algorithms can significantly enhance model performance in video understanding tasks. In this work, we address a challenging and innovative task in video understanding: predicting the actions of an agent in a video based on a partial video. We introduce the Gaze-guided Action Anticipation algorithm, which establishes a visual-semantic graph from the video input. Our method utilizes a Graph Neural Network to recognize the agent's intention and predict the action sequence to fulfill this intention. To assess the efficiency of our approach, we collect a dataset containing household activities generated in the VirtualHome environment, accompanied by human gaze data of viewing videos. Our method outperforms state-of-the-art techniques, achieving a 7% improvement in accuracy for 18-class intention recognition. This highlights the efficiency of our method in learning important features from human gaze data.\"]},\"119\":{\"h\":\"Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality\",\"t\":[\"Authors: Sungwon In, Erick Krokos, Kirsten Whitley, Chris North, Yalong Yang\",\"Link: http://arxiv.org/abs/2404.07161v1\",\"Abstract: The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts' workflow. To further improve comparison, we have designed and implemented a Branching&Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.\"]},\"120\":{\"h\":\"Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation\",\"t\":[\"Authors: Gianpaolo Alvari, Ersilia Vallefuoco, Melanie Cristofolini, Elio Salvadori, Marco Dianti, Alessia Moltani, Davide Dal Castello, Paola Venuti, Cesare Furlanello\",\"Link: http://arxiv.org/abs/2404.07159v1\",\"Abstract: Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions. Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment. We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.\"]},\"121\":{\"h\":\"How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models\",\"t\":[\"Authors: Unnseo Park, Venkatesh Sivaraman, Adam Perer\",\"Link: http://arxiv.org/abs/2404.07148v1\",\"Abstract: Reinforcement learning (RL) is a promising approach to generate treatment policies for sepsis patients in intensive care. While retrospective evaluation metrics show decreased mortality when these policies are followed, studies with clinicians suggest their recommendations are often spurious. We propose that these shortcomings may be due to lack of diversity in observed actions and outcomes in the training data, and we construct experiments to investigate the feasibility of predicting sepsis disease severity changes due to clinician actions. Preliminary results suggest incorporating action information does not significantly improve model performance, indicating that clinician actions may not be sufficiently variable to yield measurable effects on disease progression. We discuss the implications of these findings for optimizing sepsis treatment.\"]},\"122\":{\"h\":\"\\\"My toxic trait is thinking I'll remember this\\\": gaps in the learner experience of video tutorials for feature-rich software\",\"t\":[\"Authors: Ian Drosos, Advait Sarkar, Andrew D. Gordon\",\"Link: http://arxiv.org/abs/2404.07114v1\",\"Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.\"]},\"123\":{\"h\":\"VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning\",\"t\":[\"Authors: Alexandros Xenos, Niki Maria Foteinopoulou, Ioanna Ntinou, Ioannis Patras, Georgios Tzimiropoulos\",\"Link: http://arxiv.org/abs/2404.07078v1\",\"Abstract: Recognising emotions in context involves identifying the apparent emotions of an individual, taking into account contextual cues from the surrounding scene. Previous approaches to this task have involved the design of explicit scene-encoding architectures or the incorporation of external scene-related information, such as captions. However, these methods often utilise limited contextual information or rely on intricate training pipelines. In this work, we leverage the groundbreaking capabilities of Vision-and-Large-Language Models (VLLMs) to enhance in-context emotion classification without introducing complexity to the training process in a two-stage approach. In the first stage, we propose prompting VLLMs to generate descriptions in natural language of the subject's apparent emotion relative to the visual context. In the second stage, the descriptions are used as contextual information and, along with the image input, are used to train a transformer-based architecture that fuses text and visual features before the final classification task. Our experimental results show that the text and image features have complementary information, and our fused architecture significantly outperforms the individual modalities without any complex training methods. We evaluate our approach on three different datasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or comparable accuracy across all datasets and metrics compared to much more complex approaches. The code will be made publicly available on github: https://github.com/NickyFot/EmoCommonSense.git\"]},\"124\":{\"h\":\"WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers\",\"t\":[\"Authors: Yuexi Chen, Zhicheng Liu\",\"Link: http://arxiv.org/abs/2404.07005v1\",\"Abstract: Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES.\"]},\"125\":{\"h\":\"Untangling Critical Interaction with AI in Students Written Assessment\",\"t\":[\"Authors: Antonette Shibani, Simon Knight, Kirsty Kitto, Ajanie Karunanayake, Simon Buckingham Shum\",\"Link: http://arxiv.org/abs/2404.06955v1\",\"Abstract: Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations. These skills are particularly important for learners to develop in the age of generative AI where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human. To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI. Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process. We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI.\"]},\"126\":{\"h\":\"ChildCIdbLong: Longitudinal Child-Computer Interaction Database and Quantitative Analysis for Child Development\",\"t\":[\"Authors: Juan Carlos Ruiz-Garcia, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Jaime Herreros-Rodriguez\",\"Link: http://arxiv.org/abs/2404.06919v1\",\"Abstract: This article provides a comprehensive overview of recent research in the area of Child-Computer Interaction (CCI). The main contributions of the present article are two-fold. First, we present a novel longitudinal CCI database named ChildCIdbLong, which comprises over 600 children aged 18 months to 8 years old, acquired continuously over 4 academic years (2019-2023). As a result, ChildCIdbLong comprises over 12K test acquisitions over a tablet device. Different tests are considered in ChildCIdbLong, requiring different touch and stylus gestures, enabling evaluation of skills like hand-eye coordination, fine motor skills, planning, and visual tracking, among others. In addition to the ChildCIdbLong database, we propose a novel quantitative metric called Test Quality (Q), designed to measure the motor and cognitive development of children through their interaction with a tablet device. In order to provide a better comprehension of the proposed Q metric, popular percentile-based growth representations are introduced for each test, providing a two-dimensional space to compare children's development with respect to the typical age skills of the population. The results achieved in the present article highlight the potential of the novel ChildCIdbLong database in conjunction with the proposed Q metric to measure the motor and cognitive development of children as they grow up. The proposed framework could be very useful as an automatic tool to support child experts (e.g., paediatricians, educators, or neurologists) for early detection of potential physical/cognitive impairments during children's development.\"]},\"127\":{\"h\":\"SARA: Smart AI Reading Assistant for Reading Comprehension\",\"t\":[\"Authors: Enkeleda Thaqi, Mohamed Mantawy, Enkelejda Kasneci\",\"Link: http://arxiv.org/abs/2404.06906v1\",\"Abstract: SARA integrates Eye Tracking and state-of-the-art large language models in a mixed reality framework to enhance the reading experience by providing personalized assistance in real-time. By tracking eye movements, SARA identifies the text segments that attract the user's attention the most and potentially indicate uncertain areas and comprehension issues. The process involves these key steps: text detection and extraction, gaze tracking and alignment, and assessment of detected reading difficulty. The results are customized solutions presented directly within the user's field of view as virtual overlays on identified difficult text areas. This support enables users to overcome challenges like unfamiliar vocabulary and complex sentences by offering additional context, rephrased solutions, and multilingual help. SARA's innovative approach demonstrates it has the potential to transform the reading experience and improve reading proficiency.\"]},\"128\":{\"h\":\"Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome\",\"t\":[\"Authors: Bihui Jin, Heng Li, Ying Zou\",\"Link: http://arxiv.org/abs/2404.06827v1\",\"Abstract: Web browsers have been used widely by users to conduct various online activities, such as information seeking or online shopping. To improve user experience and extend the functionality of browsers, practitioners provide mechanisms to allow users to install third-party-provided plugins (i.e., extensions) on their browsers. However, little is known about the performance implications caused by such extensions. In this paper, we conduct an empirical study to understand the impact of extensions on the user-perceived performance (i.e., energy consumption and page load time) of Google Chrome, the most popular browser. We study a total of 72 representative extensions from 11 categories (e.g., Developer Tools and Sports). We observe that browser performance can be negatively impacted by the use of extensions, even when the extensions are used in unintended circumstances (e.g., when logging into an extension is not granted but required, or when an extension is not used for designated websites). We also identify a set of factors that significantly influence the performance impact of extensions, such as code complexity and privacy practices (i.e., collection of user data) adopted by the extensions. Based on our empirical observations, we provide recommendations for developers and users to mitigate the performance impact of browser extensions, such as conducting performance testing and optimization for unintended usage scenarios of extensions, or adhering to proper usage practices of extensions (e.g., logging into an extension when required).\"]},\"129\":{\"h\":\"A proposal for a revised meta-architecture of intelligent tutoring systems to foster explainability and transparency for educators\",\"t\":[\"Authors: Florian Gnadlinger, Simone Kriglstein\",\"Link: http://arxiv.org/abs/2404.06820v1\",\"Abstract: This contribution draws attention to implications connected with meta-architectural design decisions for intelligent tutoring systems in the context of formative assessments. As a first result of addressing this issue, this contribution presents a meta-architectural system design that includes the role of educators.\"]},\"130\":{\"h\":\"Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems\",\"t\":[\"Authors: Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen\",\"Link: http://arxiv.org/abs/2404.06762v1\",\"Abstract: Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.\"]},\"131\":{\"h\":\"Incremental XAI: Memorable Understanding of AI with Incremental Explanations\",\"t\":[\"Authors: Jessica Y. Bo, Pan Hao, Brian Y. Lim\",\"Link: http://arxiv.org/abs/2404.06733v1\",\"Abstract: Many explainable AI (XAI) techniques strive for interpretability by providing concise salient information, such as sparse linear factors. However, users either only see inaccurate global explanations, or highly-varying local explanations. We propose to provide more detailed explanations by leveraging the human cognitive capacity to accumulate knowledge by incrementally receiving more details. Focusing on linear factor explanations (factors $\\\\times$ values = outcome), we introduce Incremental XAI to automatically partition explanations for general and atypical instances by providing Base + Incremental factors to help users read and remember more faithful explanations. Memorability is improved by reusing base factors and reducing the number of factors shown in atypical cases. In modeling, formative, and summative user studies, we evaluated the faithfulness, memorability and understandability of Incremental XAI against baseline explanation methods. This work contributes towards more usable explanation that users can better ingrain to facilitate intuitive engagement with AI.\"]},\"132\":{\"h\":\"MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education\",\"t\":[\"Authors: Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, Ziyu Yao\",\"Link: http://arxiv.org/abs/2404.06711v1\",\"Abstract: Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, large language models (LLMs) have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of LLMs, in this work, we present MATHVC, the very first LLM-powered virtual classroom containing multiple LLM-simulated student characters, with whom a human student can practice their MM skill. To encourage each LLM character's behaviors to be aligned with their specified math-relevant properties (termed \\\"characteristics alignment\\\") and the overall conversational procedure to be close to an authentic student MM discussion (termed \\\"conversational procedural alignment\\\"), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our simulation approach and showed the promise for MATHVC to benefit real-life students in the future.\"]},\"133\":{\"h\":\"CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge\",\"t\":[\"Authors: Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, Yejin Choi\",\"Link: http://arxiv.org/abs/2404.06664v1\",\"Abstract: Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks. Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences. Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency.\"]},\"134\":{\"h\":\"2024-04-09\"},\"135\":{\"h\":\"Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids -- A Gig Driver Case Study\",\"t\":[\"Authors: Rex Chen, Ruiyi Wang, Norman Sadeh, Fei Fang\",\"Link: http://arxiv.org/abs/2404.06432v1\",\"Abstract: Decision aids based on artificial intelligence (AI) are becoming increasingly common. When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes. In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes. More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions? We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool. Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low. Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users.\"]},\"136\":{\"h\":\"Apprentices to Research Assistants: Advancing Research with Large Language Models\",\"t\":[\"Authors: M. Namvarpour, A. Razi\",\"Link: http://arxiv.org/abs/2404.06404v1\",\"Abstract: Large Language Models (LLMs) have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise. This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.\"]},\"137\":{\"h\":\"ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos\",\"t\":[\"Authors: Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan\",\"Link: http://arxiv.org/abs/2404.06243v1\",\"Abstract: Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more. Traditional supervised methods require large annotated datasets for training, which are expensive and time-consuming to acquire. This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with contrastive learning for semi-supervised action recognition. Our framework leverages both labeled and unlabelled data to robustly learn action representations in videos, combining pseudo-labeling with contrastive learning for effective learning from both types of samples. We introduce a novel cross-architecture approach where 3D Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are utilised to capture different aspects of action representations; hence we call it ActNetFormer. The 3D CNNs excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames. By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action. This comprehensive representation learning enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures. Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data. The official website of this work is available at: https://github.com/rana2149/ActNetFormer.\"]},\"138\":{\"h\":\"Multimodal Road Network Generation Based on Large Language Model\",\"t\":[\"Authors: Jiajing Chen, Weihang Xu, Haiming Cao, Zihuan Xu, Yu Zhang, Zhao Zhang, Siyao Zhang\",\"Link: http://arxiv.org/abs/2404.06227v1\",\"Abstract: With the increasing popularity of ChatGPT, large language models (LLMs) have demonstrated their capabilities in communication and reasoning, promising for transportation sector intelligentization. However, they still face challenges in domain-specific knowledge. This paper aims to leverage LLMs' reasoning and recognition abilities to replace traditional user interfaces and create an \\\"intelligent operating system\\\" for transportation simulation software, exploring their potential with transportation modeling and simulation. We introduce Network Generation AI (NGAI), integrating LLMs with road network modeling plugins, validated through experiments for accuracy and robustness. NGAI's effective use has reduced modeling costs, revolutionized transportation simulations, optimized user steps, and proposed a novel approach for LLM integration in the transportation field.\"]},\"139\":{\"h\":\"Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking\",\"t\":[\"Authors: Suleyman Ozdel, Efe Bozkir, Enkelejda Kasneci\",\"Link: http://arxiv.org/abs/2404.06216v1\",\"Abstract: As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated. While state-of-the-art approaches for privacy-preserving eye tracking mostly involve differential privacy and empirical data manipulations, previous research has not focused on methods for scanpaths. We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm. Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed. Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs. This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process. We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible.\"]},\"140\":{\"h\":\"EVE: Enabling Anyone to Train Robot using Augmented Reality\",\"t\":[\"Authors: Jun Wang, Chun-Cheng Chang, Jiafei Duan, Dieter Fox, Ranjay Krishna\",\"Link: http://arxiv.org/abs/2404.06089v1\",\"Abstract: The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.\"]},\"141\":{\"h\":\"Breathing New Life into Existing Visualizations: A Natural Language-Driven Manipulation Framework\",\"t\":[\"Authors: Can Liu, Jiacheng Yu, Yuhan Guo, Jiayi Zhuang, Yuchu Luo, Xiaoru Yuan\",\"Link: http://arxiv.org/abs/2404.06039v1\",\"Abstract: We propose an approach to manipulate existing interactive visualizations to answer users' natural language queries. We analyze the natural language tasks and propose a design space of a hierarchical task structure, which allows for a systematic decomposition of complex queries. We introduce a four-level visualization manipulation space to facilitate in-situ manipulations for visualizations, enabling a fine-grained control over the visualization elements. Our methods comprise two essential components: the natural language-to-task translator and the visualization manipulation parser. The natural language-to-task translator employs advanced NLP techniques to extract structured, hierarchical tasks from natural language queries, even those with varying degrees of ambiguity. The visualization manipulation parser leverages the hierarchical task structure to streamline these tasks into a sequence of atomic visualization manipulations. To illustrate the effectiveness of our approach, we provide real-world examples and experimental results. The evaluation highlights the precision of our natural language parsing capabilities and underscores the smooth transformation of visualization manipulations.\"]},\"142\":{\"h\":\"Cymatics Cup: Shape-Changing Drinks by Leveraging Cymatics\",\"t\":[\"Authors: Weijen Chen, Yang Yang, Kao-Hua Liu, Yun Suen Pai, Junichi Yamaoka, Kouta Minamizawa\",\"Link: http://arxiv.org/abs/2404.06027v1\",\"Abstract: To enhance the dining experience, prior studies in Human-Computer Interaction (HCI) and gastrophysics have demonstrated that modifying the static shape of solid foods can amplify taste perception. However, the exploration of dynamic shape-changing mechanisms in liquid foods remains largely untapped. In the present study, we employ cymatics, a scientific discipline focused on utilizing sound frequencies to generate patterns in liquids and particles to augment the drinking experience. Utilizing speakers, we dynamically reshaped liquids exhibiting five distinct taste profiles and evaluated resultant changes in taste perception and drinking experience. Our research objectives extend beyond merely augmenting taste from visual to tactile sensations; we also prioritize the experiential aspects of drinking. Through a series of experiments and workshops, we revealed a significant impact on taste perception and overall drinking experience when mediated by cymatics effects. Building upon these findings, we designed and developed tableware to integrate cymatics principles into gastronomic experiences.\"]},\"143\":{\"h\":\"Combinational Nonuniform Timeslicing of Dynamic Networks\",\"t\":[\"Authors: Seokweon Jung, DongHwa Shin, Hyeon Jeon, Jinwook Seo\",\"Link: http://arxiv.org/abs/2404.06021v1\",\"Abstract: Dynamic networks represent the complex and evolving interrelationships between real-world entities. Given the scale and variability of these networks, finding an optimal slicing interval is essential for meaningful analysis. Nonuniform timeslicing, which adapts to density changes within the network, is drawing attention as a solution to this problem. In this research, we categorized existing algorithms into two domains -- data mining and visualization -- according to their approach to the problem. Data mining approach focuses on capturing temporal patterns of dynamic networks, while visualization approach emphasizes lessening the burden of analysis. We then introduce a novel nonuniform timeslicing method that synthesizes the strengths of both approaches, demonstrating its efficacy with a real-world data. The findings suggest that combining the two approaches offers the potential for more effective network analysis.\"]},\"144\":{\"h\":\"Inclusive Practices for Child-Centered AI Design and Testing\",\"t\":[\"Authors: Emani Dotch, Vitica Arnold\",\"Link: http://arxiv.org/abs/2404.05920v1\",\"Abstract: We explore ideas and inclusive practices for designing and testing child-centered artificially intelligent technologies for neurodivergent children. AI is promising for supporting social communication, self-regulation, and sensory processing challenges common for neurodivergent children. The authors, both neurodivergent individuals and related to neurodivergent people, draw from their professional and personal experiences to offer insights on creating AI technologies that are accessible and include input from neurodivergent children. We offer ideas for designing AI technologies for neurodivergent children and considerations for including them in the design process while accounting for their sensory sensitivities. We conclude by emphasizing the importance of adaptable and supportive AI technologies and design processes and call for further conversation to refine child-centered AI design and testing methods.\"]},\"145\":{\"h\":\"2024-04-08\"},\"146\":{\"h\":\"ClusterRadar: an Interactive Web-Tool for the Multi-Method Exploration of Spatial Clusters Over Time\",\"t\":[\"Authors: Lee Mason, Blánaid Hicks, Jonas S. Almeida\",\"Link: http://arxiv.org/abs/2404.05897v1\",\"Abstract: Spatial cluster analysis, the detection of localized patterns of similarity in geospatial data, has a wide-range of applications for scientific discovery and practical decision making. One way to detect spatial clusters is by using local indicators of spatial association, such as Local Moran's I or Getis-Ord Gi*. However, different indicators tend to produce substantially different results due to their distinct operational characteristics. Choosing a suitable method or comparing results from multiple methods is a complex task. Furthermore, spatial clusters are dynamic and it is often useful to track their evolution over time, which adds an additional layer of complexity. ClusterRadar is a web-tool designed to address these analytical challenges. The tool allows users to easily perform spatial clustering and analyze the results in an interactive environment, uniquely prioritizing temporal analysis and the comparison of multiple methods. The tool's interactive dashboard presents several visualizations, each offering a distinct perspective of the temporal and methodological aspects of the spatial clustering results. ClusterRadar has several features designed to maximize its utility to a broad user-base, including support for various geospatial formats, and a fully in-browser execution environment to preserve the privacy of sensitive data. Feedback from a varied set of researchers suggests ClusterRadar's potential for enhancing the temporal analysis of spatial clusters.\"]},\"147\":{\"h\":\"With or Without Permission: Site-Specific Augmented Reality for Social Justice CHI 2024 Workshop Proceedings\",\"t\":[\"Authors: Rafael M. L. Silva, Ana María Cárdenas Gasca, Joshua A. Fisher, Erica Principe Cruz, Cinthya Jauregui, Amy Lueck, Fannie Liu, Andrés Monroy-Hernández, Kai Lukoff\",\"Link: http://arxiv.org/abs/2404.05889v1\",\"Abstract: This volume represents the proceedings of With or Without Permission: Site-Specific Augmented Reality for Social Justice CHI 2024 workshop.\"]},\"148\":{\"h\":\"Youth as Peer Auditors: Engaging Teenagers with Algorithm Auditing of Machine Learning Applications\",\"t\":[\"Authors: Luis Morales-Navarro, Yasmin B. Kafai, Vedya Konda, Danaë Metaxa\",\"Link: http://arxiv.org/abs/2404.05874v1\",\"Abstract: As artificial intelligence/machine learning (AI/ML) applications become more pervasive in youth lives, supporting them to interact, design, and evaluate applications is crucial. This paper positions youth as auditors of their peers' ML-powered applications to better understand algorithmic systems' opaque inner workings and external impacts. In a two-week workshop, 13 youth (ages 14-15) designed and audited ML-powered applications. We analyzed pre/post clinical interviews in which youth were presented with auditing tasks. The analyses show that after the workshop all youth identified algorithmic biases and inferred dataset and model design issues. Youth also discussed algorithmic justice issues and ML model improvements. Furthermore, youth reflected that auditing provided them new perspectives on model functionality and ideas to improve their own models. This work contributes (1) a conceptualization of algorithm auditing for youth; and (2) empirical evidence of the potential benefits of auditing. We discuss potential uses of algorithm auditing in learning and child-computer interaction research.\"]},\"149\":{\"h\":\"An empirical evaluation for defining a mid-air gesture dictionary for web-based interaction\",\"t\":[\"Authors: Thomas Pasquale, Cristina Gena, Fabiana Vernero\",\"Link: http://arxiv.org/abs/2404.05842v1\",\"Abstract: This paper presents an empirical evaluation of mid-air gestures in a web setting. Fifty-six (56) subjects, all of them HCI students, were divided into 16 groups and involved as designers. Each group worked separately with the same requirements. Firstly, designers identified the main actions required for a web-based interaction with a university classroom search service. Secondly, they proposed a set of mid-air gestures to carry out the identified actions: 99 different mid-air gestures for 16 different web actions were produced in total. Then, designers validated their proposals involving external subjects, namely 248 users in total. Finally, we analyzed their results and identified the most recurring or intuitive gestures as well as the potential criticalities associated with their proposals. Hence, we defined a mid-air gesture dictionary that contains, according to our analysis, the most suitable gestures for each identified web action. Our results suggest that most people tend to replicate gestures used in touch-based and mouse-based interfaces also in touchless interactions, ignoring the fact that they can be problematic due to the different distance between the user and the device in each interaction context.\"]},\"150\":{\"h\":\"Human-Machine Interaction in Automated Vehicles: Reducing Voluntary Driver Intervention\",\"t\":[\"Authors: Xinzhi Zhong, Yang Zhou, Varshini Kamaraj, Zhenhao Zhou, Wissam Kontar, Dan Negrut, John D. Lee, Soyoung Ahn\",\"Link: http://arxiv.org/abs/2404.05832v1\",\"Abstract: This paper develops a novel car-following control method to reduce voluntary driver interventions and improve traffic stability in Automated Vehicles (AVs). Through a combination of experimental and empirical analysis, we show how voluntary driver interventions can instigate substantial traffic disturbances that are amplified along the traffic upstream. Motivated by these findings, we present a framework for driver intervention based on evidence accumulation (EA), which describes the evolution of the driver's distrust in automation, ultimately resulting in intervention. Informed through the EA framework, we propose a deep reinforcement learning (DRL)-based car-following control for AVs that is strategically designed to mitigate unnecessary driver intervention and improve traffic stability. Numerical experiments are conducted to demonstrate the effectiveness of the proposed control model.\"]},\"151\":{\"h\":\"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs\",\"t\":[\"Authors: Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan\",\"Link: http://arxiv.org/abs/2404.05719v1\",\"Abstract: Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate \\\"any resolution\\\" on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens). Both sub-images are encoded separately before being sent to LLMs. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.\"]},\"152\":{\"h\":\"Eye Tracking on Text Reading with Visual Enhancements\",\"t\":[\"Authors: Franziska Huth, Maurice Koch, Miriam Awad, Daniel Weiskopf, Kuno Kurzhals\",\"Link: http://arxiv.org/abs/2404.05572v1\",\"Abstract: The interplay between text and visualization is gaining importance for media where traditional text is enriched by visual elements to improve readability and emphasize facts. In two controlled eye-tracking experiments ($N=12$), we approach answers to the question: How do visualization techniques influence reading behavior? We compare plain text to that marked with highlights, icons, and word-sized data visualizations. We assess quantitative metrics~(eye movement, completion time, error rate) and subjective feedback~(personal preference and ratings). The results indicate that visualization techniques, especially in the first experiment, show promising trends for improved reading behavior. The results also show the need for further research to make reading more effective and inform suggestions for future studies.\"]},\"153\":{\"h\":\"Interactive Formal Specification for Mathematical Problems of Engineers\",\"t\":[\"Authors: Walther Neuper\",\"Link: http://arxiv.org/abs/2404.05462v1\",\"Abstract: The paper presents the second part of a precise description of the prototype that has been developed in the course of the ISAC project over the last two decades. This part describes the \\\"specify-phase\\\", while the first part describing the \\\"solve-phase\\\" is already published. In the specify-phase a student interactively constructs a formal specification. The ISAC prototype implements formal specifications as established in theoretical computer science, however, the input language for the construction avoids requiring users to have knowledge of logic; this makes the system useful for various engineering faculties (and also for high school). The paper discusses not only ISAC's design of the specify-phase in detail, but also gives a brief introduction to implementation with the aim of advertising the re-use of formal frameworks (inclusive respective front-ends) with their generic tools for language definition and their rich pool of software components for formal mathematics.\"]},\"154\":{\"h\":\"Unlocking Adaptive User Experience with Generative AI\",\"t\":[\"Authors: Yutan Huang, Tanjila Kanij, Anuradha Madugalla, Shruti Mahajan, Chetan Arora, John Grundy\",\"Link: http://arxiv.org/abs/2404.05442v1\",\"Abstract: Developing user-centred applications that address diverse user needs requires rigorous user research. This is time, effort and cost-consuming. With the recent rise of generative AI techniques based on Large Language Models (LLMs), there is a possibility that these powerful tools can be used to develop adaptive interfaces. This paper presents a novel approach to develop user personas and adaptive interface candidates for a specific domain using ChatGPT. We develop user personas and adaptive interfaces using both ChatGPT and a traditional manual process and compare these outcomes. To obtain data for the personas we collected data from 37 survey participants and 4 interviews in collaboration with a not-for-profit organisation. The comparison of ChatGPT generated content and manual content indicates promising results that encourage using LLMs in the adaptive interfaces design process.\"]},\"155\":{\"h\":\"Re-Ranking News Comments by Constructiveness and Curiosity Significantly Increases Perceived Respect, Trustworthiness, and Interest\",\"t\":[\"Authors: Emily Saltz, Zaria Howard, Tin Acosta\",\"Link: http://arxiv.org/abs/2404.05429v1\",\"Abstract: Online commenting platforms have commonly developed systems to address online harms by removing and down-ranking content. An alternative, under-explored approach is to focus on up-ranking content to proactively prioritize prosocial commentary and set better conversational norms. We present a study with 460 English-speaking US-based news readers to understand the effects of re-ranking comments by constructiveness, curiosity, and personal stories on a variety of outcomes related to willingness to participate and engage, as well as perceived credibility and polarization in a comment section. In our rich-media survey experiment, participants across these four ranking conditions and a control group reviewed prototypes of comment sections of a Politics op-ed and Dining article. We found that outcomes varied significantly by article type. Up-ranking curiosity and constructiveness improved a number of measures for the Politics article, including perceived \\\\textit{Respect}, \\\\textit{Trustworthiness}, and \\\\textit{Interestingness} of the comment section. Constructiveness also increased perceptions that the comments were favorable to Republicans, with no condition worsening perceptions of partisans. Additionally, in the Dining article, personal stories and constructiveness rankings significantly improved the perceived informativeness of the comments. Overall, these findings indicate that incorporating prosocial qualities of speech into ranking could be a promising approach to promote healthier, less polarized dialogue in online comment sections.\"]},\"156\":{\"h\":\"Indexing Analytics to Instances: How Integrating a Dashboard can Support Design Education\",\"t\":[\"Authors: Ajit Jain, Andruid Kerne, Nic Lupfer, Gabriel Britain, Aaron Perrine, Yoonsuck Choe, John Keyser, Ruihong Huang, Jinsil Seo, Annie Sungkajun, Robert Lightfoot, Timothy McGuire\",\"Link: http://arxiv.org/abs/2404.05417v1\",\"Abstract: We investigate how to use AI-based analytics to support design education. The analytics at hand measure multiscale design, that is, students' use of space and scale to visually and conceptually organize their design work. With the goal of making the analytics intelligible to instructors, we developed a research artifact integrating a design analytics dashboard with design instances, and the design environment that students use to create them. We theorize about how Suchman's notion of mutual intelligibility requires contextualized investigation of AI in order to develop findings about how analytics work for people. We studied the research artifact in 5 situated course contexts, in 3 departments. A total of 236 students used the multiscale design environment. The 9 instructors who taught those students experienced the analytics via the new research artifact. We derive findings from a qualitative analysis of interviews with instructors regarding their experiences. Instructors reflected on how the analytics and their presentation in the dashboard have the potential to affect design education. We develop research implications addressing: (1) how indexing design analytics in the dashboard to actual design work instances helps design instructors reflect on what they mean and, more broadly, is a technique for how AI-based design analytics can support instructors' assessment and feedback experiences in situated course contexts; and (2) how multiscale design analytics, in particular, have the potential to support design education. By indexing, we mean linking which provides context, here connecting the numbers of the analytics with visually annotated design work instances.\"]},\"157\":{\"h\":\"WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A Conceptual Architecture\",\"t\":[\"Authors: Giuseppe Macario\",\"Link: http://arxiv.org/abs/2404.05317v1\",\"Abstract: This work proposes a WebXR-based cross-platform conceptual architecture, leveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate the development of an open, accessible, and interoperable metaverse. By introducing the concept of spatial web app, this research contributes to the discourse on the metaverse, offering an architecture that democratizes access to virtual environments and extended reality through the web, and aligns with Tim Berners-Lee's original vision of the World Wide Web as an open platform in the digital realm.\"]},\"158\":{\"h\":\"Exploiting Preference Elicitation in Interactive and User-centered Algorithmic Recourse: An Initial Exploration\",\"t\":[\"Authors: Seyedehdelaram Esfahani, Giovanni De Toni, Bruno Lepri, Andrea Passerini, Katya Tentori, Massimo Zancanaro\",\"Link: http://arxiv.org/abs/2404.05270v1\",\"Abstract: Algorithmic Recourse aims to provide actionable explanations, or recourse plans, to overturn potentially unfavourable decisions taken by automated machine learning models. In this paper, we propose an interaction paradigm based on a guided interaction pattern aimed at both eliciting the users' preferences and heading them toward effective recourse interventions. In a fictional task of money lending, we compare this approach with an exploratory interaction pattern based on a combination of alternative plans and the possibility of freely changing the configurations by the users themselves. Our results suggest that users may recognize that the guided interaction paradigm improves efficiency. However, they also feel less freedom to experiment with \\\"what-if\\\" scenarios. Nevertheless, the time spent on the purely exploratory interface tends to be perceived as a lack of efficiency, which reduces attractiveness, perspicuity, and dependability. Conversely, for the guided interface, more time on the interface seems to increase its attractiveness, perspicuity, and dependability while not impacting the perceived efficiency. That might suggest that this type of interfaces should combine these two approaches by trying to support exploratory behavior while gently pushing toward a guided effective solution.\"]},\"159\":{\"h\":\"Allowing humans to interactively guide machines where to look does not always improve a human-AI team's classification accuracy\",\"t\":[\"Authors: Giang Nguyen, Mohammad Reza Taesiri, Sunnie S. Y. Kim, Anh Nguyen\",\"Link: http://arxiv.org/abs/2404.05238v1\",\"Abstract: Via thousands of papers in Explainable AI (XAI), attention maps \\\\cite{vaswani2017attention} and feature attribution maps \\\\cite{bansal2020sam} have been established as a common means for explaining the input features that are important to AI's decisions. It is an interesting but unexplored question whether allowing users to edit the importance scores of input features at test time would improve the human-AI team's accuracy on downstream tasks. In this paper, we address this question by taking CHM-Corr, a state-of-the-art, ante-hoc explanation method \\\\cite{taesiri2022visual} that first predicts patch-wise correspondences between the input and the training-set images, and then uses them to make classification decisions. We build an interactive interface on top of CHM-Corr, enabling users to directly edit the initial feature attribution map provided by CHM-Corr. Via our CHM-Corr++ interface, users gain insights into if, when, and how the model changes its outputs, enhancing understanding beyond static explanations. Our user study with 18 machine learning researchers who performed $\\\\sim$1,400 decisions shows that our interactive approach does not improve user accuracy on CUB-200 bird image classification over static explanations. This challenges the belief that interactivity inherently boosts XAI effectiveness~\\\\cite{sokol2020one,sun2022exploring,shen2024towards,singh2024rethinking,mindlin2024beyond,lakkaraju2022rethinking,cheng2019explaining,liu2021understanding} and raises needs for future research. Our work contributes to the field by open-sourcing an interactive tool for manipulating model attention, and it lays the groundwork for future research to enable effective human-AI interaction in computer vision. We release code and data on \\\\href{https://anonymous.4open.science/r/CHMCorrPlusPlus/}{github}. Our interface are available \\\\href{http://137.184.82.109:7080/}{here}.\"]},\"160\":{\"h\":\"Fair Machine Guidance to Enhance Fair Decision Making in Biased People\",\"t\":[\"Authors: Mingzhe Yang, Hiromi Arai, Naomi Yamashita, Yukino Baba\",\"Link: http://arxiv.org/abs/2404.05228v1\",\"Abstract: Teaching unbiased decision-making is crucial for addressing biased decision-making in daily life. Although both raising awareness of personal biases and providing guidance on unbiased decision-making are essential, the latter topics remains under-researched. In this study, we developed and evaluated an AI system aimed at educating individuals on making unbiased decisions using fairness-aware machine learning. In a between-subjects experimental design, 99 participants who were prone to bias performed personal assessment tasks. They were divided into two groups: a) those who received AI guidance for fair decision-making before the task and b) those who received no such guidance but were informed of their biases. The results suggest that although several participants doubted the fairness of the AI system, fair machine guidance prompted them to reassess their views regarding fairness, reflect on their biases, and modify their decision-making criteria. Our findings provide insights into the design of AI systems for guiding fair decision-making in humans.\"]},\"161\":{\"h\":\"Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor When Adopting LLMs in HCI Research\",\"t\":[\"Authors: Gionnieve Lim, Simon T. Perrault\",\"Link: http://arxiv.org/abs/2404.05213v1\",\"Abstract: There is increasing interest in the adoption of LLMs in HCI research. However, LLMs may often be regarded as a panacea because of their powerful capabilities with an accompanying oversight on whether they are suitable for their intended tasks. We contend that LLMs should be adopted in a critical manner following rigorous evaluation. Accordingly, we present the evaluation of an LLM in identifying logical fallacies that will form part of a digital misinformation intervention. By comparing to a labeled dataset, we found that GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes invalid or unidentified instances, an accuracy of 0.90. This gives us the confidence to proceed with the application of the LLM while keeping in mind the areas where it still falls short. The paper describes our evaluation approach, results and reflections on the use of the LLM for our intended task.\"]},\"162\":{\"h\":\"2024-04-07\"},\"163\":{\"h\":\"Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring\",\"t\":[\"Authors: Nazar Ponochevnyi, Anastasia Kuzminykh\",\"Link: http://arxiv.org/abs/2404.05103v1\",\"Abstract: Recent chart-authoring systems, such as Amazon Q in QuickSight and Copilot for Power BI, demonstrate an emergent focus on supporting natural language input to share meaningful insights from data through chart creation. Currently, chart-authoring systems tend to integrate voice input capabilities by relying on speech-to-text transcription, processing spoken and typed input similarly. However, cross-modality input comparisons in other interaction domains suggest that the structure of spoken and typed-in interactions could notably differ, reflecting variations in user expectations based on interface affordances. Thus, in this work, we compare spoken and typed instructions for chart creation. Findings suggest that while both text and voice instructions cover chart elements and element organization, voice descriptions have a variety of command formats, element characteristics, and complex linguistic features. Based on these findings, we developed guidelines for designing voice-based authoring-oriented systems and additional features that can be incorporated into existing text-based systems to support speech modality.\"]},\"164\":{\"h\":\"Co-design Accessible Public Robots: Insights from People with Mobility Disability, Robotic Practitioners and Their Collaborations\",\"t\":[\"Authors: Howard Ziyu Han, Franklin Mingzhe Li, Alesandra Baca Vazquez, Daragh Byrne, Nikolas Martelaro, Sarah E Fox\",\"Link: http://arxiv.org/abs/2404.05050v1\",\"Abstract: Sidewalk robots are increasingly common across the globe. Yet, their operation on public paths poses challenges for people with mobility disabilities (PwMD) who face barriers to accessibility, such as insufficient curb cuts. We interviewed 15 PwMD to understand how they perceive sidewalk robots. Findings indicated that PwMD feel they have to compete for space on the sidewalk when robots are introduced. We next interviewed eight robotics practitioners to learn about their attitudes towards accessibility. Practitioners described how issues often stem from robotic companies addressing accessibility only after problems arise. Both interview groups underscored the importance of integrating accessibility from the outset. Building on this finding, we held four co-design workshops with PwMD and practitioners in pairs. These convenings brought to bear accessibility needs around robots operating in public spaces and in the public interest. Our study aims to set the stage for a more inclusive future around public service robots.\"]},\"165\":{\"h\":\"Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments\",\"t\":[\"Authors: Takato Mizuho, Takuji Narumi, Hideaki Kuzuoka\",\"Link: http://arxiv.org/abs/2404.05007v1\",\"Abstract: Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using VR. Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings.\"]},\"166\":{\"h\":\"Towards Developing Brain-Computer Interfaces for People with Multiple Sclerosis\",\"t\":[\"Authors: John S. Russo, Tim Mahoney, Kirill Kokorin, Ashley Reynolds, Chin-Hsuan Sophie Lin, Sam E. John, David B. Grayden\",\"Link: http://arxiv.org/abs/2404.04965v2\",\"Abstract: Multiple Sclerosis (MS) is a severely disabling condition that leads to various neurological symptoms. A Brain-Computer Interface (BCI) may substitute some lost function; however, there is a lack of BCI research in people with MS. To progress this research area effectively and efficiently, we aimed to evaluate user needs and assess the feasibility and user-centric requirements of a BCI for people with MS. We conducted an online survey of 34 people with MS to qualitatively assess user preferences and establish the initial steps of user-centred design. The survey aimed to understand their interest and preferences in BCI and bionic applications. We demonstrated widespread interest for BCI applications in all stages of MS, with a preference for a non-invasive (n = 12) or minimally invasive (n = 15) BCI over carer assistance (n = 6). Qualitative assessment indicated that this preference was not influenced by level of independence. Additionally, strong interest was noted in bionic technology for sensory and autonomic functions. Considering the potential to enhance independence and quality of life for people living with MS, the results emphasise the importance of user-centred design for future advancement of BCIs that account for the unique pathological changes associated with MS.\"]},\"167\":{\"h\":\"Balancing Information Perception with Yin-Yang: Agent-Based Information Neutrality Model for Recommendation Systems\",\"t\":[\"Authors: Mengyan Wang, Yuxuan Hu, Shiqing Wu, Weihua Li, Quan Bai, Verica Rupar\",\"Link: http://arxiv.org/abs/2404.04906v1\",\"Abstract: While preference-based recommendation algorithms effectively enhance user engagement by recommending personalized content, they often result in the creation of ``filter bubbles''. These bubbles restrict the range of information users interact with, inadvertently reinforcing their existing viewpoints. Previous research has focused on modifying these underlying algorithms to tackle this issue. Yet, approaches that maintain the integrity of the original algorithms remain largely unexplored. This paper introduces an Agent-based Information Neutrality model grounded in the Yin-Yang theory, namely, AbIN. This innovative approach targets the imbalance in information perception within existing recommendation systems. It is designed to integrate with these preference-based systems, ensuring the delivery of recommendations with neutral information. Our empirical evaluation of this model proved its efficacy, showcasing its capacity to expand information diversity while respecting user preferences. Consequently, AbIN emerges as an instrumental tool in mitigating the negative impact of filter bubbles on information consumption.\"]},\"168\":{\"h\":\"2024-04-06\"},\"169\":{\"h\":\"Navigating the Landscape of Hint Generation Research: From the Past to the Future\",\"t\":[\"Authors: Anubhav Jangra, Jamshid Mozafari, Adam Jatowt, Smaranda Muresan\",\"Link: http://arxiv.org/abs/2404.04728v1\",\"Abstract: Digital education has gained popularity in the last decade, especially after the COVID-19 pandemic. With the improving capabilities of large language models to reason and communicate with users, envisioning intelligent tutoring systems (ITSs) that can facilitate self-learning is not very far-fetched. One integral component to fulfill this vision is the ability to give accurate and effective feedback via hints to scaffold the learning process. In this survey article, we present a comprehensive review of prior research on hint generation, aiming to bridge the gap between research in education and cognitive science, and research in AI and Natural Language Processing. Informed by our findings, we propose a formal definition of the hint generation task, and discuss the roadmap of building an effective hint generation system aligned with the formal definition, including open challenges, future directions and ethical considerations.\"]},\"170\":{\"h\":\"\\\"Don't Step on My Toes\\\": Resolving Editing Conflicts in Real-Time Collaboration in Computational Notebooks\",\"t\":[\"Authors: April Yi Wang, Zihan Wu, Christopher Brooks, Steve Oney\",\"Link: http://arxiv.org/abs/2404.04695v1\",\"Abstract: Real-time collaborative editing in computational notebooks can improve the efficiency of teamwork for data scientists. However, working together through synchronous editing of notebooks introduces new challenges. Data scientists may inadvertently interfere with each others' work by altering the shared codebase and runtime state if they do not set up a social protocol for working together and monitoring their collaborators' progress. In this paper, we propose a real-time collaborative editing model for resolving conflict edits in computational notebooks that introduces three levels of edit protection to help collaborators avoid introducing errors to both the program source code and changes to the runtime state.\"]},\"171\":{\"h\":\"Designing for Complementarity: A Conceptual Framework to Go Beyond the Current Paradigm of Using XAI in Healthcare\",\"t\":[\"Authors: Elisa Rubegni, Omran Ayoub, Stefania Maria Rita Rizzo, Marco Barbero, Guenda Bernegger, Francesca Faraci, Francesca Mangili, Emiliano Soldini, Pierpaolo Trimboli, Alessandro Facchini\",\"Link: http://arxiv.org/abs/2404.04638v1\",\"Abstract: The widespread use of Artificial Intelligence-based tools in the healthcare sector raises many ethical and legal problems, one of the main reasons being their black-box nature and therefore the seemingly opacity and inscrutability of their characteristics and decision-making process. Literature extensively discusses how this can lead to phenomena of over-reliance and under-reliance, ultimately limiting the adoption of AI. We addressed these issues by building a theoretical framework based on three concepts: Feature Importance, Counterexample Explanations, and Similar-Case Explanations. Grounded in the literature, the model was deployed within a case study in which, using a participatory design approach, we designed and developed a high-fidelity prototype. Through the co-design and development of the prototype and the underlying model, we advanced the knowledge on how to design AI-based systems for enabling complementarity in the decision-making process in the healthcare domain. Our work aims at contributing to the current discourse on designing AI systems to support clinicians' decision-making processes.\"]},\"172\":{\"h\":\"Analyzing LLM Usage in an Advanced Computing Class in India\",\"t\":[\"Authors: Chaitanya Arora, Utkarsh Venaik, Pavit Singh, Sahil Goyal, Jatin Tyagi, Shyama Goel, Ujjwal Singhal, Dhruv Kumar\",\"Link: http://arxiv.org/abs/2404.04603v1\",\"Abstract: This paper investigates the usage patterns of undergraduate and graduate students when engaging with large language models (LLMs) to tackle programming assignments in the context of advanced computing courses. Existing work predominantly focuses on the influence of LLMs in introductory programming contexts. Additionally, there is a scarcity of studies analyzing actual conversations between students and LLMs. Our study provides a comprehensive quantitative and qualitative analysis of raw interactions between students and LLMs within an advanced computing course (Distributed Systems) at an Indian University. We further complement this by conducting student interviews to gain deeper insights into their usage patterns. Our study shows that students make use of large language models (LLMs) in various ways: generating code or debugging code by identifying and fixing errors. They also copy and paste assignment descriptions into LLM interfaces for specific solutions, ask conceptual questions about complex programming ideas or theoretical concepts, and generate test cases to check code functionality and robustness. Our analysis includes over 4,000 prompts from 411 students and conducting interviews with 10 students. Our analysis shows that LLMs excel at generating boilerplate code and assisting in debugging, while students handle the integration of components and system troubleshooting. This aligns with the learning objectives of advanced computing courses, which are oriented towards teaching students how to build systems and troubleshoot, with less emphasis on generating code from scratch. Therefore, LLM tools can be leveraged to increase student productivity, as shown by the data we collected. This study contributes to the ongoing discussion on LLM use in education, advocating for their usefulness in advanced computing courses to complement higher-level learning and productivity.\"]},\"173\":{\"h\":\"TeleAware Robot: Designing Awareness-augmented Telepresence Robot for Remote Collaborative Locomotion\",\"t\":[\"Authors: Ruyi Li, Yaxin Zhu, Min Liu, Yihang Zeng, Shanning Zhuang, Jiayi Fu, Yi Lu, Guyue Zhou, Can Liu, Jiangtao Gong\",\"Link: http://arxiv.org/abs/2404.04579v1\",\"Abstract: Telepresence robots can be used to support users to navigate an environment remotely and share the visiting experience with their social partners. Although such systems allow users to see and hear the remote environment and communicate with their partners via live video feed, this does not provide enough awareness of the environment and their remote partner's activities. In this paper, we introduce an awareness framework for collaborative locomotion in scenarios of onsite and remote users visiting a place together. From an observational study of small groups of people visiting exhibitions, we derived four design goals for enhancing the environmental and social awareness between social partners, and developed a set of awareness-enhancing techniques to add to a standard telepresence robot - named TeleAware robot. Through a controlled experiment simulating a guided exhibition visiting task, TeleAware robot showed the ability to lower the workload, facilitate closer social proximity, and improve mutual awareness and social presence compared with the standard one. We discuss the impact of mobility and roles of local and remote users, and provide insights for the future design of awareness-enhancing telepresence robot systems that facilitate collaborative locomotion.\"]},\"174\":{\"h\":\"A Map of Exploring Human Interaction patterns with LLM: Insights into Collaboration and Creativity\",\"t\":[\"Authors: Jiayang Li, Jiale Li\",\"Link: http://arxiv.org/abs/2404.04570v1\",\"Abstract: The outstanding performance capabilities of large language model have driven the evolution of current AI system interaction patterns. This has led to considerable discussion within the Human-AI Interaction (HAII) community. Numerous studies explore this interaction from technical, design, and empirical perspectives. However, the majority of current literature reviews concentrate on interactions across the wider spectrum of AI, with limited attention given to the specific realm of interaction with LLM. We searched for articles on human interaction with LLM, selecting 110 relevant publications meeting consensus definition of Human-AI interaction. Subsequently, we developed a comprehensive Mapping Procedure, structured in five distinct stages, to systematically analyze and categorize the collected publications. Applying this methodical approach, we meticulously mapped the chosen studies, culminating in a detailed and insightful representation of the research landscape. Overall, our review presents an novel approach, introducing a distinctive mapping method, specifically tailored to evaluate human-LLM interaction patterns. We conducted a comprehensive analysis of the current research in related fields, employing clustering techniques for categorization, which enabled us to clearly delineate the status and challenges prevalent in each identified area.\"]},\"175\":{\"h\":\"Language Models as Critical Thinking Tools: A Case Study of Philosophers\",\"t\":[\"Authors: Andre Ye, Jared Moore, Rose Novick, Amy X. Zhang\",\"Link: http://arxiv.org/abs/2404.04516v1\",\"Abstract: Current work in language models (LMs) helps us speed up or even skip thinking by accelerating and automating cognitive work. But can LMs help us with critical thinking -- thinking in deeper, more reflective ways which challenge assumptions, clarify ideas, and engineer new concepts? We treat philosophy as a case study in critical thinking, and interview 21 professional philosophers about how they engage in critical thinking and on their experiences with LMs. We find that philosophers do not find LMs to be useful because they lack a sense of selfhood (memory, beliefs, consistency) and initiative (curiosity, proactivity). We propose the selfhood-initiative model for critical thinking tools to characterize this gap. Using the model, we formulate three roles LMs could play as critical thinking tools: the Interlocutor, the Monitor, and the Respondent. We hope that our work inspires LM researchers to further develop LMs as critical thinking tools and philosophers and other 'critical thinkers' to imagine intellectually substantive uses of LMs.\"]},\"176\":{\"h\":\"Majority Voting of Doctors Improves Appropriateness of AI Reliance in Pathology\",\"t\":[\"Authors: Hongyan Gu. Chunxu Yang, Shino Magaki, Neda Zarrin-Khameh, Nelli S. Lakis, Inma Cobos, Negar Khanlou, Xinhai R. Zhang, Jasmeet Assi, Joshua T. Byers, Ameer Hamza, Karam Han, Anders Meyer, Hilda Mirbaha, Carrie A. Mohila, Todd M. Stevens, Sara L. Stone, Wenzhong Yan, Mohammad Haeri, Xiang 'Anthony' Chen\",\"Link: http://arxiv.org/abs/2404.04485v1\",\"Abstract: As Artificial Intelligence (AI) making advancements in medical decision-making, there is a growing need to ensure doctors develop appropriate reliance on AI to avoid adverse outcomes. However, existing methods in enabling appropriate AI reliance might encounter challenges while being applied in the medical domain. With this regard, this work employs and provides the validation of an alternative approach -- majority voting -- to facilitate appropriate reliance on AI in medical decision-making. This is achieved by a multi-institutional user study involving 32 medical professionals with various backgrounds, focusing on the pathology task of visually detecting a pattern, mitoses, in tumor images. Here, the majority voting process was conducted by synthesizing decisions under AI assistance from a group of pathology doctors (pathologists). Two metrics were used to evaluate the appropriateness of AI reliance: Relative AI Reliance (RAIR) and Relative Self-Reliance (RSR). Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR -- by approximately 9% and 31%, respectively -- compared to decisions made by one pathologist collaborating with AI. This increased appropriateness resulted in better precision and recall in the detection of mitoses. While our study is centered on pathology, we believe these insights can be extended to general high-stakes decision-making processes involving similar visual tasks.\"]},\"177\":{\"h\":\"2024-04-05\"},\"178\":{\"h\":\"HIV Client Perspectives on Digital Health in Malawi\",\"t\":[\"Authors: Lisa Orii, Caryl Feldacker, Jacqueline Madalitso Huwa, Agness Thawani, Evelyn Viola, Christine Kiruthu-Kamamia, Odala Sande, Hannock Tweya, Richard Anderson\",\"Link: http://arxiv.org/abs/2404.04444v1\",\"Abstract: eHealth has strong potential to advance HIV care in low- and middle-income countries. Given the sensitivity of HIV-related information and the risks associated with unintended HIV status disclosure, clients' privacy perceptions towards eHealth applications should be examined to develop client-centered technologies. Through focus group discussions with antiretroviral therapy (ART) clients from Lighthouse Trust, Malawi's public HIV care program, we explored perceptions of data security and privacy, including their understanding of data flow and their concerns about data confidentiality across several layers of data use. Our findings highlight the broad privacy concerns that affect ART clients' day-to-day choices, clients' trust in Malawi's health system, and their acceptance of, and familiarity with, point-of-care technologies used in HIV care. Based on our findings, we provide recommendations for building robust digital health systems in low- and middle-income countries with limited resources, nascent privacy regulations, and political will to take action to protect client data.\"]},\"179\":{\"h\":\"Humanoid Robots at work: where are we ?\",\"t\":[\"Authors: Fabrice R. Noreils\",\"Link: http://arxiv.org/abs/2404.04249v1\",\"Abstract: Launched by Elon Musk and its Optimus, we are witnessing a new race in which many companies have already engaged. The objective it to put at work a new generation of humanoid robots in demanding industrial environments within 2 or 3 years. Is this objective realistic ? The aim of this document and its main contributions is to provide some hints by covering the following topics: First an analysis of 12 companies based on eight criteria that will help us to distinguish companies based on their maturity and approach to the market; second as these humanoids are very complex systems we will provide an overview of the technological challenges to be addressed; third when humanoids are deployed at scale, Operation and Maintenance become critical and the we will explore what is new with these complex machines; Finally Pilots are the last step to test the feasibility of a new system before mass deployment. This is an important step to test the maturity of a product and the strategy of the humanoid supplier to address a market and two pragmatic approaches will be discussed.\"]},\"180\":{\"h\":\"Social Skill Training with Large Language Models\",\"t\":[\"Authors: Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S. Bernstein, John Mitchell\",\"Link: http://arxiv.org/abs/2404.04204v1\",\"Abstract: People rely on social skills like conflict resolution to communicate effectively and to thrive in both work and personal life. However, practice environments for social skills are typically out of reach for most people. How can we make social skill training more available, accessible, and inviting? Drawing upon interdisciplinary research from communication and psychology, this perspective paper identifies social skill barriers to enter specialized fields. Then we present a solution that leverages large language models for social skill training via a generic framework. Our AI Partner, AI Mentor framework merges experiential learning with realistic practice and tailored feedback. This work ultimately calls for cross-disciplinary innovation to address the broader implications for workforce development and social equality.\"]},\"181\":{\"h\":\"Designing Robots to Help Women\",\"t\":[\"Authors: Martin Cooney, Lena Klasén, Fernando Alonso-Fernandez\",\"Link: http://arxiv.org/abs/2404.04123v1\",\"Abstract: Robots are being designed to help people in an increasing variety of settings--but seemingly little attention has been given so far to the specific needs of women, who represent roughly half of the world's population but are highly underrepresented in robotics. Here we used a speculative prototyping approach to explore this expansive design space: First, we identified some potential challenges of interest, including crimes and illnesses that disproportionately affect women, as well as potential opportunities for designers, which were visualized in five sketches. Then, one of the sketched scenarios was further explored by developing a prototype, of a robotic helper drone equipped with computer vision to detect hidden cameras that could be used to spy on women. While object detection introduced some errors, hidden cameras were identified with a reasonable accuracy of 80% (Intersection over Union (IoU) score: 0.40). Our aim is that the identified challenges and opportunities could help spark discussion and inspire designers, toward realizing a safer, more inclusive future through responsible use of technology.\"]},\"182\":{\"h\":\"ChoreoVis: Planning and Assessing Formations in Dance Choreographies\",\"t\":[\"Authors: Samuel Beck, Nina Doerr, Kuno Kurzhals, Alexander Riedlinger, Fabian Schmierer, Michael Sedlmair, Steffen Koch\",\"Link: http://arxiv.org/abs/2404.04100v1\",\"Abstract: Sports visualization has developed into an active research field over the last decades. Many approaches focus on analyzing movement data recorded from unstructured situations, such as soccer. For the analysis of choreographed activities like formation dancing, however, the goal differs, as dancers follow specific formations in coordinated movement trajectories. To date, little work exists on how visual analytics methods can support such choreographed performances. To fill this gap, we introduce a new visual approach for planning and assessing dance choreographies. In terms of planning choreographies, we contribute a web application with interactive authoring tools and views for the dancers' positions and orientations, movement trajectories, poses, dance floor utilization, and movement distances. For assessing dancers' real-world movement trajectories, extracted by manual bounding box annotations, we developed a timeline showing aggregated trajectory deviations and a dance floor view for detailed trajectory comparison. Our approach was developed and evaluated in collaboration with dance instructors, showing that introducing visual analytics into this domain promises improvements in training efficiency for the future.\"]},\"183\":{\"h\":\"Hierarchical Neural Additive Models for Interpretable Demand Forecasts\",\"t\":[\"Authors: Leif Feddersen, Catherine Cleophas\",\"Link: http://arxiv.org/abs/2404.04070v1\",\"Abstract: Demand forecasts are the crucial basis for numerous business decisions, ranging from inventory management to strategic facility planning. While machine learning (ML) approaches offer accuracy gains, their interpretability and acceptance are notoriously lacking. Addressing this dilemma, we introduce Hierarchical Neural Additive Models for time series (HNAM). HNAM expands upon Neural Additive Models (NAM) by introducing a time-series specific additive model with a level and interacting covariate components. Covariate interactions are only allowed according to a user-specified interaction hierarchy. For example, weekday effects may be estimated independently of other covariates, whereas a holiday effect may depend on the weekday and an additional promotion may depend on both former covariates that are lower in the interaction hierarchy. Thereby, HNAM yields an intuitive forecasting interface in which analysts can observe the contribution for each known covariate. We evaluate the proposed approach and benchmark its performance against other state-of-the-art machine learning and statistical models extensively on real-world retail data. The results reveal that HNAM offers competitive prediction performance whilst providing plausible explanations.\"]},\"184\":{\"h\":\"VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots\",\"t\":[\"Authors: Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson\",\"Link: http://arxiv.org/abs/2404.04066v1\",\"Abstract: Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos and supporting files are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/\"]},\"185\":{\"h\":\"Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations\",\"t\":[\"Authors: Sita A. Vriend, Sandeep Vidyapu, Amer Rama, Kun-Ting Chen, Daniel Weiskopf\",\"Link: http://arxiv.org/abs/2404.04036v1\",\"Abstract: We conducted an eye-tracking user study with 13 participants to investigate the influence of stimulus-question ordering and question modality on participants using visual question-answering (VQA) tasks. We examined cognitive load, task performance, and gaze allocations across five distinct experimental designs, aiming to identify setups that minimize the cognitive burden on participants. The collected performance and gaze data were analyzed using quantitative and qualitative methods. Our results indicate a significant impact of stimulus-question ordering on cognitive load and task performance, as well as a noteworthy effect of question modality on task performance. These findings offer insights for the experimental design of controlled user studies in visualization research.\"]},\"186\":{\"h\":\"Validation of critical maneuvers based on shared control\",\"t\":[\"Authors: Mauricio Marcano, Joseba Sarabia, Asier Zubizarreta, Sergio Díaz\",\"Link: http://arxiv.org/abs/2404.04011v1\",\"Abstract: This paper presents the validation of shared control strategies for critical maneuvers in automated driving systems. Shared control involves collaboration between the driver and automation, allowing both parties to actively engage and cooperate at different levels of the driving task. The involvement of the driver adds complexity to the control loop, necessitating comprehensive validation methodologies. The proposed approach focuses on two critical maneuvers: overtaking in low visibility scenarios and lateral evasive actions. A modular architecture with an arbitration module and shared control algorithms is implemented, primarily focusing on the lateral control of the vehicle. The validation is conducted using a dynamic simulator, involving 8 real drivers interacting with a virtual environment. The results demonstrate improved safety and user acceptance, indicating the effectiveness of the shared control strategies in comparison with no shared-control support. Future work involves implementing shared control in drive-by-wire systems to enhance safety and driver comfort during critical maneuvers. Overall, this research contributes to the development and validation of shared control approaches in automated driving systems.\"]},\"187\":{\"h\":\"From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity\",\"t\":[\"Authors: Saskia Nuñez von Voigt, Luise Mehner, Florian Tschorsch\",\"Link: http://arxiv.org/abs/2404.04006v1\",\"Abstract: The notion of $\\\\varepsilon$-differential privacy is a widely used concept of providing quantifiable privacy to individuals. However, it is unclear how to explain the level of privacy protection provided by a differential privacy mechanism with a set $\\\\varepsilon$. In this study, we focus on users' comprehension of the privacy protection provided by a differential privacy mechanism. To do so, we study three variants of explaining the privacy protection provided by differential privacy: (1) the original mathematical definition; (2) $\\\\varepsilon$ translated into a specific privacy risk; and (3) an explanation using the randomized response technique. We compare users' comprehension of privacy protection employing these explanatory models with their comprehension of privacy protection of $k$-anonymity as baseline comprehensibility. Our findings suggest that participants' comprehension of differential privacy protection is enhanced by the privacy risk model and the randomized response-based model. Moreover, our results confirm our intuition that privacy protection provided by $k$-anonymity is more comprehensible.\"]},\"188\":{\"h\":\"Approximate UMAP allows for high-rate online visualization of high-dimensional data streams\",\"t\":[\"Authors: Peter Wassenaar, Pierre Guetschel, Michael Tangermann\",\"Link: http://arxiv.org/abs/2404.04001v1\",\"Abstract: In the BCI field, introspection and interpretation of brain signals are desired for providing feedback or to guide rapid paradigm prototyping but are challenging due to the high noise level and dimensionality of the signals. Deep neural networks are often introspected by transforming their learned feature representations into 2- or 3-dimensional subspace visualizations using projection algorithms like Uniform Manifold Approximation and Projection (UMAP). Unfortunately, these methods are computationally expensive, making the projection of data streams in real-time a non-trivial task. In this study, we introduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at generating rapid projections for real-time introspection. To study its suitability for real-time projecting, we benchmark the methods against standard UMAP and its neural network counterpart parametric UMAP. Our results show that approximate UMAP delivers projections that replicate the projection space of standard UMAP while decreasing projection speed by an order of magnitude and maintaining the same training time.\"]},\"189\":{\"h\":\"Tensions between Preference and Performance: Designing for Visual Exploration of Multi-frequency Medical Network Data\",\"t\":[\"Authors: Christian Knoll, Laura Koesten, Isotta Rigoni, Serge Vulliémoz, Torsten Möller\",\"Link: http://arxiv.org/abs/2404.03965v1\",\"Abstract: The analysis of complex high-dimensional data is a common task in many domains, resulting in bespoke visual exploration tools. Expectations and practices of domain experts as users do not always align with visualization theory. In this paper, we report on a design study in the medical domain where we developed two high-fidelity prototypes encoding EEG-derived brain network data with different types of visualizations. We evaluate these prototypes regarding effectiveness, efficiency, and preference with two groups: participants with domain knowledge (domain experts in medical research) and those without domain knowledge, both groups having little or no visualization experience. A requirement analysis and study of low-fidelity prototypes revealed a strong preference for a novel and aesthetically pleasing visualization design, as opposed to a design that is considered more optimal based on visualization theory. Our study highlights the pros and cons of both approaches, discussing trade-offs between task-specific measurements and subjective preference. While the aesthetically pleasing and novel low-fidelity prototype was favored, the results of our evaluation show that, in most cases, this was not reflected in participants' performance or subjective preference for the high-fidelity prototypes.\"]},\"190\":{\"h\":\"Open vocabulary keyword spotting through transfer learning from speech synthesis\",\"t\":[\"Authors: Kesavaraj V, Anil Kumar Vuppala\",\"Link: http://arxiv.org/abs/2404.03914v1\",\"Abstract: Identifying keywords in an open-vocabulary context is crucial for personalizing interactions with smart devices. Previous approaches to open vocabulary keyword spotting dependon a shared embedding space created by audio and text encoders. However, these approaches suffer from heterogeneous modality representations (i.e., audio-text mismatch). To address this issue, our proposed framework leverages knowledge acquired from a pre-trained text-to-speech (TTS) system. This knowledge transfer allows for the incorporation of awareness of audio projections into the text representations derived from the text encoder. The performance of the proposed approach is compared with various baseline methods across four different datasets. The robustness of our proposed model is evaluated by assessing its performance across different word lengths and in an Out-of-Vocabulary (OOV) scenario. Additionally, the effectiveness of transfer learning from the TTS system is investigated by analyzing its different intermediate representations. The experimental results indicate that, in the challenging LibriPhrase Hard dataset, the proposed approach outperformed the cross-modality correspondence detector (CMCD) method by a significant improvement of 8.22% in area under the curve (AUC) and 12.56% in equal error rate (EER).\"]},\"191\":{\"h\":\"Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction\",\"t\":[\"Authors: Hyunyoung Jang, Jinwook Kim, Jeongmi Lee\",\"Link: http://arxiv.org/abs/2404.03899v1\",\"Abstract: Retargeting methods that modify the visual representation of real movements have been widely used to expand the interaction space and create engaging virtual reality experiences. For optimal user experience and performance, it is essential to specify the perception of retargeting and utilize the appropriate range of modification parameters. However, previous studies mostly concentrated on whether users perceived the target sense or not and rarely examined the perceptual accuracy and sensitivity to retargeting. Moreover, it is unknown how the perception and performance in hand-retargeted interactions are influenced by multisensory feedback. In this study, we used rigorous psychophysical methods to specify users' perceptual accuracy and sensitivity to hand-retargeting and provide acceptable ranges of retargeting parameters. We also presented different multisensory feedback simultaneously with the retargeting to probe its effect on users' perception and task performance. The experimental results showed that providing continuous multisensory feedback, proportionate to the distance between the virtual hand and the targeted destination, heightened the accuracy of users' perception of hand retargeting without altering their perceptual sensitivity. Furthermore, the utilization of multisensory feedback considerably improved the precision of task performance, particularly at lower gain factors. Based on these findings, we propose design guidelines and potential applications of VR hand-retargeted interactions and multisensory feedback for optimal user experience and performance.\"]},\"192\":{\"h\":\"Buck You: Designing Easy-to-Onboard Blockchain Applications with Zero-Knowledge Login and Sponsored Transactions on Sui\",\"t\":[\"Authors: Eason Chen, Zimo Xiao, Justa Liang, Damien Chen, Pierce Hung, Kostas Kryptos Chalkias\",\"Link: http://arxiv.org/abs/2404.03845v1\",\"Abstract: In this paper, we developed a blockchain application to demonstrate the functionality of Sui's recent innovations: Zero Knowledge Login and Sponsored Transactions. Zero Knowledge Login allows users to create and access their blockchain wallets just with their OAuth accounts (e.g., Google, Facebook, Twitch), while Sponsored Transactions eliminate the need for users to prepare transaction fees, as they can delegate fees to sponsors' accounts. Additionally, thanks to Sui's Storage Rebate feature, sponsors in Sponsored Transactions can profit from the sponsorship, achieving a win-win and sustainable service model. Zero Knowledge Login and Sponsored Transactions are pivotal in overcoming key challenges novice blockchain users face, particularly in managing private keys and depositing initial transaction fees. By addressing these challenges in the user experience of blockchain, Sui makes the blockchain more accessible and engaging for novice users and paves the way for the broader adoption of blockchain applications in everyday life.\"]},\"193\":{\"h\":\"2024-04-04\"},\"194\":{\"h\":\"SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers\",\"t\":[\"Authors: Jonathan F. Carter, João Jorge, Oliver Gibson, Lionel Tarassenko\",\"Link: http://arxiv.org/abs/2404.03831v1\",\"Abstract: Advances in camera-based physiological monitoring have enabled the robust, non-contact measurement of respiration and the cardiac pulse, which are known to be indicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alternative to \\\"gold-standard\\\" polysomnography, which is cumbersome, expensive to administer, and hence unsuitable for longer-term clinical studies. In this paper, we introduce SleepVST, a transformer model which enables state-of-the-art performance in camera-based sleep stage classification (sleep staging). After pre-training on contact sensor data, SleepVST outperforms existing methods for cardio-respiratory sleep staging on the SHHS and MESA datasets, achieving total Cohen's kappa scores of 0.75 and 0.77 respectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video, enabling fully contact-free sleep staging. Using a video dataset of 50 nights, we achieve a total accuracy of 78.8% and a Cohen's $\\\\kappa$ of 0.71 in four-class video-based sleep staging, setting a new state-of-the-art in the domain.\"]},\"195\":{\"h\":\"I Did Not Notice: A Comparison of Immersive Analytics with Augmented and Virtual Reality\",\"t\":[\"Authors: Xiaoyan Zhou, Anil Ufuk Batmaz, Adam S. Williams, Dylan Schreiber, Francisco Ortega\",\"Link: http://arxiv.org/abs/2404.03814v1\",\"Abstract: Immersive environments enable users to engage in embodied interaction, enhancing the sensemaking processes involved in completing tasks such as immersive analytics. Previous comparative studies on immersive analytics using augmented and virtual realities have revealed that users employ different strategies for data interpretation and text-based analytics depending on the environment. Our study seeks to investigate how augmented and virtual reality influences sensemaking processes in quantitative immersive analytics. Our results, derived from a diverse group of participants, indicate that users demonstrate comparable performance in both environments. However, it was observed that users exhibit a higher tolerance for cognitive load in VR and travel further in AR. Based on our findings, we recommend providing users with the option to switch between AR and VR, thereby enabling them to select an environment that aligns with their preferences and task requirements.\"]},\"196\":{\"h\":\"Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement\",\"t\":[\"Authors: Mukund Telukunta, Sukruth Rao, Gabriella Stickney, Venkata Sriram Siddardh Nadendla, Casey Canfield\",\"Link: http://arxiv.org/abs/2404.03800v1\",\"Abstract: Modern kidney placement incorporates several intelligent recommendation systems which exhibit social discrimination due to biases inherited from training data. Although initial attempts were made in the literature to study algorithmic fairness in kidney placement, these methods replace true outcomes with surgeons' decisions due to the long delays involved in recording such outcomes reliably. However, the replacement of true outcomes with surgeons' decisions disregards expert stakeholders' biases as well as social opinions of other stakeholders who do not possess medical expertise. This paper alleviates the latter concern and designs a novel fairness feedback survey to evaluate an acceptance rate predictor (ARP) that predicts a kidney's acceptance rate in a given kidney-match pair. The survey is launched on Prolific, a crowdsourcing platform, and public opinions are collected from 85 anonymous crowd participants. A novel social fairness preference learning algorithm is proposed based on minimizing social feedback regret computed using a novel logit-based fairness feedback model. The proposed model and learning algorithm are both validated using simulation experiments as well as Prolific data. Public preferences towards group fairness notions in the context of kidney placement have been estimated and discussed in detail. The specific ARP tested in the Prolific survey has been deemed fair by the participants.\"]},\"197\":{\"h\":\"Revisiting Categorical Color Perception in Scatterplots: Sequential, Diverging, and Categorical Palettes\",\"t\":[\"Authors: Chin Tseng, Arran Zeyu Wang, Ghulam Jilani Quadri, Danielle Albers Szafir\",\"Link: http://arxiv.org/abs/2404.03787v1\",\"Abstract: Existing guidelines for categorical color selection are heuristic, often grounded in intuition rather than empirical studies of readers' abilities. While design conventions recommend palettes maximize hue differences, more recent exploratory findings indicate other factors, such as lightness, may play a role in effective categorical palette design. We conducted a crowdsourced experiment on mean value judgments in multi-class scatterplots using five color palette families--single-hue sequential, multi-hue sequential, perceptually-uniform multi-hue sequential, diverging, and multi-hue categorical--that differ in how they manipulate hue and lightness. Participants estimated relative mean positions in scatterplots containing 2 to 10 categories using 20 colormaps. Our results confirm heuristic guidance that hue-based categorical palettes are most effective. However, they also provide additional evidence that scalable categorical encoding relies on more than hue variance.\"]},\"198\":{\"h\":\"Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations\",\"t\":[\"Authors: Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee\",\"Link: http://arxiv.org/abs/2404.03745v1\",\"Abstract: The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank content as truthful in the order genuine > minor hallucination > major hallucination and user engagement behaviors mirror this pattern. More importantly, we observed that warning improves hallucination detection without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations.\"]},\"199\":{\"h\":\"Explaining Explainability: Understanding Concept Activation Vectors\",\"t\":[\"Authors: Angus Nicolson, Lisa Schut, J. Alison Noble, Yarin Gal\",\"Link: http://arxiv.org/abs/2404.03713v1\",\"Abstract: Recent interpretability methods propose using concept-based explanations to translate the internal representations of deep learning models into a language that humans are familiar with: concepts. This requires understanding which concepts are present in the representation space of a neural network. One popular method for finding concepts is Concept Activation Vectors (CAVs), which are learnt using a probe dataset of concept exemplars. In this work, we investigate three properties of CAVs. CAVs may be: (1) inconsistent between layers, (2) entangled with different concepts, and (3) spatially dependent. Each property provides both challenges and opportunities in interpreting models. We introduce tools designed to detect the presence of these properties, provide insight into how they affect the derived explanations, and provide recommendations to minimise their impact. Understanding these properties can be used to our advantage. For example, we introduce spatially dependent CAVs to test if a model is translation invariant with respect to a specific concept and class. Our experiments are performed on ImageNet and a new synthetic dataset, Elements. Elements is designed to capture a known ground truth relationship between concepts and classes. We release this dataset to facilitate further research in understanding and evaluating interpretability methods.\"]},\"200\":{\"h\":\"Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior\",\"t\":[\"Authors: Frederick Choi, Charlotte Lambert, Vinay Koshy, Sowmya Pratipati, Tue Do, Eshwar Chandrasekharan\",\"Link: http://arxiv.org/abs/2404.03612v1\",\"Abstract: Much of the research in online moderation focuses on punitive actions. However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms. We extend this research by studying the \\\"creator heart\\\" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given. We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users. We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time. We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users. We discuss avenues for extending our study to understanding positive signals from moderators on other platforms.\"]},\"201\":{\"h\":\"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work\",\"t\":[\"Authors: Somin Park, Carol C. Menassa, Vineet R. Kamat\",\"Link: http://arxiv.org/abs/2404.03498v1\",\"Abstract: In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.\"]},\"202\":{\"h\":\"Agora Elevator Bodily Sensation Study -- a report\",\"t\":[\"Authors: Rebekah Rousi\",\"Link: http://arxiv.org/abs/2404.03356v1\",\"Abstract: This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience. It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity? The following report documents the study, procedure, results and findings.\"]},\"203\":{\"h\":\"Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR\",\"t\":[\"Authors: Tanja Kojić, Maurizio Vergari, Simon Knuth, Maximilian Warsinke, Sebastian Möller, Jan-Niklas Voigt-Antons\",\"Link: http://arxiv.org/abs/2404.03337v1\",\"Abstract: Inside-out tracking is growing popular in consumer VR, enhancing accessibility. It uses HMD camera data and neural networks for effective hand tracking. However, limited user experience studies have compared this method to traditional controllers, with no consensus on the optimal control technique. This paper investigates the impact of control methods and gaming duration on VR user experience, hypothesizing hand tracking might be preferred for short sessions and by users new to VR due to its simplicity. Through a lab study with twenty participants, evaluating presence, emotional response, UX quality, and flow, findings revealed control type and session length affect user experience without significant interaction. Controllers were generally superior, attributed to their reliability, and longer sessions increased presence and realism. The study found that individuals with more VR experience were more inclined to recommend hand tracking to others, which contradicted predictions.\"]},\"204\":{\"h\":\"Exploring Emotions in Multi-componential Space using Interactive VR Games\",\"t\":[\"Authors: Rukshani Somarathna, Gelareh Mohammadi\",\"Link: http://arxiv.org/abs/2404.03239v1\",\"Abstract: Emotion understanding is a complex process that involves multiple components. The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction's effectiveness by perceiving and expressing emotions. Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete emotions has not yet been fully explored. Therefore, to better understand emotions underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation. Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset. These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments.\"]},\"205\":{\"h\":\"NLP4Gov: A Comprehensive Library for Computational Policy Analysis\",\"t\":[\"Authors: Mahasweta Chakraborti, Sailendra Akash Bonagiri, Santiago Virgüez-Ruiz, Seth Frey\",\"Link: http://arxiv.org/abs/2404.03206v1\",\"Abstract: Formal rules and policies are fundamental in formally specifying a social system: its operation, boundaries, processes, and even ontology. Recent scholarship has highlighted the role of formal policy in collective knowledge creation, game communities, the production of digital public goods, and national social media governance. Researchers have shown interest in how online communities convene tenable self-governance mechanisms to regulate member activities and distribute rights and privileges by designating responsibilities, roles, and hierarchies. We present NLP4Gov, an interactive kit to train and aid scholars and practitioners alike in computational policy analysis. The library explores and integrates methods and capabilities from computational linguistics and NLP to generate semantic and symbolic representations of community policies from text records. Versatile, documented, and accessible, NLP4Gov provides granular and comparative views into institutional structures and interactions, along with other information extraction capabilities for downstream analysis.\"]},\"206\":{\"h\":\"Towards Collaborative Family-Centered Design for Online Safety, Privacy and Security\",\"t\":[\"Authors: Mamtaj Akter, Zainab Agha, Ashwaq Alsoubai, Naima Ali, Pamela Wisniewski\",\"Link: http://arxiv.org/abs/2404.03165v1\",\"Abstract: Traditional online safety technologies often overly restrict teens and invade their privacy, while parents often lack knowledge regarding their digital privacy. As such, prior researchers have called for more collaborative approaches on adolescent online safety and networked privacy. In this paper, we propose family-centered approaches to foster parent-teen collaboration in ensuring their mobile privacy and online safety while respecting individual privacy, to enhance open discussion and teens' self-regulation. However, challenges such as power imbalances and conflicts with family values arise when implementing such approaches, making parent-teen collaboration difficult. Therefore, attending the family-centered design workshop will provide an invaluable opportunity for us to discuss these challenges and identify best research practices for the future of collaborative online safety and privacy within families.\"]},\"207\":{\"h\":\"Biodegradable Interactive Materials\",\"t\":[\"Authors: Zhihan Zhang, Mallory Parker, Kuotian Liao, Jerry Cao, Anandghan Waghmare, Joseph Breda, Chris Matsumura, Serena Eley, Eleftheria Roumeli, Shwetak Patel, Vikram Iyer\",\"Link: http://arxiv.org/abs/2404.03130v1\",\"Abstract: The sense of touch is fundamental to how we interact with the physical and digital world. Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects. In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties. Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data. We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties. We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods. Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes. We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors. We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces. We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach.\"]},\"208\":{\"h\":\"2024-04-03\"},\"209\":{\"h\":\"Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help\",\"t\":[\"Authors: Nikhita Joshi, Daniel Vogel\",\"Link: http://arxiv.org/abs/2404.03108v1\",\"Abstract: Feelings of something belonging to someone is called \\\"psychological ownership.\\\" A common assumption is that writing with generative AI lowers psychological ownership, but the extent to which this occurs and the role of prompt length are unclear. We report on two experiments to better understand the relationship between psychological ownership and prompt length. Participants wrote short stories either completely by themselves or wrote prompts of varying lengths, enforced through word limits. Results show that when participants wrote longer prompts, they had higher levels of psychological ownership. Their comments suggest they felt encouraged to think more about their prompts and include more details about the story plot. However, these benefits plateaued when the prompt length was 75-100% of the target story length. Based on these results, we propose prompt entry interface designs that nudge users with soft and hard constraints to write longer prompts for increased psychological ownership.\"]},\"210\":{\"h\":\"Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference\",\"t\":[\"Authors: Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen Görtler, Dominik Moritz, Jeffrey P Bigham, Zhile Ren, Cecile Foret, Qi Shan, Xiaoyi Zhang\",\"Link: http://arxiv.org/abs/2404.03085v1\",\"Abstract: On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.\"]},\"211\":{\"h\":\"Toward Safe Evolution of Artificial Intelligence (AI) based Conversational Agents to Support Adolescent Mental and Sexual Health Knowledge Discovery\",\"t\":[\"Authors: Jinkyung Park, Vivek Singh, Pamela Wisniewski\",\"Link: http://arxiv.org/abs/2404.03023v1\",\"Abstract: Following the recent release of various Artificial Intelligence (AI) based Conversation Agents (CAs), adolescents are increasingly using CAs for interactive knowledge discovery on sensitive topics, including mental and sexual health topics. Exploring such sensitive topics through online search has been an essential part of adolescent development, and CAs can support their knowledge discovery on such topics through human-like dialogues. Yet, unintended risks have been documented with adolescents' interactions with AI-based CAs, such as being exposed to inappropriate content, false information, and/or being given advice that is detrimental to their mental and physical well-being (e.g., to self-harm). In this position paper, we discuss the current landscape and opportunities for CAs to support adolescents' mental and sexual health knowledge discovery. We also discuss some of the challenges related to ensuring the safety of adolescents when interacting with CAs regarding sexual and mental health topics. We call for a discourse on how to set guardrails for the safe evolution of AI-based CAs for adolescents.\"]},\"212\":{\"h\":\"Generative AI in the Wild: Prospects, Challenges, and Strategies\",\"t\":[\"Authors: Yuan Sun, Eunchae Jang, Fenglong Ma, Ting Wang\",\"Link: http://arxiv.org/abs/2404.04101v1\",\"Abstract: Propelled by their remarkable capabilities to generate novel and engaging content, Generative Artificial Intelligence (GenAI) technologies are disrupting traditional workflows in many industries. While prior research has examined GenAI from a techno-centric perspective, there is still a lack of understanding about how users perceive and utilize GenAI in real-world scenarios. To bridge this gap, we conducted semi-structured interviews with (N=18) GenAI users in creative industries, investigating the human-GenAI co-creation process within a holistic LUA (Learning, Using and Assessing) framework. Our study uncovered an intriguingly complex landscape: Prospects-GenAI greatly fosters the co-creation between human expertise and GenAI capabilities, profoundly transforming creative workflows; Challenges-Meanwhile, users face substantial uncertainties and complexities arising from resource availability, tool usability, and regulatory compliance; Strategies-In response, users actively devise various strategies to overcome many of such challenges. Our study reveals key implications for the design of future GenAI tools.\"]},\"213\":{\"h\":\"ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale\",\"t\":[\"Authors: Jinbin Huang, Chen Chen, Aditi Mishra, Bum Chul Kwon, Zhicheng Liu, Chris Bryan\",\"Link: http://arxiv.org/abs/2404.02990v1\",\"Abstract: Generative image models have emerged as a promising technology to produce realistic images. Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues. Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images. To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views. To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact \\\"distilled\\\" representations, enriched with information for differentiating authentic and fake images. These representations generate gradients that propagate back to the attention maps of CLIP's transformer block. This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns. ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations. This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact. ASAP supports the analysis of cutting-edge generative models with the latest architectures, including GAN-based models like proGAN and diffusion models like the latent diffusion model. We demonstrate ASAP's usefulness through two usage scenarios using multiple fake image detection benchmark datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by diffusion-based techniques.\"]},\"214\":{\"h\":\"Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?\",\"t\":[\"Authors: Jian Zheng, Ge Gao\",\"Link: http://arxiv.org/abs/2404.02880v1\",\"Abstract: Everyone spends some time waiting every day. HCI research has developed tools for boosting productivity while waiting. However, little is known about how people naturally spend their waiting time. We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks. The aim of this study is to understand the activities people do while waiting and the effect of situational factors. We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities. These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day. Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance. Our findings shed light on future empirical research and system design for time management.\"]},\"215\":{\"h\":\"The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers\",\"t\":[\"Authors: Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, David Sontag\",\"Link: http://arxiv.org/abs/2404.02806v1\",\"Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.\"]},\"216\":{\"h\":\"AI and personalized learning: bridging the gap with modern educational goals\",\"t\":[\"Authors: Kristjan-Julius Laak, Jaan Aru\",\"Link: http://arxiv.org/abs/2404.02798v1\",\"Abstract: Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our analysis indicates a gap between the objectives of modern education and the current direction of PL. We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies. While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.\"]},\"217\":{\"h\":\"IEEE VIS Workshop on Visualization for Climate Action and Sustainability\",\"t\":[\"Authors: Benjamin Bach, Fanny Chevalier, Helen-Nicole Kostis, Mark Subbaro, Yvonne Jansen, Robert Soden\",\"Link: http://arxiv.org/abs/2404.02743v1\",\"Abstract: This first workshop on visualization for climate action and sustainability aims to explore and consolidate the role of data visualization in accelerating action towards addressing the current environmental crisis. Given the urgency and impact of the environmental crisis, we ask how our skills, research methods, and innovations can help by empowering people and organizations. We believe visualization holds an enormous power to aid understanding, decision making, communication, discussion, participation, education, and exploration of complex topics around climate action and sustainability. Hence, this workshop invites submissions and discussion around these topics with the goal of establishing a visible and actionable link between these fields and their respective stakeholders. The workshop solicits work-in-progress and research papers as well as pictorials and interactive demos from the whole range of visualization research (dashboards, interactive spaces, scientific visualization, storytelling, visual analytics, explainability etc.), within the context of environmentalism (climate science, sustainability, energy, circular economy, biodiversity, etc.) and across a range of scenarios from public awareness and understanding, visual analysis, expert decision making, science communication, personal decision making etc. After presentations of submissions, the workshop will feature dedicated discussion groups around data driven interactive experiences for the public, and tools for personal and professional decision making.\"]},\"218\":{\"h\":\"Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities\",\"t\":[\"Authors: Jiale Li, Jiayang Li, Jiahao Chen, Yifan Li, Shijie Wang, Hugo Zhou, Minjun Ye, Yunsheng Su\",\"Link: http://arxiv.org/abs/2404.02718v1\",\"Abstract: Human-like Agents with diverse and dynamic personality could serve as an important design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive application.In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior. The Personality system includes three modules: Cognition, Emotion and Character Growth. The Behavior system comprises two modules: Planning and Action. We also build a simulation platform that enables agents to interact with the environment and other agents. Evolving Agents can simulate the human personality evolution process. Compared to its initial state, agents' personality and behavior patterns undergo believable development after several days of simulation. Agents reflect on their behavior to reason and develop new personality traits. These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution.In our experiment, we utilized simulation platform with 10 agents for evaluation. During the evaluation, these agents experienced believable and inspirational personality evolution. Through ablation and control experiments, we demonstrated the outstanding effectiveness of agent personality evolution and all modules of our agent architecture contribute to creating believable human-like agents with diverse and dynamic personalities. We also demonstrated through workshops how Evolving Agents could inspire designers.\"]},\"219\":{\"h\":\"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM\",\"t\":[\"Authors: Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yuekai Huang, Jun Hu, Qing Wang\",\"Link: http://arxiv.org/abs/2404.02706v1\",\"Abstract: Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.\"]},\"220\":{\"h\":\"Spatial Summation of Localized Pressure for Haptic Sensory Prostheses\",\"t\":[\"Authors: Sreela Kodali, Cihualpilli Camino Cruz, Thomas C. Bulea, Kevin S. Rao Diana Bharucha-Goebel, Alexander T. Chesler, Carsten G. Bonnemann, Allison M. Okamura\",\"Link: http://arxiv.org/abs/2404.02565v1\",\"Abstract: A host of medical conditions, including amputations, diabetes, stroke, and genetic disease, result in loss of touch sensation. Because most types of sensory loss have no pharmacological treatment or rehabilitative therapy, we propose a haptic sensory prosthesis that provides substitutive feedback. The wrist and forearm are compelling locations for feedback due to available skin area and not occluding the hands, but have reduced mechanoreceptor density compared to the fingertips. Focusing on localized pressure as the feedback modality, we hypothesize that we can improve on prior devices by invoking a wider range of stimulus intensity using multiple points of pressure to evoke spatial summation, which is the cumulative perceptual experience from multiple points of stimuli. We conducted a preliminary perceptual test to investigate this idea and found that just noticeable difference is reduced with two points of pressure compared to one, motivating future work using spatial summation in sensory prostheses.\"]},\"221\":{\"h\":\"Cultural influence on autonomous vehicles acceptance\",\"t\":[\"Authors: Chowdhury Shahriar Muzammel, Maria Spichkova, James Harland\",\"Link: http://arxiv.org/abs/2404.03694v1\",\"Abstract: Autonomous vehicles and other intelligent transport systems have been evolving rapidly and are being increasingly deployed worldwide. Previous work has shown that perceptions of autonomous vehicles and attitudes towards them depend on various attributes, including the respondent's age, education level and background. These findings with respect to age and educational level are generally uniform, such as showing that younger respondents are typically more accepting of autonomous vehicles, as are those with higher education levels. However the influence of factors such as culture are much less clear cut. In this paper we analyse the relationship between acceptance of autonomous vehicles and national culture by means of the well-known Hofstede cultural model.\"]},\"222\":{\"h\":\"PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts\",\"t\":[\"Authors: Tian Huang, Chun Yu, Weinan Shi, Zijian Peng, David Yang, Weiqi Sun, Yuanchun Shi\",\"Link: http://arxiv.org/abs/2404.02475v1\",\"Abstract: Robotic Process Automation (RPA) offers a valuable solution for efficiently automating tasks on the graphical user interface (GUI), by emulating human interactions, without modifying existing code. However, its broader adoption is constrained by the need for expertise in both scripting languages and workflow design. To address this challenge, we present PromptRPA, a system designed to comprehend various task-related textual prompts (e.g., goals, procedures), thereby generating and performing corresponding RPA tasks. PromptRPA incorporates a suite of intelligent agents that mimic human cognitive functions, specializing in interpreting user intent, managing external information for RPA generation, and executing operations on smartphones. The agents can learn from user feedback and continuously improve their performance based on the accumulated knowledge. Experimental results indicated a performance jump from a 22.28% success rate in the baseline to 95.21% with PromptRPA, requiring an average of 1.66 user interventions for each new task. PromptRPA presents promising applications in fields such as tutorial creation, smart assistance, and customer service.\"]},\"223\":{\"h\":\"A neuroergonomics model to evaluating nuclear power plants operators' performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model\",\"t\":[\"Authors: Yan Zhang, Ming Jia, Meng Li, JianYu Wang, XiangMin Hu, ZhiHui Xu, Tao Chen\",\"Link: http://arxiv.org/abs/2404.02439v1\",\"Abstract: Operators experience complicated physiological and psychological states when exposed to extreme heat stress, which can impair cognitive function and decrease performance significantly, ultimately leading to severe secondary disasters. Therefore, there is an urgent need for a feasible technique to identify their abnormal states to enhance the reliability of human-cybernetics systems. With the advancement of deep learning in physiological modeling, a model for evaluating operators' performance driven by electrocardiogram (ECG) and functional near-infrared spectroscopy (fNIRS) was proposed, demonstrating high ecological validity. The model fused a convolutional neural network (CNN) backbone and a graph attention network (GAT) backbone to extract discriminative features from ECG time-frequency spectrums and fNIRS prefrontal cortex (PFC) network respectively with deeper neuroscience domain knowledge, and eventually achieved 0.90 AUC. Results supported that handcrafted features extracted by specialized neuroscience methods can alleviate overfitting. Inspired by the small-world nature of the brain network, the fNIRS PFC network was organized as an undirected graph and embedded by GAT. It is proven to perform better in information aggregation and delivery compared to a simple non-linear transformation. The model provides a potential neuroergonomics application for evaluating the human state in vital human-cybernetics systems under industry 5.0 scenarios.\"]},\"224\":{\"h\":\"A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion\",\"t\":[\"Authors: Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, Shuwu Zhang\",\"Link: http://arxiv.org/abs/2404.02411v1\",\"Abstract: Diffusion models have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions. However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning. To resolve this, we propose a unified framework utilizing diffusion inversion that enables multi-level editing capabilities for co-speech gesture generation without re-training. The method takes advantage of two key capabilities of invertible diffusion models. The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise. This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions. The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory. With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization. Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing.\"]},\"225\":{\"h\":\"2024-04-02\"},\"226\":{\"h\":\"From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization\",\"t\":[\"Authors: Chase Stokes, Chelsea Sanker, Bridget Cogley, Vidya Setlur\",\"Link: http://arxiv.org/abs/2404.02317v1\",\"Abstract: Understanding and communicating data uncertainty is crucial for making informed decisions in sectors like finance and healthcare. Previous work has explored how to express uncertainty in various modes. For example, uncertainty can be expressed visually with quantile dot plots or linguistically with hedge words and prosody. Our research aims to systematically explore how variations within each mode contribute to communicating uncertainty to the user; this allows us to better understand each mode's affordances and limitations. We completed an exploration of the uncertainty design space based on pilot studies and ran two crowdsourced experiments examining how speech, text, and visualization modes and variants within them impact decision-making with uncertain data. Visualization and text were most effective for rational decision-making, though text resulted in lower confidence. Speech garnered the highest trust despite sometimes leading to risky decisions. Results from these studies indicate meaningful trade-offs among modes of information and encourage exploration of multimodal data representations.\"]},\"227\":{\"h\":\"A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other\",\"t\":[\"Authors: Seraphina Yong, Leo Cui, Evan Suma Rosenberg, Svetlana Yarosh\",\"Link: http://arxiv.org/abs/2404.02277v1\",\"Abstract: Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others' reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner's experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of `embodied social cognition,' and envisioning socially-embodied experiences as an interactive context.\"]},\"228\":{\"h\":\"Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices\",\"t\":[\"Authors: Ruiwei Xiao, Xinying Hou, John Stamper\",\"Link: http://arxiv.org/abs/2404.02213v1\",\"Abstract: Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing LLM-based hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students' problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students' learning needs.\"]},\"229\":{\"h\":\"Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools\",\"t\":[\"Authors: Md Naimul Hoque, Sungbok Shin, Niklas Elmqvist\",\"Link: http://arxiv.org/abs/2404.02147v1\",\"Abstract: Human-centered AI (HCAI), rather than replacing the human, puts the human user in the driver's seat of so-called human-centered AI-infused tools (HCAI tools): interactive software tools that amplify, augment, empower, and enhance human performance using AI models; often novel generative or foundation AI ones. In this paper, we discuss how interactive visualization can be a key enabling technology for creating such human-centered AI tools. Visualization has already been shown to be a fundamental component in explainable AI models, and coupling this with data-driven, semantic, and unified interaction feedback loops will enable a human-centered approach to integrating AI models in the loop with human users. We present several examples of our past and current work on such HCAI tools, including for creative writing, temporal prediction, and user experience analysis. We then draw parallels between these tools to suggest common themes on how interactive visualization can support the design of future HCAI tools.\"]},\"230\":{\"h\":\"The Effects of Group Sanctions on Participation and Toxicity: Quasi-experimental Evidence from the Fediverse\",\"t\":[\"Authors: Carl Colglazier, Nathan TeBlunthuis, Aaron Shaw\",\"Link: http://arxiv.org/abs/2404.02109v1\",\"Abstract: Online communities often overlap and coexist, despite incongruent norms and approaches to content moderation. When communities diverge, decentralized and federated communities may pursue group-level sanctions, including defederation (disconnection) to block communication between members of specific communities. We investigate the effects of defederation in the context of the Fediverse, a set of decentralized, interconnected social networks with independent governance. Mastodon and Pleroma, the most popular software powering the Fediverse, allow administrators on one server to defederate from another. We use a difference-in-differences approach and matched controls to estimate the effects of defederation events on participation and message toxicity among affected members of the blocked and blocking servers. We find that defederation causes a drop in activity for accounts on the blocked servers, but not on the blocking servers. Also, we find no evidence of an effect of defederation on message toxicity.\"]},\"231\":{\"h\":\"Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows\",\"t\":[\"Authors: Grace Guo, Dustin Arendt, Alex Endert\",\"Link: http://arxiv.org/abs/2404.02081v1\",\"Abstract: Explainable AI (XAI) tools represent a turn to more human-centered and human-in-the-loop AI approaches that emphasize user needs and perspectives in machine learning model development workflows. However, while the majority of ML resources available today are developed for Python computational environments such as JupyterLab and Jupyter Notebook, the same has not been true of interactive XAI systems, which are often still implemented as standalone interfaces. In this paper, we address this mismatch by identifying three design patterns for embedding front-end XAI interfaces into Jupyter, namely: 1) One-way communication from Python to JavaScript, 2) Two-way data synchronization, and 3) Bi-directional callbacks. We also provide an open-source toolkit, bonXAI, that demonstrates how each design pattern might be used to build interactive XAI tools for a Pytorch text classification workflow. Finally, we conclude with a discussion of best practices and open questions. Our aims for this paper are to discuss how interactive XAI tools might be developed for computational notebooks, and how they can better integrate into existing model development workflows to support more collaborative, human-centered AI.\"]},\"232\":{\"h\":\"Preuve de concept d'un bot vocal dialoguant en wolof\",\"t\":[\"Authors: Elodie Gauthier, Papa-Séga Wade, Thierry Moudenc, Patrice Collen, Emilie De Neef, Oumar Ba, Ndeye Khoyane Cama, Cheikh Ahmadou Bamba Kebe, Ndeye Aissatou Gningue, Thomas Mendo'o Aristide\",\"Link: http://arxiv.org/abs/2404.02009v1\",\"Abstract: This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22% of WER for the ASR task and 78% of F1-score on the NLU task.\"]},\"233\":{\"h\":\"Cash or Non-Cash? Unveiling Ideators' Incentive Preferences in Crowdsourcing Contests\",\"t\":[\"Authors: Christoph Riedl, Johann Füller, Katja Hutter, Gerard J. Tellis\",\"Link: http://arxiv.org/abs/2404.01997v1\",\"Abstract: Even though research has repeatedly shown that non-cash incentives can be effective, cash incentives are the de facto standard in crowdsourcing contests. In this multi-study research, we quantify ideators' preferences for non-cash incentives and investigate how allowing ideators to self-select their preferred incentive -- offering ideators a choice between cash and non-cash incentives -- affects their creative performance. We further explore whether the market context of the organization hosting the contest -- social (non-profit) or monetary (for-profit) -- moderates incentive preferences and their effectiveness. We find that individuals exhibit heterogeneous incentive preferences and often prefer non-cash incentives, even in for-profit contexts. Offering ideators a choice of incentives can enhance creative performance. Market context moderates the effect of incentives, such that ideators who receive non-cash incentives in for-profit contexts tend to exert less effort. We show that heterogeneity of ideators' preferences (and the ability to satisfy diverse preferences with suitably diverse incentive options) is a critical boundary condition to realizing benefits from offering ideators a choice of incentives. We provide managers with guidance to design effective incentives by improving incentive-preference fit for ideators.\"]},\"234\":{\"h\":\"Fast and Adaptive Questionnaires for Voting Advice Applications\",\"t\":[\"Authors: Fynn Bachmann, Cristina Sarasua, Abraham Bernstein\",\"Link: http://arxiv.org/abs/2404.01872v1\",\"Abstract: The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions. We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy. Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version.\"]},\"235\":{\"h\":\"Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model\",\"t\":[\"Authors: Xu He, Qiaochu Huang, Zhensong Zhang, Zhiwei Lin, Zhiyong Wu, Sicheng Yang, Minglei Li, Zhiyi Chen, Songcen Xu, Xiaofei Wu\",\"Link: http://arxiv.org/abs/2404.01862v1\",\"Abstract: Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.\"]},\"236\":{\"h\":\"\\\"That's Not Good Science!\\\": An Argument for the Thoughtful Use of Formative Situations in Research through Design\",\"t\":[\"Authors: Raquel B Robinson, Anya Osborne, Chen Ji, James Collin Fey, Ella Dagan, Katherine Isbister\",\"Link: http://arxiv.org/abs/2404.01848v1\",\"Abstract: Most currently accepted approaches to evaluating Research through Design (RtD) presume that design prototypes are finalized and ready for robust testing in laboratory or in-the-wild settings. However, it is also valuable to assess designs at intermediate phases with mid-fidelity prototypes, not just to inform an ongoing design process, but also to glean knowledge of broader use to the research community. We propose 'formative situations' as a frame for examining mid-fidelity prototypes-in-process in this way. We articulate a set of criteria to help the community better assess the rigor of formative situations, in the service of opening conversation about establishing formative situations as a valuable contribution type within the RtD community.\"]},\"237\":{\"h\":\"Unmasking the Nuances of Loneliness: Using Digital Biomarkers to Understand Social and Emotional Loneliness in College Students\",\"t\":[\"Authors: Malik Muhammad Qirtas, Evi Zafeirid, Dirk Pesch, Eleanor Bantry White\",\"Link: http://arxiv.org/abs/2404.01845v1\",\"Abstract: Background: Loneliness among students is increasing across the world, with potential consequences for mental health and academic success. To address this growing problem, accurate methods of detection are needed to identify loneliness and to differentiate social and emotional loneliness so that intervention can be personalized to individual need. Passive sensing technology provides a unique technique to capture behavioral patterns linked with distinct loneliness forms, allowing for more nuanced understanding and interventions for loneliness. Methods: To differentiate between social and emotional loneliness using digital biomarkers, our study included statistical tests, machine learning for predictive modeling, and SHAP values for feature importance analysis, revealing important factors in loneliness classification. Results: Our analysis revealed significant behavioral differences between socially and emotionally lonely groups, particularly in terms of phone usage and location-based features , with machine learning models demonstrating substantial predictive power in classifying loneliness levels. The XGBoost model, in particular, showed high accuracy and was effective in identifying key digital biomarkers, including phone usage duration and location-based features, as significant predictors of loneliness categories. Conclusion: This study underscores the potential of passive sensing data, combined with machine learning techniques, to provide insights into the behavioral manifestations of social and emotional loneliness among students. The identification of key digital biomarkers paves the way for targeted interventions aimed at mitigating loneliness in this population.\"]},\"238\":{\"h\":\"Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods\",\"t\":[\"Authors: Zdravko Marinov, Moon Kim, Jens Kleesiek, Rainer Stiefelhagen\",\"Link: http://arxiv.org/abs/2404.01816v1\",\"Abstract: Interactive segmentation plays a crucial role in accelerating the annotation, particularly in domains requiring specialized expertise such as nuclear medicine. For example, annotating lesions in whole-body Positron Emission Tomography (PET) images can require over an hour per volume. While previous works evaluate interactive segmentation models through either real user studies or simulated annotators, both approaches present challenges. Real user studies are expensive and often limited in scale, while simulated annotators, also known as robot users, tend to overestimate model performance due to their idealized nature. To address these limitations, we introduce four evaluation metrics that quantify the user shift between real and simulated annotators. In an initial user study involving four annotators, we assess existing robot users using our proposed metrics and find that robot users significantly deviate in performance and annotation behavior compared to real annotators. Based on these findings, we propose a more realistic robot user that reduces the user shift by incorporating human factors such as click variation and inter-annotator disagreement. We validate our robot user in a second user study, involving four other annotators, and show it consistently reduces the simulated-to-real user shift compared to traditional robot users. By employing our robot user, we can conduct more large-scale and cost-efficient evaluations of interactive segmentation models, while preserving the fidelity of real user studies. Our implementation is based on MONAI Label and will be made publicly available.\"]},\"239\":{\"h\":\"Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G\",\"t\":[\"Authors: Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku Jäntti, Mérouane Debbah\",\"Link: http://arxiv.org/abs/2404.01713v1\",\"Abstract: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.\"]},\"240\":{\"h\":\"Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot\",\"t\":[\"Authors: Petr Vanc, Radoslav Skoviera, Karla Stepanova\",\"Link: http://arxiv.org/abs/2404.01702v1\",\"Abstract: As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds.\"]},\"241\":{\"h\":\"NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps\",\"t\":[\"Authors: Kristina Gligoric, Myra Cheng, Lucia Zheng, Esin Durmus, Dan Jurafsky\",\"Link: http://arxiv.org/abs/2404.01651v1\",\"Abstract: The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.\"]},\"242\":{\"h\":\"InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis\",\"t\":[\"Authors: Luoxuan Weng, Xingbo Wang, Junyu Lu, Yingchaojie Feng, Yihan Liu, Wei Chen\",\"Link: http://arxiv.org/abs/2404.01644v1\",\"Abstract: The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users' analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of LLMs. In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during LLM-powered data analysis. Then, we propose an LLM-based multi-agent framework to automatically extract, associate, and organize insights along with the analysis process. Based on this, we introduce InsightLens, an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users' manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.\"]},\"243\":{\"h\":\"Gen4DS: Workshop on Data Storytelling in an Era of Generative AI\",\"t\":[\"Authors: Xingyu Lan, Leni Yang, Zezhong Wang, Danqing Shi, Sheelagh Carpendale\",\"Link: http://arxiv.org/abs/2404.01622v1\",\"Abstract: Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories? How might generative AI alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (including hands-on creation, group discussion pods, and debates on controversial issues) for the workshop. We hope that participants will learn about the latest advances and pioneering work in data storytelling, engage in critical conversations with each other, and have an enjoyable, unforgettable, and meaningful experience at the event.\"]},\"244\":{\"h\":\"Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration\",\"t\":[\"Authors: Melanie J. McGrath, Andreas Duenser, Justine Lacey, Cecile Paris\",\"Link: http://arxiv.org/abs/2404.01615v1\",\"Abstract: Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors. The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time. Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts. These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams.\"]},\"245\":{\"h\":\"Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game\",\"t\":[\"Authors: Silin Du, Xiaowei Zhang\",\"Link: http://arxiv.org/abs/2404.01602v1\",\"Abstract: Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs and few LLMs possess the capacity for opinion leadership.\"]},\"246\":{\"h\":\"Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment\",\"t\":[\"Authors: Jesudara Omidokun, Darlington Egeonu, Bochen Jia, Liang Yang\",\"Link: http://arxiv.org/abs/2404.01576v1\",\"Abstract: This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks.\"]},\"247\":{\"h\":\"2024-04-01\"},\"248\":{\"h\":\"PlayFutures: Imagining Civic Futures with AI and Puppets\",\"t\":[\"Authors: Supratim Pait, Sumita Sharma, Ashley Frith, Michael Nitsche, Noura Howell\",\"Link: http://arxiv.org/abs/2404.01527v1\",\"Abstract: Children are the builders of the future and crucial to how the technologies around us develop. They are not voters but are participants in how the public spaces in a city are used. Through a workshop designed around kids of age 9-12, we investigate if novel technologies like artificial intelligence can be integrated in existing ways of play and performance to 1) re-imagine the future of civic spaces, 2) reflect on these novel technologies in the process and 3) build ways of civic engagement through play. We do this using a blend AI image generation and Puppet making to ultimately build future scenarios, perform debate and discussion around the futures and reflect on AI, its role and potential in their process. We present our findings of how AI helped envision these futures, aid performances, and report some initial reflections from children about the technology.\"]},\"249\":{\"h\":\"DeLVE into Earth's Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts\",\"t\":[\"Authors: Mara Solen, Nigar Sultana, Laura Lukes, Tamara Munzner\",\"Link: http://arxiv.org/abs/2404.01488v1\",\"Abstract: While previous work has found success in deploying visualizations as museum exhibits, differences in visitor behaviour across varying museum contexts are understudied. We present an interactive Deep-time Literacy Visualization Exhibit (DeLVE) to help museum visitors understand deep time (lengths of extremely long geological processes) by improving proportional reasoning skills through comparison of different time periods. DeLVE uses a new visualization idiom, Connected Multi-Tier Ranges, to visualize curated datasets of past events across multiple scales of time, relating extreme scales with concrete scales that have more familiar magnitudes and units. Museum staff at three separate museums approved the deployment of DeLVE as a digital kiosk, and devoted time to curating a unique dataset in each of them. We collect data from two sources, an observational study and system trace logs, yielding evidence of successfully meeting our requirements. We discuss the importance of context: similar museum exhibits in different contexts were received very differently by visitors. We additionally discuss differences in our process from standard design study methodology which is focused on design studies for data analysis purposes, rather than for presentation. Supplemental materials are available at: https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541\"]},\"250\":{\"h\":\"A Design Space for Visualization with Large Scale-Item Ratios\",\"t\":[\"Authors: Mara Solen, Tamara Munzner\",\"Link: http://arxiv.org/abs/2404.01485v1\",\"Abstract: The scale-item ratio is the relationship between the largest scale and the smallest item in a visualization. Designing visualizations when this ratio is large can be challenging, and designers have developed many approaches to overcome this challenge. We present a design space for visualization with large scale-item ratios. The design space includes three dimensions, with eight total subdimensions. We demonstrate its descriptive power by using it to code approaches from a corpus we compiled of 54 examples, created by a mix of academics and practitioners. We then partition these examples into five strategies, which are shared approaches with respect to design space dimension choices. We demonstrate generative power by analyzing missed opportunities within the corpus of examples, identified through analysis of the design space, where we note how certain examples could have benefited from different choices. Supplemental materials: https://osf.io/wbrdm/?view_only=04389a2101a04e71a2c208a93bf2f7f2\"]},\"251\":{\"h\":\"Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs\",\"t\":[\"Authors: Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald\",\"Link: http://arxiv.org/abs/2404.01461v1\",\"Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.\"]},\"252\":{\"h\":\"A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs\",\"t\":[\"Authors: Harry Li, Gabriel Appleby, Ashley Suh\",\"Link: http://arxiv.org/abs/2404.01425v1\",\"Abstract: We present a mixed-methods study to explore how large language models (LLMs) can assist users in the visual exploration and analysis of knowledge graphs (KGs). We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with KGs and LLMs, either collaboratively or concurrently. Our findings show that participants overwhelmingly want an LLM to facilitate data retrieval from KGs through joint query construction, to identify interesting relationships in the KG through multi-turn conversation, and to create on-demand visualizations from the KG that enhance their trust in the LLM's outputs. To interact with an LLM, participants strongly prefer a chat-based 'widget,' built on top of their regular analysis workflows, with the ability to guide the LLM using their interactions with a visualization. When viewing an LLM's outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the KG) alongside summarizing text. However, participants also expressed concerns about an LLM's ability to maintain semantic intent when translating natural language questions into KG queries, the risk of an LLM 'hallucinating' false data from the KG, and the difficulties of engineering a 'perfect prompt.' From the analysis of our interviews, we contribute a preliminary roadmap for the design of LLM-driven knowledge graph exploration systems and outline future opportunities in this emergent design space.\"]},\"253\":{\"h\":\"Towards a potential paradigm shift in health data collection and analysis\",\"t\":[\"Authors: David Josef Herzog, Nitsa Judith Herzog\",\"Link: http://arxiv.org/abs/2404.01403v1\",\"Abstract: Industrial Revolution 4.0 transforms healthcare systems. The first three technological revolutions changed the relationship between human and machine interaction due to the exponential growth of machine numbers. The fourth revolution put humans into a situation where heterogeneous data is produced with unmatched quantity and quality not only by traditional methods, enforced by digitization, but also by ubiquitous computing, machine-to-machine interactions and smart environment. The modern cyber-physical space underlines the role of the person in the expanding context of computerization and big data processing. In healthcare, where data collection and analysis particularly depend on human efforts, the disruptive nature of these developments is evident. Adaptation to this process requires deep scrutiny of the trends and recognition of future medical data technologies` evolution. Significant difficulties arise from discrepancies in requirements by healthcare, administrative and technology stakeholders. Black box and grey box decisions made in medical imaging and diagnostic Decision Support Software are often not transparent enough for the professional, social and medico-legal requirements. While Explainable AI proposes a partial solution for AI applications in medicine, the approach has to be wider and multiplex. LLM potential and limitations are also discussed. This paper lists the most significant issues in these topics and describes possible solutions.\"]},\"254\":{\"h\":\"Evaluating Privacy Perceptions, Experience, and Behavior of Software Development Teams\",\"t\":[\"Authors: Maxwell Prybylo, Sara Haghighi, Sai Teja Peddinti, Sepideh Ghanavati\",\"Link: http://arxiv.org/abs/2404.01283v1\",\"Abstract: With the increase in the number of privacy regulations, small development teams are forced to make privacy decisions on their own. In this paper, we conduct a mixed-method survey study, including statistical and qualitative analysis, to evaluate the privacy perceptions, practices, and knowledge of members involved in various phases of software development (SDLC). Our survey includes 362 participants from 23 countries, encompassing roles such as product managers, developers, and testers. Our results show diverse definitions of privacy across SDLC roles, emphasizing the need for a holistic privacy approach throughout SDLC. We find that software teams, regardless of their region, are less familiar with privacy concepts (such as anonymization), relying on self-teaching and forums. Most participants are more familiar with GDPR and HIPAA than other regulations, with multi-jurisdictional compliance being their primary concern. Our results advocate the need for role-dependent solutions to address the privacy challenges, and we highlight research directions and educational takeaways to help improve privacy-aware software development.\"]},\"255\":{\"h\":\"Information Plane Analysis Visualization in Deep Learning via Transfer Entropy\",\"t\":[\"Authors: Adrian Moldovan, Angel Cataron, Razvan Andonie\",\"Link: http://arxiv.org/abs/2404.01364v1\",\"Abstract: In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training. According to the Information Bottleneck principle, a neural model's internal representation should compress the input data as much as possible while still retaining sufficient information about the output. Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation. The claim that there is a causal link between information-theoretic compression and generalization, measured by mutual information, is plausible, but results from different studies are conflicting. In contrast to mutual information, TE can capture temporal relationships between variables. To explore such links, in our novel approach we use TE to quantify information transfer between neural layers and perform Information Plane analysis. We obtained encouraging experimental results, opening the possibility for further investigations.\"]},\"256\":{\"h\":\"Image Reconstruction from Electroencephalography Using Latent Diffusion\",\"t\":[\"Authors: Teng Fei, Virginia de Sa\",\"Link: http://arxiv.org/abs/2404.01250v1\",\"Abstract: In this work, we have adopted the diffusion-based image reconstruction pipeline previously used for fMRI image reconstruction and applied it to Electroencephalography (EEG). The EEG encoding method is very simple, and forms a baseline from which more sophisticated EEG encoding methods can be compared. We have also evaluated the fidelity of the generated image using the same metrics used in the previous functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) works. Our results show that while the reconstruction from EEG recorded to rapidly presented images is not as good as reconstructions from fMRI to slower presented images, it holds a surprising amount of information that could be applied in specific use cases. Also, EEG-based image reconstruction works better in some categories-such as land animals and food-than others, shedding new light on previous findings of EEG's sensitivity to those categories and revealing potential for these methods to further understand EEG responses to human visual coding. More investigation should use longer-duration image stimulations to elucidate the later components that might be salient to the different image categories.\"]},\"257\":{\"h\":\"AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding\",\"t\":[\"Authors: Safwat Ali Khan, Wenyu Wang, Yiran Ren, Bin Zhu, Jiangfan Shi, Alyssa McGowan, Wing Lam, Kevin Moran\",\"Link: http://arxiv.org/abs/2404.01240v1\",\"Abstract: Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate. In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store. Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage. The improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques.\"]},\"258\":{\"h\":\"LLM Attributor: Interactive Visual Attribution for LLM Generation\",\"t\":[\"Authors: Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng\",\"Link: http://arxiv.org/abs/2404.01361v1\",\"Abstract: While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.\"]},\"259\":{\"h\":\"Chat Modeling: Natural Language-based Procedural Modeling of Biological Structures without Training\",\"t\":[\"Authors: Donggang Jia, Yunhai Wang, Ivan Viola\",\"Link: http://arxiv.org/abs/2404.01063v1\",\"Abstract: 3D modeling of biological structures is an inherently complex process, necessitating both biological and geometric understanding. Additionally, the complexity of user interfaces of 3D modeling tools and the associated steep learning curve further exacerbate the difficulty of authoring a 3D model. In this paper, we introduce a novel framework to address the challenge of using 3D modeling software by converting users' textual inputs into modeling actions within an interactive procedural modeling system. The framework incorporates a code generator of a novel code format and a corresponding code interpreter. The major technical innovation includes the user-refinement mechanism that captures the degree of user dissatisfaction with the modeling outcome, offers an interactive revision, and leverages this feedback for future improved 3D modeling. This entire framework is powered by large language models and eliminates the need for a traditional training process. We develop a prototype tool named Chat Modeling, offering both automatic and step-by-step 3D modeling approaches. Our evaluation of the framework with structural biologists highlights the potential of our approach being utilized in their scientific workflows. All supplemental materials are available at https://osf.io/x4qb7/.\"]},\"260\":{\"h\":\"Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation\",\"t\":[\"Authors: Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, Shengfeng He\",\"Link: http://arxiv.org/abs/2404.01050v1\",\"Abstract: Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofengl/DragNoise.\"]},\"261\":{\"h\":\"How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey\",\"t\":[\"Authors: Zhonghao Shi, Ellen Landrum, Amy O' Connell, Mina Kian, Leticia Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J Matarić\",\"Link: http://arxiv.org/abs/2404.00938v1\",\"Abstract: Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of LLMs in SAR technologies, and discuss the potentials and risks of applying LLMs to the following three major technical challenges of SAR: 1) natural language dialog; 2) multimodal understanding; 3) LLMs as robot policies.\"]},\"262\":{\"h\":\"2024-03-31\"},\"263\":{\"h\":\"Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration\",\"t\":[\"Authors: Samuel Aleksander Sánchez Olszewski\",\"Link: http://arxiv.org/abs/2404.00634v1\",\"Abstract: AI is redefining how humans interact with technology, leading to a synergetic collaboration between the two. Nevertheless, the effects of human cognition on this collaboration remain unclear. This study investigates the implications of two cognitive biases, anthropomorphism and framing effect, on human-AI collaboration within a hiring setting. Subjects were asked to select job candidates with the help of an AI-powered recommendation tool. The tool was manipulated to have either human-like or robot-like characteristics and presented its recommendations in either positive or negative frames. The results revealed that the framing of AI's recommendations had no significant influence on subjects' decisions. In contrast, anthropomorphism significantly affected subjects' agreement with AI recommendations. Contrary to expectations, subjects were less likely to agree with the AI if it had human-like characteristics. These findings demonstrate that cognitive biases can impact human-AI collaboration and highlight the need for tailored approaches to AI product design, rather than a single, universal solution.\"]},\"264\":{\"h\":\"\\\"My agent understands me better\\\": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents\",\"t\":[\"Authors: Yuki Hou, Haruki Tamoto, Homei Miyashita\",\"Link: http://arxiv.org/abs/2404.00573v1\",\"Abstract: In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.\"]},\"265\":{\"h\":\"The Emotional Impact of Game Duration: A Framework for Understanding Player Emotions in Extended Gameplay Sessions\",\"t\":[\"Authors: Anoop Kumar, Suresh Dodda, Navin Kamuni, Venkata Sai Mahesh Vuppalapati\",\"Link: http://arxiv.org/abs/2404.00526v1\",\"Abstract: Video games have played a crucial role in entertainment since their development in the 1970s, becoming even more prominent during the lockdown period when people were looking for ways to entertain them. However, at that time, players were unaware of the significant impact that playtime could have on their feelings. This has made it challenging for designers and developers to create new games since they have to control the emotional impact that these games will take on players. Thus, the purpose of this study is to look at how a player's emotions are affected by the duration of the game. In order to achieve this goal, a framework for emotion detection is created. According to the experiment's results, the volunteers' general ability to express emotions increased from 20 to 60 minutes. In comparison to shorter gameplay sessions, the experiment found that extended gameplay sessions did significantly affect the player's emotions. According to the results, it was recommended that in order to lessen the potential emotional impact that playing computer and video games may have in the future, game producers should think about creating shorter, entertaining games.\"]},\"266\":{\"h\":\"Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation\",\"t\":[\"Authors: Rohan Chaudhury, Mihir Godbole, Aakash Garg, Jinsil Hwaryoung Seo\",\"Link: http://arxiv.org/abs/2404.01339v1\",\"Abstract: Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a zero-shot setting. These intricacies are seamlessly integrated into the generated text by the language model during text generation, allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. These generated elements are then adeptly transformed into corresponding speech patterns and emotive sounds using a rule-based approach during the text-to-speech phase. Based on our experiments, our novel system produces synthesized speech that's almost indistinguishable from genuine human communication, making each interaction feel more personal and authentic.\"]},\"267\":{\"h\":\"2024-03-30\"},\"268\":{\"h\":\"Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App\",\"t\":[\"Authors: Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell\",\"Link: http://arxiv.org/abs/2404.00487v1\",\"Abstract: MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.\"]},\"269\":{\"h\":\"Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment\",\"t\":[\"Authors: Catie Cuan, Kyle Jeffrey, Kim Kleiven, Adrian Li, Emre Fisher, Matt Harrison, Benjie Holson, Allison Okamura, Matt Bennice\",\"Link: http://arxiv.org/abs/2404.00442v1\",\"Abstract: For decades, robotics researchers have pursued various tasks for multi-robot systems, from cooperative manipulation to search and rescue. These tasks are multi-robot extensions of classical robotic tasks and often optimized on dimensions such as speed or efficiency. As robots transition from commercial and research settings into everyday environments, social task aims such as engagement or entertainment become increasingly relevant. This work presents a compelling multi-robot task, in which the main aim is to enthrall and interest. In this task, the goal is for a human to be drawn to move alongside and participate in a dynamic, expressive robot flock. Towards this aim, the research team created algorithms for robot movements and engaging interaction modes such as gestures and sound. The contributions are as follows: (1) a novel group navigation algorithm involving human and robot agents, (2) a gesture responsive algorithm for real-time, human-robot flocking interaction, (3) a weight mode characterization system for modifying flocking behavior, and (4) a method of encoding a choreographer's preferences inside a dynamic, adaptive, learned system. An experiment was performed to understand individual human behavior while interacting with the flock under three conditions: weight modes selected by a human choreographer, a learned model, or subset list. Results from the experiment showed that the perception of the experience was not influenced by the weight mode selection. This work elucidates how differing task aims such as engagement manifest in multi-robot system design and execution, and broadens the domain of multi-robot tasks.\"]},\"270\":{\"h\":\"Visualizing Routes with AI-Discovered Street-View Patterns\",\"t\":[\"Authors: Tsung Heng Wu, Md Amiruzzaman, Ye Zhao, Deepshikha Bhati, Jing Yang\",\"Link: http://arxiv.org/abs/2404.00431v1\",\"Abstract: Street-level visual appearances play an important role in studying social systems, such as understanding the built environment, driving routes, and associated social and economic factors. It has not been integrated into a typical geographical visualization interface (e.g., map services) for planning driving routes. In this paper, we study this new visualization task with several new contributions. First, we experiment with a set of AI techniques and propose a solution of using semantic latent vectors for quantifying visual appearance features. Second, we calculate image similarities among a large set of street-view images and then discover spatial imagery patterns. Third, we integrate these discovered patterns into driving route planners with new visualization techniques. Finally, we present VivaRoutes, an interactive visualization prototype, to show how visualizations leveraged with these discovered patterns can help users effectively and interactively explore multiple routes. Furthermore, we conducted a user study to assess the usefulness and utility of VivaRoutes.\"]},\"271\":{\"h\":\"A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration\",\"t\":[\"Authors: Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone\",\"Link: http://arxiv.org/abs/2404.00405v1\",\"Abstract: With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the \\\"5W1H\\\" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.\"]},\"272\":{\"h\":\"Designing a User-centric Framework for Information Quality Ranking of Large-scale Street View Images\",\"t\":[\"Authors: Tahiya Chowdhury, Ilan Mandel, Jorge Ortiz, Wendy Ju\",\"Link: http://arxiv.org/abs/2404.00392v1\",\"Abstract: Street view imagery (SVI), largely captured via outfitted fleets or mounted dashcams in consumer vehicles is a rapidly growing source of geospatial data used in urban sensing and development. These datasets are often collected opportunistically, are massive in size, and vary in quality which limits the scope and extent of their use in urban planning. Thus far there has not been much work to identify the obstacles experienced and tools needed by the users of such datasets. This severely limits the opportunities of using emerging street view images in supporting novel research questions that can improve the quality of urban life. This work includes a formative interview study with 5 expert users of large-scale street view datasets from academia, urban planning, and related professions which identifies novel use cases, challenges, and opportunities to increase the utility of these datasets. Based on the user findings, we present a framework to evaluate the quality of information for street images across three attributes (spatial, temporal, and content) that stakeholders can utilize for estimating the value of a dataset, and to improve it over time for their respective use case. We then present a case study using novel street view images where we evaluate our framework and present practical use cases for users. We discuss the implications for designing future systems to support the collection and use of street view data to assist in sensing and planning the urban environment.\"]},\"273\":{\"h\":\"On Task and in Sync: Examining the Relationship between Gaze Synchrony and Self-Reported Attention During Video Lecture Learning\",\"t\":[\"Authors: Babette Bühler, Efe Bozkir, Hannah Deininger, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci\",\"Link: http://arxiv.org/abs/2404.00333v1\",\"Abstract: Successful learning depends on learners' ability to sustain attention, which is particularly challenging in online education due to limited teacher interaction. A potential indicator for attention is gaze synchrony, demonstrating predictive power for learning achievements in video-based learning in controlled experiments focusing on manipulating attention. This study (N=84) examines the relationship between gaze synchronization and self-reported attention of learners, using experience sampling, during realistic online video learning. Gaze synchrony was assessed through Kullback-Leibler Divergence of gaze density maps and MultiMatch algorithm scanpath comparisons. Results indicated significantly higher gaze synchronization in attentive participants for both measures and self-reported attention significantly predicted post-test scores. In contrast, synchrony measures did not correlate with learning outcomes. While supporting the hypothesis that attentive learners exhibit similar eye movements, the direct use of synchrony as an attention indicator poses challenges, requiring further research on the interplay of attention, gaze synchrony, and video content type.\"]},\"274\":{\"h\":\"Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation\",\"t\":[\"Authors: Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn\",\"Link: http://arxiv.org/abs/2404.00300v1\",\"Abstract: A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players' mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and multimodal feedback. We conducted an experiment to investigate the intervention's effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments.\"]},\"275\":{\"h\":\"Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World\",\"t\":[\"Authors: Guande Wu, Chen Zhao, Claudio Silva, He He\",\"Link: http://arxiv.org/abs/2404.00246v1\",\"Abstract: Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.\"]},\"276\":{\"h\":\"2024-03-29\"},\"277\":{\"h\":\"Tools and Tasks in Sensemaking: A Visual Accessibility Perspective\",\"t\":[\"Authors: Yichun Zhao, Miguel A. Nacenta\",\"Link: http://arxiv.org/abs/2404.00192v1\",\"Abstract: Our previous interview study explores the needs and uses of diagrammatic information by the Blind and Low Vision (BLV) community, resulting in a framework called the Ladder of Diagram Access. The framework outlines five levels of information access when interacting with a diagram. In this paper, we connect this framework to include the global activity of sensemaking and discuss its (in)accessibility to the BLV demographic. We also discuss the integration of this framework into the sensemaking process and explore the current sensemaking practices and strategies employed by the BLV community, the challenges they face at different levels of the ladder, and potential solutions to enhance inclusivity towards a data-driven workforce.\"]},\"278\":{\"h\":\"No Risk, No Reward: Towards An Automated Measure of Psychological Safety from Online Communication\",\"t\":[\"Authors: Sharon Ferguson, Georgia Van de Zande, Alison Olechowski\",\"Link: http://arxiv.org/abs/2404.00171v1\",\"Abstract: The data created from virtual communication platforms presents the opportunity to explore automated measures for monitoring team performance. In this work, we explore one important characteristic of successful teams - Psychological Safety - or the belief that a team is safe for interpersonal risk-taking. To move towards an automated measure of this phenomenon, we derive virtual communication characteristics and message keywords related to elements of Psychological Safety from the literature. Using a mixed methods approach, we investigate whether these characteristics are present in the Slack messages from two design teams - one high in Psychological Safety, and one low. We find that some usage characteristics, such as replies, reactions, and user mentions, might be promising metrics to indicate higher levels of Psychological Safety, while simple keyword searches may not be nuanced enough. We present the first step towards the automated detection of this important, yet complex, team characteristic.\"]},\"279\":{\"h\":\"Circle Back Next Week: The Effect of Meeting-Free Weeks on Distributed Workers' Unstructured Time and Attention Negotiation\",\"t\":[\"Authors: Sharon Ferguson, Michael Massimi\",\"Link: http://arxiv.org/abs/2404.00161v1\",\"Abstract: While distributed workers rely on scheduled meetings for coordination and collaboration, these meetings can also challenge their ability to focus. Protecting worker focus has been addressed from a technical perspective, but companies are now attempting organizational interventions, such as meeting-free weeks. Recognizing distributed collaboration as a sociotechnical challenge, we first present an interview study with distributed workers participating in meeting-free weeks at an enterprise software company. We identify three orientations workers exhibit during these weeks: Focus, Collaborative, and Time-Bound, each with varying levels and use of unstructured time. These different orientations result in challenges in attention negotiation, which may be suited for technical interventions. This motivated a follow-up study investigating attention negotiation and the compensating mechanisms workers developed during meeting-free weeks. Our framework identified tensions between the attention-getting and attention-delegation strategies. We extend past work to show how workers adapt their virtual collaboration mechanisms in response to organizational interventions\"]},\"280\":{\"h\":\"Give Text A Chance: Advocating for Equal Consideration for Language and Visualization\",\"t\":[\"Authors: Chase Stokes, Marti A. Hearst\",\"Link: http://arxiv.org/abs/2404.00131v1\",\"Abstract: Visualization research tends to de-emphasize consideration of the textual context in which its images are placed. We argue that visualization research should consider textual representations as a primary alternative to visual options when assessing designs, and when assessing designs, equal attention should be given to the construction of the language as to the visualizations. We also call for a consideration of readability when integrating visualizations with written text. In highlighting these points, visualization research would be elevated in efficacy and demonstrate thorough accounting for viewers' needs and responses.\"]},\"281\":{\"h\":\"Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids\",\"t\":[\"Authors: Daniel B. Hier, Tayo Obafemi-Ajayi, Gayla R. Olbricht, Devin M. Burns, Sasha Petrenko, Donald C. Wunsch II\",\"Link: http://arxiv.org/abs/2403.20246v1\",\"Abstract: Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability. When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot. A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction. This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots. This method connects the low-dimension space to the original high-dimensional space. We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots.\"]},\"282\":{\"h\":\"Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities\",\"t\":[\"Authors: Silvia García-Méndez, Francisco de Arriba-Pérez, Francisco J. González-Castaño, José A. Regueiro-Janeiro, Felipe Gil-Castiñeira\",\"Link: http://arxiv.org/abs/2404.01327v1\",\"Abstract: Current language processing technologies allow the creation of conversational chatbot platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER chatbot, designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user's mood. Its novelty lies in the concept of \\\"intelligent radio\\\", according to which, instead of simplifying a digital information system to make it accessible to the elderly, a traditional channel they find familiar -- background news -- is augmented with interactions via voice dialogues. We make it possible by combining Artificial Intelligence Modelling Language, automatic Natural Language Generation and Sentiment Analysis. The system allows accessing digital content of interest by combining words extracted from user answers to chatbot questions with keywords extracted from the news items. This approach permits defining metrics of the abstraction capabilities of the users depending on a spatial representation of the word space. To prove the suitability of the proposed solution we present results of real experiments conducted with elderly people that provided valuable insights. Our approach was considered satisfactory during the tests and improved the information search capabilities of the participants.\"]},\"283\":{\"h\":\"ITCMA: A Generative Agent Based on a Computational Consciousness Structure\",\"t\":[\"Authors: Hanzhong Zhang, Jibin Yin, Haoyang Wang, Ziwei Xiang\",\"Link: http://arxiv.org/abs/2403.20097v1\",\"Abstract: Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.\"]},\"284\":{\"h\":\"MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm System\",\"t\":[\"Authors: Maha Nawaz, Abdul Basit, Muhammad Shafique\",\"Link: http://arxiv.org/abs/2403.19992v1\",\"Abstract: Currently, people with disability or difficulty to move their arms (referred to as \\\"patients\\\") have very limited technological solutions to efficiently address their physiological limitations. It is mainly due to two reasons: (1) the non-invasive solutions like mind-controlled prosthetic devices are typically very costly and require expensive maintenance; and (2) other solutions require costly invasive brain surgery, which is high risk to perform, expensive, and difficult to maintain. Therefore, current technological solutions are not accessible for all patients with different financial backgrounds. Toward this, we propose a low-cost technological solution called MindArm, a mechanized intelligent non-invasive neuro-driven prosthetic arm system. Our MindArm system employs a deep neural network (DNN) engine to translate brain signals into the intended prosthetic arm motion, thereby helping patients to perform many activities despite their physiological limitations. Here, our MindArm system utilizes widely accessible and low-cost surface electroencephalogram (EEG) electrodes coupled with an Open Brain Computer Interface and UDP networking for acquiring brain signals and transmitting them to the compute module for signal processing. In the compute module, we run a trained DNN model to interpret normalized micro-voltage of the brain signals, and then translate them into a prosthetic arm action via serial communication seamlessly. The experimental results on a fully working prototype demonstrate that, from the three defined actions, our MindArm system achieves positive success rates, i.e., 91% for idle/stationary, 85% for shake hand, and 84% for pick-up cup. This demonstrates that our MindArm provides a novel approach for an alternate low-cost mind-controlled prosthetic devices for all patients.\"]},\"285\":{\"h\":\"2024-03-28\"},\"286\":{\"h\":\"\\\"I'm categorizing LLM as a productivity tool\\\": Examining ethics of LLM use in HCI research practices\",\"t\":[\"Authors: Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen\",\"Link: http://arxiv.org/abs/2403.19876v1\",\"Abstract: Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey conducted with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.\"]},\"287\":{\"h\":\"Creating Aesthetic Sonifications on the Web with SIREN\",\"t\":[\"Authors: Tristan Peng, Hongchan Choi, Jonathan Berger\",\"Link: http://arxiv.org/abs/2403.19763v1\",\"Abstract: SIREN is a flexible, extensible, and customizable web-based general-purpose interface for auditory data display (sonification). Designed as a digital audio workstation for sonification, synthesizers written in JavaScript using the Web Audio API facilitate intuitive mapping of data to auditory parameters for a wide range of purposes. This paper explores the breadth of sound synthesis techniques supported by SIREN, and details the structure and definition of a SIREN synthesizer module. The paper proposes further development that will increase SIREN's utility.\"]},\"288\":{\"h\":\"Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies\",\"t\":[\"Authors: Benjamin Kraske, Zakariya Laouar, Zachary Sunberg\",\"Link: http://arxiv.org/abs/2403.19760v1\",\"Abstract: As humans come to rely on autonomous systems more, ensuring the transparency of such systems is important to their continued adoption. Explainable Artificial Intelligence (XAI) aims to reduce confusion and foster trust in systems by providing explanations of agent behavior. Partially observable Markov decision processes (POMDPs) provide a flexible framework capable of reasoning over transition and state uncertainty, while also being amenable to explanation. This work investigates the use of user-provided counterfactuals to generate contrastive explanations of POMDP policies. Feature expectations are used as a means of contrasting the performance of these policies. We demonstrate our approach in a Search and Rescue (SAR) setting. We analyze and discuss the associated challenges through two case studies.\"]},\"289\":{\"h\":\"Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models\",\"t\":[\"Authors: Ole Hall, Anil Yaman\",\"Link: http://arxiv.org/abs/2403.19620v1\",\"Abstract: Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the images through local search based on an aesthetic measure. We evaluate the effectiveness of this approach by comparing the results produced by the automatic and collaborative interactive evolution. The results show that the proposed approach can generate highly attractive art images when the evolution is guided by collaborative human feedback.\"]},\"290\":{\"h\":\"Exploring Communication Dynamics: Eye-tracking Analysis in Pair Programming of Computer Science Education\",\"t\":[\"Authors: Wunmin Jang, Hong Gao, Tilman Michaeli, Enkelejda Kasneci\",\"Link: http://arxiv.org/abs/2403.19560v1\",\"Abstract: Pair programming is widely recognized as an effective educational tool in computer science that promotes collaborative learning and mirrors real-world work dynamics. However, communication breakdowns within pairs significantly challenge this learning process. In this study, we use eye-tracking data recorded during pair programming sessions to study communication dynamics between various pair programming roles across different student, expert, and mixed group cohorts containing 19 participants. By combining eye-tracking data analysis with focus group interviews and questionnaires, we provide insights into communication's multifaceted nature in pair programming. Our findings highlight distinct eye-tracking patterns indicating changes in communication skills across group compositions, with participants prioritizing code exploration over communication, especially during challenging tasks. Further, students showed a preference for pairing with experts, emphasizing the importance of understanding group formation in pair programming scenarios. These insights emphasize the importance of understanding group dynamics and enhancing communication skills through pair programming for successful outcomes in computer science education.\"]},\"291\":{\"h\":\"LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae\",\"t\":[\"Authors: Celia Chen, Alex Leitch\",\"Link: http://arxiv.org/abs/2403.19506v1\",\"Abstract: This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.\"]},\"292\":{\"h\":\"A theoretical framework for the design and analysis of computational thinking problems in education\",\"t\":[\"Authors: Giorgia Adorni, Alberto Piatti, Engin Bumbacher, Lucio Negrini, Francesco Mondada, Dorit Assaf, Francesca Mangili, Luca Gambardella\",\"Link: http://arxiv.org/abs/2403.19475v1\",\"Abstract: The field of computational thinking education has grown in recent years as researchers and educators have sought to develop and assess students' computational thinking abilities. While much of the research in this area has focused on defining computational thinking, the competencies it involves and how to assess them in teaching and learning contexts, this work takes a different approach. We provide a more situated perspective on computational thinking, focusing on the types of problems that require computational thinking skills to be solved and the features that support these processes. We develop a framework for analysing existing computational thinking problems in an educational context. We conduct a comprehensive literature review to identify prototypical activities from areas where computational thinking is typically pursued in education. We identify the main components and characteristics of these activities, along with their influence on activating computational thinking competencies. The framework provides a catalogue of computational thinking skills that can be used to understand the relationship between problem features and competencies activated. This study contributes to the field of computational thinking education by offering a tool for evaluating and revising existing problems to activate specific skills and for assisting in designing new problems that target the development of particular competencies. The results of this study may be of interest to researchers and educators working in computational thinking education.\"]},\"293\":{\"h\":\"\\\"At the end of the day, I am accountable\\\": Gig Workers' Self-Tracking for Multi-Dimensional Accountability Management\",\"t\":[\"Authors: Rie Helene, Hernandez, Qiurong Song, Yubo Kou, Xinning Gui\",\"Link: http://arxiv.org/abs/2403.19436v1\",\"Abstract: Tracking is inherent in and central to the gig economy. Platforms track gig workers' performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics -- with some studies briefly acknowledging the phenomenon of self-tracking among workers -- there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-tracking to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers' self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers' multi-dimensional self-tracking.\"]},\"294\":{\"h\":\"An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations\",\"t\":[\"Authors: Jonathan Erskine, Matt Clifford, Alexander Hepburn, Raúl Santos-Rodríguez\",\"Link: http://arxiv.org/abs/2403.19339v1\",\"Abstract: Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.\"]},\"295\":{\"h\":\"CogniDot: Vasoactivity-based Cognitive Load Monitoring with a Miniature On-skin Sensor\",\"t\":[\"Authors: Hongbo Lan, Yanrong Li, Shixuan Li, Xin Yi, Tengxiang Zhang\",\"Link: http://arxiv.org/abs/2403.19206v1\",\"Abstract: Vascular activities offer valuable signatures for psychological monitoring applications. We present CogniDot, an affordable, miniature skin sensor placed on the temporal area on the head that senses cognitive loads with a single-pixel color sensor. With its energy-efficient design, bio-compatible adhesive, and compact size (22mm diameter, 8.5mm thickness), it is ideal for long-term monitoring of mind status. We showed in detail the hardware design of our sensor. The user study results with 12 participants show that CogniDot can accurately differentiate between three levels of cognitive loads with a within-user accuracy of 97%. We also discuss its potential for broader applications.\"]},\"296\":{\"h\":\"Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration\",\"t\":[\"Authors: Louie Søs Meyer, Johanne Engel Aaen, Anitamalina Regitse Tranberg, Peter Kun, Matthias Freiberger, Sebastian Risi, Anders Sundnes Løvlie\",\"Link: http://arxiv.org/abs/2403.19174v1\",\"Abstract: This Research through Design paper explores how object detection may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art. We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum's digital collection of paintings by browsing through objects detected in the images, as a novel form of open-ended exploration. We provide three contributions. First, we show how an object detection pipeline can be integrated into a design process for visual exploration. Second, we present the design and development of an app that enables exploration of an art museum's collection. Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate object detection techniques into the digitalization of museums.\"]},\"297\":{\"h\":\"Exploring Holistic HMI Design for Automated Vehicles: Insights from a Participatory Workshop to Bridge In-Vehicle and External Communication\",\"t\":[\"Authors: Haoyu Dong, Tram Thi Minh Tran, Rutger Verstegen, Silvia Cazacu, Ruolin Gao, Marius Hoggenmüller, Debargha Dey, Mervyn Franssen, Markus Sasalovici, Pavlo Bazilinskyy, Marieke Martens\",\"Link: http://arxiv.org/abs/2403.19153v1\",\"Abstract: Human-Machine Interfaces (HMIs) for automated vehicles (AVs) are typically divided into two categories: internal HMIs for interactions within the vehicle, and external HMIs for communication with other road users. In this work, we examine the prospects of bridging these two seemingly distinct domains. Through a participatory workshop with automotive user interface researchers and practitioners, we facilitated a critical exploration of holistic HMI design by having workshop participants collaboratively develop interaction scenarios involving AVs, in-vehicle users, and external road users. The discussion offers insights into the escalation of interface elements as an HMI design strategy, the direct interactions between different users, and an expanded understanding of holistic HMI design. This work reflects a collaborative effort to understand the practical aspects of this holistic design approach, offering new perspectives and encouraging further investigation into this underexplored aspect of automotive user interfaces.\"]},\"298\":{\"h\":\"Real-time accident detection and physiological signal monitoring to enhance motorbike safety and emergency response\",\"t\":[\"Authors: S. M. Kayser Mehbub Siam, Khadiza Islam Sumaiya, Md Rakib Al-Amin, Tamim Hasan Turjo, Ahsanul Islam, A. H. M. A. Rahim, Md Rakibul Hasan\",\"Link: http://arxiv.org/abs/2403.19085v1\",\"Abstract: Rapid urbanization and improved living standards have led to a substantial increase in the number of vehicles on the road, consequently resulting in a rise in the frequency of accidents. Among these accidents, motorbike accidents pose a particularly high risk, often resulting in serious injuries or deaths. A significant number of these fatalities occur due to delayed or inadequate medical attention. To this end, we propose a novel automatic detection and notification system specifically designed for motorbike accidents. The proposed system comprises two key components: a detection system and a physiological signal monitoring system. The detection system is integrated into the helmet and consists of a microcontroller, accelerometer, GPS, GSM, and Wi-Fi modules. The physio-monitoring system incorporates a sensor for monitoring pulse rate and SpO${2}$ saturation. All collected data are presented on an LCD display and wirelessly transmitted to the detection system through the microcontroller of the physiological signal monitoring system. If the accelerometer readings consistently deviate from the specified threshold decided through extensive experimentation, the system identifies the event as an accident and transmits the victim's information -- including the GPS location, pulse rate, and SpO${2}$ saturation rate -- to the designated emergency contacts. Preliminary results demonstrate the efficacy of the proposed system in accurately detecting motorbike accidents and promptly alerting emergency contacts. We firmly believe that the proposed system has the potential to significantly mitigate the risks associated with motorbike accidents and save lives.\"]},\"299\":{\"h\":\"2024-03-27\"},\"300\":{\"h\":\"Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers\",\"t\":[\"Authors: Yuning Wu, Jiaying Wei, Jean Oh, Daniel Cardoso Llach\",\"Link: http://arxiv.org/abs/2403.19060v1\",\"Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover\\\" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce.\"]},\"301\":{\"h\":\"Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE\",\"t\":[\"Authors: Pavlin G. Poličar, Blaž Zupan\",\"Link: http://arxiv.org/abs/2403.19040v1\",\"Abstract: Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows connecting two adjacent time series points to point in the same direction, while the Edge Length Loss (ELL) penalizes arrows - which effectively represent time gaps in the visualized embedding - based on their length. Both loss terms are differentiable and can be easily incorporated into existing dimensionality reduction techniques. By promoting local directionality of the directed edges, our procedure produces more temporally meaningful and less cluttered visualizations. We demonstrate the effectiveness of our approach on a toy dataset and two real-world datasets.\"]},\"302\":{\"h\":\"Women are less comfortable expressing opinions online than men and report heightened fears for safety: Surveying gender differences in experiences of online harms\",\"t\":[\"Authors: Francesca Stevens, Florence E. Enock, Tvesha Sippy, Jonathan Bright, Miranda Cross, Pica Johansson, Judy Wajcman, Helen Z. Margetts\",\"Link: http://arxiv.org/abs/2403.19037v1\",\"Abstract: Online harms, such as hate speech, trolling and self-harm promotion, continue to be widespread. While some work suggests women are disproportionately affected, other studies find mixed evidence for gender differences in experiences with content of this kind. Using a nationally representative survey of UK adults (N=1992), we examine exposure to a variety of harms, fears surrounding being targeted, the psychological impact of online experiences, the use of safety tools to protect against harm, and comfort with various forms of online participation across men and women. We find that while men and women see harmful content online to a roughly similar extent, women are more at risk than men of being targeted by harms including online misogyny, cyberstalking and cyberflashing. Women are significantly more fearful of being targeted by harms overall, and report greater negative psychological impact as a result of particular experiences. Perhaps in an attempt to mitigate risk, women report higher use of a range of safety tools and less comfort with several forms of online participation, with just 23% of women comfortable expressing political views online compared to 40% of men. We also find direct associations between fears surrounding harms and comfort with online behaviours. For example, fear of being trolled significantly decreases comfort expressing opinions, and fear of being targeted by misogyny significantly decreases comfort sharing photos. Our results are important because with much public discourse happening online, we must ensure all members of society feel safe and able to participate in online spaces.\"]},\"303\":{\"h\":\"Should I Help a Delivery Robot? Cultivating Prosocial Norms through Observations\",\"t\":[\"Authors: Vivienne Bihe Chi, Shashank Mehrotra, Teruhisa Misu, Kumar Akash\",\"Link: http://arxiv.org/abs/2403.19027v1\",\"Abstract: We propose leveraging prosocial observations to cultivate new social norms to encourage prosocial behaviors toward delivery robots. With an online experiment, we quantitatively assess updates in norm beliefs regarding human-robot prosocial behaviors through observational learning. Results demonstrate the initially perceived normativity of helping robots is influenced by familiarity with delivery robots and perceptions of robots' social intelligence. Observing human-robot prosocial interactions notably shifts peoples' normative beliefs about prosocial actions; thereby changing their perceived obligations to offer help to delivery robots. Additionally, we found that observing robots offering help to humans, rather than receiving help, more significantly increased participants' feelings of obligation to help robots. Our findings provide insights into prosocial design for future mobility systems. Improved familiarity with robot capabilities and portraying them as desirable social partners can help foster wider acceptance. Furthermore, robots need to be designed to exhibit higher levels of interactivity and reciprocal capabilities for prosocial behavior.\"]},\"304\":{\"h\":\"The Correlations of Scene Complexity, Workload, Presence, and Cybersickness in a Task-Based VR Game\",\"t\":[\"Authors: Mohammadamin Sanaei, Stephen B. Gilbert, Nikoo Javadpour, Hila Sabouni, Michael C. Dorneich, Jonathan W. Kelly\",\"Link: http://arxiv.org/abs/2403.19019v1\",\"Abstract: This investigation examined the relationships among scene complexity, workload, presence, and cybersickness in virtual reality (VR) environments. Numerous factors can influence the overall VR experience, and existing research on this matter is not yet conclusive, warranting further investigation. In this between-subjects experimental setup, 44 participants engaged in the Pendulum Chair game, with half exposed to a simple scene with lower optic flow and lower familiarity, and the remaining half to a complex scene characterized by higher optic flow and greater familiarity. The study measured the dependent variables workload, presence, and cybersickness and analyzed their correlations. Equivalence testing was also used to compare the simple and complex environments. Results revealed that despite the visible differences between the environments, within the 10% boundaries of the maximum possible value for workload and presence, and 13.6% of the maximum SSQ value, a statistically significant equivalence was observed between the simple and complex scenes. Additionally, a moderate, negative correlation emerged between workload and SSQ scores. The findings suggest two key points: (1) the nature of the task can mitigate the impact of scene complexity factors such as optic flow and familiarity, and (2) the correlation between workload and cybersickness may vary, showing either a positive or negative relationship.\"]},\"305\":{\"h\":\"Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine Learning\",\"t\":[\"Authors: Darlene Barker, Haim Levkowitz\",\"Link: http://arxiv.org/abs/2403.19014v1\",\"Abstract: In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino\\\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for future advancements in virtual touch technology.\"]},\"306\":{\"h\":\"SolderlessPCB: Reusing Electronic Components in PCB Prototyping through Detachable 3D Printed Housings\",\"t\":[\"Authors: Zeyu Yan, Jiasheng Li, Zining Zhang, Huaishu Peng\",\"Link: http://arxiv.org/abs/2403.18797v1\",\"Abstract: The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste. This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components. Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering. We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability. To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes. We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission. The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges.\"]},\"307\":{\"h\":\"Teaching Introductory HRI: UChicago Course \\\"Human-Robot Interaction: Research and Practice\\\"\",\"t\":[\"Authors: Sarah Sebo\",\"Link: http://arxiv.org/abs/2403.18692v1\",\"Abstract: In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago. Since 2020, I have taught and refined this course each academic year. Human-Robot Interaction: Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI. Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications. Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot. In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course.\"]},\"308\":{\"h\":\"An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project\",\"t\":[\"Authors: Ben Arie Tanay, Lexy Arinze, Siddhant S. Joshi, Kirsten A. Davis, James C. Davis\",\"Link: http://arxiv.org/abs/2403.18679v1\",\"Abstract: Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are influencing software engineering practice. Software engineering educators must teach future software engineers how to use such tools well. As of yet, there have been few studies that report on the use of LLMs in the classroom. It is, therefore, important to evaluate students' perception of LLMs and possible ways of adapting the computing curriculum to these shifting paradigms. Purpose: The purpose of this study is to explore computing students' experiences and approaches to using LLMs during a semester-long software engineering project. Design/Method: We collected data from a senior-level software engineering course at Purdue University. This course uses a project-based learning (PBL) design. The students used LLMs such as ChatGPT and Copilot in their projects. A sample of these student teams were interviewed to understand (1) how they used LLMs in their projects; and (2) whether and how their perspectives on LLMs changed over the course of the semester. We analyzed the data to identify themes related to students' usage patterns and learning outcomes. Results/Discussion: When computing students utilize LLMs within a project, their use cases cover both technical and professional applications. In addition, these students perceive LLMs to be efficient tools in obtaining information and completion of tasks. However, there were concerns about the responsible use of LLMs without being detrimental to their own learning outcomes. Based on our findings, we recommend future research to investigate the usage of LLM's in lower-level computer engineering courses to understand whether and how LLMs can be integrated as a learning aid without hurting the learning outcomes.\"]},\"309\":{\"h\":\"Aiming for Relevance\",\"t\":[\"Authors: Bar Eini Porat, Danny Eytan, Uri Shalit\",\"Link: http://arxiv.org/abs/2403.18668v1\",\"Abstract: Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care. 10 pages, 9 figures.\"]},\"310\":{\"h\":\"LLM\"},\"311\":{\"h\":\"2024-04-11\"},\"312\":{\"h\":\"Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation\",\"t\":[\"Authors: Jinkyung Park, Pamela Wisniewski, Vivek Singh\",\"Link: http://arxiv.org/abs/2404.07926v1\",\"Abstract: In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized. Therefore, we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI community's methods for working with data.\"]},\"313\":{\"h\":\"AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs\",\"t\":[\"Authors: Zeyi Liao, Huan Sun\",\"Link: http://arxiv.org/abs/2404.07921v1\",\"Abstract: As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\\\\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.\"]},\"314\":{\"h\":\"ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs\",\"t\":[\"Authors: Lei Sun, Zhengwei Tao, Youdi Li, Hiroshi Arakawa\",\"Link: http://arxiv.org/abs/2404.07677v1\",\"Abstract: The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.\"]},\"315\":{\"h\":\"Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain\",\"t\":[\"Authors: Iker García-Ferrero, Rodrigo Agerri, Aitziber Atutxa Salazar, Elena Cabrio, Iker de la Iglesia, Alberto Lavelli, Bernardo Magnini, Benjamin Molinet, Johana Ramirez-Romero, German Rigau, Jose Maria Villa-Gonzalez, Serena Villata, Andrea Zaninello\",\"Link: http://arxiv.org/abs/2404.07613v1\",\"Abstract: Research on language technology for the development of medical applications is currently a hot topic in Natural Language Understanding and Generation. Thus, a number of large language models (LLMs) have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction. While these LLMs display competitive performance on automated medical texts benchmarks, they have been pre-trained and evaluated with a focus on a single language (English mostly). This is particularly true of text-to-text models, which typically require large amounts of domain-specific pre-training data, often not easily accessible for many languages. In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish. This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we present two new evaluation benchmarks for all four languages with the aim of facilitating multilingual research in this domain. A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian benchmarks, while being competitive with current state-of-the-art LLMs in English.\"]},\"316\":{\"h\":\"UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs\",\"t\":[\"Authors: Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, Maosong Sun\",\"Link: http://arxiv.org/abs/2404.07584v1\",\"Abstract: Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements. The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher's workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration. UltraEval is now available for researchers publicly~\\\\footnote{Website is at \\\\url{https://github.com/OpenBMB/UltraEval}}.\"]},\"317\":{\"h\":\"Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning\",\"t\":[\"Authors: Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan\",\"Link: http://arxiv.org/abs/2404.07546v1\",\"Abstract: In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format which helps LLMs to respond in desired label words. We then demonstrate this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL and find that retrieving the most semantically similar examples notably boosts model's discriminative capability.\"]},\"318\":{\"h\":\"WESE: Weak Exploration to Strong Exploitation for LLM Agents\",\"t\":[\"Authors: Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen\",\"Link: http://arxiv.org/abs/2404.07456v1\",\"Abstract: Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.\"]},\"319\":{\"h\":\"Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs\",\"t\":[\"Authors: Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, Tsung-Yu Lin\",\"Link: http://arxiv.org/abs/2404.07449v1\",\"Abstract: Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.\"]},\"320\":{\"h\":\"2024-04-10\"},\"321\":{\"h\":\"BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks\",\"t\":[\"Authors: Ruijia Cheng, Titus Barik, Alan Leung, Fred Hohman, Jeffrey Nichols\",\"Link: http://arxiv.org/abs/2404.07387v1\",\"Abstract: Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs). However, they encounter difficulties in understanding and working with code produced by LLMs. To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user prompts and code generation. We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code. Through 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas. We discuss the implications of our findings for UI-centric interactive paradigm in code generation LLMs.\"]},\"322\":{\"h\":\"Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving\",\"t\":[\"Authors: Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang\",\"Link: http://arxiv.org/abs/2404.07382v1\",\"Abstract: Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.\"]},\"323\":{\"h\":\"LLMs in Biomedicine: A study on clinical Named Entity Recognition\",\"t\":[\"Authors: Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang\",\"Link: http://arxiv.org/abs/2404.07376v1\",\"Abstract: Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the medical domain by exploring strategies to enhance their performance for the Named-Entity Recognition (NER) task. Specifically, our study reveals the importance of meticulously designed prompts in biomedicine. Strategic selection of in-context examples yields a notable improvement, showcasing ~15-20% increase in F1 score across all benchmark datasets for few-shot clinical NER. Additionally, our findings suggest that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER. Leveraging a medical knowledge base, our proposed method inspired by Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for zero-shot clinical NER. We will release the code upon publication.\"]},\"324\":{\"h\":\"From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications\",\"t\":[\"Authors: Yongqiang Ma, Lizhi Qing, Jiawei Liu, Yangyang Kang, Yue Zhang, Wei Lu, Xiaozhong Liu, Qikai Cheng\",\"Link: http://arxiv.org/abs/2404.07108v2\",\"Abstract: Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.\"]},\"325\":{\"h\":\"MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models\",\"t\":[\"Authors: Rahul Mehta, Andrew Hoblitzell, Jack O'Keefe, Hyeju Jang, Vasudeva Varma\",\"Link: http://arxiv.org/abs/2404.06948v2\",\"Abstract: Hallucinations in large language models (LLMs) have recently become a significant problem. A recent effort in this direction is a shared task at Semeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. This paper describes our winning solution ranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks respectively. We propose a meta-regressor framework of LLMs for model evaluation and integration that achieves the highest scores on the leaderboard. We also experiment with various transformer-based models and black box methods like ChatGPT, Vectara, and others. In addition, we perform an error analysis comparing GPT4 against our best model which shows the limitations of the former.\"]},\"326\":{\"h\":\"GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications\",\"t\":[\"Authors: Shishir G. Patil, Tianjun Zhang, Vivian Fang, Noppapon C., Roy Huang, Aaron Hao, Martin Casado, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica\",\"Link: http://arxiv.org/abs/2404.06921v1\",\"Abstract: Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. We argue that in many cases, \\\"post-facto validation\\\" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned \\\"pre-facto validation\\\" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.\"]},\"327\":{\"h\":\"Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora?\",\"t\":[\"Authors: Miriam Anschütz, Edoardo Mosca, Georg Groh\",\"Link: http://arxiv.org/abs/2404.06838v1\",\"Abstract: Text simplification seeks to improve readability while retaining the original content and meaning. Our study investigates whether pre-trained classifiers also maintain such coherence by comparing their predictions on both original and simplified inputs. We conduct experiments using 11 pre-trained models, including BERT and OpenAI's GPT 3.5, across six datasets spanning three languages. Additionally, we conduct a detailed analysis of the correlation between prediction change rates and simplification types/strengths. Our findings reveal alarming inconsistencies across all languages and models. If not promptly addressed, simplified inputs can be easily exploited to craft zero-iteration model-agnostic adversarial attacks with success rates of up to 50%\"]},\"328\":{\"h\":\"Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge\",\"t\":[\"Authors: Li Zhou, Taelin Karidi, Nicolas Garneau, Yong Cao, Wanlong Liu, Wenyu Chen, Daniel Hershcovich\",\"Link: http://arxiv.org/abs/2404.06833v1\",\"Abstract: Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains.\"]},\"329\":{\"h\":\"Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation\",\"t\":[\"Authors: Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun\",\"Link: http://arxiv.org/abs/2404.06809v1\",\"Abstract: The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models' capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance. Moreover, our model supports customized credibility, offering a wide range of potential applications.\"]},\"330\":{\"h\":\"MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education\",\"t\":[\"Authors: Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, Ziyu Yao\",\"Link: http://arxiv.org/abs/2404.06711v1\",\"Abstract: Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, large language models (LLMs) have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of LLMs, in this work, we present MATHVC, the very first LLM-powered virtual classroom containing multiple LLM-simulated student characters, with whom a human student can practice their MM skill. To encourage each LLM character's behaviors to be aligned with their specified math-relevant properties (termed \\\"characteristics alignment\\\") and the overall conversational procedure to be close to an authentic student MM discussion (termed \\\"conversational procedural alignment\\\"), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our simulation approach and showed the promise for MATHVC to benefit real-life students in the future.\"]},\"331\":{\"h\":\"CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge\",\"t\":[\"Authors: Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, Yejin Choi\",\"Link: http://arxiv.org/abs/2404.06664v1\",\"Abstract: Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks. Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences. Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency.\"]},\"332\":{\"h\":\"2024-04-09\"},\"333\":{\"h\":\"Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?\",\"t\":[\"Authors: Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban\",\"Link: http://arxiv.org/abs/2404.06644v1\",\"Abstract: Evaluating Large Language Models (LLMs) is challenging due to their generative nature, necessitating precise evaluation methodologies. Additionally, non-English LLM evaluation lags behind English, resulting in the absence or weakness of LLMs for many languages. In response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,192 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages. The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of LLMs that support the Persian language. Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc., aimed at assessing different facets of LLMs such as language comprehension, reasoning, and information retrieval across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort. Previous works lacked an evaluation framework that combined all of these features into a single comprehensive benchmark. Furthermore, we evaluate a wide range of existing LLMs that support the Persian language, with statistical analyses and interpretations of their outputs.\"]},\"334\":{\"h\":\"Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs\",\"t\":[\"Authors: Bibek Upadhayay, Vahid Behzadan\",\"Link: http://arxiv.org/abs/2404.07242v1\",\"Abstract: Large Language Models (LLMs) are increasingly being developed and applied, but their widespread use faces challenges. These include aligning LLMs' responses with human values to prevent harmful outputs, which is addressed through safety training methods. Even so, bad actors and malicious users have succeeded in attempts to manipulate the LLMs to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights. Another challenge is the multilingual capabilities of LLMs, which enable the model to understand and respond in multiple languages. Consequently, attackers exploit the unbalanced pre-training datasets of LLMs in different languages and the comparatively lower model performance in low-resource languages than high-resource ones. As a result, attackers use a low-resource languages to intentionally manipulate the model to create harmful responses. Many of the similar attack vectors have been patched by model providers, making the LLMs more robust against language-based manipulation. In this paper, we introduce a new black-box attack vector called the \\\\emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses. Our experiments with five different models, namely Google's Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse.\"]},\"335\":{\"h\":\"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\"t\":[\"Authors: Nathan Brake, Thomas Schaaf\",\"Link: http://arxiv.org/abs/2404.06503v1\",\"Abstract: Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"]},\"336\":{\"h\":\"Pitfalls of Conversational LLMs on News Debiasing\",\"t\":[\"Authors: Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek\",\"Link: http://arxiv.org/abs/2404.06488v1\",\"Abstract: This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task. We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.\"]},\"337\":{\"h\":\"Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks\",\"t\":[\"Authors: Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen\",\"Link: http://arxiv.org/abs/2404.06480v1\",\"Abstract: Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.\"]},\"338\":{\"h\":\"AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents\",\"t\":[\"Authors: Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence\",\"Link: http://arxiv.org/abs/2404.06411v1\",\"Abstract: The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.\"]},\"339\":{\"h\":\"Model Generation from Requirements with LLMs: an Exploratory Study\",\"t\":[\"Authors: Alessio Ferrari, Sallam Abualhaija, Chetan Arora\",\"Link: http://arxiv.org/abs/2404.06371v1\",\"Abstract: Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.\"]},\"340\":{\"h\":\"LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements\",\"t\":[\"Authors: Victoria Basmov, Yoav Goldberg, Reut Tsarfaty\",\"Link: http://arxiv.org/abs/2404.06283v1\",\"Abstract: The task of reading comprehension (RC), often implemented as context-based question answering (QA), provides a primary means to assess language models' natural language understanding (NLU) capabilities. Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive. If the context aligns with the LLMs' internal knowledge, it is hard to discern whether the models' answers stem from context comprehension or from LLMs' internal information. Conversely, using data that conflicts with the models' knowledge creates erroneous trends which distort the results. To address this issue, we suggest to use RC on imaginary data, based on fictitious facts and entities. This task is entirely independent of the models' world knowledge, enabling us to evaluate LLMs' linguistic abilities without the interference of parametric knowledge. Testing ChatGPT, GPT-4, LLaMA 2 and Mixtral on such imaginary data, we uncover a class of linguistic phenomena posing a challenge to current LLMs, involving thinking in terms of alternative, hypothetical scenarios. While all the models handle simple affirmative and negative contexts with high accuracy, they are much more prone to error when dealing with modal and conditional contexts. Crucially, these phenomena also trigger the LLMs' vulnerability to knowledge-conflicts again. In particular, while some models prove virtually unaffected by knowledge conflicts in affirmative and negative contexts, when faced with more semantically involved modal and conditional environments, they often fail to separate the text from their internal knowledge.\"]},\"341\":{\"h\":\"A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs\",\"t\":[\"Authors: Toshihiro Kamiya\",\"Link: http://arxiv.org/abs/2404.06082v1\",\"Abstract: Although the context length limitation of large language models (LLMs) has been mitigated, it still hinders their application to software development tasks. This study proposes a method incorporating execution traces into RAG for inquiries about source code. Small-scale experiments confirm a tendency for the method to contribute to improving LLM response quality.\"]},\"342\":{\"h\":\"On Evaluating the Efficiency of Source Code Generated by LLMs\",\"t\":[\"Authors: Changan Niu, Ting Zhang, Chuanyi Li, Bin Luo, Vincent Ng\",\"Link: http://arxiv.org/abs/2404.06041v1\",\"Abstract: Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.\"]},\"343\":{\"h\":\"PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs\",\"t\":[\"Authors: Alessandro Berti\",\"Link: http://arxiv.org/abs/2404.06035v1\",\"Abstract: pm4py is a process mining library for Python implementing several process mining (PM) artifacts and algorithms. It also offers methods to integrate PM with large language models (LLMs). This paper examines how the current paradigms of PM on LLM are implemented in pm4py, identifying challenges such as privacy, hallucinations, and the context window limit.\"]},\"344\":{\"h\":\"AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts\",\"t\":[\"Authors: Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien\",\"Link: http://arxiv.org/abs/2404.05993v1\",\"Abstract: As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment\"]},\"345\":{\"h\":\"VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?\",\"t\":[\"Authors: Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue\",\"Link: http://arxiv.org/abs/2404.05955v1\",\"Abstract: Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding. In this paper, we introduce \\\\bench{}, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. \\\\bench{} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on \\\\bench{}, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs. We believe \\\\bench{} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications.\"]},\"346\":{\"h\":\"2024-04-08\"},\"347\":{\"h\":\"LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding\",\"t\":[\"Authors: Mingrui Wu, Sheng Cao\",\"Link: http://arxiv.org/abs/2404.05825v1\",\"Abstract: Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches. This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.\"]},\"348\":{\"h\":\"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs\",\"t\":[\"Authors: Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan\",\"Link: http://arxiv.org/abs/2404.05719v1\",\"Abstract: Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate \\\"any resolution\\\" on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens). Both sub-images are encoded separately before being sent to LLMs. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.\"]},\"349\":{\"h\":\"MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation\",\"t\":[\"Authors: Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang\",\"Link: http://arxiv.org/abs/2404.05674v1\",\"Abstract: In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.\"]},\"350\":{\"h\":\"The Fact Selection Problem in LLM-Based Program Repair\",\"t\":[\"Authors: Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, Sergey Mechtaev\",\"Link: http://arxiv.org/abs/2404.05520v2\",\"Abstract: Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.\"]},\"351\":{\"h\":\"PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM for Emotion-Cause Pair Extraction in Conversations\",\"t\":[\"Authors: Roman Kazakov, Kseniia Petukhova, Ekaterina Kochmar\",\"Link: http://arxiv.org/abs/2404.05502v1\",\"Abstract: In this paper, we present our submission to the SemEval-2023 Task~3 \\\"The Competition of Multimodal Emotion Cause Analysis in Conversations\\\", focusing on extracting emotion-cause pairs from dialogs. Specifically, our approach relies on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based neural network to detect causes. We score 2nd in the ranking for Subtask 1, demonstrating the effectiveness of our approach through one of the highest weighted-average proportional F1 scores recorded at 0.264.\"]},\"352\":{\"h\":\"PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of LLM-generated Text?\",\"t\":[\"Authors: Kseniia Petukhova, Roman Kazakov, Ekaterina Kochmar\",\"Link: http://arxiv.org/abs/2404.05483v1\",\"Abstract: In this paper, we present our submission to the SemEval-2024 Task 8 \\\"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection\\\", focusing on the detection of machine-generated texts (MGTs) in English. Specifically, our approach relies on combining embeddings from the RoBERTa-base with diversity features and uses a resampled training set. We score 12th from 124 in the ranking for Subtask A (monolingual track), and our results show that our approach is generalizable across unseen models and domains, achieving an accuracy of 0.91.\"]},\"353\":{\"h\":\"LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models\",\"t\":[\"Authors: Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu\",\"Link: http://arxiv.org/abs/2404.05221v1\",\"Abstract: Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.\"]},\"354\":{\"h\":\"Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor When Adopting LLMs in HCI Research\",\"t\":[\"Authors: Gionnieve Lim, Simon T. Perrault\",\"Link: http://arxiv.org/abs/2404.05213v1\",\"Abstract: There is increasing interest in the adoption of LLMs in HCI research. However, LLMs may often be regarded as a panacea because of their powerful capabilities with an accompanying oversight on whether they are suitable for their intended tasks. We contend that LLMs should be adopted in a critical manner following rigorous evaluation. Accordingly, we present the evaluation of an LLM in identifying logical fallacies that will form part of a digital misinformation intervention. By comparing to a labeled dataset, we found that GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes invalid or unidentified instances, an accuracy of 0.90. This gives us the confidence to proceed with the application of the LLM while keeping in mind the areas where it still falls short. The paper describes our evaluation approach, results and reflections on the use of the LLM for our intended task.\"]},\"355\":{\"h\":\"Progressive Alignment with VLM-LLM Feature to Augment Defect Classification for the ASE Dataset\",\"t\":[\"Authors: Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu\",\"Link: http://arxiv.org/abs/2404.05183v1\",\"Abstract: Traditional defect classification approaches are facing with two barriers. (1) Insufficient training data and unstable data quality. Collecting sufficient defective sample is expensive and time-costing, consequently leading to dataset variance. It introduces the difficulty on recognition and learning. (2) Over-dependence on visual modality. When the image pattern and texture is monotonic for all defect classes in a given dataset, the performance of conventional AOI system cannot be guaranteed. In scenarios where image quality is compromised due to mechanical failures or when defect information is inherently difficult to discern, the performance of deep models cannot be guaranteed. A main question is, \\\"how to solve those two problems when they occur at the same time?\\\" The feasible strategy is to explore another feature within dataset and combine an eminent vision-language model (VLM) and Large-Language model (LLM) with their astonishing zero-shot capability. In this work, we propose the special ASE dataset, including rich data description recorded on image, for defect classification, but the defect feature is uneasy to learn directly. Secondly, We present the prompting for VLM-LLM against defect classification with the proposed ASE dataset to activate extra-modality feature from images to enhance performance. Then, We design the novel progressive feature alignment (PFA) block to refine image-text feature to alleviate the difficulty of alignment under few-shot scenario. Finally, the proposed Cross-modality attention fusion (CMAF) module can effectively fuse different modality feature. Experiment results have demonstrated our method's effectiveness over several defect classification methods for the ASE dataset.\"]},\"356\":{\"h\":\"Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac Patients\",\"t\":[\"Authors: HyoJe Jung, Yunha Kim, Heejung Choi, Hyeram Seo, Minkyoung Kim, JiYe Han, Gaeun Kee, Seohyun Park, Soyoung Ko, Byeolhee Kim, Suyeon Kim, Tae Joon Jun, Young-Hak Kim\",\"Link: http://arxiv.org/abs/2404.05144v1\",\"Abstract: Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication. However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors. The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in healthcare. This study directly addresses the inefficiencies and inaccuracies in creating discharge notes manually, particularly for cardiac patients, by employing AI techniques, specifically large language model (LLM). Utilizing a substantial dataset from a cardiology center, encompassing wide-ranging medical records and physician assessments, our research evaluates the capability of LLM to enhance the documentation process. Among the various models assessed, Mistral-7B distinguished itself by accurately generating discharge notes that significantly improve both documentation efficiency and the continuity of care for patients. These notes underwent rigorous qualitative evaluation by medical expert, receiving high marks for their clinical relevance, completeness, readability, and contribution to informed decision-making and care planning. Coupled with quantitative analyses, these results confirm Mistral-7B's efficacy in distilling complex medical information into concise, coherent summaries. Overall, our findings illuminate the considerable promise of specialized LLM, such as Mistral-7B, in refining healthcare documentation workflows and advancing patient care. This study lays the groundwork for further integrating advanced AI technologies in healthcare, demonstrating their potential to revolutionize patient documentation and support better care outcomes.\"]},\"357\":{\"h\":\"2024-04-07\"},\"358\":{\"h\":\"Clinical Trials Protocol Authoring using LLMs\",\"t\":[\"Authors: Morteza Maleki\",\"Link: http://arxiv.org/abs/2404.05044v1\",\"Abstract: This report embarks on a mission to revolutionize clinical trial protocol development through the integration of advanced AI technologies. With a focus on leveraging the capabilities of generative AI, specifically GPT-4, this initiative aimed to streamline and enhance the efficiency and accuracy of clinical trial protocols. The methodology encompassed a detailed analysis and preparation of comprehensive drug and study level metadata, followed by the deployment of GPT-4 for automated protocol section generation. Results demonstrated a significant improvement in protocol authoring, highlighted by increases in efficiency, accuracy, and the customization of protocols to specific trial requirements. Challenges encountered during model selection and prompt engineering were systematically addressed, leading to refined methodologies that capitalized on the advanced text generation capabilities of GPT-4. This project not only showcases the practical applications and benefits of generative AI in clinical trial design but also sets a foundation for future innovations in the field.\"]},\"359\":{\"h\":\"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression\",\"t\":[\"Authors: Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian Floyd\",\"Link: http://arxiv.org/abs/2404.04997v1\",\"Abstract: The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.\"]},\"360\":{\"h\":\"Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis\",\"t\":[\"Authors: Chen Yang, Junjie Chen, Bin Lin, Jianyi Zhou, Ziqi Wang\",\"Link: http://arxiv.org/abs/2404.04966v1\",\"Abstract: Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage.\"]},\"361\":{\"h\":\"AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications\",\"t\":[\"Authors: Xin Pang, Zhucong Li, Jiaxiang Chen, Yuan Cheng, Yinghui Xu, Yuan Qi\",\"Link: http://arxiv.org/abs/2404.04902v1\",\"Abstract: We introduce AI2Apps, a Visual Integrated Development Environment (Visual IDE) with full-cycle capabilities that accelerates developers to build deployable LLM-based AI agent Applications. This Visual IDE prioritizes both the Integrity of its development tools and the Visuality of its components, ensuring a smooth and efficient building experience.On one hand, AI2Apps integrates a comprehensive development toolkit ranging from a prototyping canvas and AI-assisted code editor to agent debugger, management system, and deployment tools all within a web-based graphical user interface. On the other hand, AI2Apps visualizes reusable front-end and back-end code as intuitive drag-and-drop components. Furthermore, a plugin system named AI2Apps Extension (AAE) is designed for Extensibility, showcasing how a new plugin with 20 components enables web agent to mimic human-like browsing behavior. Our case study demonstrates substantial efficiency improvements, with AI2Apps reducing token consumption and API calls when debugging a specific sophisticated multimodal agent by approximately 90% and 80%, respectively. The AI2Apps, including an online demo, open-source code, and a screencast video, is now publicly accessible.\"]},\"362\":{\"h\":\"Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs\",\"t\":[\"Authors: Yiqun Duan, Qiang Zhang, Renjing Xu\",\"Link: http://arxiv.org/abs/2404.04869v1\",\"Abstract: The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a `pure-language' strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models.\"]},\"363\":{\"h\":\"Explaining EDA synthesis errors with LLMs\",\"t\":[\"Authors: Siyu Qiu, Benjamin Tan, Hammond Pearce\",\"Link: http://arxiv.org/abs/2404.07235v1\",\"Abstract: Training new engineers in digital design is a challenge, particularly when it comes to teaching the complex electronic design automation (EDA) tooling used in this domain. Learners will typically deploy designs in the Verilog and VHDL hardware description languages to Field Programmable Gate Arrays (FPGAs) from Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains (Quartus Prime and Vivado, respectively). These tools are complex and difficult to use -- yet, as they are the tools used in industry, they are an essential first step in this space. In this work, we examine how recent advances in artificial intelligence may be leveraged to address aspects of this challenge. Specifically, we investigate if Large Language Models (LLMs), which have demonstrated text comprehension and question-answering capabilities, can be used to generate novice-friendly explanations of compile-time synthesis error messages from Quartus Prime and Vivado. To perform this study we generate 936 error message explanations using three OpenAI LLMs over 21 different buggy code samples. These are then graded for relevance and correctness, and we find that in approximately 71% of cases the LLMs give correct & complete explanations suitable for novice learners.\"]},\"364\":{\"h\":\"LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead\",\"t\":[\"Authors: Junda He, Christoph Treude, David Lo\",\"Link: http://arxiv.org/abs/2404.04834v1\",\"Abstract: Integrating Large Language Models(LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities competitive to human planning and reasoning. This paper envisions the evolution of LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted software engineering challenges. LMA systems introduce numerous benefits, including enhanced robustness through collaborative cross-examination, autonomous problem-solving, and scalable solutions to complex software projects. By examining the role of LMA systems in future software engineering practices, this vision paper highlights the potential applications and emerging challenges. We further point to specific opportunities for research and conclude with a research agenda with a set of research questions to guide future research directions.\"]},\"365\":{\"h\":\"Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language\",\"t\":[\"Authors: Raphaël Merx, Aso Mahmudi, Katrina Langford, Leo Alberto de Araujo, Ekaterina Vylomova\",\"Link: http://arxiv.org/abs/2404.04809v1\",\"Abstract: This study explores the use of large language models (LLMs) for translating English into Mambai, a low-resource Austronesian language spoken in Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, we examine the efficacy of few-shot LLM prompting for machine translation (MT) in this low-resource context. Our methodology involves the strategic selection of parallel sentences and dictionary entries for prompting, aiming to enhance translation accuracy, using open-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality. However, our findings reveal stark disparities in translation performance across test sets, with BLEU scores reaching as high as 21.2 on materials from the language manual, in contrast to a maximum of 4.4 on a test set provided by a native speaker. These results underscore the importance of diverse and representative corpora in assessing MT for low-resource languages. Our research provides insights into few-shot LLM prompting for low-resource MT, and makes available an initial corpus for the Mambai language.\"]},\"366\":{\"h\":\"SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget\",\"t\":[\"Authors: Zihao Wang, Shaoduo Gan\",\"Link: http://arxiv.org/abs/2404.04793v1\",\"Abstract: Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions. Based on our observations regarding layer-wise importance in inference, we propose SqueezeAttention to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative token sparsification algorithms to compress the KV-cache for each layer with its very own budget. By optimizing the KV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves around 30% to 70% of the memory reductions and up to 2.2 times of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention.\"]},\"367\":{\"h\":\"2024-04-06\"},\"368\":{\"h\":\"Multicalibration for Confidence Scoring in LLMs\",\"t\":[\"Authors: Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth\",\"Link: http://arxiv.org/abs/2404.04689v1\",\"Abstract: This paper proposes the use of \\\"multicalibration\\\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \\\"self-annotation\\\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.\"]},\"369\":{\"h\":\"On the Limitations of Large Language Models (LLMs): False Attribution\",\"t\":[\"Authors: Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney\",\"Link: http://arxiv.org/abs/2404.04631v1\",\"Abstract: In this work, we provide insight into one important limitation of large language models (LLMs), i.e. false attribution, and introduce a new hallucination metric - Simple Hallucination Index (SHI). The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (LLaMA-2-13B, Mixtral 8x7B, and Gemma-7B), especially as human annotation can be costly. We collected the top 10 most popular books, according to Project Gutenberg, divided each one into equal chunks of 400 words, and asked each LLM to predict the author. We then randomly sampled 162 chunks for human evaluation from each of the annotated books, based on the error margin of 7% and a confidence level of 95% for the book with the most chunks (Great Expectations by Charles Dickens, having 922 chunks). The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.737, 0.249, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as an SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which is generalizable to other tasks. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.\"]},\"370\":{\"h\":\"Analyzing LLM Usage in an Advanced Computing Class in India\",\"t\":[\"Authors: Chaitanya Arora, Utkarsh Venaik, Pavit Singh, Sahil Goyal, Jatin Tyagi, Shyama Goel, Ujjwal Singhal, Dhruv Kumar\",\"Link: http://arxiv.org/abs/2404.04603v1\",\"Abstract: This paper investigates the usage patterns of undergraduate and graduate students when engaging with large language models (LLMs) to tackle programming assignments in the context of advanced computing courses. Existing work predominantly focuses on the influence of LLMs in introductory programming contexts. Additionally, there is a scarcity of studies analyzing actual conversations between students and LLMs. Our study provides a comprehensive quantitative and qualitative analysis of raw interactions between students and LLMs within an advanced computing course (Distributed Systems) at an Indian University. We further complement this by conducting student interviews to gain deeper insights into their usage patterns. Our study shows that students make use of large language models (LLMs) in various ways: generating code or debugging code by identifying and fixing errors. They also copy and paste assignment descriptions into LLM interfaces for specific solutions, ask conceptual questions about complex programming ideas or theoretical concepts, and generate test cases to check code functionality and robustness. Our analysis includes over 4,000 prompts from 411 students and conducting interviews with 10 students. Our analysis shows that LLMs excel at generating boilerplate code and assisting in debugging, while students handle the integration of components and system troubleshooting. This aligns with the learning objectives of advanced computing courses, which are oriented towards teaching students how to build systems and troubleshoot, with less emphasis on generating code from scratch. Therefore, LLM tools can be leveraged to increase student productivity, as shown by the data we collected. This study contributes to the ongoing discussion on LLM use in education, advocating for their usefulness in advanced computing courses to complement higher-level learning and productivity.\"]},\"371\":{\"h\":\"A Map of Exploring Human Interaction patterns with LLM: Insights into Collaboration and Creativity\",\"t\":[\"Authors: Jiayang Li, Jiale Li\",\"Link: http://arxiv.org/abs/2404.04570v1\",\"Abstract: The outstanding performance capabilities of large language model have driven the evolution of current AI system interaction patterns. This has led to considerable discussion within the Human-AI Interaction (HAII) community. Numerous studies explore this interaction from technical, design, and empirical perspectives. However, the majority of current literature reviews concentrate on interactions across the wider spectrum of AI, with limited attention given to the specific realm of interaction with LLM. We searched for articles on human interaction with LLM, selecting 110 relevant publications meeting consensus definition of Human-AI interaction. Subsequently, we developed a comprehensive Mapping Procedure, structured in five distinct stages, to systematically analyze and categorize the collected publications. Applying this methodical approach, we meticulously mapped the chosen studies, culminating in a detailed and insightful representation of the research landscape. Overall, our review presents an novel approach, introducing a distinctive mapping method, specifically tailored to evaluate human-LLM interaction patterns. We conducted a comprehensive analysis of the current research in related fields, employing clustering techniques for categorization, which enabled us to clearly delineate the status and challenges prevalent in each identified area.\"]},\"372\":{\"h\":\"IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials\",\"t\":[\"Authors: Shreyasi Mandal, Ashutosh Modi\",\"Link: http://arxiv.org/abs/2404.04510v1\",\"Abstract: Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates LLMs' robustness, consistency, and faithful reasoning when performing Natural Language Inference (NLI) on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains. The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset.\"]},\"373\":{\"h\":\"2024-04-05\"},\"374\":{\"h\":\"Increased LLM Vulnerabilities from Fine-tuning and Quantization\",\"t\":[\"Authors: Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth Harshangi\",\"Link: http://arxiv.org/abs/2404.04392v1\",\"Abstract: Large Language Models (LLMs) have become very popular and have found use cases in many domains, such as chatbots, auto-task completion agents, and much more. However, LLMs are vulnerable to different types of attacks, such as jailbreaking, prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo adversarial and alignment training to learn not to generate malicious and toxic content. For specialized use cases, these foundational LLMs are subjected to fine-tuning or quantization for better performance and efficiency. We examine the impact of downstream tasks such as fine-tuning and quantization on LLM vulnerability. We test foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions. Our research shows that fine-tuning and quantization reduces jailbreak resistance significantly, leading to increased LLM vulnerabilities. Finally, we demonstrate the utility of external guardrails in reducing LLM vulnerabilities.\"]},\"375\":{\"h\":\"ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing\",\"t\":[\"Authors: Alec Helbling, Seongmin Lee, Polo Chau\",\"Link: http://arxiv.org/abs/2404.04376v1\",\"Abstract: Recently, researchers have proposed powerful systems for generating and manipulating images using natural language instructions. However, it is difficult to precisely specify many common classes of image transformations with text alone. For example, a user may wish to change the location and breed of a particular dog in an image with several similar dogs. This task is quite difficult with natural language alone, and would require a user to write a laboriously complex prompt that both disambiguates the target dog and describes the destination. We propose ClickDiffusion, a system for precise image manipulation and generation that combines natural language instructions with visual feedback provided by the user through a direct manipulation interface. We demonstrate that by serializing both an image and a multi-modal instruction into a textual representation it is possible to leverage LLMs to perform precise transformations of the layout and appearance of an image. Code available at https://github.com/poloclub/ClickDiffusion.\"]},\"376\":{\"h\":\"Koala: Key frame-conditioned long video-LLM\",\"t\":[\"Authors: Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko\",\"Link: http://arxiv.org/abs/2404.04346v1\",\"Abstract: Long video question answering is a challenging task that involves recognizing short-term activities and reasoning about their fine-grained relationships. State-of-the-art video Large Language Models (vLLMs) hold promise as a viable solution due to their demonstrated emergent capabilities on new tasks. However, despite being trained on millions of short seconds-long videos, vLLMs are unable to understand minutes-long videos and accurately answer questions about them. To address this limitation, we propose a lightweight and self-supervised approach, Key frame-conditioned long video-LLM (Koala), that introduces learnable spatiotemporal queries to adapt pretrained vLLMs for generalizing to longer videos. Our approach introduces two new tokenizers that condition on visual tokens computed from sparse video key frames for understanding short and long video moments. We train our proposed approach on HowTo100M and demonstrate its effectiveness on zero-shot long video understanding benchmarks, where it outperforms state-of-the-art large models by 3 - 6% in absolute accuracy across all tasks. Surprisingly, we also empirically show that our approach not only helps a pretrained vLLM to understand long videos but also improves its accuracy on short-term action recognition.\"]},\"377\":{\"h\":\"Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model\",\"t\":[\"Authors: Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang\",\"Link: http://arxiv.org/abs/2404.04167v1\",\"Abstract: In this study, we introduce CT-LLM, a 2B large language model (LLM) that illustrates a pivotal shift towards prioritizing the Chinese language in developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This strategic composition facilitates the model's exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques. Demonstrating remarkable performance on the CHC-Bench, CT-LLM excels in Chinese language tasks, and showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training LLMs predominantly on English corpora and then adapting them to other languages, broadening the horizons for LLM training methodologies. By open-sourcing the full process of training a Chinese LLM, including a detailed data processing procedure with the obtained Massive Appropriate Pretraining Chinese Corpus (MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark (CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster further exploration and innovation in both academia and industry, paving the way for more inclusive and versatile language models.\"]},\"378\":{\"h\":\"Robust Preference Optimization with Provable Noise Tolerance for LLMs\",\"t\":[\"Authors: Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye\",\"Link: http://arxiv.org/abs/2404.04102v1\",\"Abstract: The preference alignment aims to enable large language models (LLMs) to generate responses that conform to human values, which is essential for developing general AI systems. Ranking-based methods -- a promising class of alignment approaches -- learn human preferences from datasets containing response pairs by optimizing the log-likelihood margins between preferred and dis-preferred responses. However, due to the inherent differences in annotators' preferences, ranking labels of comparisons for response pairs are unavoidably noisy. This seriously hurts the reliability of existing ranking-based methods. To address this problem, we propose a provably noise-tolerant preference alignment method, namely RObust Preference Optimization (ROPO). To the best of our knowledge, ROPO is the first preference alignment method with noise-tolerance guarantees. The key idea of ROPO is to dynamically assign conservative gradient weights to response pairs with high label uncertainty, based on the log-likelihood margins between the responses. By effectively suppressing the gradients of noisy samples, our weighting strategy ensures that the expected risk has the same gradient direction independent of the presence and proportion of noise. Experiments on three open-ended text generation tasks with four base models ranging in size from 2.8B to 13B demonstrate that ROPO significantly outperforms existing ranking-based methods.\"]},\"379\":{\"h\":\"CLUE: A Clinical Language Understanding Evaluation for LLMs\",\"t\":[\"Authors: Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek\",\"Link: http://arxiv.org/abs/2404.04067v1\",\"Abstract: Large Language Models (LLMs) have shown the potential to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs address healthcare-specific challenges, including privacy demands and computational constraints. However, evaluation of these models has primarily been limited to non-clinical tasks, which do not reflect the complexity of practical clinical applications. Additionally, there has been no thorough comparison between biomedical and general-domain LLMs for clinical tasks. To fill this gap, we present the Clinical Language Understanding Evaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters and four existing tasks designed to test the practical applicability of LLMs in healthcare settings. Our evaluation covers several biomedical and general domain LLMs, providing insights into their clinical performance and applicability. CLUE represents a step towards a standardized approach to evaluating and developing LLMs in healthcare to align future model development with the real-world needs of clinical application. We publish our evaluation and data generation scripts: https://github.com/dadaamin/CLUE\"]},\"380\":{\"h\":\"VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots\",\"t\":[\"Authors: Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson\",\"Link: http://arxiv.org/abs/2404.04066v1\",\"Abstract: Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos and supporting files are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/\"]},\"381\":{\"h\":\"Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning\",\"t\":[\"Authors: Gawon Choi, Hyemin Ahn\",\"Link: http://arxiv.org/abs/2404.03891v1\",\"Abstract: In robotics, the use of Large Language Models (LLMs) is becoming prevalent, especially for understanding human commands. In particular, LLMs are utilized as domain-agnostic task planners for high-level human commands. LLMs are capable of Chain-of-Thought (CoT) reasoning, and this allows LLMs to be task planners. However, we need to consider that modern robots still struggle to perform complex actions, and the domains where robots can be deployed are limited in practice. This leads us to pose a question: If small LMs can be trained to reason in chains within a single domain, would even small LMs be good task planners for the robots? To train smaller LMs to reason in chains, we build `COmmand-STeps datasets' (COST) consisting of high-level commands along with corresponding actionable low-level steps, via LLMs. We release not only our datasets but also the prompt templates used to generate them, to allow anyone to build datasets for their domain. We compare GPT3.5 and GPT4 with the finetuned GPT2 for task domains, in tabletop and kitchen environments, and the result shows that GPT2-medium is comparable to GPT3.5 for task planning in a specific domain. Our dataset, code, and more output samples can be found in https://github.com/Gawon-Choi/small-LMs-Task-Planning\"]},\"382\":{\"h\":\"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction\",\"t\":[\"Authors: Bowen Zhang, Harold Soh\",\"Link: http://arxiv.org/abs/2404.03868v1\",\"Abstract: In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schema easily exceed the LLMs' context window length. To address this problem, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.\"]},\"383\":{\"h\":\"2024-04-04\"},\"384\":{\"h\":\"CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering\",\"t\":[\"Authors: Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, Bruno Fleisch\",\"Link: http://arxiv.org/abs/2404.04302v1\",\"Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.\"]},\"385\":{\"h\":\"SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses\",\"t\":[\"Authors: Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, Daniel Khashabi\",\"Link: http://arxiv.org/abs/2404.04298v1\",\"Abstract: Can LLMs continually improve their previous outputs for better results? An affirmative answer would require LLMs to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first introduce a unified framework that allows us to compare the generative and discriminative capability of any model on any task. Then, in our resulting experimental analysis of several LLMs, we do not observe the performance of those models on discrimination to be reliably better than generation. We hope these findings inform the growing literature on self-improvement AI systems.\"]},\"386\":{\"h\":\"GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation\",\"t\":[\"Authors: Kaustubh Dhole, Eugene Agichtein\",\"Link: http://arxiv.org/abs/2404.03746v1\",\"Abstract: Query Reformulation(QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in large language models. By taking inspiration from the success of ensemble prompting strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.\"]},\"387\":{\"h\":\"Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations\",\"t\":[\"Authors: Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee\",\"Link: http://arxiv.org/abs/2404.03745v1\",\"Abstract: The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank content as truthful in the order genuine > minor hallucination > major hallucination and user engagement behaviors mirror this pattern. More importantly, we observed that warning improves hallucination detection without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations.\"]},\"388\":{\"h\":\"SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection\",\"t\":[\"Authors: Bradley P. Allen, Fina Polat, Paul Groth\",\"Link: http://arxiv.org/abs/2404.03732v1\",\"Abstract: We describe the University of Amsterdam Intelligent Data Engineering Lab team's entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system builds on previous work on using prompt programming and in-context learning with large language models (LLMs) to build classifiers for hallucination detection, and extends that work through the incorporation of context-specific definition of task, role, and target concept, and automated generation of examples for use in a few-shot prompting approach. The resulting system achieved fourth-best and sixth-best performance in the model-agnostic track and model-aware tracks for Task 6, respectively, and evaluation using the validation sets showed that the system's classification decisions were consistent with those of the crowd-sourced human labellers. We further found that a zero-shot approach provided better accuracy than a few-shot approach using automatically generated examples. Code for the system described in this paper is available on Github.\"]},\"389\":{\"h\":\"Training LLMs over Neurally Compressed Text\",\"t\":[\"Authors: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant\",\"Link: http://arxiv.org/abs/2404.03626v1\",\"Abstract: In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\\\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.\"]},\"390\":{\"h\":\"Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph\",\"t\":[\"Authors: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini\",\"Link: http://arxiv.org/abs/2404.03623v1\",\"Abstract: Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information. However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation. Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph. Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information). By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs.\"]},\"391\":{\"h\":\"Evaluating LLMs at Detecting Errors in LLM Responses\",\"t\":[\"Authors: Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang\",\"Link: http://arxiv.org/abs/2404.03602v1\",\"Abstract: With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.\"]},\"392\":{\"h\":\"Personalized LLM Response Generation with Parameterized Memory Injection\",\"t\":[\"Authors: Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu\",\"Link: http://arxiv.org/abs/2404.03565v1\",\"Abstract: Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \\\\textbf{M}emory-\\\\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \\\\textbf{L}LM \\\\textbf{P}ersonalization(\\\\textbf{MiLP}).\"]},\"393\":{\"h\":\"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens\",\"t\":[\"Authors: Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny\",\"Link: http://arxiv.org/abs/2404.03413v1\",\"Abstract: This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/\"]},\"394\":{\"h\":\"Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers\",\"t\":[\"Authors: Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang\",\"Link: http://arxiv.org/abs/2404.03192v1\",\"Abstract: The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works (e.g., RankGPT) have also demonstrated that the LLMs exhibit better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.\"]},\"395\":{\"h\":\"Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?\",\"t\":[\"Authors: Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich Klakow\",\"Link: http://arxiv.org/abs/2404.03134v1\",\"Abstract: Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them.\"]},\"396\":{\"h\":\"Towards Standards-Compliant Assistive Technology Product Specifications via LLMs\",\"t\":[\"Authors: Chetan Arora, John Grundy, Louise Puli, Natasha Layton\",\"Link: http://arxiv.org/abs/2404.03122v1\",\"Abstract: In the rapidly evolving field of assistive technology (AT), ensuring that products meet national and international standards is essential for user safety, efficacy, and accessibility. In this vision paper, we introduce CompliAT, a pioneering framework designed to streamline the compliance process of AT product specifications with these standards through the innovative use of Large Language Models (LLMs). CompliAT addresses three critical tasks: checking terminology consistency, classifying products according to standards, and tracing key product specifications to standard requirements. We tackle the challenge of terminology consistency to ensure that the language used in product specifications aligns with relevant standards, reducing misunderstandings and non-compliance risks. We propose a novel approach for product classification, leveraging a retrieval-augmented generation model to accurately categorize AT products aligning to international standards, despite the sparse availability of training data. Finally, CompliAT implements a traceability and compliance mechanism from key product specifications to standard requirements, ensuring all aspects of an AT product are thoroughly vetted against the corresponding standards. By semi-automating these processes, CompliAT aims to significantly reduce the time and effort required for AT product standards compliance and uphold quality and safety standards. We outline our planned implementation and evaluation plan for CompliAT.\"]},\"397\":{\"h\":\"2024-04-03\"},\"398\":{\"h\":\"The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies\",\"t\":[\"Authors: Marcin P. Joachimiak, Mark A. Miller, J. Harry Caufield, Ryan Ly, Nomi L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard\",\"Link: http://arxiv.org/abs/2404.03044v1\",\"Abstract: The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. The ontology is structured around six top-level branches: Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI. AIO's development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support. This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies. The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO).\"]},\"399\":{\"h\":\"I-Design: Personalized LLM Interior Designer\",\"t\":[\"Authors: Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang\",\"Link: http://arxiv.org/abs/2404.02838v1\",\"Abstract: Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.\"]},\"400\":{\"h\":\"AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs\",\"t\":[\"Authors: Maike Behrendt, Stefan Sylvius Wagner, Marc Ziegele, Lena Wilms, Anke Stoll, Dominique Heinbach, Stefan Harmeling\",\"Link: http://arxiv.org/abs/2404.02761v1\",\"Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature.\"]},\"401\":{\"h\":\"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM\",\"t\":[\"Authors: Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yuekai Huang, Jun Hu, Qing Wang\",\"Link: http://arxiv.org/abs/2404.02706v1\",\"Abstract: Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.\"]},\"402\":{\"h\":\"Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation\",\"t\":[\"Authors: Yizhu Liu, Ran Tao, Shengyu Guo, Yifan Yang\",\"Link: http://arxiv.org/abs/2404.02616v1\",\"Abstract: Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training data, which can construct new query-document pairs as training data. Extensive offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling.\"]},\"403\":{\"h\":\"Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game\",\"t\":[\"Authors: Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li\",\"Link: http://arxiv.org/abs/2404.02532v1\",\"Abstract: With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.\"]},\"404\":{\"h\":\"uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?\",\"t\":[\"Authors: Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh\",\"Link: http://arxiv.org/abs/2404.02474v1\",\"Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.\"]},\"405\":{\"h\":\"Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data\",\"t\":[\"Authors: Parth Patwa, Simone Filice, Zhiyu Chen, Giuseppe Castellucci, Oleg Rokhlenko, Shervin Malmasi\",\"Link: http://arxiv.org/abs/2404.02422v1\",\"Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.\"]},\"406\":{\"h\":\"2024-04-02\"},\"407\":{\"h\":\"Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs\",\"t\":[\"Authors: Faraz Lotfi, Farnoosh Faraji, Nikhil Kakodkar, Travis Manderson, David Meger, Gregory Dudek\",\"Link: http://arxiv.org/abs/2404.02294v1\",\"Abstract: This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.\"]},\"408\":{\"h\":\"LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages\",\"t\":[\"Authors: Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer\",\"Link: http://arxiv.org/abs/2404.02261v1\",\"Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings. By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.\"]},\"409\":{\"h\":\"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks\",\"t\":[\"Authors: Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion\",\"Link: http://arxiv.org/abs/2404.02151v1\",\"Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token \\\"Sure\\\"), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.\"]},\"410\":{\"h\":\"Topic-based Watermarks for LLM-Generated Text\",\"t\":[\"Authors: Alexander Nemecek, Yuzhou Jiang, Erman Ayday\",\"Link: http://arxiv.org/abs/2404.02138v1\",\"Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a \\\"topic-based watermarking algorithm\\\" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.\"]},\"411\":{\"h\":\"Advancing LLM Reasoning Generalists with Preference Trees\",\"t\":[\"Authors: Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun\",\"Link: http://arxiv.org/abs/2404.02078v1\",\"Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.\"]},\"412\":{\"h\":\"Long-context LLMs Struggle with Long In-context Learning\",\"t\":[\"Authors: Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen\",\"Link: http://arxiv.org/abs/2404.02060v1\",\"Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.\"]},\"413\":{\"h\":\"Multitask-based Evaluation of Open-Source LLM on Software Vulnerability\",\"t\":[\"Authors: Xin Yin, Chao Ni\",\"Link: http://arxiv.org/abs/2404.02056v1\",\"Abstract: This paper proposes a pipeline for quantitatively evaluating interactive LLMs using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of LLMs based on this dataset. We find that the existing state-of-the-art methods are generally superior to LLMs in software vulnerability detection. Although LLMs improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types. In addition, LLMs demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types. Finally, LLMs show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a few-shot setting. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights for further enhancing LLMs' software vulnerability handling capabilities.\"]},\"414\":{\"h\":\"MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving\",\"t\":[\"Authors: Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion Stoica, Hao Zhang\",\"Link: http://arxiv.org/abs/2404.02015v1\",\"Abstract: Large language models (LLMs) have demon- strated remarkable performance, and organiza- tions are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally for- mulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal coloca- tions and maximize utilization. MuxServe de- signs a unified resource manager to enable flexi- ble and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\\\times$ higher throughput or processes $2.9\\\\times$ more requests within $99%$ SLO attainment.\"]},\"415\":{\"h\":\"Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization\",\"t\":[\"Authors: Yoichi Ishibashi, Yoshimasa Nishimura\",\"Link: http://arxiv.org/abs/2404.02183v1\",\"Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.\"]},\"416\":{\"h\":\"Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation\",\"t\":[\"Authors: Veronica Valeros, Anna Širokova, Carlos Catania, Sebastian Garcia\",\"Link: http://arxiv.org/abs/2404.01940v1\",\"Abstract: Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.\"]},\"417\":{\"h\":\"Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation\",\"t\":[\"Authors: Shanshan Feng, Haoming Lyu, Caishun Chen, Yew-Soon Ong\",\"Link: http://arxiv.org/abs/2404.01855v1\",\"Abstract: Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.\"]},\"418\":{\"h\":\"Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack\",\"t\":[\"Authors: Mark Russinovich, Ahmed Salem, Ronen Eldan\",\"Link: http://arxiv.org/abs/2404.01833v1\",\"Abstract: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as \\\"jailbreaks\\\", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.\"]},\"419\":{\"h\":\"Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems\",\"t\":[\"Authors: Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego\",\"Link: http://arxiv.org/abs/2404.01616v2\",\"Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.\"]},\"420\":{\"h\":\"Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems\",\"t\":[\"Authors: Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego\",\"Link: http://arxiv.org/abs/2404.01616v1\",\"Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.\"]},\"421\":{\"h\":\"2024-04-01\"},\"422\":{\"h\":\"Syntactic Robustness for LLM-based Code Generation\",\"t\":[\"Authors: Laboni Sarker, Mara Downing, Achintya Desai, Tevfik Bultan\",\"Link: http://arxiv.org/abs/2404.01535v1\",\"Abstract: Rapid advances in the field of Large Language Models (LLMs) have made LLM-based code generation an important area for investigation. An LLM-based code generator takes a prompt as input and produces code that implements the requirements specified in the prompt. Many software requirements include mathematical formulas that specify the expected behavior of the code to be generated. Given a code generation prompt that includes a mathematical formula, a reasonable expectation is that, if the formula is syntactically modified without changing its semantics, the generated code for the modified prompt should be semantically equivalent. We formalize this concept as syntactic robustness and investigate the syntactic robustness of GPT-3.5-Turbo and GPT-4 as code generators. To test syntactic robustness, we generate syntactically different but semantically equivalent versions of prompts using a set of mutators that only modify mathematical formulas in prompts. In this paper, we focus on prompts that ask for code that generates solutions to variables in an equation, when given coefficients of the equation as input. Our experimental evaluation demonstrates that GPT-3.5-Turbo and GPT-4 are not syntactically robust for this type of prompts. To improve syntactic robustness, we define a set of reductions that transform the formulas to a simplified form and use these reductions as a pre-processing step. Our experimental results indicate that the syntactic robustness of LLM-based code generation can be improved using our approach.\"]},\"423\":{\"h\":\"Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs\",\"t\":[\"Authors: Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald\",\"Link: http://arxiv.org/abs/2404.01461v1\",\"Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.\"]},\"424\":{\"h\":\"Unveiling Divergent Inductive Biases of LLMs on Temporal Data\",\"t\":[\"Authors: Sindhu Kishore, Hangfeng He\",\"Link: http://arxiv.org/abs/2404.01453v1\",\"Abstract: Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for \\\"AFTER'' in the QA format for both implicit and explicit events, while GPT-4 leans towards \\\"BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards \\\"TRUE'', and GPT-4 exhibits a preference for \\\"FALSE'' in the TE format for both implicit and explicit events. This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.\"]},\"425\":{\"h\":\"Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs\",\"t\":[\"Authors: Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao, Chengyuan Ma, Liang Zhao, Yang Liu\",\"Link: http://arxiv.org/abs/2404.01430v1\",\"Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augmentation technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context. Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving LLMs' effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge.\"]},\"426\":{\"h\":\"A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs\",\"t\":[\"Authors: Harry Li, Gabriel Appleby, Ashley Suh\",\"Link: http://arxiv.org/abs/2404.01425v1\",\"Abstract: We present a mixed-methods study to explore how large language models (LLMs) can assist users in the visual exploration and analysis of knowledge graphs (KGs). We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with KGs and LLMs, either collaboratively or concurrently. Our findings show that participants overwhelmingly want an LLM to facilitate data retrieval from KGs through joint query construction, to identify interesting relationships in the KG through multi-turn conversation, and to create on-demand visualizations from the KG that enhance their trust in the LLM's outputs. To interact with an LLM, participants strongly prefer a chat-based 'widget,' built on top of their regular analysis workflows, with the ability to guide the LLM using their interactions with a visualization. When viewing an LLM's outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the KG) alongside summarizing text. However, participants also expressed concerns about an LLM's ability to maintain semantic intent when translating natural language questions into KG queries, the risk of an LLM 'hallucinating' false data from the KG, and the difficulties of engineering a 'perfect prompt.' From the analysis of our interviews, we contribute a preliminary roadmap for the design of LLM-driven knowledge graph exploration systems and outline future opportunities in this emergent design space.\"]},\"427\":{\"h\":\"Prompt-prompted Mixture of Experts for Efficient LLM Generation\",\"t\":[\"Authors: Harry Dong, Beidi Chen, Yuejie Chi\",\"Link: http://arxiv.org/abs/2404.01365v1\",\"Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\\\\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code will be available at https://github.com/hdong920/GRIFFIN.\"]},\"428\":{\"h\":\"Mapping the Increasing Use of LLMs in Scientific Papers\",\"t\":[\"Authors: Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou\",\"Link: http://arxiv.org/abs/2404.01268v1\",\"Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.\"]},\"429\":{\"h\":\"LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models\",\"t\":[\"Authors: Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei\",\"Link: http://arxiv.org/abs/2404.01230v1\",\"Abstract: This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.\"]},\"430\":{\"h\":\"Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs\",\"t\":[\"Authors: Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo\",\"Link: http://arxiv.org/abs/2404.01151v1\",\"Abstract: Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce \\\"Detect2Interact\\\", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.\"]},\"431\":{\"h\":\"Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit\",\"t\":[\"Authors: Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, Sarah Masud Preum\",\"Link: http://arxiv.org/abs/2404.01147v1\",\"Abstract: Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.\"]},\"432\":{\"h\":\"Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation\",\"t\":[\"Authors: Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin\",\"Link: http://arxiv.org/abs/2404.01129v1\",\"Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs) with LLMs. The SLMs can explicitly incorporate Abstract Meaning Representation (AMR) graph information of the dialogue through a gating mechanism for enhanced semantic representation learning. The evaluation result of SLMs and AMR graph information are plugged into the prompt of LLM, for the enhanced in-context learning performance. Experimental results on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at https://github.com/Bernard-Yang/SIMAMR.\"]},\"433\":{\"h\":\"LLM Attributor: Interactive Visual Attribution for LLM Generation\",\"t\":[\"Authors: Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng\",\"Link: http://arxiv.org/abs/2404.01361v1\",\"Abstract: While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.\"]},\"434\":{\"h\":\"Enabling Memory Safety of C Programs using LLMs\",\"t\":[\"Authors: Nausheen Mohammed, Akash Lal, Aseem Rastogi, Subhajit Roy, Rahul Sharma\",\"Link: http://arxiv.org/abs/2404.01096v1\",\"Abstract: Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.\"]},\"435\":{\"h\":\"Can LLMs get help from other LLMs without revealing private information?\",\"t\":[\"Authors: Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor Cărbune, Blaise Aguera y Arcas\",\"Link: http://arxiv.org/abs/2404.01041v2\",\"Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.\"]},\"436\":{\"h\":\"Efficiently Distilling LLMs for Edge Applications\",\"t\":[\"Authors: Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih Lee\",\"Link: http://arxiv.org/abs/2404.01353v1\",\"Abstract: Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.\"]},\"437\":{\"h\":\"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\"t\":[\"Authors: Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang\",\"Link: http://arxiv.org/abs/2404.00971v1\",\"Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"]},\"438\":{\"h\":\"LLMs are Good Sign Language Translators\",\"t\":[\"Authors: Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, Jun Liu\",\"Link: http://arxiv.org/abs/2404.00925v1\",\"Abstract: Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.\"]},\"439\":{\"h\":\"TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text\",\"t\":[\"Authors: Xiaoyan Qu, Xiangfeng Meng\",\"Link: http://arxiv.org/abs/2404.00899v1\",\"Abstract: With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecting boundaries within mixed texts, including the incorporation of extra layers on top of LLMs, combination of segmentation loss, and the impact of pretraining. Our findings aim to provide valuable insights for future research in this area.\"]},\"440\":{\"h\":\"2024-03-31\"},\"441\":{\"h\":\"The Larger the Better? Improved LLM Code-Generation via Budget Reallocation\",\"t\":[\"Authors: Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi\",\"Link: http://arxiv.org/abs/2404.00725v1\",\"Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.\"]},\"442\":{\"h\":\"Training-Free Semantic Segmentation via LLM-Supervision\",\"t\":[\"Authors: Wenfang Sun, Yingjun Du, Gaowen Liu, Ramana Kompella, Cees G. M. Snoek\",\"Link: http://arxiv.org/abs/2404.00701v1\",\"Abstract: Recent advancements in open vocabulary models, like CLIP, have notably advanced zero-shot classification and segmentation by utilizing natural language for class-specific embeddings. However, most research has focused on improving model accuracy through prompt engineering, prompt learning, or fine-tuning with limited labeled data, thereby overlooking the importance of refining the class descriptors. This paper introduces a new approach to text-supervised semantic segmentation using supervision by a large language model (LLM) that does not require extra training. Our method starts from an LLM, like GPT-3, to generate a detailed set of subclasses for more accurate class representation. We then employ an advanced text-supervised semantic segmentation model to apply the generated subclasses as target labels, resulting in diverse segmentation results tailored to each subclass's unique characteristics. Additionally, we propose an assembly that merges the segmentation maps from the various subclass descriptors to ensure a more comprehensive representation of the different aspects in the test images. Through comprehensive experiments on three standard benchmarks, our method outperforms traditional text-supervised semantic segmentation methods by a marked margin.\"]},\"443\":{\"h\":\"How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library\",\"t\":[\"Authors: Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty\",\"Link: http://arxiv.org/abs/2404.00699v1\",\"Abstract: With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with LLMs, and help the community track contamination levels of LLMs by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: https://github.com/ntunlp/LLMSanitize.\"]},\"444\":{\"h\":\"LLM meets Vision-Language Models for Zero-Shot One-Class Classification\",\"t\":[\"Authors: Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Vincent Gripon\",\"Link: http://arxiv.org/abs/2404.00675v2\",\"Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label\"]},\"445\":{\"h\":\"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs\",\"t\":[\"Authors: Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, Zibin Zheng\",\"Link: http://arxiv.org/abs/2404.00640v2\",\"Abstract: Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.\"]},\"446\":{\"h\":\"AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight\",\"t\":[\"Authors: Nicola Fabiano\",\"Link: http://arxiv.org/abs/2404.00600v2\",\"Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.\"]},\"447\":{\"h\":\"CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs\",\"t\":[\"Authors: Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li\",\"Link: http://arxiv.org/abs/2404.01343v1\",\"Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and large LLMs to achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and QA pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how LLMs can enhance or serve as alternatives to human customer service. Our code and dataset will be open-sourced soon.\"]},\"448\":{\"h\":\"\\\"My agent understands me better\\\": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents\",\"t\":[\"Authors: Yuki Hou, Haruki Tamoto, Homei Miyashita\",\"Link: http://arxiv.org/abs/2404.00573v1\",\"Abstract: In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.\"]},\"449\":{\"h\":\"DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations\",\"t\":[\"Authors: Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu\",\"Link: http://arxiv.org/abs/2404.00557v1\",\"Abstract: Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.\"]},\"450\":{\"h\":\"LLMs are Good Action Recognizers\",\"t\":[\"Authors: Haoxuan Qu, Yujun Cai, Jun Liu\",\"Link: http://arxiv.org/abs/2404.00532v1\",\"Abstract: Skeleton-based action recognition has attracted lots of research attention. Recently, to build an accurate skeleton-based action recognizer, a variety of works have been proposed. Among them, some works use large model architectures as backbones of their recognizers to boost the skeleton data representation capability, while some other works pre-train their recognizers on external data to enrich the knowledge. In this work, we observe that large language models which have been extensively used in various natural language processing tasks generally hold both large model architectures and rich implicit knowledge. Motivated by this, we propose a novel LLM-AR framework, in which we investigate treating the Large Language Model as an Action Recognizer. In our framework, we propose a linguistic projection process to project each input action signal (i.e., each skeleton sequence) into its sentence format'' (i.e., an action sentence''). Moreover, we also incorporate our framework with several designs to further facilitate this linguistic projection process. Extensive experiments demonstrate the efficacy of our proposed framework.\"]},\"451\":{\"h\":\"2024-03-30\"},\"452\":{\"h\":\"Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App\",\"t\":[\"Authors: Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell\",\"Link: http://arxiv.org/abs/2404.00487v1\",\"Abstract: MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.\"]},\"453\":{\"h\":\"Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs\",\"t\":[\"Authors: Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang\",\"Link: http://arxiv.org/abs/2404.00486v1\",\"Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of you may be attacked to the LLMs' context window.\"]},\"454\":{\"h\":\"NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\",\"t\":[\"Authors: Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle\",\"Link: http://arxiv.org/abs/2404.00459v1\",\"Abstract: Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of \\\"42\\\", we suggest using \\\"{2:42}\\\" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.\"]},\"455\":{\"h\":\"MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks\",\"t\":[\"Authors: Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang\",\"Link: http://arxiv.org/abs/2404.00457v1\",\"Abstract: Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract \\\"important information\\\", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of \\\"important information\\\". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.\"]},\"456\":{\"h\":\"QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs\",\"t\":[\"Authors: Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman\",\"Link: http://arxiv.org/abs/2404.00456v1\",\"Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.\"]},\"457\":{\"h\":\"A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration\",\"t\":[\"Authors: Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone\",\"Link: http://arxiv.org/abs/2404.00405v1\",\"Abstract: With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the \\\"5W1H\\\" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.\"]},\"458\":{\"h\":\"Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange\",\"t\":[\"Authors: Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp\",\"Link: http://arxiv.org/abs/2404.00344v1\",\"Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \\\\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}\"]},\"459\":{\"h\":\"Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation\",\"t\":[\"Authors: Yuji Naraki, Ryosuke Yamaki, Yoshikazu Ikeda, Takafumi Horie, Hiroki Naganuma\",\"Link: http://arxiv.org/abs/2404.01334v1\",\"Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under constrained budget conditions. This study illuminates the potential of leveraging LLMs to improve dataset quality, introduces a novel technique to mitigate class imbalances, and demonstrates the feasibility of achieving high-performance NER in a cost-effective way.\"]},\"460\":{\"h\":\"A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs\",\"t\":[\"Authors: Md Saroar Jahan, Mourad Oussalah, Djamila Romaissa Beddia, Jhuma kabir Mim, Nabil Arhab\",\"Link: http://arxiv.org/abs/2404.00303v1\",\"Abstract: The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.\"]},\"461\":{\"h\":\"Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits\",\"t\":[\"Authors: Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan Wen, Ala Tak, Fred Morstatter, Morteza Dehghani\",\"Link: http://arxiv.org/abs/2404.00267v1\",\"Abstract: Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not fully diminish the predictive power of authors' linguistic patterns over their personal traits. We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process. Our findings have important implications for the study of linguistic markers of personal traits in the age of LLMs.\"]},\"462\":{\"h\":\"DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference\",\"t\":[\"Authors: Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin\",\"Link: http://arxiv.org/abs/2404.00242v1\",\"Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\\\\times$, along with an additional reduction in IO for $\\\\mathbf{Q} \\\\mathbf{K}^\\\\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\\\\times$ in end-to-end latency across two practical reasoning tasks over the SOTA attention algorithms.\"]},\"463\":{\"h\":\"Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark\",\"t\":[\"Authors: Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng\",\"Link: http://arxiv.org/abs/2404.00216v1\",\"Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.\"]},\"464\":{\"h\":\"2024-03-29\"},\"465\":{\"h\":\"Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference\",\"t\":[\"Authors: Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, Josep Torrellas\",\"Link: http://arxiv.org/abs/2403.20306v1\",\"Abstract: With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.\"]},\"466\":{\"h\":\"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain\",\"t\":[\"Authors: Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini\",\"Link: http://arxiv.org/abs/2403.20288v1\",\"Abstract: We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.\"]},\"467\":{\"h\":\"LUQ: Long-text Uncertainty Quantification for LLMs\",\"t\":[\"Authors: Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier\",\"Link: http://arxiv.org/abs/2403.20279v1\",\"Abstract: Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\\\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \\\\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \\\\textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular LLMs' response confidence spectrum and how that interplays with the response' factuality. We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about. To further improve the factual accuracy of LLM responses, we propose a method called \\\\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.\"]},\"468\":{\"h\":\"Using LLMs to Model the Beliefs and Preferences of Targeted Populations\",\"t\":[\"Authors: Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Arechiga\",\"Link: http://arxiv.org/abs/2403.20252v1\",\"Abstract: We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.\"]},\"469\":{\"h\":\"Accurate Block Quantization in LLMs with Outliers\",\"t\":[\"Authors: Nikita Trukhanov, Ilya Soloveychik\",\"Link: http://arxiv.org/abs/2403.20137v1\",\"Abstract: The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and provide extremely good quantization accuracy. The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block. In this paper, we focus on the most critical problem of limited KV-cache storage. We propose a novel approach enabling usage of low precision BFP formats without compromising the resulting model accuracy. We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their quantization quality is significantly improved. The methodology yields 2x savings in the memory footprint without significant degradation of the model's accuracy. Importantly, the rearrangement of channels happens at the compile time and thus has no impact on the inference latency.\"]},\"470\":{\"h\":\"Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning\",\"t\":[\"Authors: Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang\",\"Link: http://arxiv.org/abs/2403.20046v1\",\"Abstract: Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\\\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\\\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\\\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\\\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \\\\textsc{CoTErrorSet} will be published soon on \\\\texttt{Anonymity Link}.\"]},\"471\":{\"h\":\"Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning\",\"t\":[\"Authors: Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li\",\"Link: http://arxiv.org/abs/2403.19962v1\",\"Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.\"]},\"472\":{\"h\":\"Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching\",\"t\":[\"Authors: Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang\",\"Link: http://arxiv.org/abs/2403.19930v1\",\"Abstract: The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats.\"]},\"473\":{\"h\":\"2024-03-28\"},\"474\":{\"h\":\"\\\"I'm categorizing LLM as a productivity tool\\\": Examining ethics of LLM use in HCI research practices\",\"t\":[\"Authors: Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen\",\"Link: http://arxiv.org/abs/2403.19876v1\",\"Abstract: Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey conducted with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.\"]},\"475\":{\"h\":\"LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces\",\"t\":[\"Authors: Xiaomin Ouyang, Mani Srivastava\",\"Link: http://arxiv.org/abs/2403.19857v1\",\"Abstract: Most studies on machine learning in sensing systems focus on low-level perception tasks that process raw sensory data within a short time window. However, many practical applications, such as human routine modeling and occupancy tracking, require high-level reasoning abilities to comprehend concepts and make inferences based on long-term sensor traces. Existing machine learning-based approaches for handling such complex tasks struggle to generalize due to the limited training samples and the high dimensionality of sensor traces, necessitating the integration of human knowledge for designing first-principle models or logic reasoning methods. We pose a fundamental question: Can we harness the reasoning capabilities and world knowledge of Large Language Models (LLMs) to recognize complex events from long-term spatiotemporal sensor traces? To answer this question, we design an effective prompting framework for LLMs on high-level reasoning tasks, which can handle traces from the raw sensor data as well as the low-level perception results. We also design two strategies to enhance performance with long sensor traces, including summarization before reasoning and selective inclusion of historical traces. Our framework can be implemented in an edge-cloud setup, running small LLMs on the edge for data summarization and performing high-level reasoning on the cloud for privacy preservation. The results show that LLMSense can achieve over 80% accuracy on two high-level reasoning tasks such as dementia diagnosis with behavior traces and occupancy tracking with environmental sensor traces. This paper provides a few insights and guidelines for leveraging LLM for high-level reasoning on sensor traces and highlights several directions for future work.\"]},\"476\":{\"h\":\"LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae\",\"t\":[\"Authors: Celia Chen, Alex Leitch\",\"Link: http://arxiv.org/abs/2403.19506v1\",\"Abstract: This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.\"]},\"477\":{\"h\":\"Enhancing Anomaly Detection in Financial Markets with an LLM-based Multi-Agent Framework\",\"t\":[\"Authors: Taejin Park\",\"Link: http://arxiv.org/abs/2403.19735v1\",\"Abstract: This paper introduces a Large Language Model (LLM)-based multi-agent framework designed to enhance anomaly detection within financial market data, tackling the longstanding challenge of manually verifying system-generated anomaly alerts. The framework harnesses a collaborative network of AI agents, each specialised in distinct functions including data conversion, expert analysis via web research, institutional knowledge utilization or cross-checking and report consolidation and management roles. By coordinating these agents towards a common objective, the framework provides a comprehensive and automated approach for validating and interpreting financial data anomalies. I analyse the S&P 500 index to demonstrate the framework's proficiency in enhancing the efficiency, accuracy and reduction of human intervention in financial market monitoring. The integration of AI's autonomous functionalities with established analytical methods not only underscores the framework's effectiveness in anomaly detection but also signals its broader applicability in supporting financial market monitoring.\"]},\"478\":{\"h\":\"Checkpoint Merging via Bayesian Optimization in LLM Pretraining\",\"t\":[\"Authors: Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui\",\"Link: http://arxiv.org/abs/2403.19390v1\",\"Abstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.\"]},\"479\":{\"h\":\"Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors\",\"t\":[\"Authors: Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, Linjian Mo\",\"Link: http://arxiv.org/abs/2403.19347v1\",\"Abstract: With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database. Subsequently, the deeper, trainable layers of the LLM facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings. This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity. Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores. Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using LLMs, especially with longer user sequences. BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR prediction.\"]},\"480\":{\"h\":\"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios\",\"t\":[\"Authors: Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, Jie Tang\",\"Link: http://arxiv.org/abs/2403.19318v1\",\"Abstract: We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction.\"]},\"481\":{\"h\":\"Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators\",\"t\":[\"Authors: Zahra Abbasiantaeb, Mohammad Aliannejadi\",\"Link: http://arxiv.org/abs/2403.19302v1\",\"Abstract: CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset.\"]},\"482\":{\"h\":\"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM\",\"t\":[\"Authors: Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang\",\"Link: http://arxiv.org/abs/2403.19114v1\",\"Abstract: LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval\"]},\"483\":{\"h\":\"Learning From Correctness Without Prompting Makes LLM Efficient Reasoner\",\"t\":[\"Authors: Yuxuan Yao, Han Wu, Zhijiang Guo, Biyan Zhou, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, Linqi Song\",\"Link: http://arxiv.org/abs/2403.19094v1\",\"Abstract: Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\\\textbf{Le}arning from \\\\textbf{Co}rrectness (\\\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption.\"]},\"484\":{\"h\":\"2024-03-27\"},\"485\":{\"h\":\"Towards LLM-RecSys Alignment with Textual ID Learning\",\"t\":[\"Authors: Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang\",\"Link: http://arxiv.org/abs/2403.19021v1\",\"Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.\"]},\"486\":{\"h\":\"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations\",\"t\":[\"Authors: Ehsan Latif, Ramviyas Parasuraman, Xiaoming Zhai\",\"Link: http://arxiv.org/abs/2403.18721v1\",\"Abstract: Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.\"]},\"487\":{\"h\":\"SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens\",\"t\":[\"Authors: Chengbo Liu, Yong Zhu\",\"Link: http://arxiv.org/abs/2403.18647v1\",\"Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the \\\"two-step-draft-then-verify\\\" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.\"]},\"488\":{\"h\":\"FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs\",\"t\":[\"Authors: Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Yanming Zhang, Weiming Zhang, Nenghai Yu\",\"Link: http://arxiv.org/abs/2403.18403v1\",\"Abstract: Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoCBinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.\"]},\"489\":{\"h\":\"Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback\",\"t\":[\"Authors: Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu\",\"Link: http://arxiv.org/abs/2403.18349v1\",\"Abstract: Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.\"]},\"490\":{\"h\":\"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications\",\"t\":[\"Authors: Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava\",\"Link: http://arxiv.org/abs/2403.18327v1\",\"Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.\"]},\"491\":{\"h\":\"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\"t\":[\"Authors: Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu\",\"Link: http://arxiv.org/abs/2403.18249v1\",\"Abstract: Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"]},\"492\":{\"h\":\"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices\",\"t\":[\"Authors: Neda Taghizadeh Serajeh, Iman Mohammadi, Vittorio Fuccella, Mattia De Rosa\",\"Link: http://arxiv.org/abs/2403.18173v1\",\"Abstract: Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.\"]},\"493\":{\"h\":\"2024-03-26\"},\"494\":{\"h\":\"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization\",\"t\":[\"Authors: Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu\",\"Link: http://arxiv.org/abs/2403.18120v1\",\"Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"]},\"495\":{\"h\":\"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution\",\"t\":[\"Authors: Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng\",\"Link: http://arxiv.org/abs/2403.17927v1\",\"Abstract: In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.\"]},\"496\":{\"h\":\"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications\",\"t\":[\"Authors: Philip Lippmann, Matthijs Spaan, Jie Yang\",\"Link: http://arxiv.org/abs/2403.17860v1\",\"Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.\"]},\"497\":{\"h\":\"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs\",\"t\":[\"Authors: David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler\",\"Link: http://arxiv.org/abs/2403.17856v1\",\"Abstract: Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4.\"]},\"498\":{\"h\":\"Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)\",\"t\":[\"Authors: Amir Ghasemi, Paul Guinand\",\"Link: http://arxiv.org/abs/2403.17819v1\",\"Abstract: Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks. In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.\"]},\"499\":{\"h\":\"MOODFLOW\",\"t\":[\"我通过研究社会流动人口发现，这类群体的精神关注十分不足，我发现他们经常因为生存压力和狭窄的生活社交圈子产生情绪问题且无法合理宣泄，我以此出发做了一个社会创新设计项目。我通过线上的APP设计一个符号体系来创造情绪社交空间，设置模块化、社区化的线下互动装置并融合音画交互技术，帮助用户快速排解情绪。整个项目通过服务设计、艺术、交互空间来找到情绪宣泄的创新方式。\"]},\"500\":{\"h\":\"The City Of Desires  欲望之城\",\"t\":[\"我组织了一个工作坊，我们构建了一个反乌托邦的叙事世界，来思考在人工智能飞速发展的今天，人类日益增长的危险欲望。我们设置了不同的传感器，让玩家在互动过程中体验不舒适的身体感受，并反思人类欲望过度所带来的影响。通过跨学科的碰撞和团队协作，将游戏娱乐与人机交互相结合，回应假设，并思考未来的商业形式。\"]},\"501\":{\"h\":\"\"},\"502\":{\"h\":\"职业/行业思考\",\"t\":[\"谈谈设计师的未来\",\"聊聊建筑转行\",\"毕业逆袭，看我如何挑战传统建筑\",\"六帽子思考法则——通过玩游戏思考留学\",\"《用户体验要素：以用户为中心的产品设计》读书总结\",\"商业工程师概念的思考\",\"”不适体验“游戏的商业思考\",\"设计界的新魔咒：知识诅咒\",\"创新跨越：从个人到系统层面的创造力培养\",\"数据驱动设计\",\"用户体验设计中的决策心理学：理解大脑如何做出选择\",\"用艺术思维革新商业——设计思维vs艺术思维\",\"2024 年最热门的技能\",\"为何我们需要转向新的设计哲学与实践\",\"Design Engineering in the Age of Industry 4.0 论文分析\"]},\"503\":{\"h\":\"留学思考\",\"t\":[\"通过EDBI来思考设计师的未来\",\"为什么选择留学\",\"艺术装置和择校思考\"]},\"504\":{\"h\":\"灵感分析\",\"t\":[\"Project UrbEx——通过摄影和游戏来寻找灵感\",\"通过音频增强现实技术思考\",\"、\"]},\"505\":{\"h\":\"\",\"t\":[\"5 个适合个人和专业用途的 AI 电子邮件生成器\"]},\"506\":{\"h\":\"为什么选择留学\",\"t\":[\"24Fall进入尾声了，之前开了贴子讨论转行跨界的话题，得到不同看法，今天先从我个人角度聊聊留学申请这一年，希望尽一点微薄之力给25、26Fall申请的同学一点方向指引。\"]},\"507\":{\"h\":\"1.职业发展\",\"t\":[\"对我来说，硕士留学最重要就是一个更综合，有跨学科环境的发展平台，还有学校带来的资源和人脉。这句话听起来很老套，但对我已经工作了几年的人来说确实是这样，我本科是美院景观设计，虽然毕业后算转行，但也是和空间设计相关，后面又辞职做自媒体做了个人工作室，作为数字艺术家荣幸受邀得到小红书、芒果TV等媒体活动的合作机会，赶上了一些小风口取得了成绩但是也会遇到瓶颈，算是以“超级个体”的身份在设计艺术领域有一段快速上升期，打造了一个IP。未来我也希望这个IP可以集合到更多人，技术更新迭代太快了，但是只要你不断保持对新事物的思考，将自己的特点挖掘，就可能会产生很多灵感爆炸，找到不同的商业应用环境。\",\"在数字艺术领域的技能我都是靠自学的，也算是打造了一个个人体系（不是软件技术方面的，以后有机会会再分享），但是我还是想通过更体系化的学习帮助在该领域深造，同时未来我也希望在多媒体行业有更多发展和找到更多人去往未来新领域去冲刺，所以选择了交互这个大类的留学专业。\"]},\"508\":{\"h\":\"2.地区与择校\",\"t\":[\"我目标很明确，首选香港地区，学校就是香港理工大学，当时的第一志愿专业是Met，也就是现在申请上的Ime前身。我未来的职业发展也是希望和多媒体行业有关的，专业设置也最匹配我过往创业的方向，未来再去结合不同的技术，娱乐方式，迭代商业模式，包括之前对于met的研究，也是具备很强的跨学科环境，游戏、用户心理、社交互联网、商业策略等，不只是从传统设计的思维出发了，而是帮助你站在更多维度去思考媒体技术的不同发展，我认为未来这些传统概念都会被更新，岗位匹配人才也更需要拥有复合技能，跨学科优势。\",\"我认为我过去的工作经历抛开职位名字的刻板印象，比如xxx设计师，其实我学到的都是挺多元化的，甚至对我来说设计之外的技能更加珍贵，可能也是让我在当时自媒体创业前期能够扛得住的一部分原因。所以我对于留学专业的选择也一定是交叉多元，而且我更希望去接触一些商科体系的内容来作为我的补充。\",\"那为什么会选香港，说实话我大学很向往英国的教育，包括大四做毕业设计学习了UCL的一些理念最后还让我的作品广泛被讨论，但目前对于我来说，香港环境和教学对于未来整合资源肯定优于英国，我更在意有更多机会去对接真实市场，甚至企业合作。客观来说，每个国家地区留学各有优劣，还是先问问自己什么是当下最想得到的，就如有些同学未来希望从事编制，在意落户问题，那可能排名就是一个首要重点。\",\"当然，虽说是有种all in的感觉，但我依然选择在这个时候多去争取不同地区和学校专业面试和录取的机会，这一部分后面我会再复盘给后面申请的同学一些参考。\",\"坦白说，如今的留学，绝对没有什么保录取一说，招生官最希望看到的是除了硬背景外的个人特性，最真实的你，作品集也是一种IP的打造，最大化去告诉招生官“你是谁”，“如何被记住”，这些一定是申请准备时候最应该的提前筹划的。\"]},\"509\":{\"h\":\"艺术装置思考和择校思考\",\"t\":[\"不要为了做艺术装置而做艺术装置，不要为了交互而做交互，如果你是从设计产品的角度出发，那你要想的是一个策略，它应该是有客观依据，有逻辑性的推理，而不是一堆交互模块堆叠而成的视觉艺术品。\",\"在我的从业经历中，包括我留学准备的作品集里，都有相关的项目经历，所以我先用两个不同角度来和大家分享，一是技术点，二是结果呈现。\",\"技术点来说，分软件和硬件层面，软件主要有两种分类，一是建模软件 比如c4d，blender，二十游戏引擎，比如unity，UE，硬件也可以大致分为两种，一种是aduino这类传感器模块，一种是kinect或者leapmotion这种现成的深度摄像头，硬件再往大层面拓展，可能是像服装、空间结构等等，那这时候你需要的材料就更多了，听起来非常多东西，但是实际上我们在制作作品集的时候，一定在明确自己定位的时候去选择相应的软件，我们没有办法去精通每一款软件硬件，甚至怎么去组装，那就是回到最开始我说的，有客观理性的逻辑去找到技术点所需要的工具，再精准去攻破，这样才能有效率。\",\"那在结果层面来说，我认为大家不要在这个阶段追求绝对的完美，反观有时候教授就喜欢看你的实验过程，你的迭代方式，这种才是真实的，你如何去挑战自己，找到突破口，这才是一个具有探索心的人应该被看到的东西，当然如果有时间有精力，我本身也非常支持同学们可以去深化，不乏有同学的作品质量在自媒体平台或者线下展览这些场合被观众赏识，也许通过这种方式也会让你未来有更多可能性，这也是我们在准备筹划的事情。\",\"借此话题顺便聊聊伦敦大学金史密斯学院，\",\"首先从我圈子资源的角度来说，\",\"我身边有认识从这里毕业的同学，包括以前有和学校的人进行合作，有些成为艺术家，有些成为品牌主理人，包括很多很厉害的自媒体博主也在就读于这两所大学，他们对于美的事物确实非常有见解，这也是他们能够创意领域引人瞩目的原因，那具体艺术院校确实就读体验方面，我觉得去找就读的学长学姐是最直接的。\",\"其实咱们应该把目光看的更远些，我知道RCA和UAL是大家非常热门的讨论话题，但是全球也不乏有许多优秀的艺术学院，就比如伦敦大学金史密斯学院，也就是大家口中的金匠。今年我也非常荣幸拿到金匠拓展实践的无条件offer，因为我本身雅思并没有过关嘛，我想说的一点是，金匠的面试真的是我觉得体验感最好的了，简单总结就是：交流感，很多面试其实是让人感觉有压力的，或者就是很结构化，但是在金史密斯学院拓展实践项目的面试中就感受到他们独特的个性在，甚至教授有时候都比你说得多，其实这是一种很正向的交流，包括教授有时候就直接对着你的点去发散，他当时告诉我未来可以介入一些神经系统的测试，问我有没有兴趣继续把这些作品完善成人机交互游戏，要知道，如果教授没有认真去阅览你的作品集或者只是简单地走过场，他也不会说出这些想法。\",\"那说了这么多，回归问题本身，值不值得这个问题，我觉得得先问问自己。\",\"什么样的学习叫做有价值，如果你喜欢填鸭式教学，那肯定一点价值都没有，如果你想对于艺术、创意十分感兴趣，会自主思考，对于未知的事物有强烈的好奇心，那我觉得这所学校还是非常适合你去探索和实验的，所以平时有同学来问我们这个问题的时候，我们也都会先反问回去，得自己先了解自己，从点到线再到面，最后形成你的个体空间，这个问题答案就出来了。\"]},\"510\":{\"h\":\"通过EDBI来思考设计师的未来\",\"t\":[\"最近看到一篇论文叫做Design Engineering in the Age of Industry 4.0（工业4.0时代的设计工程学），想到我留学申请中对于专业选择的研究，我发现这几年交互设计体系中，EDBI这几个字母经常被重新组合出现。解释一下，E=工程（engineering)，D=设计(Design)，B=商业(Business)，I=创新/整合（innovative/integrated）。\",\"特别是新加坡国立大学出现的挺多类似的专业，今年还新开了一个整合设计（Integrated design），其实香港理工大学的几个专业都是如此的概念，Mdes学位的IBD/SSD/ISD，Msc学位的IME，这几个字母只是一个启示，并不是非得是用上才能表达这个理念。\",\"说到底就是这几年的专业越来越倾向于建立跨学科环境，去培养具有实际应用能力的工程师和设计师，或者应该说复合型人才，包括鼓励他们结合商业思考，适应市场需求。在数字化创新时代下也更鼓励学生跳出传统学科界限，融合不同领域以产生新的解决问题的方法，培养创新精神。甚至这些专业可能还与法律、艺术等其他学科相结合。\",\"举几个例子，比如\",\"D+B： 设计领域的创新结合商业应用，推动新产品或服务的开发，如我们常常提到的用户体验（UX）、服务设计、商业咨询等等。\",\"D+I+E： 创新技术与工程设计相结合，产生新的科技解决方案，例如绿色能源、物联网或者自动化系统的革新，催生新的硬件产品设计，如智能设备或环保材料的应用。\",\"B+I： 商业与创新结合，推动商业模式的创新，比如数字营销策略、社交电商等等，为企业解决问题，开发独特的产品和服务。\",\"可能性非常多，甚至也不包括提到的字母。说到这里，也想结合我的申请经历给大家一点启发，既然全球各地的名校也在组合这些字母，那我们在作品集里是不是也可以有这样一种思维去打造项目呢，同时这也是一个人设的打造，就像MBTI人格一样，对自己划分一个基础定位，再配合自身的背景情况去打开你的开题项目。\",\"所以，通过这几个简单例子，我想告诉大家一些小启发，先定位，再下手，先找找自己的作品集人设，再利用这个人设来展开你的故事，这样才容易打动教授。\"]},\"511\":{\"h\":\"Project UrbEx\",\"t\":[\"欢迎来到车车灵感一分钟，今天通过摄影和游戏来寻找灵感。\",\"作品集灵感｜探索城市废墟和游戏创作🎴 - 小红书\",\"游戏设计师中村育美通过名为《Project UrbEx》的新书分享了她二十年来探索废弃城市的经历，其中的灵感来源于遗忘的城市风景。她的电子游戏如《恶灵附身》和《幽灵线：东京》，展示了这种爱好如何融入她的职业生涯，并创造出富有细节和沉浸感的虚拟世界。\",\"她认为人类建造并遗弃的地方也有其美感，并且可以成为创造游戏的灵感来源。这种观点启示我们，即使看似荒芜的地方也可能隐藏着故事和创意，值得我们去探索和尊重。这些地方可以成为回忆和理解过去时代的线索，即使看似普通也可能具有一种独特的吸引力。在游戏设计中，她巧妙地将这些元素融入了故事，让玩家感受到与现实世界的共鸣。\",\"值得思考的问题：\",\"这种对废弃空间的热爱是如何转化为在电子游戏中创造独特世界观的？\",\"人类如何能在日常生活中发现并欣赏那些被遗忘的美，而不是仅仅破坏它们？\",\"如何能从游戏交互体验中理解和感知这种来自废弃空间的情感？\",\"如何看待城市建筑和人类历史的“遗忘之美丽”？\",\"如果你是一个摄影爱好者，或者热爱探索城市的人，不妨试着思考如何将这些来自于身边的记忆通过设计、技术手段转换成更深入的话题。尝试用摄影记录下你的发现，这也可能会成为创作游戏时的独特素材，而做出来的作品集项目也更加真实和打动人。\",\"好了，今天的灵感一分钟就到这里了，我们下期再见。\"]},\"512\":{\"h\":\"音频增强现实\",\"t\":[\"借着今天 GPT-4o的发布，想到之前的一门技术，也分享一些案例给大家作为灵感参考。\",\"音频增强现实（Audio AR）是一种新兴的技术，它将音频信息与现实环境相结合，提供一种沉浸式的体验。这种技术通过耳机和语音交互来传递信息，让用户能够在听音乐、看电影或进行其他活动时，同时感受到音效和环境声音。\",\"当然，Audio AR的概念并不是全新的。 Microsoft 的Soundscape和 FourSquare 的MarsBot是提供音频 AR 体验的两项早期尝试。比如，Microsoft 的 Soundscape 主要是作为视障人士的辅助工具而开发的。当用户穿过城市时，空间音频信号会通知他们周围的兴趣点，从自助洗衣店到餐馆。它展示了使用空间音频和基于位置的警报来增强现实的潜力。然而，在密集的城市环境中，它会产生压倒性的刺耳声音。而在郊区，它太稀疏了，没有什么用处。\",\"这些应用程序以及类似的实验取得了重要进展，并表明音频 AR 不仅仅可以用于简单的路线导航。 Soundscape 展示了空间音频和基于位置的寻路通知的强大功能。\"]},\"513\":{\"h\":\"原理\"},\"514\":{\"h\":\"1.1. 技术架构\",\"t\":[\"Audio AR的核心是将音频元素融入到用户的现实环境中。这通常涉及一个头戴式设备，用户可以将其连接到智能手机或其他电子设备上。这些设备配备了麦克风和扬声器，允许音频信号在空间中传播并触发特定的反应。例如，用户可以在听歌时看到歌词同步显示，或者在观看电影时听到环绕音效。\"]},\"515\":{\"h\":\"1.2. 交互方式\",\"t\":[\"Audio AR的交互主要基于语音命令，用户可以通过耳机与设备进行对话。这种交互方式比传统的屏幕触摸更为自然，因为它允许用户专注于手头的任务，而不必时刻盯着屏幕。例如，在户外运动或驾驶时，人们可以继续听音乐。\"]},\"516\":{\"h\":\"1.3. 技术挑战\",\"t\":[\"Audio AR 应用程序的核心 UX 挑战是在正确的时间提供正确的信息。尽管音频增强现实带来了诸多便利，但实现起来也面临一些挑战。首先，需要开放标准和互操作性，以确保设备之间的兼容性和内容的无缝传输。此外，设计时必须考虑到声音如何在空间中传播，以避免干扰或噪音。还有，如何适配不同的环境和场景，以便音频可以与周围环境融合，而不是突兀地插入。\",\"音频 AR 代表了我们与数字信息交互方式的重大转变。通过将界面从屏幕移至耳朵，我们可以保持清醒并与周围的世界互动。它有可能让技术感觉更少侵入性、更环境化，并且更少受到设备外形的限制。但要实现这一目标，我们需要解决一些关键的交互挑战。我们需要开放标准来与耳机集成并理解上下文,我们需要深思熟虑的设计，以便在正确的时间提供正确的信息。耳机、语音接口和人工智能已经达到了音频 AR 实用化的成熟程度。我们看到了引人注目的产品——而不仅仅是实验——比如 Meta 的 Wayfarer 眼镜。\",\"关键词：音频增强现实（Audio AR）、用户体验（UX）、技术趋势、交互挑战、语音接口\"]},\"517\":{\"h\":\"灵感启发\",\"t\":[\"设计者可以借鉴电影中的“指导人物”角色，通过整合各类技术（如耳机、语音助手和AI），为用户提供即时且相关的信息，减少用户注意力转移的需求。\",\"企业应考虑将音频AR纳入产品设计中，使其成为增强用户体验的一种方式。例如，在健身应用中，集成音频AR可以提供实时反馈和指导，让用户在运动时不会错过关键信息。 电影、游戏等娱乐领域也正在探索Audio AR的应用，它能够为观众或玩家带来更丰富的感官体验，如增强现实的环境声音和互动元素，提高沉浸感。音频增强现实还可以用于教育场景，例如语言学习应用可以实时提供翻译，或者在训练模拟环境中提供音效指导。\",\"在Audio AR中，如何让用户感觉到科技的便利而非侵入？\",\"如何设计用户界面和交互模式，使音频AR既能提供即时反馈又能适应不同的环境需求？\"]},\"518\":{\"h\":\"2024 年最热门的技能\",\"t\":[\" LinkedIn发布了一份关于2024年蕞抢手技能的报告，指出在人工智能时代，沟通、适应能力、客户服务和领导力等软技能的重要性日益凸显。报告基于对LinkedIn全球1亿成员的数据分析，显示在过去12个月中，成员们增加了6.8亿个技能到他们的个人资料中，同比增长80%。报告强调，在人工智能快速发展的背景下，持续提升员工的软技能对企业和个人职业发展至关重要。\",\"在AI时代，软技能和硬技能的平衡是关键。AI可以处理大量数据和执行重复性任务，但软技能如沟通、协作和批判性思维是人类特有的优势。我们应该多思考如何通过设计更具创造性和战略性的工作内容，让AI负责流程化和重复性的任务，同时提升人类工作者的决策能力、人际交往能力和批判性思维能力，从而在AI时代保持其独特的价值。**终身学习，终身学习，终身学习！**不断更新和扩展自己的技能组合，特别是那些不易被AI取代的软技能，如沟通、领导力和问题解决能力，以保持职业竞争力。\",\"接下来我们来思考AI时代下我们该如何建立个人技能树\",\"沟通技能： 在AI时代，沟通技能的挑战在于如何跨越数字鸿沟，而其机会则在于构建更加高效、多元化的沟通网络，促进跨文化理解和团队合作。\",\"客户服务： AI的发展可能侵蚀传统客户服务岗位，但同时也提供了新的服务模式，我们可以把握机会，结合AI技术提升服务质量和客户满意度。\",\"领导力： 领导力面临的挑战是如何在AI辅助下保持团队凝聚力，而其机会则在于培养领导者的创新能力和适应变革的能力，引领团队取得成果。\",\"项目管理： AI时代项目管理的关键在于如何利用AI工具优化流程，AI可以更加高效建立团队管理流程，利于提升项目管理者的战略思维和团队协作能力。\",\"管理： AI带来的管理变革，对于管理者的决策能力和领导力十分考验，危机在于管理技能可能因AI自动化而变得过时，只能不断提升人际管理能力和战略思维，才能适应动态管理环境，推动企业实现创新和发展。\",\"数据分析： 数据分析面临的挑战在于如何确保数据的准确性和可靠性，而其机会则在于通过数据分析洞察市场趋势，为企业决策提供有力支持。\",\"团队合作： AI可能加剧团队隔阂，但另一面思考却也可以加强跨职能团队协作，促进知识共享和创新能力。管理者该思考如何打破信息壁垒，去构建开放、包容的团队文化，提升团队整体协作效率。\",\"销售： 传统销售模式可能因AI而失效，或许该思考如何应对AI带来的市场变化、避免重复无情绪价值的销售，而同时利用AI技术也可以精准定位客户，提升销售转化率。\",\"问题解决： AI时代的问题复杂多变，领导者和员工将面临更多新挑战，团队都需要培养批判性思维和战略规划能力以应对变化。\",\"研究： 危机在于对AI的过度依赖可能导致缺乏独立思考，如何确保研究结果的客观性和准确性，而机会在于提升研究技能，保持批判性和创造性思维，可以利用AI技术拓宽研究领域，推动个人和团队的知识库数据。\"]},\"519\":{\"h\":\"Design Engineering in the Age of Industry 4.0 论文分析\",\"t\":[\"关键词：Industry 4.0、设计工程、数字化、集成、服务化产品、智能X、数据驱动、动态风险管理、可持续设计、灵活制造\",\"简介： Industry 4.0,也被称为第四工业革命，正在对设计工程（DE4.0）产生深远影响。该章节讨论了在Industry 4.0时代，设计工程的新原则、方法和角色如何适应，并利用大数据、机器学习和其他数字技术来提升产品设计、生命周期管理以及供应链效率。随着工业4.0的到来，设计领域正面临前所未有的挑战，包括人类-数据交互的新形式、系统复杂性和不确定性增加，以及网络安全等问题。为了应对这些挑战，文章强调了在产品开发和制造中融入设计思维的重要性，并提出了几个关键的解决方案。\",\"首先，设计思维（Design Thinking）成为战略规划的核心，通过系统性地利用员工创造力进行决策制定，以实现动态集成制造创新（Integrated Manufacturing）。设计流程不仅关注传统产品的完成，还扩展到市场营销和客户服务，如数字孪生的应用，这有助于提升生产效率并适应变化。\",\"其次，智能技术的应用提供了数据驱动的决策支持。例如，使用眼动系统可以收集关于产品位置或状态的数据，并通过数字双生（Digital Twin）进行实时分享，从而优化生产流程中的安全性和生产力。\",\"此外，文章还讨论了如何在设计中处理不确定性和复杂性，如在设计参数的选择和确定上引入灵活性，以及利用数据科学方法来管理和风险评估。这些技术能够帮助设计师预见并应对可能出现的问题，确保系统的可靠性和适应性。\",\"最后，文章提出组织需要从利润导向型企业转变为学习型组织，以社会利益为导向，并安全地集成机器人于生产线。这种转变不仅要求制造业更新其生产模式，还需员工具备跨学科能力和创新技能。\",\"总结来说，工业4.0时代的挑战呼唤设计思维的全面应用，通过数据驱动、系统灵活性和可持续性设计，企业能够应对复杂环境并实现高效制造。此外，组织层面的转型是确保这一目标的关键，这将有助于推动制造业朝着更智能、灵活的方向发展。\",\"关键议题包括：Innovation Ecosystem：随着Industry 4.0的发展，消费者和设计工程师的角色变得更加重要，他们共同塑造未来的系统并确保其可持续性。这要求设计工程必须聚焦于培养创新生态系统，通过实时数据决策来支持产品和服务的进化。\",\"Cyber-Physical-Systems (CPS) and Internet of People：随着互联网和个人物品的融合，设计者需要应对“数字自我”与物理世界的界限模糊问题，这涉及如何在技术进步的同时处理人类互动和合作的问题。\",\"学术关键词1：Industry 4.0 - Product Ecosystem Design\",\"解释：这个关键词指的是在Industry 4.0时代，产品生态系统设计的概念，它涉及到动态的产品单元和用户互动，设计师需要应对不断增长的技术挑战。\",\"学术关键词2: Cloud-based Digital Platform - Open Innovation\",\"解释：指利用云平台支持开放设计和制造，促进服务化产品的实现过程，并协调物理流。\",\"关系：与工业4.0中的数字化生产和服务化有关，体现了技术进步对产品开发的影响。\",\"学术关键词3: Sharing Economy - Resource Sharing\",\"解释：共享经济是基于资源分享的经济社会体系，它通过众包和数字平台驱动创新，促进产品的分发和市场可行性。\",\"关键点：在设计思维中，这种模式强调合作与资源的流动。\",\"学术关键词4: Human-Cyber-Physical Systems (HCPS)\",\"解释：融合了人类互动、合作以及物理系统，是工业化4.0中的概念，涉及到复杂的社会技术系统设计。\",\"关键点：在工业设计和工程中，理解并适应这种系统的动态变化至关重要。\",\"学术关键词5: Data-driven Design\",\"解释：数据驱动的设计是指基于大量过程数据来指导产品开发，强调数据在设计决策中的作用。\",\"关系：它反映了现代设计中的数据分析和应用，强调的是设计方法和技术的创新。\",\"问题：如何利用数字化技术和人机交互解决DE4.0中面临的问题？\",\"解决方案：通过云设计和制造，利用数字平台进行协作，减少技术依赖并提高决策效率。同时，设计新产品如服务系统，利用CSCW技术理解和应对复杂性，以及通过数据分析优化产品和服务。\",\"结果：这种方法使得产品能够适应消费者生态系统，提供实时价值，并可能通过订阅费实现价值捕获。此外，它还强调可持续性和设计的循环经济思维，例如通过系统动力学模拟和模拟来管理资源，以及利用数据科学方法进行知识合成和集成任务。\",\"总结：DE4.0通过结合技术与人性化的设计，以及对实时事件数据的利用，实现了智能生产，并在服务化产品设计、风险管理、可持续性设计等多个方面产生了深远影响。这促使制造商从单纯的利润导向转变为学习型组织，以适应快速变化的市场环境和技术发展。\",\"随着工业4.0的到来，设计工程领域面临着资源共享生产网络、智能制造和跨组织协调等问题。这些问题主要体现在如何在数字化时代中实现高效决策制定、数字通信框架的构建以及智能系统的应用等方面。\",\"首先，文章讨论了数字通信中的“数字编织物”（Digital Thread），这是一个关键概念，它通过集成设计信息和模型，促进生产流程的自动化和信息流动。然而，如何建立并维护这个框架是一个挑战，因为涉及到数据的安全性和隐私保护。\",\"其次，智能制造在制造业中的应用也带来了一系列问题，如生产效率、质量和创新决策等方面的需求。智能X（Smart X）的概念被提出，旨在通过缩短供应链周期来提高竞争力，但其实施需要解决软件和硬件的集成问题，以及如何利用这些技术带来的数据以实现更快速的响应。\",\"此外，文章还提到了诸如业务到消费者（B2C）、互联网服务、产品和服务的数字化共享，以及在设计和制造过程中的情绪预测等具体议题。这涉及到数据驱动的设计优化、情绪识别技术的应用以及信息的价值化等问题。\",\"解决方案方面，作者们强调了网络安全管理的重要性，通过使用数字孪生技术来保护设计数据，同时利用智能决策支持系统进行连续的决策制定和评估。此外，技术和创新策略也在被提出，如通过模型驱动框架进行人机交互设计（Industry 4.0），以及应用生物测量设备进行用户体验分析。\",\"最后，文章总结了这些解决方案的应用效果，包括通过数字技术提升生产效率、通过服务化思维促进产品和服务的创新，并通过对非技术能力的要求来适应数字化工作场所的变化。然而，实际效果的具体数据并未在文中提供，需要进一步查阅具体研究或报告。\",\"《工业4.0时代的工程设计：集成与创新》这篇文章探讨了在行业4.0时代，设计工程的新挑战和机遇。首先，文章指出随着技术进步和互联网的普及，如物联网、大数据和人工智能的应用，设计环节不再局限于产品功能的完成，而是扩展到营销、客户服务等多个领域，形成了“数字链条”（Digital Thread），这强调了设计的数字化沟通和连贯性的重要性。\",\"设计工程正从传统的实体制造转向更复杂的人机交互系统，如人-机器-物理系统（Human-Cyber-Physical Systems, HCPMS）。作者提到了如何在设计初期理解和管理不确定性，以及如何应对网络安全威胁的问题，例如如何预防和保护系统免受潜在的威胁。此外，文章还提到设计过程需要更加迭代决策和评估，强调了设计思维与服务导向的重要性。\",\"同时，文章指出教育方式可能也需要适应变化，如课程长度可能缩短以保证知识不过时，并探讨在线教育（MOOCs）作为新教育模式的可能性。这反映了工业4.0中对创新、快速学习和终身学习的需求。\",\"此外，文中还讨论了如何从利润导向的企业转变为“学习组织”，并融入社会利益，例如通过循环经济的挑战，以及如何利用技术如大数据分析来提升决策效率和质量保证。作者指出非技术能力（如创新思维、适应性等）在数字化工作场所中的重要性，并强调教育应该培养未来的工作力。\",\"本文的观点是工业4.0对设计工程提出了多维度的挑战与机遇，设计师需要具备跨学科理解和快速适应的能力，同时企业也需要转变商业模式，拥抱学习和可持续发展。这启示我们，在面对技术进步时，设计不仅仅关注产品功能，更要考虑整体效率和用户体验；教育应注重培养创新能力和终身学习；企业则需在追求利润的同时，承担社会责任，推动循环经济的发展。\",\"通过这篇文章，我们可以理解工业4.0时代的设计工程需要综合多方面的能力，并结合数字化、服务化等趋势进行调整和优化。\"]},\"520\":{\"h\":\"”不适体验“游戏的商业思考\",\"t\":[\"我之前做了一个项目The City Of Desires，利用一些不舒适的游戏体验来完成人机项目。我们也完成了一个产品蓝图，后面我也思考对于这类非常规体验游戏是否未来可能有更大的发展空间，比如当下已经有的分娩疼痛体验，也是一种不舒适的体验，让男性可以站在女性视角去理解分娩的痛苦，那不舒适的体验在娱乐、教育领域是否也有一些潜在的商业空间呢？\",\"根据对不舒适体验的简单调研，我尝试去思考其在未来的商业价值、商业架构以及商业模式中的作用。随着现代社会压力的增加，不舒适体验产品有望成为心理健康的创新服务形式，满足人们提升心理适应性和抗压能力的愿望。\",\"商业价值体现在多个维度。首先，这类产品能够作为教育资源，协助学生和成年人应对生活挑战，增强他们的适应能力和解决问题的能力。其次，企业或许可以将其纳入员工福利计划，以提升团队凝聚力和员工的心理承受力，从而提高整体生产效率。此外，不舒适体验产品也可用于社会问题的宣传教育，提高公众对贫困、健康等社会问题的关注度。\",\"商业架构可能倾向于平台化运营，整合多种不舒适的体验，实现资源共享和经济效益。同时，与心理咨询机构、教育机构、企业以及非盈利组织建立合作关系，共同推进产品的开发和应用。\",\"商业模式可能包括订阅服务，让用户定期获得新的体验内容；B2B服务模式为企业定制解决方案；内容销售涉及虚拟现实场景和在线课程的购买或租赁；另外，广告赞助也是一个重要的收入来源，特别是对于关注心理健康、教育和公益事业的品牌。对于具有社会意义的项目，可以采取公益模式，通过捐赠、赞助或政府资助来维持运营。\",\"不舒适体验的商业化也许可以创造更多经济价值，还为解决社会问题提供了创新思路，同时也提升了品牌形象和社会责任感。有望将个人成长、企业发展与社会进步紧密联系，是否未来有巨大的潜力和广阔的发展前景呢，大家可以聊聊。\"]},\"521\":{\"h\":\"战略层\",\"t\":[\"用户体验设计——用户心理感受和行为\",\"功能+视觉=综合形成\",\"网站 1、已内容为主的网络产品 2、交互为主的网络应用\",\"转化率——衡量用户体验 转化率=访问者/注册量（购买、认证、入驻......）\",\"优质转化——浏览→购买\",\"ps：小红书类转化率机制 点击率（问题在于推广曝光不足还是质量不够） 、赞收藏与粉丝比例（用户只是被你某一条笔记吸引或者是喜欢你的社区内容）\",\"五个层面（由上到下：具体→抽象）\",\"表现层：视觉元素（让用户看到的）\",\"框架层：布局罗列（用户可以干什么）\",\"结构层：逻辑串联（要怎么做这些事，更有效率，体验更好，达到目的）\",\"范围层：优化特性功能的组合方式（哪些需要被强调）\",\"战略层：经营者和用户的目标目的\",\"下面层决定上面层，底层逻辑\",\"下层完成在相邻上层完成前，而不是上层出现前就完成了\",\"基础目标——品牌识别（概念系统、情绪反应等等，不仅仅局限于视觉设计）\",\"建立好成功标准\",\"用户模型：人物用户可以是从研究数据中抽取出来可以作为样例的虚拟形象\",\"人物信息内容含量与这个虚拟形象所要表现的\",\"头像是否重要？虚拟人or真实人照片？\",\"所包含的信息详细度，甚至涉及到视觉版面内容\",\"重要性：你知道什么该做什么不该做才不会永远在循环和发展没有一个定性的目标\",\"*从为什么、是什么→做什么、什么内容去满足\",\"战略层的人物形象→加入场景\",\"要去想怎么防止不好的事情发生而不是先下定不应该产生某事\",\"具体化、明确，抽象的词少用（欢迎、受关注等等，这些词描绘的并没有具体化的数据指标，是点击量？播放量？评论数？）\",\"减少主观性——时尚？符合谁的时尚？（这些需要得到某类群体的认同，会有非常多的可能性出现）\",\"一个概念模型的产生不是要去告诉用户怎么使用，而是让他们产生直觉形成的交互行为能够与·预期的符合\",\"高效的结构优点应该是容纳成长、适应变动（时间长了，你的组织框架、某些分类方式的改变是否能够适配）（以历史记录来说，每个平台确实都有自己的管理方式，如抖音、微博没有浏览记录，原因可能他并不是一个记录型的平台软件，也许更希望通过用户的互动来产生记录而不是单纯的浏览产生痕迹）\",\"到了结构层，应该去识别用户需要的至关信息，预知期望并纳入设计中\",\"*让用户了解你的结构——命名原则（描述标签等等）+受控词典（使用用户语言和保持一致性）或更精细的类词词典（补充同义词的表达，缩写、俚语等等，甚至是广狭义的拓展）\",\"界面设计：提供用户做某事，元素布局\",\"导航设计：提供用户去哪儿，引导元素的安排\",\"信息设计：传达给用户想法，信息要素的排布（功能型产品：以任务为导向，信息型产品：以信息为导向，信息设计跨越这两者的边界，容纳更大的范围）\",\"布局和习惯，什么是该创新打破传统的，什么东西一旦被创新反而给用户增加困扰，记住我们使用东西的习惯。同时用户的不同文化背景也可能对一个信息图产生不同的联想，容易混淆概念，尽量减少不同人群的猜测。\",\"视觉设计，忠于眼睛——视觉落点与你的战略目标要素是否匹配\",\"设计系统迭代过程的保留和记录\",\"风格指南的重要性——提供足够细节来保持决策的方向（产品品牌视觉的规范性及全局标准，以及到模块、网站的具体功能）\"]},\"522\":{\"h\":\"为何我们需要转向新的设计哲学与实践\",\"t\":[\" 设计存在粒度问题 为什么我们需要转向新的设计理念和实践？\",\"作者：Kevin Richard\",\"读书笔记和思考\",\"尽管设计领域试图超越对个体经验的关注，但实际上，UX设计和相关实践在反映整个社会群体的共享社会文化实践方面存在不足。用户体验设计（UXD）和以人为中心的设计（HCD）是两种设计方法，它们都强调用户的需求和体验，但它们的焦点和方法有所不同。\",\"用户体验设计（UXD） 用户体验设计（UXD）是一种以用户为中心的设计方法，它关注的是产品的用户如何与产品互动，以及用户在使用过程中的整体体验。UXD的目标是创建直观、高效、愉悦的用户体验。\",\"焦点：用户交互和用户体验。 方法：用户研究、用户测试、原型设计、可用性评估等。 目标：提高产品的易用性、吸引力和满意度。\",\"以人为中心的设计（HCD） 以人为中心的设计（HCD）是一种更加广泛的设计方法，它不仅关注产品设计，还包括产品、服务、系统甚至整个工作流程的设计。HCD的核心是理解人、环境、任务之间的相互作用。\",\"焦点：人、环境、任务之间的交互。 方法：用户研究、情境分析、设计迭代、跨学科协作等。 目标：创造满足人们需求、符合人的能力和限制的设计解决方案。\",\"设计领域的局限性过度关注个体特征：UXD和HCD在实践中往往过于关注个体的心理特征、需求和偏好，而忽略了更广泛的社会文化背景。 忽视社会文化背景：设计方法缺乏对用户的社会文化背景的深入理解，导致设计产品无法真正反映和满足整个群体的需求。\",\"设计实践的根源与导向工程背景：设计实践的根源在于工程学，这导致设计倾向于从技术角度解决问题，而不是从人的角度出发。 认知心理学的影响：与认知心理学知识的结合使得设计实践更加倾向于将问题归结于个体心理层面，而忽略了外部环境和文化因素的影响。\",\"设计转向与策略关注社会文化背景：设计师需要更多地关注社会文化背景，理解不同文化对用户行为和需求的影响。 用户研究和市场调研：通过深入的用户研究和市场调研，设计师可以更好地理解用户的真实需求，从而设计出更符合用户的需求的产品。 多元设计方法：采用多元设计方法，如社区参与式设计，可以帮助设计师从更广泛的角度考虑问题，并确保设计能够满足不同用户的需求。\",\"设计哲学的转变从个体到群体：设计哲学需要从以个体为中心转向以群体为中心，考虑整个社会群体的需求和期望。 从问题解决到系统设计：设计实践需要从简单的问题解决转向系统设计，考虑整个社会文化背景和生态景观。\",\"结论 设计领域需要转向新的设计哲学与实践，以解决当前存在的“粒度问题”。这意味着设计师需要超越对个体用户的关注，更多地考虑社会文化背景，通过深入的用户研究和多元设计方法，创造出能够满足整个社会群体需求的设计作品。这种转变不仅能够更好地服务于用户，还能够促进社会的整体福祉。\"]},\"523\":{\"h\":\"六帽子思考法则——通过玩游戏思考留学\",\"t\":[\"先简单介绍六帽子思考法则\",\"蓝帽子：“指挥家的帽子” 当你或你的团队处于蓝帽模式时，你会专注于控制自己的思维和管理决策过程。你会制定议程、要求总结并得出结论。\",\"绿帽子：“创意帽” 绿帽代表创造性思维。当你“戴上”这顶帽子时，你会探索一系列的想法和可能的前进方向。\",\"红帽子：“心脏之帽” 这顶帽子代表着感觉和本能。当你进行这种思考时，你可以表达自己的感受，而不必用逻辑来证明它们。\",\"黄帽子：“乐观主义者的帽子” 运用黄帽思维，你会以最积极的眼光看待问题。你会强调你的想法可能带来的好处和附加值。\",\"黑帽：“法官的帽子” 这顶帽子代表谨慎和评估风险。你要运用批判性判断，并准确解释你为什么有顾虑。\",\"白帽子：“事实帽” 白帽代表信息收集。想想你已经收集的知识和见解，还有你缺少的信息，以及你可以从哪里获得这些信息。\",\"这个思考法可以是由一人也可以是团队来完成，帽子的顺序可以大部分情况可以自行决定。\",\"以我亲身经历为例子\",\"蓝帽：我现在要思考的问题是，当下的我需要通过留学来提升自我吗？\",\"红帽：本能感受，当下我确实遇到一些瓶颈，我通过自学得到了很多技能，也通过自己创业打开了圈子，但是当时还是处在一个阶层难以突破，我需要更多专业引导、思维认知、人脉资源等来帮我更好面对未来的挑战。\",\"白帽：查资料， 留学地区选择，留学学校，专业介绍等等？比如相关专业与我现在的和将来的联系，课程知识可以帮我提升的内容。\",\"绿帽：我需要关注留学读书学习以外的内容，通过留学如何来帮我创造价值和提升？对我而言，留学读研如果还都是纸上谈兵的构想就没意思了，学校对我而言既是学习的地方，更重要的也是平台，如果可以充分利用好平台资源也是一种价值实现。\",\"黄帽：既然我关注到读书以外的内容，在申请阶段，我关注的便不只有专业的层面了，学校是否有一些校企合作可以有更落地的项目实践，学校的校友圈与当下的发展是否可以产生更好的联系呢（这个问题像是回到了以前高考对于学校和专业排位选择的思考）。经过这一层，我已经把香港理工锁定好了，在当下的留学选择中是最适合我的，学校的设计学院也是闻名国际，我也非常荣幸最后收到了dream offer。\",\"黑帽：反面思考，当下的学历贬值，可能一张硕士文凭难以让我有飞跃性的提升，同时辛苦准备的申请最后都可能泡汤，努力化为乌有。\"]},\"524\":{\"h\":\"创新跨越：从个人到系统层面的创造力培养\",\"t\":[\" 新加坡在2022年的PISA评估中脱颖而出，成为全球在创意思考方面表现最出色的教育体系。同时，当地的教育专家、学府、行业专业人士和政策制定者联手制定了一个全面的设计教育改革蓝图，以重塑设计教育。\",\"新加坡教育部长陈春声先生积极推动设计思维的实践，发起一个由13个机构组成的全球设计教育联盟，致力于用设计思维解决实际问题。然而，尽管新加坡学生在PISA测试中表现出色，但根据数据，只有64%的学生对自身创造力有信心，低于OECD的平均值73%，显示出信心和勇气在创新中的重要性。\",\"“创意系统画布”工具，该工具强调文化在形成个体和群体创新行为规范中的核心地位。\",\"发现问题：新加坡学生在创意思考上表现出色，但在自信心方面存在不足，仅有64%的学生对自己的创造力有信心，低于OECD平均值。\",\"解决方案：通过设计教育改革，包括推广设计思维，建立全球设计教育联盟，以及使用创意系统画布工具，以提升文化中的创新行为规范。\",\"结果：目标是建立一个鼓励创新的环境，提高学生们的自信心和勇气，从而促进系统层面的创新。这将有助于克服教育成就与个人创新自信之间的差距。\",\"关键知识点和问题思考\",\"创意系统画布（Creative Systems Canvas）：一种工具，用于帮助组织识别不同类型的创造力，以实现系统化的方法。\",\"PISA（Programme for International Student Assessment）：一个国际性的学生评估项目，评估各国学生的阅读、数学和科学能力，以及在本文中的创意思考能力。\",\"设计思维（Design Thinking）：一种以人为本的解决问题方法，强调理解用户需求，通常涉及迭代和原型创建。\",\"创造力自信（Creativity Confidence）：个人对自己创新能力和创造力的信念程度，对于激发创新至关重要。\",\"文化（Culture）：在创意系统画布中，文化被视为创新行为规范形成的中心，影响个体和群体的创新活动。\",\"Q1: 为什么新加坡在PISA评估中表现出色，但学生对自身创造力的信心却不足？ A1: 这可能是因为教育系统过于注重学术成就，而忽视了培养学生的自信心和勇气，导致他们在实际应用和自我评价时感到不确定。\",\"Q2: 创意系统画布如何帮助组织或社会层面推动创新？ A2: 创意系统画布强调文化在创新中的核心作用，它帮助识别不同类型的创造力，鼓励在组织内部形成支持创新的行为规范，从而推动系统层面的创新，不仅限于个人层面的创新活动。\"]},\"525\":{\"h\":\"商业工程师概念的思考\",\"t\":[\"根据 FourWeekM BA 对商业工程的定义，商业工程师是介于企业家、高管和商业战略家之间的混合体，它结合了对业务建模的深刻理解以加速实验，设计思维以开发具有客户痴迷方法的产品。通过非线性竞争、过渡性业务建模和技术建模等概念，对业务扩展有深刻的理解。\",\"作者Gennaro Cuofano提出了商业工程师这一角色，他们具备深入理解技术、设计实验流程以测试业务模型以及逆向工程业务核心资产的能力。（逆向工程业务指通过分析和解构一个公司的现有运作模式，来识别其最根本、最具价值的部分，即“核心资产”。这可能包括技术、产品、分销渠道、客户关系、品牌影响力等。商业工程师运用逆向工程的思路，从结果（如收入模式、财务表现）出发，逐步深入到公司的内部运作机制，理解是什么使得这个公司能够在市场上取得成功或保持竞争优势。逆向工程这一过程旨在理解业务是如何运作的，哪些因素驱动了它的增长和盈利能力，以及如何复制或改进这些关键元素。这种能力对于创新、改进现有业务模型或创建新的商业模式至关重要。通过这种方式，商业工程师可以发现潜在的改进点，或者找到颠覆现有市场格局的新机会。）\",\"商业工程的核心是基于业务建模的实验，通过理解和操作公司的底层经济结构，如资金流动、核心竞争力等。\",\"商业工程师借鉴了设计思维，通过关注客户来理解和创造价值。他们不仅理解客户的需要，还能通过实验和迭代来测试业务模型，以构建封闭的反馈循环。 商业建模作为基础，帮助他们分析业务，从外在如收入到内在的核心资产进行层层剖析。\",\"结果：\",\"商业工程师结合了设计思维和商业建模，形成了一种创新的工作方法，即“以客户为中心”的商业工程。这种方法使他们能够通过实验和迭代来创造指数级的机会，打破竞争模式，并从客户的反馈中获得价值。\",\"商业工程师理解如何分发技术，以及如何在现实世界中测试和验证这些技术的假设。\",\"结果体现在业务的增长和发展上，即能够适应市场变化，快速做出反应，并通过实验来优化业务模型。\",\"给传统设计师们的启发：\",\"以客户为中心：设计思维强调客户体验，但商业工程师更进一步，将客户置于业务战略的核心，强调自下而上的创新和指数级增长机会。设计师可以思考如何更深入地理解和响应客户需求，创造不仅美观而且具有高附加值的设计。\",\"实验与迭代：商业工程师利用业务建模进行实验，快速测试假设。设计师也可以借鉴这种快速原型和迭代的过程，不断优化设计方案，确保它们能够适应市场的变化和用户的需求。\",\"商业模式理解：了解商业模式对于设计师来说非常重要，因为它影响产品的功能、形态和市场定位。设计师可以学习如何分析商业模式，以便更好地将设计与商业目标相结合。\",\"复杂系统的理解：商业世界是一个复杂的系统，设计师需要意识到他们的设计决策可能产生多米诺效应。考虑设计如何影响整个系统，以及它如何与其他业务元素相互作用，可以帮助创造出更有影响力的设计解决方案。\",\"线性与非线性竞争：在快速变化的市场中，设计师应认识到短期的竞争可能是线性的，而长期竞争则可能引发行业重组。这意味着设计师必须保持灵活，预见未来趋势，并设计出能够适应这些变化的产品和服务。\",\"增量与突破性思维：设计师应该学会在何时采用渐进式改进，何时需要颠覆性创新。在某些情况下，微小的改进足以保持竞争力，而在其他情况下，大胆的创新可能是必要的。\",\"技术与经济激励：商业工程师关注技术的经济影响。设计师应关注技术如何改变设计的可能性，以及新技术如何影响成本结构和用户体验。\"]},\"526\":{\"h\":\"数据驱动设计\"},\"527\":{\"h\":\"概念\",\"t\":[\"在当前充满不确定性的时期，设计师面临着裁员和行业未来发展的担忧。然而，数据驱动设计（Data-Informed Design）提供了一种应对策略，它并非意味着AI取代设计师的工作，而是一种轻量级的“恰到好处的数据”方法，由前Facebook设计副总裁Julie Zhuo推崇。这种方法特别适用于UX/设计成熟度较低的环境，可以增强设计的影响力。\",\"数据驱动设计强调在设计过程中加入适量的数据，以支持决策，而不是让数据完全主导。通过收集和分析用户行为数据，设计师可以更好地理解用户需求，优化用户体验，并提高设计项目的成功率。这种方法有助于在设计决策中找到平衡，同时保持创新性。\",\"几个思考的关键点：\",\"1️⃣数据驱动设计是设计师在动荡时期的生存工具，可以增强设计的决策影响力。\",\"2️⃣数据不是取代设计师，而是作为设计过程的有力补充。\",\"3️⃣“恰到好处的数据”方法，结合设计师的直觉，能够提升设计效果。\",\"4️⃣数据分析可以帮助识别设计中的瓶颈，优化用户体验和转化率。\",\"5️⃣通过数据驱动设计，设计师可以更有效地解决问题，重新调整设计优先级，并在行业中保持竞争力。尽管面临挑战，但数据的融入为设计带来了新的机遇，帮助设计师在快速变化的环境中导航。\",\"6️⃣在数据驱动设计中，设计师的角色转变为数据分析师和决策者，他们需要理解数据背后的意义，用数据来支持和验证设计决策，同时也需保持对用户需求的敏锐洞察。\"]},\"528\":{\"h\":\"数据驱动设计的应用案例\",\"t\":[\"初创公司的产品优化\",\"用户反馈收集\",\"行为跟踪\",\"界面与交互调整\",\"转化率与留存率提升\",\"电子商务平台的购物体验\",\"用户购买路径分析\",\"浏览习惯研究\",\"购物车放弃率识别\",\"结账流程与商品展示优化\",\"销售业绩提升\",\"新闻网站的内容推荐\",\"阅读偏好理解\",\"用户行为分析\",\"个性化推荐算法\",\"用户满意度与粘性增强\",\"广告收入增长\",\"教育应用程序的用户体验迭代\",\"学习路径改进\",\"课程内容与互动元素优化\",\"学习进度与反馈收集\",\"功能有效性评估\",\"学习成果与参与度提升\"]},\"529\":{\"h\":\"数据驱动设计的概念与应用\",\"t\":[\"AI与UX的融合\",\"AI-Enhanced UX Design\",\"用户行为分析自动化\",\"需求预测\",\"快速设计优化\",\"数据科学在设计中的角色\",\"Data-Driven Design Transformation\",\"决策支持与流程优化\",\"设计项目优先级确定\",\"设计效率提升\",\"未来UX的适应性框架\",\"Future-Ready UX Adaptability Framework\",\"灵活设计原则\",\"数据监控机制\",\"快速迭代能力\",\"轻量化数据设计\",\"Zhuo's Lightweight Data-Driven Design Strategy\",\"关键指标指导设计\",\"复杂性降低\",\"团队响应速度与创新能力提升\",\"智能设计的未来洞察\",\"Intelligent Design Future Insights\",\"AI预测能力结合\",\"行业变化深度学习\",\"设计趋势预见\",\"前瞻性设计决策\"]},\"530\":{\"h\":\"\",\"t\":[\" 今年刚好准备重新回归校园入读硕士，又是临近一年一度的毕业季，zui近各大美院的毕业展也开始准备举行了，和大家分享一些过去的故事，也希望给大家带来一些新的启发。\",\"2019年，我用建筑叙事的思维完成了毕业设计——“Phantasisland”。\",\"通过两段叙事：一是我构想了 2199 年虚构的垃圾城市故事，另一则是追溯了 1943 年垃圾游乐场的诞生和发展历史。\",\"我通过虚幻的构造和真实的过去去提出一个设想，但是这种畅想也是一种对孩子们内心世界的回应，同时借由废弃材料的媒介去发挥，引出问题去思考，如果我们能借由某种手段去帮助孩子们又能让他们更自由去创造、探索、冒险，这样的世界里会产生怎么样的反应，而我也通过一个美好的假想去回应这个结果。在这个杂糅的世界中，孩子们得到自由、体验到Qian所未有的冒险之旅，没有大人的看管，内心的力量被放大，他们又重回那种自由无畏的精神状态，而废弃材料的利用又经由孩子之手得到创造，帮助孩子们圆梦。创造可以是物质性的，亦可以是精神性的，冒险、探索也是一种创造，一种属于孩子们才能读懂的创造。\",\"这是一个开放性的命题，也许答案应该由更多的观众去解读。\",\"当时在导师的指导下学习了《short stories》的叙事建筑理论，对我启发甚大，篇幅太长，下回可以再和大家分析这本书的内容\",\"对于这种项目如何把握，我总结了三点。\",\"抽象和感性 尝试去完全打开自己的内心，把自己当成小说家，置身进入你zui想去的世界，用文本或者加入一些抽象拼贴把它描绘出来。\",\"客观和理性 寻找世界上的客观事件，历史、新闻等等都是你的理论依据，切记你是以一个设计师的视角去挖掘现象，而不是一个艺术家在自由发挥。\",\"理性和感性的叠加 再次回归当下的生活，寻找结合点。你写了一个虚构小说，你是以一个“当事人”的视角在构思一些“痛点”，而你寻找了某些“痛点”客观存在的事实，这时候的再设计，便是一个能立得住脚的观点了。而通常这类题材更适合去做一个开放性的反思话题，我们要清楚，设计师并不能jiejue所有问题，但是我们可以通过设计师的思维来给予观众反思的空间。\",\"相关阅读Phantasisland(造梦城)\"]},\"531\":{\"h\":\"洞察消费设计与数字营销\",\"t\":[\"在当今这个时代，我们的生活仿佛陷入了一个无休止的消费狂热。我们不断被诱导购买我们不需要甚至不想要的东西。这种趋势的背后，是消费设计的力量。那消费设计是什么呢？\",\"1️⃣_消费设计的定义与历史_\",\"消费设计，顾名思义，是一种通过设计来影响消费者行为的力量。它包括一系列旨在操纵消费者购买决策的策略和方法。这一概念源于20世纪初的消费者工程，当时为了解决过度生产的问题，经济学家和设计师开始训练人们成为顺从的消费者。\",\"2️⃣_消费设计的影响_\",\"过度消费 消费设计通过算法、营销策略和用户体验设计等手段，诱导消费者进行过度消费，导致消费者陷入消费陷阱，无法理性消费。\",\"环境污染 过度消费导致资源浪费、环境污染和气候变化等问题。消费设计在推动过度消费的过程中，加剧了这些问题。\",\"供应链剥削 为了降低成本，提高利润，消费设计往往牺牲了供应链中工人的权益，导致工人剥削。\",\"我最近学习一些数字营销相关的内容，发现数字营销与消费设计都是现代营销策略中的两个关键组成部分，它们在推动销售和品牌建设方面发挥着重要作用。\",\"1️⃣_相同点_ 目标一致 数字营销和消费设计的目标都是为了提升品牌知名度、吸引潜在客户并最终实现销售增长。\",\"依赖技术 两者都依赖于现代技术，如互联网、社交媒体、大数据和人工智能等，以实现更高效的营销效果。\",\"用户体验 两者都强调用户体验，通过优化产品和服务设计来满足消费者的需求，提升满意度。\",\"数据驱动 数字营销和消费设计都注重数据分析，通过收集用户行为数据来优化营销策略和产品设计。\",\"2️⃣_不同点_ 定义范围 数字营销是指的是通过数字渠道（如网站、社交媒体、电子邮件等）进行的营销活动，强调的是信息的传播和与消费者的互动，而消费设计侧重于产品或服务的物理和用户体验设计，旨在影响消费者的购买决策和品牌忠诚度。\",\"营销渠道 数字营销涵盖多种渠道，如搜索引擎、社交媒体、电子邮件、网站等，而消费设计涉及产品设计、包装设计、用户界面（UI）设计、用户体验（UX）设计等。。\",\"交互方式 数字营销通过互动广告、社交媒体互动等方式与消费者建立联系，而消费设计则通过产品和服务本身的体验来影响消费者。\",\"作用环节 数字营销在消费者接触产品或服务之前发挥作用，帮助建立品牌认知和吸引潜在客户。\",\"消费设计在消费者接触产品或服务时发挥作用，直接影响消费者的购买决策和使用体验。\",\"营销策略： 数字营销侧重于推广和传播，而消费设计则侧重于创造欲望和需求。\",\"3️⃣_发散思考_ 跨界融合 数字营销与消费设计可以相互融合，形成更加全面的营销策略。例如，通过社交媒体平台推广产品，并结合消费设计理念，提升产品吸引力和用户体验。或者利用新技术发展例如，通过增强现实（AR）和虚拟现实（VR）技术，消费者可以在数字环境中体验产品，这是数字营销和消费设计相结合的例子。\",\"可持续发展 随着消费者对环保和可持续发展的关注增加，数字营销和消费设计应更加注重社会责任。企业可以通过绿色设计、环保材料和可持续生产方式来提升品牌形象。\",\"个性化营销 利用大数据和人工智能技术，数字营销和消费设计可以实现个性化营销。通过分析消费者行为和偏好，为企业提供更精准的产品推荐和营销方案。\",\"跨界合作 企业可以与设计师、艺术家等跨界合作，创造出具有独特魅力的产品，吸引更多消费者。\",\"用户体验至上 无论是消费设计还是数字营销，都将更加注重用户体验，通过提供优质的产品和服务来提升消费者满意度。\",\"消费设计与数字营销的未来将是一个不断进化、相互融合的过程。随着技术的发展和社会的变化，两者将共同推动营销领域的创新，为消费者创造更加丰富、可持续的购物体验。\"]},\"532\":{\"h\":\"用户体验设计中的决策心理学：理解大脑如何做出选择\",\"t\":[\" 近来读到一文分析探讨大脑的决策系统，以及如何将这些知识应用于用户体验设计，总结了一下。\",\"每天，我们都在进行着大量的决策，从早晨起床决定是否按闹钟，到选择穿哪件衣服，再到决定早餐吃什么。据研究者称，成年人每天大约要做出35,000个决策。这些决策不仅包括日常琐事，还包括更重要的决定，如买商品或发送重要email。\",\"我们的决策是如何形成的？答案是，大脑中有两个系统在起着作用。\",\"大脑的决策系统\",\"行为经济学是一个结合了经济学和心理学的学科，它帮助我们理解人们在现实世界中的行为。丹尼尔·卡尼曼和阿莫斯·特沃斯基提出了双系统决策模型，该模型有助于我们更有效地理解用户行为和设计。\",\"快速系统（系统1）：这是一个无意识、本能和情绪化的系统，它自动、快速地工作，消耗的能量很少。这个系统负责处理日常生活中的简单决策，如刷牙或浏览社交媒体。\",\"慢速系统（系统2）：这是一个有意识、需要集中精力的系统，它消耗的能量较多。这个系统负责处理复杂的决策，如invest决策或solve复杂的数学问题。\",\"如何将决策心理学应用于用户体验设计\",\"理解这两个决策系统对于用户体验设计师来说至关重要。以下是一些将决策心理学应用于设计的策略：\",\"Forecast用户反应：设计接口时，应考虑到系统1的快速、直观的反应。例如，Netflix的recommendation算法快速向用户展示基于其观看历史的选项，从而实现快速、直观的决策。\",\"支持深思熟虑的过程：提供清晰、详细的信息来支持系统2的分析需求。例如，Airbnb提供详细的图片和房东信息，帮助旅行者做出明智的住宿选择。\",\"减轻认知偏差：意识到潜在的偏差，并设计以抵消它们。例如，Notion提供模板和指南，帮助用户避免因选择过多而导致的决策疲劳。\",\"情境意识：了解用户的情境，并根据情况提供信息。例如，Spotify使用情境数据提供个性化的播放列表和recommendation，增强听众的体验。\",\"用户体验设计师的决策\",\"作为用户体验设计师，我们自身也受到双系统理论的影响。为了确保公正性，我们应该分析并了解用户的情境，确定他们需要立即了解的信息，以及可以稍后了解的信息，并将信息分解成可消化的块。这种方法有助于创建能够有效引导用户决策的设计。\",\"总结\",\"了解大脑的决策过程对于用户体验设计师来说至关重要。通过应用决策心理学的原则，我们可以设计出更有效、更以人为本的产品和服务，从而提升用户体验。设计师可以通过使用多样化的用户测试组来避免认知偏差，确保设计决策基于客观数据而非个人偏见。在设计过程中，应避免隐藏信息或使内容过于复杂，以防止用户做出非知情决策。此外，设计师应保持开放的心态，接受来自不同背景的反馈，以便从多个角度审视设计。\"]},\"533\":{\"h\":\"用艺术思维革新商业——设计思维vs艺术思维\",\"t\":[\" 在不断发展的商业和技术格局中，艺术的作用越来越重要。ChatGPT 背后的 OpenAI 等公司正在拥抱 Alex Reben 这样的艺术家，标志着商业创新的重大转变。将艺术融入技术进步不仅仅是为了美学，而是为了促进颠覆性创新。\",\"艺术思维是由 John Maeda 等人物倡导的概念，与更结构化的设计思维方法不同。设计思维侧重于以用户为重点的实际问题解jue，而艺术思维则强调创造性的探索和质疑。它是关于询问而不是解决、探索而不是深化、质疑现实而不是接受现实。\",\"这种创新方法体现在 Airbnb 商业模式中，该模式由美术专业毕业生 Brian Chesky 构想。他的艺术方法不仅改变了酒店业，还重新定义了人们如何看待和与家的概念互动。\",\"同样，Waze 和谷歌地图进入我们日常生活也说明了艺术思维如何挑战和改变社会规范。出租车司机 Seu Lauro 曾以熟悉自己所在城市的每条街道而自豪，但他现在开始质疑世界对记忆的依赖。Simon Weckert 将99部连接到Google Maps的智能手机地图上显示了一个严重的交通拥堵点，但实际上那里并没有人，进一步凸显了质疑现实的力量。\",\"艺术与技术的融合，尤其是通过艺术思维的视角，不仅是一种趋势，也是企业在AI时代保持领先地位的必要条件。这是关于开辟新道路，而不是深化现有道路。Airbnb 的例子以及 Waze 和谷歌地图对社会的影响凸显了质疑和创新的变革力量。\",\"归根结底，商业和艺术的世界并没有那么截然不同。两者都在寻求以创新和有意义的方式创造、改变和影响社会。工作和创新的未来可能确实需要一种新的语言，或者两种。\",\"在我们进入AI时代时，接受艺术思维对质疑现实和创造新可能性的强调至关重要。未来的旅程不仅仅是关于答案，而是关于我们提出的问题，这些问题将引领我们走向更有趣的现实。\",\"总之，通过艺术思维将艺术与技术融合不仅仅是一种时尚，也是企业在AI时代蓬勃发展的必要条件。这是关于突破创造力的界限，质疑现状，创造新的现实。\",\"在各行各业中，艺术思维可以通过引入新颖的视角和方法来激发创新。例如，在产品设计领域，除了解决问题的设计思维还可以通过艺术思维探索用户的情感需求，创造出更具有吸引力的产品。在市场营销领域，可以通过艺术思维来构建独特的品牌形象，吸引消费者的注意力。艺术思维也可以帮助团队或组织应对未来的挑战，因为它鼓励人们从不同的角度思考问题，从而发现新的解决方案。特别是在面对复杂和不确定的情况时，艺术思维能够激发团队成员的创造力，帮助他们找到非传统的解决方向。\",\"我个人并不特别倾向某种思维，而是更应该去互补，和我之前提到的知识诅咒类似，有时候应该将自己跳脱出来，从另一个视角去看待事物。以前有时候我做的工作很杂，看着很全能，其实会花费很多时间，也让我惯于在某一条线上持续一段时间，当你停下来的时候，转换思维转换角色，有时候就会收获良多。假如你是一个设计师，当你和客户汇报方案时，你便是一个销售的角色，你也可以从销售思维你理解你做设计的优劣势，痛点在哪，又如何进步，诸如此类。时代变革太快，接触不同的思维也可以更好适应不稳定的环境。\",\"部分案例参考来自 Art Thinking: creating desirable futures in the AI era. ---Carolina Guimarãesa. ---Carolina Guimarães\"]},\"534\":{\"h\":\"聊聊建筑转行\",\"t\":[\"契机： 18年大三实习的时候，被行业鞭策了几个月，自毕业设计开始已经决定转行，也希望通过一个独一无二的作品来打开我的转行道路。很幸运也很感谢当时带我毕业设计的老师，当时做了一个开放性的思辨设计项目，利用叙事跳脱了传统的建筑学科边界，最后也很幸运成为了优秀作品被学院留存在教学楼，在毕业展被许多媒体传播和观众喜爱。\",\"心得总结： 一，选对一个课题和适合你的导师非常重要，同时在适当的时候坚持自己的观点，特别对于美院学生，大学也就这么一次机会能够真正地表达你的思维。\",\"二，我把毕业设计当成一门“生意”去看待，我先确定了我的目标人群是大众，再来才是业内人士，因为本身我选择了一个冒险的开题方式，能不能得到各评委老师的认同我在前期是无法预知的（当然能做好这一点肯定也很重要），我更希望透过毕业展的传播，可以打开更多方向，所以从开题到最后的成果，我从一开始就得有个框架了，如果最后一步“营销”没有做好，那这个模型就散了。所以我基本抓住这一点，假如中间项目崩盘了，我也得让我的成果能最大化在展览现场表现。\",\"第一次工作跨界： 当时有头部的建筑景观公司找到我，都推辞了，包括我也放弃了本校保研的资格。虽然脱离了建筑景观行业，但还是和空间设计相关，比如舞台，装置，灯光，美陈，宴会等等。也是因为毕业设计的思维被挖掘，刚开始算是有挺多项目资源，无奈不到一年，yq就出现了，而且对于这类行业影响甚大。\",\"心得总结： 一，为什么会选择这类行业？因为美院学生在当时对于一些审美要求的行业还是挺吃香，再者，刚毕业也还怀着许多成为大设计师的落地理想，加上这类行业的项目周期短可以快速积累项目，而且那几年确实也有一定红利期。\",\"二，从商业思维，资源环境角度，我更看重设计之外的学习，比如我有许多机会可以和客户打交道，讲方案，学会销售自己的设计，控制利润成本等等。相比同履历的设计背景同学，这类经历在当时我会多更多。\"]},\"535\":{\"h\":\"设计界的新魔咒：知识诅咒\",\"t\":[\"🎨【设计界的新魔咒：知识诅咒】\",\"🔍最近get了一个词叫“知识诅咒”。有时候，我们的专业知识反而成了沟通的障碍，比如当设计师深陷用户行为的迷宫，却忘了门外的人可能还在找入口。这种现象阻碍了有效沟通，尤其在跨学科合作中显得尤为突出。\",\"💡想象一下，你在用户测试中发现了那些看似反常的行为，兴奋地想要分享，但团队的反应却像是听天书。这就是知识诅咒在悄悄起作用。当我们过于沉浸在专业领域，往往忘记用普通人能理解的方式去表达。\",\"💡解决之道在于“翻译”你的专业知识。用故事代替术语，用直观的示意图替换复杂的流程。比如，不要说“信息架构”，要说“如何让用户像逛公园一样轻松导航你的产品”。\",\"🏆下次当你感觉团队对你的眼中洞察无感时，别忘了，有效的沟通比专业知识本身更重要。\",\"💡知识诅咒的本质是信息不对等，它限制了创新的潜力。在快速发展的未来，每个人除了精通自己的专业，更需要具备跨学科的视角。例如，一个传统设计师将数据科学、人工智能（AI）和用户体验（UX）融合，同时需要能够用非技术人员的语言解释复杂的技术概念。🌐想象一下，一个设计师正在尝试将AI技术融入产品设计。如果他或她不能有效地解释机器学习如何改善用户体验，那么团队可能无法完全理解和接纳这个想法。因此，设计师需要学会“翻译”技术术语，转化为直观的故事和视觉表现。\",\"🛠️其实具备跨学科能力也是打破知识诅咒的一种方式。\",\"🌱跨学科合作的关键在于共情和沟通。跳出舒适区，学习其他领域的基础知识，才能更好地与工程师、产品经理甚至市场营销人员对话。也更容易共同构建更加人性化、智能且富有创新的产品和服务。\",\"🔻最后，\",\"▪️每当你觉得“为什么他们不懂？”的时候，就是你需要转换视角，用更接地气的方式去沟通的时候。\",\"▪️设计是为了连接人，而非隔离。\",\"▪️人人都应该拥有“设计能力”，而非设计师独有。\"]},\"536\":{\"h\":\"设计师的未来\",\"t\":[\" 简单翻译图片内容就是，2019年yq前UX用户体验行业开始越发成熟并成为主流，2020-2022年yq期间，大批新人进入这个行业，充斥着数字化内容和用户体验设计，当下从2023开始，AI带来的挑战和行业裁员让设计师陷入一个黑暗期，而往后对于设计行业可能面临越来越不平等的市场竞争，单一的输出型岗位、停留在传统设计规则上的人已经很难在市场立足。\"]},\"537\":{\"h\":\"心得体会\",\"t\":[\"作为一个多次经历跨界的设计师，也想分享一点心得。的确，设计行业正在经历一场深刻的变革，由于全球经济环境的变化和自动化技术的发展，设计师们面临着前所未有的挑战。但我认为这仍是一个机会，可以帮助推动设计领域向更商业化方向发展。此时，跨学科的能力和对AI技术的理解也将成为竞争力的关键，以往的设计创新可能更多地依赖于个人技能和创意，而未来，将会转向为更加系统化、以数据驱动的策略。\",\"我们团队本身包括和其他创业者的交流，都是非常认可“复合型人才”这个概念，和之前所提的“超级个体”也有异曲同工的意义，渐渐地，跨学科可能不再是一个绝对的优势，而是一个必备的能力，缺乏交叉思维的人可能也会因为这种变化导扩大了在细分领域上的差距。设计不再局限于所谓的“造型”领域，而是需要结合实际商业价值，设计师也要学会理解产品经济学和商业模式，以一个策略、系统、运营的思维在调动你的设计产出。\",\"参考内容来自互联网 Jan Takacs\"]},\"538\":{\"h\":\"参考理念\",\"t\":[\"如果你是一名设计师，现在浏览互联网，就会发现这很困难。似乎一切都在分崩离析，因为我们的收件箱和信息流中充满了令人震惊的头条新闻，例如“用户体验已死，我们杀死了它”、“设计师正面临生存危机”，甚至“设计界的大恐慌：设计”领导者正在努力应对他们的未来”、“我所有的朋友都放弃了设计”等等。难怪许多人都在重新审视自己的职业选择和行业未来。但这波悲观情绪只是一种噪音，还是确实有更深刻的事情正在发生？\",\"设计师现在面临着一个关键的适应时期，这将决定该行业的下一个十年及以后的发展。有几个新的、严酷的现实值得我们注意，因为它们影响着全球设计生态系统中的每个人，无论我们喜欢与否。主要的问题：\",\"新的市场条件要求我们从纯粹以客户为中心的方法转向更加以商业为中心的方法，在这一过程中，对商业模式和产品经济学的理解和贡献与创造力和同理心同样重要。\",\"现在的关键是影响，而不是过程。盲目追求双钻和坚持按部就班的严格流程都是行不通的，因为现实情况要混乱得多，如今的数字产品很少是以线性、完美的方式构建的。(注：自我介绍、作品集或案例研究也是如此。没有人会对那些80%以上都是关于过程的案例研究感兴趣。影响会把过程当做早午餐）。\",\"现在的构思是最便宜的（这得益于新的人工智能技术），而且如今只有 \\\"设计 \\\"概念不会让人惊叹。无论是个人还是团队，越来越有必要（也越来越有利可图）成为产品的创造者，而不仅仅是 \\\"像素推手\\\"。\",\"一些行业正在崛起，一些行业正在衰落。不可能每家公司都需要（数字）设计（以及设计师）。这意味着会有新的领域和行业需要探索，也会有可能性消失和丧失。\",\"普通人已经无法胜任。由于市场的大量饱和，现在需要更多的奉献精神、独特性和专业性才能脱颖而出。机会仍然很多，只是现在需要付出更多的努力才能获得。\"]},\"539\":{\"h\":\"利用AI提升电邮营销效能：五大优秀电邮生成器推荐\",\"t\":[\"摘要：本文讨论了人工智能在电子邮件生成中的应用，指出AI电子邮件助手如何通过自动化和优化过程提高效率和效果。这些工具能够个性化邮件、优化主题行和发送时间，从而提升打开率和转化率。文章列举了一些顶级的AI电子邮件生成器，如Rytr、Jasper、Copy.ai、Flowrite和Hypotenuse，并提供了它们的功能和定价信息。\",\"关键词：人工智能、电子邮件生成器、自动化、个性化、效率提升\",\"发现问题：编写有效电子邮件需要时间和技巧，可能导致低转化率和客户流失。\",\"解决方案：利用AI电子邮件助手软件，这些工具基于机器学习和自然语言处理，能自动化创建、优化邮件内容，提高邮件质量和针对性。\",\"结果：AI电子邮件生成器帮助用户节省时间，提高电子邮件质量，提升打开率和参与度，促进业务增长。不同工具如Rytr、Jasper等提供了多种功能和价格选项，以适应不同用户的需求。 文章涉及的领域：信息技术/人工智能应用\",\"关键知识点：\",\"人工智能电子邮件生成器 (AI email generators): 这是利用人工智能技术自动生成电子邮件内容的工具，通过机器学习和自然语言处理来提高效率和质量。\",\"自然语言处理 (Natural Language Processing, NLP): 一种计算机科学领域，专注于使计算机理解和生成人类语言，用于此处的电子邮件生成。\",\"机器学习 (Machine Learning, ML): 人工智能的一个子领域，使系统能够从数据中学习和改进，用于电子邮件生成器以优化内容。\",\"A/B 测试 (A/B testing): 在市场营销中，通过对比两种或多种变体来确定哪种策略更有效，此处用于优化电子邮件的主题行、行动号召等。\",\"个性化 (Personalization): 根据用户或客户的特定需求和偏好定制内容，AI电子邮件生成器能根据客户数据创建定制的电子邮件。 Q1: 人工智能电子邮件生成器如何帮助提高电子邮件的打开率和参与度？ A1: 人工智能电子邮件生成器通过分析用户需求、优化主题行、个性化内容以及根据收件人的兴趣和行为进行细分，来提高电子邮件的打开率和参与度。它们还能进行A/B测试，找出最有效的邮件版本。\",\"Q2: 除了电子邮件，这些AI生成器是否还能用于创建其他类型的内容？ A2: 是的，许多AI电子邮件生成器如Rytr、Jasper和Copy.ai不仅限于电子邮件，还可以用于创建博客文章、社交媒体帖子、广告文案等各种类型的营销和非营销内容。\",\"Q3: 使用AI电子邮件生成器是否真的能节省时间和提高效率？ A3: 根据文章中的信息，使用AI电子邮件生成器可以显著减少编写和优化电子邮件所需的时间，同时通过自动化和学习过程提高效率。它们能够快速生成高质量内容，这对于需要大量撰写电子邮件的个人和团队特别有用。\",\"原文地址：https://www-forbes-com.translate.goog/sites/technology/article/ai-email-generators/?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=auto&_x_tr_pto=wapp\"]},\"540\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"除了电子邮件\",{\"1\":{\"539\":1}}],[\"除了解决问题的设计思维还可以通过艺术思维探索用户的情感需求\",{\"1\":{\"533\":1}}],[\"找出最有效的邮件版本\",{\"1\":{\"539\":1}}],[\"找到突破口\",{\"1\":{\"509\":1}}],[\"找到不同的商业应用环境\",{\"1\":{\"507\":1}}],[\"测试\",{\"1\":{\"539\":1}}],[\"测试结果亮点\",{\"0\":{\"88\":1}}],[\"编写有效电子邮件需要时间和技巧\",{\"1\":{\"539\":1}}],[\"编程相关\",{\"1\":{\"8\":1}}],[\"效率提升\",{\"1\":{\"539\":1}}],[\"摘要\",{\"1\":{\"539\":1}}],[\"独特性和专业性才能脱颖而出\",{\"1\":{\"538\":1}}],[\"普通人已经无法胜任\",{\"1\":{\"538\":1}}],[\"越来越有必要\",{\"1\":{\"538\":1}}],[\"影响会把过程当做早午餐\",{\"1\":{\"538\":1}}],[\"影响个体和群体的创新活动\",{\"1\":{\"524\":1}}],[\"完美的方式构建的\",{\"1\":{\"538\":1}}],[\"完成项目测试\",{\"1\":{\"76\":1}}],[\"完成了这个作品\",{\"1\":{\"23\":1}}],[\"盲目追求双钻和坚持按部就班的严格流程都是行不通的\",{\"1\":{\"538\":1}}],[\"严酷的现实值得我们注意\",{\"1\":{\"538\":1}}],[\"造型\",{\"1\":{\"537\":1}}],[\"造梦\",{\"0\":{\"64\":1}}],[\"造梦城又再次升起了围墙\",{\"1\":{\"73\":1}}],[\"造梦城灯光闪亮\",{\"1\":{\"73\":1}}],[\"造梦城诞生了\",{\"1\":{\"60\":1}}],[\"造梦城开始诞生\",{\"1\":{\"58\":1}}],[\"造梦城的诞生也许成为城市里的一颗最美星星\",{\"1\":{\"45\":1}}],[\"造梦城内外的反差\",{\"1\":{\"45\":1}}],[\"造梦城\",{\"1\":{\"17\":2,\"23\":1,\"43\":5,\"74\":1,\"530\":1}}],[\"缺乏交叉思维的人可能也会因为这种变化导扩大了在细分领域上的差距\",{\"1\":{\"537\":1}}],[\"渐渐地\",{\"1\":{\"537\":1}}],[\"停留在传统设计规则上的人已经很难在市场立足\",{\"1\":{\"536\":1}}],[\"停在了一束光中\",{\"1\":{\"46\":1}}],[\"单一的输出型岗位\",{\"1\":{\"536\":1}}],[\"充斥着数字化内容和用户体验设计\",{\"1\":{\"536\":1}}],[\"充满了想象力\",{\"1\":{\"43\":1}}],[\"▪️人人都应该拥有\",{\"1\":{\"535\":1}}],[\"▪️设计是为了连接人\",{\"1\":{\"535\":1}}],[\"▪️每当你觉得\",{\"1\":{\"535\":1}}],[\"跳出舒适区\",{\"1\":{\"535\":1}}],[\"跳入这个可以沉入水底的橡胶筒中\",{\"1\":{\"68\":1}}],[\"别忘了\",{\"1\":{\"535\":1}}],[\"别妨碍我工作了\",{\"1\":{\"46\":1}}],[\"🌱跨学科合作的关键在于共情和沟通\",{\"1\":{\"535\":1}}],[\"🌐想象一下\",{\"1\":{\"535\":1}}],[\"🏆下次当你感觉团队对你的眼中洞察无感时\",{\"1\":{\"535\":1}}],[\"🎨\",{\"1\":{\"535\":1}}],[\"翻译\",{\"1\":{\"535\":2}}],[\"往往忘记用普通人能理解的方式去表达\",{\"1\":{\"535\":1}}],[\"往灯塔上攀爬着\",{\"1\":{\"65\":1}}],[\"兴奋地想要分享\",{\"1\":{\"535\":1}}],[\"🔻最后\",{\"1\":{\"535\":1}}],[\"🛠️其实具备跨学科能力也是打破知识诅咒的一种方式\",{\"1\":{\"535\":1}}],[\"💡知识诅咒的本质是信息不对等\",{\"1\":{\"535\":1}}],[\"💡解决之道在于\",{\"1\":{\"535\":1}}],[\"💡想象一下\",{\"1\":{\"535\":1}}],[\"🔍最近get了一个词叫\",{\"1\":{\"535\":1}}],[\"尤其在跨学科合作中显得尤为突出\",{\"1\":{\"535\":1}}],[\"尤其是通过艺术思维的视角\",{\"1\":{\"533\":1}}],[\"尤其是在非临床的情况下\",{\"1\":{\"82\":1}}],[\"尤其是在过去的十年\",{\"1\":{\"35\":1}}],[\"尤其是儿童\",{\"1\":{\"81\":1}}],[\"却忘了门外的人可能还在找入口\",{\"1\":{\"535\":1}}],[\"控制利润成本等等\",{\"1\":{\"534\":1}}],[\"讲方案\",{\"1\":{\"534\":1}}],[\"讲述了2199年世界被垃圾堆满\",{\"1\":{\"46\":1}}],[\"讲述了在未来世界生活在废墟上的孩子们的故事\",{\"1\":{\"43\":1}}],[\"刚毕业也还怀着许多成为大设计师的落地理想\",{\"1\":{\"534\":1}}],[\"刚开始算是有挺多项目资源\",{\"1\":{\"534\":1}}],[\"宴会等等\",{\"1\":{\"534\":1}}],[\"宴会设计师\",{\"1\":{\"6\":1}}],[\"选对一个课题和适合你的导师非常重要\",{\"1\":{\"534\":1}}],[\"选择这一行业的主要原因是不仅仅停留于后端设计的角色\",{\"1\":{\"91\":1}}],[\"契机\",{\"1\":{\"534\":1}}],[\"部分案例参考来自\",{\"1\":{\"533\":1}}],[\"诸如此类\",{\"1\":{\"533\":1}}],[\"假如中间项目崩盘了\",{\"1\":{\"534\":1}}],[\"假如你是一个设计师\",{\"1\":{\"533\":1}}],[\"假设与疑问\",{\"0\":{\"53\":1}}],[\"转化为直观的故事和视觉表现\",{\"1\":{\"535\":1}}],[\"转化率与留存率提升\",{\"1\":{\"528\":1}}],[\"转化率=访问者\",{\"1\":{\"521\":1}}],[\"转化率\",{\"1\":{\"521\":1}}],[\"转换思维转换角色\",{\"1\":{\"533\":1}}],[\"总之\",{\"1\":{\"533\":1}}],[\"总结了一下\",{\"1\":{\"532\":1}}],[\"总结来说\",{\"1\":{\"519\":1}}],[\"总结\",{\"0\":{\"82\":1},\"1\":{\"519\":1,\"532\":1}}],[\"总结整个过程就是由文本拼贴到线稿剖面画面\",{\"1\":{\"43\":1}}],[\"归根结底\",{\"1\":{\"533\":1}}],[\"曾以熟悉自己所在城市的每条街道而自豪\",{\"1\":{\"533\":1}}],[\"曾经被灾难分开的岛屿成为了孩子们把垃圾变废为宝的魔法屋\",{\"1\":{\"23\":1}}],[\"质疑现状\",{\"1\":{\"533\":1}}],[\"质疑现实而不是接受现实\",{\"1\":{\"533\":1}}],[\"质量和创新决策等方面的需求\",{\"1\":{\"519\":1}}],[\"质量\",{\"1\":{\"79\":1}}],[\"标志着商业创新的重大转变\",{\"1\":{\"533\":1}}],[\"标准化程度低\",{\"1\":{\"36\":1}}],[\"背后的\",{\"1\":{\"533\":1}}],[\"接触不同的思维也可以更好适应不稳定的环境\",{\"1\":{\"533\":1}}],[\"接受艺术思维对质疑现实和创造新可能性的强调至关重要\",{\"1\":{\"533\":1}}],[\"接受来自不同背景的反馈\",{\"1\":{\"532\":1}}],[\"接下来我们来思考ai时代下我们该如何建立个人技能树\",{\"1\":{\"518\":1}}],[\"接下来的考虑是如何通过增强现实来增加乐趣\",{\"1\":{\"88\":1}}],[\"确定他们需要立即了解的信息\",{\"1\":{\"532\":1}}],[\"确保设计决策基于客观数据而非个人偏见\",{\"1\":{\"532\":1}}],[\"确保它们能够适应市场的变化和用户的需求\",{\"1\":{\"525\":1}}],[\"确保系统的可靠性和适应性\",{\"1\":{\"519\":1}}],[\"减轻认知偏差\",{\"1\":{\"532\":1}}],[\"减少主观性\",{\"1\":{\"521\":1}}],[\"减少技术依赖并提高决策效率\",{\"1\":{\"519\":1}}],[\"减少用户注意力转移的需求\",{\"1\":{\"517\":1}}],[\"详细的信息来支持系统2的分析需求\",{\"1\":{\"532\":1}}],[\"支持深思熟虑的过程\",{\"1\":{\"532\":1}}],[\"直观的决策\",{\"1\":{\"532\":1}}],[\"直观的反应\",{\"1\":{\"532\":1}}],[\"直接影响消费者的购买决策和使用体验\",{\"1\":{\"531\":1}}],[\"慢速系统\",{\"1\":{\"532\":1}}],[\"丹尼尔\",{\"1\":{\"532\":1}}],[\"丹麦语\",{\"1\":{\"48\":1}}],[\"丹麦哥本哈根emdrup的第一个冒险乐园被称为\",{\"1\":{\"48\":1}}],[\"丹麦景观设计师carl\",{\"1\":{\"48\":1}}],[\"答案是\",{\"1\":{\"532\":1}}],[\"据研究者称\",{\"1\":{\"532\":1}}],[\"据说100多年前出现了一个不为人知的禁地\",{\"1\":{\"23\":1}}],[\"环保材料和可持续生产方式来提升品牌形象\",{\"1\":{\"531\":1}}],[\"环境污染和气候变化等问题\",{\"1\":{\"531\":1}}],[\"环境污染\",{\"1\":{\"531\":1}}],[\"环境\",{\"1\":{\"522\":2}}],[\"营销\",{\"1\":{\"534\":1}}],[\"营销策略\",{\"1\":{\"531\":1}}],[\"营销策略和用户体验设计等手段\",{\"1\":{\"531\":1}}],[\"营销渠道\",{\"1\":{\"531\":1}}],[\"定义范围\",{\"1\":{\"531\":1}}],[\"定位得更加精准一些\",{\"1\":{\"21\":1}}],[\"依赖技术\",{\"1\":{\"531\":1}}],[\"吸引消费者的注意力\",{\"1\":{\"533\":1}}],[\"吸引更多消费者\",{\"1\":{\"531\":1}}],[\"吸引潜在客户并最终实现销售增长\",{\"1\":{\"531\":1}}],[\"吸引力和满意度\",{\"1\":{\"522\":1}}],[\"供应链剥削\",{\"1\":{\"531\":1}}],[\"供应商以及公司内部团队协调等等\",{\"1\":{\"92\":1}}],[\"供应商沟通确认\",{\"1\":{\"92\":1}}],[\"加剧了这些问题\",{\"1\":{\"531\":1}}],[\"加上这类行业的项目周期短可以快速积累项目\",{\"1\":{\"534\":1}}],[\"加上之前产生一些不良效应\",{\"1\":{\"29\":1}}],[\"加上我这时候发布的内容也多是3d建模的数字作品\",{\"1\":{\"20\":1}}],[\"诱导消费者进行过度消费\",{\"1\":{\"531\":1}}],[\"顾名思义\",{\"1\":{\"531\":1}}],[\"洞察消费设计与数字营销\",{\"0\":{\"531\":1}}],[\"便是一个能立得住脚的观点了\",{\"1\":{\"530\":1}}],[\"便能去到心中的新世界\",{\"1\":{\"46\":1}}],[\"痛点在哪\",{\"1\":{\"533\":1}}],[\"痛点\",{\"1\":{\"530\":2}}],[\"寻找结合点\",{\"1\":{\"530\":1}}],[\"寻找世界上的客观事件\",{\"1\":{\"530\":1}}],[\"理性和感性的叠加\",{\"1\":{\"530\":1}}],[\"理解这两个决策系统对于用户体验设计师来说至关重要\",{\"1\":{\"532\":1}}],[\"理解是什么使得这个公司能够在市场上取得成功或保持竞争优势\",{\"1\":{\"525\":1}}],[\"理解不同文化对用户行为和需求的影响\",{\"1\":{\"522\":1}}],[\"理解并适应这种系统的动态变化至关重要\",{\"1\":{\"519\":1}}],[\"理解大脑如何做出选择\",{\"0\":{\"532\":1},\"1\":{\"502\":1}}],[\"理解对象可以组织成逻辑顺序以及执行包含任务\",{\"1\":{\"79\":1}}],[\"切记你是以一个设计师的视角去挖掘现象\",{\"1\":{\"530\":1}}],[\"置身进入你zui想去的世界\",{\"1\":{\"530\":1}}],[\"把自己当成小说家\",{\"1\":{\"530\":1}}],[\"把前面每个点通过一张剖面表达来链接带动\",{\"1\":{\"43\":1}}],[\"抽象和感性\",{\"1\":{\"530\":1}}],[\"抽象的词少用\",{\"1\":{\"521\":1}}],[\"篇幅太长\",{\"1\":{\"530\":1}}],[\"轻量化数据设计\",{\"1\":{\"529\":1}}],[\"决策支持与流程优化\",{\"1\":{\"529\":1}}],[\"课程内容与互动元素优化\",{\"1\":{\"528\":1}}],[\"课程知识可以帮我提升的内容\",{\"1\":{\"523\":1}}],[\"阅读偏好理解\",{\"1\":{\"528\":1}}],[\"购物车放弃率识别\",{\"1\":{\"528\":1}}],[\"购买\",{\"1\":{\"521\":1}}],[\"浏览习惯研究\",{\"1\":{\"528\":1}}],[\"浏览→购买\",{\"1\":{\"521\":1}}],[\"界面与交互调整\",{\"1\":{\"528\":1}}],[\"界面设计\",{\"1\":{\"521\":1}}],[\"行动号召等\",{\"1\":{\"539\":1}}],[\"行为经济学是一个结合了经济学和心理学的学科\",{\"1\":{\"532\":1}}],[\"行为跟踪\",{\"1\":{\"528\":1}}],[\"行业变化深度学习\",{\"1\":{\"529\":1}}],[\"行业专业人士和政策制定者联手制定了一个全面的设计教育改革蓝图\",{\"1\":{\"524\":1}}],[\"行业思考\",{\"0\":{\"502\":1}}],[\"行业经历\",{\"0\":{\"6\":1}}],[\"初创公司的产品优化\",{\"1\":{\"528\":1}}],[\"几个思考的关键点\",{\"1\":{\"527\":1}}],[\"几个满脸胡子的老人突然出现\",{\"1\":{\"46\":1}}],[\"恰到好处的数据\",{\"1\":{\"527\":2}}],[\"恰逢当时因为我的毕设收到活动邀请\",{\"1\":{\"20\":1}}],[\"微小的改进足以保持竞争力\",{\"1\":{\"525\":1}}],[\"微博没有浏览记录\",{\"1\":{\"521\":1}}],[\"何时需要颠覆性创新\",{\"1\":{\"525\":1}}],[\"线性与非线性竞争\",{\"1\":{\"525\":1}}],[\"考虑设计如何影响整个系统\",{\"1\":{\"525\":1}}],[\"考虑整个社会文化背景和生态景观\",{\"1\":{\"522\":1}}],[\"考虑整个社会群体的需求和期望\",{\"1\":{\"522\":1}}],[\"复合型人才\",{\"1\":{\"537\":1}}],[\"复杂性降低\",{\"1\":{\"529\":1}}],[\"复杂系统的理解\",{\"1\":{\"525\":1}}],[\"复盘了可能在当时有着这波nft与元宇宙的营销热潮\",{\"1\":{\"31\":1}}],[\"了解大脑的决策过程对于用户体验设计师来说至关重要\",{\"1\":{\"532\":1}}],[\"了解用户的情境\",{\"1\":{\"532\":1}}],[\"了解商业模式对于设计师来说非常重要\",{\"1\":{\"525\":1}}],[\"了解不同的形状可以帮助儿童更好地理解日常生活中的物体和环境\",{\"1\":{\"81\":1}}],[\"给传统设计师们的启发\",{\"1\":{\"525\":1}}],[\"打破竞争模式\",{\"1\":{\"525\":1}}],[\"打造了一个ip\",{\"1\":{\"507\":1}}],[\"打造成一个3d数字设计\",{\"1\":{\"20\":1}}],[\"核心竞争力等\",{\"1\":{\"525\":1}}],[\"核心资产\",{\"1\":{\"525\":1}}],[\"改变和影响社会\",{\"1\":{\"533\":1}}],[\"改进现有业务模型或创建新的商业模式至关重要\",{\"1\":{\"525\":1}}],[\"改到了室内\",{\"1\":{\"26\":1}}],[\"哪些因素驱动了它的增长和盈利能力\",{\"1\":{\"525\":1}}],[\"哪些需要被强调\",{\"1\":{\"521\":1}}],[\"逆向工程这一过程旨在理解业务是如何运作的\",{\"1\":{\"525\":1}}],[\"逆向工程业务指通过分析和解构一个公司的现有运作模式\",{\"1\":{\"525\":1}}],[\"逐步深入到公司的内部运作机制\",{\"1\":{\"525\":1}}],[\"逐步建立品牌形象\",{\"1\":{\"31\":1}}],[\"财务表现\",{\"1\":{\"525\":1}}],[\"品牌影响力等\",{\"1\":{\"525\":1}}],[\"品牌识别\",{\"1\":{\"521\":1}}],[\"即能够适应市场变化\",{\"1\":{\"525\":1}}],[\"即\",{\"1\":{\"525\":2}}],[\"即使看似普通也可能具有一种独特的吸引力\",{\"1\":{\"511\":1}}],[\"即使看似荒芜的地方也可能隐藏着故事和创意\",{\"1\":{\"511\":1}}],[\"鼓励在组织内部形成支持创新的行为规范\",{\"1\":{\"524\":1}}],[\"低于oecd平均值\",{\"1\":{\"524\":1}}],[\"低于oecd的平均值73\",{\"1\":{\"524\":1}}],[\"仅有64\",{\"1\":{\"524\":1}}],[\"仅仅是技术限制了当下的发展吗\",{\"1\":{\"42\":1}}],[\"显示出信心和勇气在创新中的重要性\",{\"1\":{\"524\":1}}],[\"显示在过去12个月中\",{\"1\":{\"518\":1}}],[\"致力于用设计思维解决实际问题\",{\"1\":{\"524\":1}}],[\"致力于将创意与技术相结合\",{\"1\":{\"14\":1}}],[\"努力化为乌有\",{\"1\":{\"523\":1}}],[\"黄帽\",{\"1\":{\"523\":1}}],[\"黄帽子\",{\"1\":{\"523\":1}}],[\"查资料\",{\"1\":{\"523\":1}}],[\"思维认知\",{\"1\":{\"523\":1}}],[\"思考过程\",{\"1\":{\"31\":1}}],[\"思考后\",{\"1\":{\"21\":1}}],[\"红帽\",{\"1\":{\"523\":1}}],[\"红帽子\",{\"1\":{\"523\":1}}],[\"帽子的顺序可以大部分情况可以自行决定\",{\"1\":{\"523\":1}}],[\"事实帽\",{\"1\":{\"523\":1}}],[\"事物的数量也保持不变\",{\"1\":{\"79\":1}}],[\"白帽\",{\"1\":{\"523\":1}}],[\"白帽代表信息收集\",{\"1\":{\"523\":1}}],[\"白帽子\",{\"1\":{\"523\":1}}],[\"白天的围墙隔离着造梦城\",{\"1\":{\"73\":1}}],[\"白天\",{\"1\":{\"53\":1}}],[\"白天黑夜的不同状态又将是一种父母与孩子的某种内心丝连\",{\"1\":{\"45\":1}}],[\"法官的帽子\",{\"1\":{\"523\":1}}],[\"黑帽\",{\"1\":{\"523\":2}}],[\"黑暗中让人无法看清\",{\"1\":{\"46\":1}}],[\"心得体会\",{\"0\":{\"537\":1}}],[\"心得总结\",{\"1\":{\"534\":2}}],[\"心脏之帽\",{\"1\":{\"523\":1}}],[\"心生梦境\",{\"1\":{\"24\":1}}],[\"戴上\",{\"1\":{\"523\":1}}],[\"戴盾\",{\"1\":{\"74\":1}}],[\"绿帽\",{\"1\":{\"523\":1}}],[\"绿帽代表创造性思维\",{\"1\":{\"523\":1}}],[\"绿帽子\",{\"1\":{\"523\":1}}],[\"蓝帽\",{\"1\":{\"523\":1}}],[\"蓝帽子\",{\"1\":{\"523\":1}}],[\"蓝图设想\",{\"0\":{\"60\":1}}],[\"粒度问题\",{\"1\":{\"522\":1}}],[\"采用多元设计方法\",{\"1\":{\"522\":1}}],[\"采梦\",{\"1\":{\"24\":1}}],[\"认知心理学的影响\",{\"1\":{\"522\":1}}],[\"认证\",{\"1\":{\"521\":1}}],[\"导致工人剥削\",{\"1\":{\"531\":1}}],[\"导致消费者陷入消费陷阱\",{\"1\":{\"531\":1}}],[\"导致他们在实际应用和自我评价时感到不确定\",{\"1\":{\"524\":1}}],[\"导致设计产品无法真正反映和满足整个群体的需求\",{\"1\":{\"522\":1}}],[\"导航设计\",{\"1\":{\"521\":1}}],[\"忽视社会文化背景\",{\"1\":{\"522\":1}}],[\"需求预测\",{\"1\":{\"529\":1}}],[\"需求和偏好\",{\"1\":{\"522\":1}}],[\"需要集中精力的系统\",{\"1\":{\"532\":1}}],[\"需要进一步查阅具体研究或报告\",{\"1\":{\"519\":1}}],[\"需要开放标准和互操作性\",{\"1\":{\"516\":1}}],[\"需要更加简化游戏部分\",{\"1\":{\"89\":1}}],[\"需要改善的部分\",{\"0\":{\"89\":1}}],[\"需要各种材料\",{\"1\":{\"46\":1}}],[\"需要去持续探讨的地方\",{\"1\":{\"43\":1}}],[\"任务之间的交互\",{\"1\":{\"522\":1}}],[\"任务之间的相互作用\",{\"1\":{\"522\":1}}],[\"目标一致\",{\"1\":{\"531\":1}}],[\"目标是建立一个鼓励创新的环境\",{\"1\":{\"524\":1}}],[\"目标\",{\"1\":{\"522\":2}}],[\"目前线上教学为主\",{\"1\":{\"4\":1}}],[\"目前是igo团队的合作专题导师\",{\"1\":{\"4\":1}}],[\"方法\",{\"1\":{\"522\":2,\"527\":2}}],[\"方法和角色如何适应\",{\"1\":{\"519\":1}}],[\"焦点\",{\"1\":{\"522\":2}}],[\"愉悦的用户体验\",{\"1\":{\"522\":1}}],[\"读书笔记和思考\",{\"1\":{\"522\":1}}],[\"读书总结\",{\"1\":{\"502\":1}}],[\"风格指南的重要性\",{\"1\":{\"521\":1}}],[\"风险管理\",{\"1\":{\"519\":1}}],[\"忠于眼睛\",{\"1\":{\"521\":1}}],[\"视觉落点与你的战略目标要素是否匹配\",{\"1\":{\"521\":1}}],[\"视觉设计\",{\"1\":{\"521\":1}}],[\"视觉元素\",{\"1\":{\"521\":1}}],[\"尽量减少不同人群的猜测\",{\"1\":{\"521\":1}}],[\"尽管面临挑战\",{\"1\":{\"527\":1}}],[\"尽管新加坡学生在pisa测试中表现出色\",{\"1\":{\"524\":1}}],[\"尽管设计领域试图超越对个体经验的关注\",{\"1\":{\"522\":1}}],[\"尽管音频增强现实带来了诸多便利\",{\"1\":{\"516\":1}}],[\"尽管这看起来像是徒劳无功的\",{\"1\":{\"46\":1}}],[\"尽管具体的未来我们也难以预测\",{\"1\":{\"31\":1}}],[\"容易混淆概念\",{\"1\":{\"521\":1}}],[\"容纳更大的范围\",{\"1\":{\"521\":1}}],[\"记住我们使用东西的习惯\",{\"1\":{\"521\":1}}],[\"记得橡胶筒水底的孩子吗\",{\"1\":{\"72\":1}}],[\"功能有效性评估\",{\"1\":{\"528\":1}}],[\"功能型产品\",{\"1\":{\"521\":1}}],[\"功能+视觉=综合形成\",{\"1\":{\"521\":1}}],[\"信息技术\",{\"1\":{\"539\":1}}],[\"信息架构\",{\"1\":{\"535\":1}}],[\"信息型产品\",{\"1\":{\"521\":1}}],[\"信息要素的排布\",{\"1\":{\"521\":1}}],[\"信息设计跨越这两者的边界\",{\"1\":{\"521\":1}}],[\"信息设计\",{\"1\":{\"521\":1}}],[\"传达给用户想法\",{\"1\":{\"521\":1}}],[\"传统销售模式可能因ai而失效\",{\"1\":{\"518\":1}}],[\"传统游戏与交互式电脑游戏的结合\",{\"1\":{\"81\":1}}],[\"传统ip授权模式的授权流程长\",{\"1\":{\"36\":1}}],[\"传统ip创作和运营的模式下\",{\"1\":{\"35\":1}}],[\"元素布局\",{\"1\":{\"521\":1}}],[\"元宇宙2023\",{\"1\":{\"42\":1}}],[\"元宇宙ip可能性拓展\",{\"0\":{\"25\":1}}],[\"元宇宙这个概念正火热于市场\",{\"1\":{\"22\":1}}],[\"元宇宙\",{\"1\":{\"21\":1,\"26\":1,\"29\":1}}],[\"俚语等等\",{\"1\":{\"521\":1}}],[\"缩写\",{\"1\":{\"521\":1}}],[\"补充同义词的表达\",{\"1\":{\"521\":1}}],[\"描述标签等等\",{\"1\":{\"521\":1}}],[\"命名原则\",{\"1\":{\"521\":1}}],[\"预见未来趋势\",{\"1\":{\"525\":1}}],[\"预知期望并纳入设计中\",{\"1\":{\"521\":1}}],[\"预期的符合\",{\"1\":{\"521\":1}}],[\"某些分类方式的改变是否能够适配\",{\"1\":{\"521\":1}}],[\"某种历史的深藏\",{\"1\":{\"72\":1}}],[\"符合人的能力和限制的设计解决方案\",{\"1\":{\"522\":1}}],[\"符合谁的时尚\",{\"1\":{\"521\":1}}],[\"符合这个社会发展\",{\"1\":{\"43\":1}}],[\"评估各国学生的阅读\",{\"1\":{\"524\":1}}],[\"评估物体的稳定性和适用性等\",{\"1\":{\"81\":1}}],[\"评论数\",{\"1\":{\"521\":1}}],[\"播放量\",{\"1\":{\"521\":1}}],[\"受关注等等\",{\"1\":{\"521\":1}}],[\"受全球创意产业经验的影响\",{\"1\":{\"80\":1}}],[\"头像是否重要\",{\"1\":{\"521\":1}}],[\"头显不适合\",{\"1\":{\"81\":1}}],[\"概念不会让人惊叹\",{\"1\":{\"538\":1}}],[\"概念\",{\"0\":{\"527\":1}}],[\"概念系统\",{\"1\":{\"521\":1}}],[\"概念也成了大家连接物理和数字世界的一个钥匙\",{\"1\":{\"26\":1}}],[\"底层逻辑\",{\"1\":{\"521\":1}}],[\"范围层\",{\"1\":{\"521\":1}}],[\"达到目的\",{\"1\":{\"521\":1}}],[\"逻辑串联\",{\"1\":{\"521\":1}}],[\"布局和习惯\",{\"1\":{\"521\":1}}],[\"布局罗列\",{\"1\":{\"521\":1}}],[\"布景\",{\"1\":{\"5\":1}}],[\"框架层\",{\"1\":{\"521\":1}}],[\"表现层\",{\"1\":{\"521\":1}}],[\"具体化\",{\"1\":{\"521\":1}}],[\"具体→抽象\",{\"1\":{\"521\":1}}],[\"具足ip与nft运营\",{\"1\":{\"36\":1}}],[\"五大优秀电邮生成器推荐\",{\"0\":{\"539\":1}}],[\"五个层面\",{\"1\":{\"521\":1}}],[\"五颜六色\",{\"1\":{\"46\":1}}],[\"点击率\",{\"1\":{\"521\":1}}],[\"点进去还可以观看详情介绍\",{\"1\":{\"29\":1}}],[\"注\",{\"1\":{\"538\":1}}],[\"注册量\",{\"1\":{\"521\":1}}],[\"注意到\",{\"1\":{\"48\":1}}],[\"衡量用户体验\",{\"1\":{\"521\":1}}],[\"已内容为主的网络产品\",{\"1\":{\"521\":1}}],[\"已经施工完成\",{\"1\":{\"21\":1}}],[\"网站等\",{\"1\":{\"531\":1}}],[\"网站的具体功能\",{\"1\":{\"521\":1}}],[\"网站\",{\"1\":{\"521\":1}}],[\"战略层的人物形象→加入场景\",{\"1\":{\"521\":1}}],[\"战略层\",{\"0\":{\"521\":1},\"1\":{\"521\":1}}],[\"战争唤醒了人们对孩子内部混乱的担忧\",{\"1\":{\"49\":1}}],[\"赞收藏与粉丝比例\",{\"1\":{\"521\":1}}],[\"赞助或政府资助来维持运营\",{\"1\":{\"520\":1}}],[\"赞数据也算不错\",{\"1\":{\"20\":1}}],[\"另外\",{\"1\":{\"520\":1}}],[\"另一则是追溯了\",{\"1\":{\"530\":1}}],[\"另一次按颜色进行\",{\"1\":{\"79\":1}}],[\"另一个类型是找我聊ip合作的\",{\"1\":{\"21\":1}}],[\"健康等社会问题的关注度\",{\"1\":{\"520\":1}}],[\"满足人们提升心理适应性和抗压能力的愿望\",{\"1\":{\"520\":1}}],[\"拥抱学习和可持续发展\",{\"1\":{\"519\":1}}],[\"拥有一个美好的童年记忆等等也是需要持续关注的话题\",{\"1\":{\"43\":1}}],[\"快速地工作\",{\"1\":{\"532\":1}}],[\"快速系统\",{\"1\":{\"532\":1}}],[\"快速迭代能力\",{\"1\":{\"529\":1}}],[\"快速设计优化\",{\"1\":{\"529\":1}}],[\"快速测试假设\",{\"1\":{\"525\":1}}],[\"快速做出反应\",{\"1\":{\"525\":1}}],[\"快速学习和终身学习的需求\",{\"1\":{\"519\":1}}],[\"快闪店也是他们策划的一些新商业模式\",{\"1\":{\"28\":1}}],[\"形成更加全面的营销策略\",{\"1\":{\"531\":1}}],[\"形成了一种创新的工作方法\",{\"1\":{\"525\":1}}],[\"形成了\",{\"1\":{\"519\":1}}],[\"形态和市场定位\",{\"1\":{\"525\":1}}],[\"形状还与许多其他学科和技能密切相关\",{\"1\":{\"81\":1}}],[\"形状在儿童生活中的实际应用\",{\"1\":{\"81\":1}}],[\"形状对认知发展的影响\",{\"1\":{\"81\":1}}],[\"情境意识\",{\"1\":{\"532\":1}}],[\"情境分析\",{\"1\":{\"522\":1}}],[\"情境性和艺术性\",{\"1\":{\"78\":1}}],[\"情绪反应等等\",{\"1\":{\"521\":1}}],[\"情绪识别技术的应用以及信息的价值化等问题\",{\"1\":{\"519\":1}}],[\"互联网服务\",{\"1\":{\"519\":1}}],[\"互联网的web3生态形成仍然是任重而道远\",{\"1\":{\"42\":1}}],[\"旨在影响消费者的购买决策和品牌忠诚度\",{\"1\":{\"531\":1}}],[\"旨在通过缩短供应链周期来提高竞争力\",{\"1\":{\"519\":1}}],[\"旨在引发更多人对孩子们成长环境与教育方式的思考\",{\"1\":{\"17\":1}}],[\"解决方案方面\",{\"1\":{\"519\":1}}],[\"解决方案\",{\"1\":{\"519\":1,\"524\":1,\"539\":1}}],[\"解释\",{\"1\":{\"519\":5}}],[\"解释一下\",{\"1\":{\"510\":1}}],[\"关系\",{\"1\":{\"519\":2}}],[\"关键知识点\",{\"1\":{\"539\":1}}],[\"关键知识点和问题思考\",{\"1\":{\"524\":1}}],[\"关键指标指导设计\",{\"1\":{\"529\":1}}],[\"关键点\",{\"1\":{\"519\":2}}],[\"关键议题包括\",{\"1\":{\"519\":1}}],[\"关键词\",{\"1\":{\"516\":1,\"519\":1,\"539\":1}}],[\"消耗的能量很少\",{\"1\":{\"532\":1}}],[\"消费者可以在数字环境中体验产品\",{\"1\":{\"531\":1}}],[\"消费者和设计工程师的角色变得更加重要\",{\"1\":{\"519\":1}}],[\"消费设计与数字营销的未来将是一个不断进化\",{\"1\":{\"531\":1}}],[\"消费设计在消费者接触产品或服务时发挥作用\",{\"1\":{\"531\":1}}],[\"消费设计在推动过度消费的过程中\",{\"1\":{\"531\":1}}],[\"消费设计往往牺牲了供应链中工人的权益\",{\"1\":{\"531\":1}}],[\"消费设计通过算法\",{\"1\":{\"531\":1}}],[\"消费设计的影响\",{\"1\":{\"531\":1}}],[\"消费设计的定义与历史\",{\"1\":{\"531\":1}}],[\"消费设计\",{\"1\":{\"531\":1}}],[\"消防等等\",{\"1\":{\"92\":1}}],[\"组织层面的转型是确保这一目标的关键\",{\"1\":{\"519\":1}}],[\"企业可以与设计师\",{\"1\":{\"531\":1}}],[\"企业可以通过绿色设计\",{\"1\":{\"531\":1}}],[\"企业发展与社会进步紧密联系\",{\"1\":{\"520\":1}}],[\"企业以及非盈利组织建立合作关系\",{\"1\":{\"520\":1}}],[\"企业或许可以将其纳入员工福利计划\",{\"1\":{\"520\":1}}],[\"企业则需在追求利润的同时\",{\"1\":{\"519\":1}}],[\"企业能够应对复杂环境并实现高效制造\",{\"1\":{\"519\":1}}],[\"企业应考虑将音频ar纳入产品设计中\",{\"1\":{\"517\":1}}],[\"系统\",{\"1\":{\"537\":1}}],[\"系统2\",{\"1\":{\"532\":1}}],[\"系统1\",{\"1\":{\"532\":1}}],[\"系统甚至整个工作流程的设计\",{\"1\":{\"522\":1}}],[\"系统灵活性和可持续性设计\",{\"1\":{\"519\":1}}],[\"系统复杂性和不确定性增加\",{\"1\":{\"519\":1}}],[\"智能且富有创新的产品和服务\",{\"1\":{\"535\":1}}],[\"智能设计的未来洞察\",{\"1\":{\"529\":1}}],[\"智能制造在制造业中的应用也带来了一系列问题\",{\"1\":{\"519\":1}}],[\"智能制造和跨组织协调等问题\",{\"1\":{\"519\":1}}],[\"智能技术的应用提供了数据驱动的决策支持\",{\"1\":{\"519\":1}}],[\"智能x\",{\"1\":{\"519\":2}}],[\"随着技术的发展和社会的变化\",{\"1\":{\"531\":1}}],[\"随着消费者对环保和可持续发展的关注增加\",{\"1\":{\"531\":1}}],[\"随着现代社会压力的增加\",{\"1\":{\"520\":1}}],[\"随着互联网和个人物品的融合\",{\"1\":{\"519\":1}}],[\"随着industry\",{\"1\":{\"519\":1}}],[\"随着工业4\",{\"1\":{\"519\":2}}],[\"随后\",{\"1\":{\"78\":1}}],[\"机会仍然很多\",{\"1\":{\"538\":1}}],[\"机器学习\",{\"1\":{\"539\":1}}],[\"机器学习和其他数字技术来提升产品设计\",{\"1\":{\"519\":1}}],[\"机器\",{\"1\":{\"519\":1}}],[\"机构的负责人看到我的作品感兴趣\",{\"1\":{\"80\":1}}],[\"正在对设计工程\",{\"1\":{\"519\":1}}],[\"简单翻译图片内容就是\",{\"1\":{\"536\":1}}],[\"简单总结就是\",{\"1\":{\"509\":1}}],[\"简介\",{\"1\":{\"519\":1}}],[\"灵活设计原则\",{\"1\":{\"529\":1}}],[\"灵活的方向发展\",{\"1\":{\"519\":1}}],[\"灵活制造\",{\"1\":{\"519\":1}}],[\"灵感启发\",{\"0\":{\"517\":1}}],[\"灵感分析\",{\"0\":{\"504\":1}}],[\"动态风险管理\",{\"1\":{\"519\":1}}],[\"动植物也受到污染\",{\"1\":{\"23\":1}}],[\"集成与创新\",{\"1\":{\"519\":1}}],[\"集成\",{\"1\":{\"519\":1}}],[\"集成音频ar可以提供实时反馈和指导\",{\"1\":{\"517\":1}}],[\"危机在于对ai的过度依赖可能导致缺乏独立思考\",{\"1\":{\"518\":1}}],[\"危机在于管理技能可能因ai自动化而变得过时\",{\"1\":{\"518\":1}}],[\"研究\",{\"1\":{\"518\":1}}],[\"研究表明\",{\"1\":{\"81\":1}}],[\"团队响应速度与创新能力提升\",{\"1\":{\"529\":1}}],[\"团队都需要培养批判性思维和战略规划能力以应对变化\",{\"1\":{\"518\":1}}],[\"团队合作\",{\"1\":{\"518\":1}}],[\"问题在于推广曝光不足还是质量不够\",{\"1\":{\"521\":1}}],[\"问题\",{\"1\":{\"519\":1}}],[\"问题解决\",{\"1\":{\"518\":1}}],[\"问我有没有兴趣继续把这些作品完善成人机交互游戏\",{\"1\":{\"509\":1}}],[\"避免重复无情绪价值的销售\",{\"1\":{\"518\":1}}],[\"销售业绩提升\",{\"1\":{\"528\":1}}],[\"销售\",{\"1\":{\"518\":1}}],[\"销售冠军\",{\"1\":{\"6\":1}}],[\"包装设计\",{\"1\":{\"531\":1}}],[\"包容的团队文化\",{\"1\":{\"518\":1}}],[\"包括推广设计思维\",{\"1\":{\"524\":1}}],[\"包括通过数字技术提升生产效率\",{\"1\":{\"519\":1}}],[\"包括人类\",{\"1\":{\"519\":1}}],[\"包括鼓励他们结合商业思考\",{\"1\":{\"510\":1}}],[\"包括教授有时候就直接对着你的点去发散\",{\"1\":{\"509\":1}}],[\"包括很多很厉害的自媒体博主也在就读于这两所大学\",{\"1\":{\"509\":1}}],[\"包括以前有和学校的人进行合作\",{\"1\":{\"509\":1}}],[\"包括我也放弃了本校保研的资格\",{\"1\":{\"534\":1}}],[\"包括我留学准备的作品集里\",{\"1\":{\"509\":1}}],[\"包括我目前也在进行的留学作品集辅导服务\",{\"1\":{\"31\":1}}],[\"包括大四做毕业设计学习了ucl的一些理念最后还让我的作品广泛被讨论\",{\"1\":{\"508\":1}}],[\"包括之前对于met的研究\",{\"1\":{\"508\":1}}],[\"包括零售销售额\",{\"1\":{\"34\":1}}],[\"促进业务增长\",{\"1\":{\"539\":1}}],[\"促进生产流程的自动化和信息流动\",{\"1\":{\"519\":1}}],[\"促进产品的分发和市场可行性\",{\"1\":{\"519\":1}}],[\"促进服务化产品的实现过程\",{\"1\":{\"519\":1}}],[\"促进知识共享和创新能力\",{\"1\":{\"518\":1}}],[\"促进跨文化理解和团队合作\",{\"1\":{\"518\":1}}],[\"管理者该思考如何打破信息壁垒\",{\"1\":{\"518\":1}}],[\"管理\",{\"1\":{\"518\":1}}],[\"利于提升项目管理者的战略思维和团队协作能力\",{\"1\":{\"518\":1}}],[\"利用ai电子邮件助手软件\",{\"1\":{\"539\":1}}],[\"利用ai提升电邮营销效能\",{\"0\":{\"539\":1}}],[\"利用叙事跳脱了传统的建筑学科边界\",{\"1\":{\"534\":1}}],[\"利用大数据和人工智能技术\",{\"1\":{\"531\":1}}],[\"利用一些不舒适的游戏体验来完成人机项目\",{\"1\":{\"520\":1}}],[\"利用cscw技术理解和应对复杂性\",{\"1\":{\"519\":1}}],[\"利用数字平台进行协作\",{\"1\":{\"519\":1}}],[\"利用我空间背景的优势\",{\"1\":{\"92\":1}}],[\"利用传统儿童教育的美术适用性和比较可控的ar技术来提出一个培养儿童创造力的基础方案\",{\"1\":{\"82\":1}}],[\"利用静态图像和声音的尝试\",{\"1\":{\"23\":1}}],[\"终身学习\",{\"1\":{\"518\":3}}],[\"协助学生和成年人应对生活挑战\",{\"1\":{\"520\":1}}],[\"协作和批判性思维是人类特有的优势\",{\"1\":{\"518\":1}}],[\"协调现场事宜\",{\"1\":{\"6\":1}}],[\"软技能和硬技能的平衡是关键\",{\"1\":{\"518\":1}}],[\"软件主要有两种分类\",{\"1\":{\"509\":1}}],[\"软件偏程随之兴起\",{\"1\":{\"78\":1}}],[\"持续提升员工的软技能对企业和个人职业发展至关重要\",{\"1\":{\"518\":1}}],[\"报告强调\",{\"1\":{\"518\":1}}],[\"报告基于对linkedin全球1亿成员的数据分析\",{\"1\":{\"518\":1}}],[\"沟通技能的挑战在于如何跨越数字鸿沟\",{\"1\":{\"518\":1}}],[\"沟通技能\",{\"1\":{\"518\":1}}],[\"沟通\",{\"1\":{\"518\":1}}],[\"沟通层面\",{\"1\":{\"93\":1}}],[\"使系统能够从数据中学习和改进\",{\"1\":{\"539\":1}}],[\"使用ai电子邮件生成器可以显著减少编写和优化电子邮件所需的时间\",{\"1\":{\"539\":1}}],[\"使用ai电子邮件生成器是否真的能节省时间和提高效率\",{\"1\":{\"539\":1}}],[\"使用用户语言和保持一致性\",{\"1\":{\"521\":1}}],[\"使用眼动系统可以收集关于产品位置或状态的数据\",{\"1\":{\"519\":1}}],[\"使音频ar既能提供即时反馈又能适应不同的环境需求\",{\"1\":{\"517\":1}}],[\"使其成为增强用户体验的一种方式\",{\"1\":{\"517\":1}}],[\"使长期投资变得遥不可及\",{\"1\":{\"23\":1}}],[\"语音助手和ai\",{\"1\":{\"517\":1}}],[\"语音接口\",{\"1\":{\"516\":1}}],[\"语音接口和人工智能已经达到了音频\",{\"1\":{\"516\":1}}],[\"角色\",{\"1\":{\"517\":1}}],[\"指出ai电子邮件助手如何通过自动化和优化过程提高效率和效果\",{\"1\":{\"539\":1}}],[\"指出在人工智能时代\",{\"1\":{\"518\":1}}],[\"指挥家的帽子\",{\"1\":{\"523\":1}}],[\"指利用云平台支持开放设计和制造\",{\"1\":{\"519\":1}}],[\"指导人物\",{\"1\":{\"517\":1}}],[\"指的是一些单纯获得流量但实际账号无法取得关注的笔记\",{\"1\":{\"20\":1}}],[\"实验与迭代\",{\"1\":{\"525\":1}}],[\"实际效果的具体数据并未在文中提供\",{\"1\":{\"519\":1}}],[\"实现资源共享和经济效益\",{\"1\":{\"520\":1}}],[\"实现了智能生产\",{\"1\":{\"519\":1}}],[\"实现梦想\",{\"1\":{\"17\":1}}],[\"实用化的成熟程度\",{\"1\":{\"516\":1}}],[\"耳机\",{\"1\":{\"516\":1}}],[\"挑战是在正确的时间提供正确的信息\",{\"1\":{\"516\":1}}],[\"例如通过循环经济的挑战\",{\"1\":{\"519\":1}}],[\"例如通过系统动力学模拟和模拟来管理资源\",{\"1\":{\"519\":1}}],[\"例如如何预防和保护系统免受潜在的威胁\",{\"1\":{\"519\":1}}],[\"例如语言学习应用可以实时提供翻译\",{\"1\":{\"517\":1}}],[\"例如\",{\"1\":{\"514\":1,\"515\":1,\"517\":1,\"519\":1,\"531\":1,\"532\":4,\"533\":1,\"535\":1,\"538\":1}}],[\"例如绿色能源\",{\"1\":{\"510\":1}}],[\"允许音频信号在空间中传播并触发特定的反应\",{\"1\":{\"514\":1}}],[\"允许儿童在不同的角色中创造和探索\",{\"1\":{\"84\":1}}],[\"原文地址\",{\"1\":{\"539\":1}}],[\"原型设计\",{\"1\":{\"522\":1}}],[\"原因可能他并不是一个记录型的平台软件\",{\"1\":{\"521\":1}}],[\"原理\",{\"0\":{\"513\":1}}],[\"原始推理始于儿童的思维过程从符号思维转变为直觉思维\",{\"1\":{\"79\":1}}],[\"主要的问题\",{\"1\":{\"538\":1}}],[\"主要是作为视障人士的辅助工具而开发的\",{\"1\":{\"512\":1}}],[\"主打专精化教学模式\",{\"1\":{\"4\":1}}],[\"音频\",{\"1\":{\"516\":1}}],[\"音频增强现实还可以用于教育场景\",{\"1\":{\"517\":1}}],[\"音频增强现实\",{\"0\":{\"512\":1},\"1\":{\"512\":1,\"516\":1}}],[\"音乐等领域\",{\"1\":{\"78\":1}}],[\"音乐家上街追寻自然的声音等等\",{\"1\":{\"56\":1}}],[\"好了\",{\"1\":{\"511\":1}}],[\"好的名字\",{\"1\":{\"48\":1}}],[\"尝试去完全打开自己的内心\",{\"1\":{\"530\":1}}],[\"尝试用摄影记录下你的发现\",{\"1\":{\"511\":1}}],[\"尝试着一种逃离式的重生\",{\"1\":{\"50\":1}}],[\"遗忘之美丽\",{\"1\":{\"511\":1}}],[\"东京\",{\"1\":{\"511\":1}}],[\"幽灵线\",{\"1\":{\"511\":1}}],[\"恶灵附身\",{\"1\":{\"511\":1}}],[\"先简单介绍六帽子思考法则\",{\"1\":{\"523\":1}}],[\"先找找自己的作品集人设\",{\"1\":{\"510\":1}}],[\"先定位\",{\"1\":{\"510\":1}}],[\"先后推出市场对于ip的影响是什么\",{\"1\":{\"28\":1}}],[\"既然我关注到读书以外的内容\",{\"1\":{\"523\":1}}],[\"既然全球各地的名校也在组合这些字母\",{\"1\":{\"510\":1}}],[\"既要考虑到传统玩具的无技术性\",{\"1\":{\"81\":1}}],[\"催生新的硬件产品设计\",{\"1\":{\"510\":1}}],[\"推动循环经济的发展\",{\"1\":{\"519\":1}}],[\"推动个人和团队的知识库数据\",{\"1\":{\"518\":1}}],[\"推动企业实现创新和发展\",{\"1\":{\"518\":1}}],[\"推动商业模式的创新\",{\"1\":{\"510\":1}}],[\"推动新产品或服务的开发\",{\"1\":{\"510\":1}}],[\"推出赋能计划\",{\"1\":{\"23\":1}}],[\"推出的旨在聚集全球创作者和世界知名\",{\"1\":{\"9\":1}}],[\"举几个例子\",{\"1\":{\"510\":1}}],[\"培养创新精神\",{\"1\":{\"510\":1}}],[\"融合\",{\"1\":{\"535\":1}}],[\"融合了人类互动\",{\"1\":{\"519\":1}}],[\"融合不同领域以产生新的解决问题的方法\",{\"1\":{\"510\":1}}],[\"融合叙事空间\",{\"1\":{\"22\":1}}],[\"适应变动\",{\"1\":{\"521\":1}}],[\"适应性等\",{\"1\":{\"519\":1}}],[\"适应能力\",{\"1\":{\"518\":1}}],[\"适应市场需求\",{\"1\":{\"510\":1}}],[\"适合儿童创造力教育的年龄\",{\"0\":{\"79\":1}}],[\"特别对于美院学生\",{\"1\":{\"534\":1}}],[\"特别是在面对复杂和不确定的情况时\",{\"1\":{\"533\":1}}],[\"特别是对于关注心理健康\",{\"1\":{\"520\":1}}],[\"特别是那些不易被ai取代的软技能\",{\"1\":{\"518\":1}}],[\"特别是新加坡国立大学出现的挺多类似的专业\",{\"1\":{\"510\":1}}],[\"特沃斯基提出了双系统决策模型\",{\"1\":{\"532\":1}}],[\"特殊奖励或者与ip方互动的机会\",{\"1\":{\"40\":1}}],[\"得自己先了解自己\",{\"1\":{\"509\":1}}],[\"得到不同看法\",{\"1\":{\"506\":1}}],[\"值得思考的问题\",{\"1\":{\"511\":1}}],[\"值得我们去探索和尊重\",{\"1\":{\"511\":1}}],[\"值得注意的是\",{\"1\":{\"48\":1}}],[\"值不值得这个问题\",{\"1\":{\"509\":1}}],[\"交互为主的网络应用\",{\"1\":{\"521\":1}}],[\"交互挑战\",{\"1\":{\"516\":1}}],[\"交互方式\",{\"0\":{\"515\":1},\"1\":{\"531\":1}}],[\"交互空间来找到情绪宣泄的创新方式\",{\"1\":{\"499\":1}}],[\"交流感\",{\"1\":{\"509\":1}}],[\"首先\",{\"1\":{\"516\":1,\"519\":3,\"520\":1}}],[\"首先从我圈子资源的角度来说\",{\"1\":{\"509\":1}}],[\"首选香港地区\",{\"1\":{\"508\":1}}],[\"借着今天\",{\"1\":{\"512\":1}}],[\"借此话题顺便聊聊伦敦大学金史密斯学院\",{\"1\":{\"509\":1}}],[\"借由花语去表达\",{\"1\":{\"24\":1}}],[\"再者\",{\"1\":{\"534\":1}}],[\"再来才是业内人士\",{\"1\":{\"534\":1}}],[\"再到决定早餐吃什么\",{\"1\":{\"532\":1}}],[\"再次回归当下的生活\",{\"1\":{\"530\":1}}],[\"再利用这个人设来展开你的故事\",{\"1\":{\"510\":1}}],[\"再下手\",{\"1\":{\"510\":1}}],[\"再配合自身的背景情况去打开你的开题项目\",{\"1\":{\"510\":1}}],[\"再精准去攻破\",{\"1\":{\"509\":1}}],[\"再转换到成果的方式\",{\"1\":{\"31\":1}}],[\"听起来非常多东西\",{\"1\":{\"509\":1}}],[\"听觉方面出发去改进\",{\"1\":{\"89\":1}}],[\"分销渠道\",{\"1\":{\"525\":1}}],[\"分软件和硬件层面\",{\"1\":{\"509\":1}}],[\"分配到每个需要这种垃圾的岛上\",{\"1\":{\"46\":1}}],[\"二\",{\"1\":{\"534\":2}}],[\"二十游戏引擎\",{\"1\":{\"509\":1}}],[\"二是结果呈现\",{\"1\":{\"509\":1}}],[\"二等奖\",{\"1\":{\"7\":1}}],[\"招生官最希望看到的是除了硬背景外的个人特性\",{\"1\":{\"508\":1}}],[\"绝对没有什么保录取一说\",{\"1\":{\"508\":1}}],[\"绝对反对模板化的作品集辅导\",{\"1\":{\"4\":1}}],[\"坦白说\",{\"1\":{\"508\":1}}],[\"客观存在的事实\",{\"1\":{\"530\":1}}],[\"客观和理性\",{\"1\":{\"530\":1}}],[\"客观来说\",{\"1\":{\"508\":1}}],[\"客户关系\",{\"1\":{\"525\":1}}],[\"客户服务等多个领域\",{\"1\":{\"519\":1}}],[\"客户服务\",{\"1\":{\"518\":1}}],[\"客户服务和领导力等软技能的重要性日益凸显\",{\"1\":{\"518\":1}}],[\"客户\",{\"1\":{\"92\":1}}],[\"客户沟通与维护等工作\",{\"1\":{\"15\":1}}],[\"香港环境和教学对于未来整合资源肯定优于英国\",{\"1\":{\"508\":1}}],[\"香味被合成器处理和调配\",{\"1\":{\"71\":1}}],[\"跨界合作\",{\"1\":{\"531\":1}}],[\"跨界融合\",{\"1\":{\"531\":1}}],[\"跨学科可能不再是一个绝对的优势\",{\"1\":{\"537\":1}}],[\"跨学科的能力和对ai技术的理解也将成为竞争力的关键\",{\"1\":{\"537\":1}}],[\"跨学科协作等\",{\"1\":{\"522\":1}}],[\"跨学科优势\",{\"1\":{\"508\":1}}],[\"跨过这座石桥\",{\"1\":{\"66\":1}}],[\"岗位匹配人才也更需要拥有复合技能\",{\"1\":{\"508\":1}}],[\"迭代商业模式\",{\"1\":{\"508\":1}}],[\"娱乐方式\",{\"1\":{\"508\":1}}],[\"娱乐和商业活动\",{\"1\":{\"40\":1}}],[\"专业介绍等等\",{\"1\":{\"523\":1}}],[\"专业设置也最匹配我过往创业的方向\",{\"1\":{\"508\":1}}],[\"专注于使计算机理解和生成人类语言\",{\"1\":{\"539\":1}}],[\"专注于创造力\",{\"1\":{\"80\":1}}],[\"专注于舞美活动全案策划与设计执行\",{\"1\":{\"15\":1}}],[\"地区与择校\",{\"0\":{\"508\":1}}],[\"超级个体\",{\"1\":{\"507\":1,\"537\":1}}],[\"算是以\",{\"1\":{\"507\":1}}],[\"赶上了一些小风口取得了成绩但是也会遇到瓶颈\",{\"1\":{\"507\":1}}],[\"硕士留学最重要就是一个更综合\",{\"1\":{\"507\":1}}],[\"之前开了贴子讨论转行跨界的话题\",{\"1\":{\"506\":1}}],[\"之间的这条界线进行了谈判\",{\"1\":{\"49\":1}}],[\"论文分析\",{\"0\":{\"519\":1},\"1\":{\"502\":1}}],[\"数学和科学能力\",{\"1\":{\"524\":1}}],[\"数据监控机制\",{\"1\":{\"529\":1}}],[\"数据科学在设计中的角色\",{\"1\":{\"529\":1}}],[\"数据交互的新形式\",{\"1\":{\"519\":1}}],[\"数据驱动的设计是指基于大量过程数据来指导产品开发\",{\"1\":{\"519\":1}}],[\"数据驱动\",{\"1\":{\"519\":1,\"531\":1}}],[\"数据驱动设计的概念与应用\",{\"0\":{\"529\":1}}],[\"数据驱动设计的应用案例\",{\"0\":{\"528\":1}}],[\"数据驱动设计强调在设计过程中加入适量的数据\",{\"1\":{\"527\":1}}],[\"数据驱动设计\",{\"0\":{\"526\":1},\"1\":{\"502\":1,\"527\":1}}],[\"数据分析面临的挑战在于如何确保数据的准确性和可靠性\",{\"1\":{\"518\":1}}],[\"数据分析\",{\"1\":{\"518\":1}}],[\"数字\",{\"1\":{\"538\":1}}],[\"数字营销与消费设计可以相互融合\",{\"1\":{\"531\":1}}],[\"数字营销侧重于推广和传播\",{\"1\":{\"531\":1}}],[\"数字营销在消费者接触产品或服务之前发挥作用\",{\"1\":{\"531\":1}}],[\"数字营销通过互动广告\",{\"1\":{\"531\":1}}],[\"数字营销涵盖多种渠道\",{\"1\":{\"531\":1}}],[\"数字营销是指的是通过数字渠道\",{\"1\":{\"531\":1}}],[\"数字营销和消费设计可以实现个性化营销\",{\"1\":{\"531\":1}}],[\"数字营销和消费设计应更加注重社会责任\",{\"1\":{\"531\":1}}],[\"数字营销和消费设计都注重数据分析\",{\"1\":{\"531\":1}}],[\"数字营销和消费设计的目标都是为了提升品牌知名度\",{\"1\":{\"531\":1}}],[\"数字链条\",{\"1\":{\"519\":1}}],[\"数字编织物\",{\"1\":{\"519\":1}}],[\"数字通信框架的构建以及智能系统的应用等方面\",{\"1\":{\"519\":1}}],[\"数字自我\",{\"1\":{\"519\":1}}],[\"数字化\",{\"1\":{\"519\":1}}],[\"数字产品的一些潜在应用场景\",{\"1\":{\"29\":1}}],[\"数字资产\",{\"1\":{\"29\":1}}],[\"数字艺术ip相关的项目\",{\"1\":{\"31\":1}}],[\"数字艺术作品比起实体绘画缺乏唯一性\",{\"1\":{\"28\":1}}],[\"数字艺术家\",{\"1\":{\"9\":1}}],[\"数字媒体\",{\"1\":{\"4\":1}}],[\"谈谈设计师的未来\",{\"1\":{\"502\":1}}],[\"职业发展\",{\"0\":{\"507\":1}}],[\"职业\",{\"0\":{\"502\":1}}],[\"欲望之城\",{\"0\":{\"500\":1}}],[\"社区化的线下互动装置并融合音画交互技术\",{\"1\":{\"499\":1}}],[\"社交媒体帖子\",{\"1\":{\"539\":1}}],[\"社交媒体互动等方式与消费者建立联系\",{\"1\":{\"531\":1}}],[\"社交媒体\",{\"1\":{\"531\":3}}],[\"社交电商等等\",{\"1\":{\"510\":1}}],[\"社交互联网\",{\"1\":{\"508\":1}}],[\"社交\",{\"1\":{\"33\":1}}],[\"设置模块化\",{\"1\":{\"499\":1}}],[\"设计界的大恐慌\",{\"1\":{\"538\":1}}],[\"设计界的新魔咒\",{\"0\":{\"535\":1},\"1\":{\"502\":1,\"535\":1}}],[\"设计不再局限于所谓的\",{\"1\":{\"537\":1}}],[\"设计不仅仅关注产品功能\",{\"1\":{\"519\":1}}],[\"设计行业正在经历一场深刻的变革\",{\"1\":{\"537\":1}}],[\"设计能力\",{\"1\":{\"535\":1}}],[\"设计接口时\",{\"1\":{\"532\":1}}],[\"设计等\",{\"1\":{\"531\":1}}],[\"设计\",{\"1\":{\"531\":1,\"538\":3}}],[\"设计趋势预见\",{\"1\":{\"529\":1}}],[\"设计效率提升\",{\"1\":{\"529\":1}}],[\"设计项目优先级确定\",{\"1\":{\"529\":1}}],[\"设计成熟度较低的环境\",{\"1\":{\"527\":1}}],[\"设计成本的控制如何利用更低的成本也可以做出好的现场效果\",{\"1\":{\"92\":1}}],[\"设计实验流程以测试业务模型以及逆向工程业务核心资产的能力\",{\"1\":{\"525\":1}}],[\"设计实践需要从简单的问题解决转向系统设计\",{\"1\":{\"522\":1}}],[\"设计实践的根源在于工程学\",{\"1\":{\"522\":1}}],[\"设计实践的根源与导向工程背景\",{\"1\":{\"522\":1}}],[\"设计哲学需要从以个体为中心转向以群体为中心\",{\"1\":{\"522\":1}}],[\"设计哲学的转变从个体到群体\",{\"1\":{\"522\":1}}],[\"设计转向与策略关注社会文化背景\",{\"1\":{\"522\":1}}],[\"设计方法缺乏对用户的社会文化背景的深入理解\",{\"1\":{\"522\":1}}],[\"设计方案的效果图我也是比较熟悉了\",{\"1\":{\"20\":1}}],[\"设计迭代\",{\"1\":{\"522\":1}}],[\"设计存在粒度问题\",{\"1\":{\"522\":1}}],[\"设计系统迭代过程的保留和记录\",{\"1\":{\"521\":1}}],[\"设计环节不再局限于产品功能的完成\",{\"1\":{\"519\":1}}],[\"设计新产品如服务系统\",{\"1\":{\"519\":1}}],[\"设计者需要应对\",{\"1\":{\"519\":1}}],[\"设计者可以借鉴电影中的\",{\"1\":{\"517\":1}}],[\"设计流程不仅关注传统产品的完成\",{\"1\":{\"519\":1}}],[\"设计思维侧重于以用户为重点的实际问题解jue\",{\"1\":{\"533\":1}}],[\"设计思维强调客户体验\",{\"1\":{\"525\":1}}],[\"设计思维以开发具有客户痴迷方法的产品\",{\"1\":{\"525\":1}}],[\"设计思维\",{\"1\":{\"519\":1,\"524\":1}}],[\"设计思维vs艺术思维\",{\"0\":{\"533\":1},\"1\":{\"502\":1}}],[\"设计领域需要转向新的设计哲学与实践\",{\"1\":{\"522\":1}}],[\"设计领域的局限性过度关注个体特征\",{\"1\":{\"522\":1}}],[\"设计领域的创新结合商业应用\",{\"1\":{\"510\":1}}],[\"设计领域正面临前所未有的挑战\",{\"1\":{\"519\":1}}],[\"设计工程正从传统的实体制造转向更复杂的人机交互系统\",{\"1\":{\"519\":1}}],[\"设计工程的新挑战和机遇\",{\"1\":{\"519\":1}}],[\"设计工程的新原则\",{\"1\":{\"519\":1}}],[\"设计工程领域面临着资源共享生产网络\",{\"1\":{\"519\":1}}],[\"设计工程\",{\"1\":{\"519\":1}}],[\"设计时必须考虑到声音如何在空间中传播\",{\"1\":{\"516\":1}}],[\"设计结构问题\",{\"1\":{\"92\":1}}],[\"设计执行\",{\"0\":{\"91\":1}}],[\"设计概念\",{\"0\":{\"84\":1}}],[\"设计概念开发\",{\"1\":{\"6\":1}}],[\"设计出一个高要求的混合现实玩具\",{\"1\":{\"81\":1}}],[\"设计师现在面临着一个关键的适应时期\",{\"1\":{\"538\":1}}],[\"设计师正面临生存危机\",{\"1\":{\"538\":1}}],[\"设计师也要学会理解产品经济学和商业模式\",{\"1\":{\"537\":1}}],[\"设计师也可以借鉴这种快速原型和迭代的过程\",{\"1\":{\"525\":1}}],[\"设计师们面临着前所未有的挑战\",{\"1\":{\"537\":1}}],[\"设计师的未来\",{\"0\":{\"536\":1}}],[\"设计师的角色转变为数据分析师和决策者\",{\"1\":{\"527\":1}}],[\"设计师并不能jiejue所有问题\",{\"1\":{\"530\":1}}],[\"设计师面临着裁员和行业未来发展的担忧\",{\"1\":{\"527\":1}}],[\"设计师应保持开放的心态\",{\"1\":{\"532\":1}}],[\"设计师应关注技术如何改变设计的可能性\",{\"1\":{\"525\":1}}],[\"设计师应该学会在何时采用渐进式改进\",{\"1\":{\"525\":1}}],[\"设计师应认识到短期的竞争可能是线性的\",{\"1\":{\"525\":1}}],[\"设计师可以通过使用多样化的用户测试组来避免认知偏差\",{\"1\":{\"532\":1}}],[\"设计师可以更有效地解决问题\",{\"1\":{\"527\":1}}],[\"设计师可以更好地理解用户需求\",{\"1\":{\"527\":1}}],[\"设计师可以更好地理解用户的真实需求\",{\"1\":{\"522\":1}}],[\"设计师可以学习如何分析商业模式\",{\"1\":{\"525\":1}}],[\"设计师可以思考如何更深入地理解和响应客户需求\",{\"1\":{\"525\":1}}],[\"设计师需要学会\",{\"1\":{\"535\":1}}],[\"设计师需要意识到他们的设计决策可能产生多米诺效应\",{\"1\":{\"525\":1}}],[\"设计师需要更多地关注社会文化背景\",{\"1\":{\"522\":1}}],[\"设计师需要具备跨学科理解和快速适应的能力\",{\"1\":{\"519\":1}}],[\"设计师需要应对不断增长的技术挑战\",{\"1\":{\"519\":1}}],[\"设计师和设备制造商偷走了设计版本的冒险游乐设备\",{\"1\":{\"49\":1}}],[\"设计师一起构建游戏的未来\",{\"1\":{\"40\":1}}],[\"设计师称号\",{\"1\":{\"6\":1}}],[\"设计的工作室\",{\"1\":{\"20\":1}}],[\"设计相关\",{\"1\":{\"20\":1}}],[\"设计软件\",{\"1\":{\"8\":1}}],[\"设计主管\",{\"1\":{\"6\":1}}],[\"<\",{\"1\":{\"486\":2}}],[\"^\",{\"1\":{\"462\":1}}],[\"širokova\",{\"1\":{\"416\":1}}],[\"çelen\",{\"1\":{\"399\":1}}],[\"~15\",{\"1\":{\"323\":1}}],[\">\",{\"1\":{\"198\":2,\"387\":2}}],[\"`command\",{\"1\":{\"381\":1}}],[\"`pure\",{\"1\":{\"362\":1}}],[\"``work\",{\"1\":{\"300\":1}}],[\"``filter\",{\"1\":{\"167\":1}}],[\"`mention\",{\"1\":{\"241\":1}}],[\"`embodied\",{\"1\":{\"227\":1}}],[\"`hallucinations\",{\"1\":{\"198\":1,\"387\":1}}],[\"94\",{\"1\":{\"495\":1}}],[\"922\",{\"1\":{\"369\":1}}],[\"98\",{\"1\":{\"305\":1}}],[\"97\",{\"1\":{\"295\":1}}],[\"91\",{\"1\":{\"284\":1,\"352\":1,\"362\":1,\"445\":1}}],[\"965\",{\"1\":{\"428\":1}}],[\"96\",{\"1\":{\"283\":1}}],[\"936\",{\"1\":{\"363\":1}}],[\"93\",{\"1\":{\"239\":1}}],[\"950\",{\"1\":{\"428\":1}}],[\"95\",{\"1\":{\"222\":1,\"369\":1}}],[\"9\",{\"0\":{\"404\":1},\"1\":{\"156\":1,\"176\":1,\"248\":1,\"283\":1,\"305\":1,\"309\":1,\"314\":1,\"344\":1,\"386\":1,\"404\":1,\"414\":1}}],[\"9996\",{\"1\":{\"369\":1}}],[\"99\",{\"1\":{\"149\":1,\"160\":1,\"239\":1,\"313\":1,\"445\":1,\"456\":1}}],[\"90\",{\"1\":{\"122\":1,\"161\":1,\"223\":1,\"354\":1,\"361\":1,\"372\":1}}],[\"=0\",{\"1\":{\"140\":1}}],[\"=\",{\"0\":{\"482\":1},\"1\":{\"131\":1,\"135\":1,\"166\":3}}],[\"$99\",{\"1\":{\"414\":1}}],[\"$1\",{\"1\":{\"414\":1}}],[\"$k$\",{\"0\":{\"187\":1},\"1\":{\"187\":2}}],[\"$n=12$\",{\"1\":{\"152\":1}}],[\"$n=14$\",{\"1\":{\"140\":1}}],[\"$mean\",{\"1\":{\"140\":1}}],[\"$d=30$\",{\"1\":{\"140\":1}}],[\"$\",{\"1\":{\"131\":1,\"135\":2,\"159\":1,\"187\":3,\"194\":1,\"298\":2,\"414\":1,\"462\":1}}],[\"$2\",{\"1\":{\"107\":2,\"414\":1}}],[\"+受控词典\",{\"1\":{\"521\":1}}],[\"+\",{\"1\":{\"107\":2,\"131\":1}}],[\"q3\",{\"1\":{\"539\":1}}],[\"q2\",{\"1\":{\"524\":1,\"539\":1}}],[\"q1\",{\"1\":{\"524\":1,\"539\":1}}],[\"qkv\",{\"1\":{\"462\":3}}],[\"qr\",{\"1\":{\"386\":2}}],[\"qa\",{\"1\":{\"337\":1,\"340\":1,\"424\":2,\"447\":1}}],[\"qinhao\",{\"1\":{\"471\":1}}],[\"qinwei\",{\"1\":{\"447\":1}}],[\"qin\",{\"1\":{\"391\":1,\"443\":1}}],[\"qing\",{\"1\":{\"219\":1,\"324\":1,\"349\":1,\"392\":1,\"401\":1}}],[\"qiu\",{\"1\":{\"363\":1}}],[\"qiurong\",{\"1\":{\"293\":1}}],[\"qianqiao\",{\"1\":{\"403\":1}}],[\"qiang\",{\"1\":{\"362\":1}}],[\"qiaochu\",{\"1\":{\"105\":1,\"235\":1}}],[\"qiyue\",{\"1\":{\"353\":1}}],[\"qikai\",{\"1\":{\"324\":1}}],[\"qihao\",{\"1\":{\"322\":1}}],[\"qirtas\",{\"1\":{\"237\":1}}],[\"qi\",{\"1\":{\"210\":1,\"361\":1}}],[\"q\",{\"1\":{\"126\":3,\"163\":1,\"198\":1,\"387\":1,\"462\":1,\"494\":1}}],[\"quy\",{\"1\":{\"412\":1}}],[\"qu\",{\"1\":{\"377\":1,\"439\":1,\"450\":1}}],[\"quite\",{\"1\":{\"375\":1}}],[\"quickly\",{\"1\":{\"243\":1,\"258\":1,\"433\":1,\"443\":1}}],[\"quicksight\",{\"1\":{\"163\":1}}],[\"qudsi\",{\"1\":{\"350\":1}}],[\"quoting\",{\"1\":{\"241\":1}}],[\"quarot\",{\"0\":{\"456\":1},\"1\":{\"456\":3}}],[\"quartus\",{\"1\":{\"363\":2}}],[\"quarter\",{\"1\":{\"307\":1}}],[\"quadruped\",{\"1\":{\"283\":1}}],[\"quadri\",{\"1\":{\"197\":1}}],[\"quasi\",{\"0\":{\"230\":1}}],[\"quanyu\",{\"1\":{\"317\":1}}],[\"quantize\",{\"1\":{\"456\":1}}],[\"quantized\",{\"1\":{\"438\":1,\"456\":2}}],[\"quantization\",{\"0\":{\"374\":1,\"469\":1},\"1\":{\"374\":3,\"456\":2,\"469\":4}}],[\"quantity\",{\"1\":{\"253\":1,\"482\":1}}],[\"quantitatively\",{\"1\":{\"303\":1,\"413\":1}}],[\"quantitative\",{\"0\":{\"126\":1,\"494\":1},\"1\":{\"120\":1,\"126\":1,\"135\":1,\"152\":1,\"172\":1,\"184\":1,\"185\":1,\"195\":1,\"291\":1,\"356\":1,\"370\":1,\"380\":1,\"399\":1,\"476\":1,\"494\":1}}],[\"quantile\",{\"1\":{\"226\":1}}],[\"quantify\",{\"1\":{\"233\":1,\"238\":1,\"255\":1,\"435\":1}}],[\"quantifying\",{\"1\":{\"200\":1,\"255\":1,\"270\":1}}],[\"quantification\",{\"0\":{\"467\":1},\"1\":{\"467\":1}}],[\"quantifies\",{\"1\":{\"213\":2,\"264\":1,\"448\":1}}],[\"quantifiable\",{\"1\":{\"187\":1}}],[\"quan\",{\"1\":{\"167\":1}}],[\"qualities\",{\"1\":{\"155\":1}}],[\"qualitatively\",{\"1\":{\"166\":1}}],[\"qualitative\",{\"1\":{\"98\":1,\"136\":1,\"156\":1,\"166\":1,\"172\":1,\"184\":1,\"185\":1,\"210\":1,\"213\":1,\"227\":1,\"254\":1,\"291\":2,\"339\":1,\"356\":1,\"370\":1,\"380\":1,\"399\":1,\"430\":1,\"476\":2}}],[\"quality\",{\"0\":{\"272\":1,\"400\":1},\"1\":{\"96\":1,\"105\":1,\"106\":1,\"126\":1,\"133\":1,\"166\":1,\"203\":1,\"219\":1,\"224\":1,\"253\":1,\"272\":3,\"289\":2,\"331\":1,\"335\":1,\"336\":2,\"341\":1,\"344\":1,\"355\":2,\"356\":1,\"360\":1,\"365\":1,\"382\":1,\"384\":1,\"396\":1,\"399\":1,\"400\":5,\"401\":1,\"404\":1,\"411\":1,\"436\":1,\"458\":1,\"459\":2,\"462\":1,\"469\":1,\"480\":1,\"486\":1,\"487\":1,\"495\":1}}],[\"qualifications\",{\"0\":{\"8\":1}}],[\"querying\",{\"1\":{\"368\":1,\"435\":1}}],[\"query\",{\"0\":{\"386\":1,\"481\":1},\"1\":{\"240\":1,\"252\":1,\"313\":2,\"384\":1,\"386\":3,\"402\":7,\"426\":1,\"444\":2,\"481\":3}}],[\"queries\",{\"1\":{\"141\":3,\"252\":1,\"313\":2,\"376\":1,\"384\":1,\"392\":1,\"393\":1,\"394\":1,\"402\":1,\"419\":1,\"420\":1,\"426\":1,\"444\":1,\"481\":2,\"486\":1}}],[\"queried\",{\"1\":{\"111\":1,\"408\":1,\"435\":1}}],[\"questionnaire\",{\"1\":{\"234\":2}}],[\"questionnaires\",{\"0\":{\"234\":1},\"1\":{\"234\":1,\"290\":1}}],[\"questions\",{\"0\":{\"431\":1,\"489\":1},\"1\":{\"133\":2,\"172\":1,\"231\":1,\"234\":4,\"243\":3,\"252\":1,\"272\":1,\"282\":1,\"326\":1,\"331\":2,\"333\":1,\"334\":1,\"364\":1,\"368\":1,\"370\":1,\"376\":1,\"384\":1,\"426\":1,\"431\":5,\"458\":4,\"466\":1,\"467\":1,\"470\":1,\"489\":4,\"492\":1}}],[\"question\",{\"0\":{\"384\":1,\"430\":1},\"1\":{\"102\":2,\"135\":1,\"152\":1,\"159\":2,\"185\":5,\"234\":2,\"245\":1,\"258\":1,\"307\":1,\"314\":1,\"319\":1,\"328\":1,\"340\":1,\"350\":2,\"355\":1,\"363\":1,\"368\":1,\"376\":1,\"381\":1,\"384\":2,\"418\":1,\"424\":1,\"433\":1,\"441\":2,\"443\":1,\"458\":1,\"461\":1,\"470\":1,\"475\":2}}],[\"quest\",{\"1\":{\"81\":2,\"100\":1}}],[\"带着不同的角色去思考问题\",{\"1\":{\"93\":1}}],[\"多元设计方法\",{\"1\":{\"522\":1}}],[\"多元化的沟通网络\",{\"1\":{\"518\":1}}],[\"多站在其他角色的立场思考设计怎么打动客户\",{\"1\":{\"93\":1}}],[\"多年来\",{\"1\":{\"48\":1}}],[\"执行层面\",{\"1\":{\"93\":1}}],[\"执行到完成的项目管理\",{\"1\":{\"6\":1}}],[\"通常涉及迭代和原型创建\",{\"1\":{\"524\":1}}],[\"通常这些项目时间都不长\",{\"1\":{\"92\":1}}],[\"通过对比两种或多种变体来确定哪种策略更有效\",{\"1\":{\"539\":1}}],[\"通过机器学习和自然语言处理来提高效率和质量\",{\"1\":{\"539\":1}}],[\"通过艺术思维将艺术与技术融合不仅仅是一种时尚\",{\"1\":{\"533\":1}}],[\"通过应用决策心理学的原则\",{\"1\":{\"532\":1}}],[\"通过提供优质的产品和服务来提升消费者满意度\",{\"1\":{\"531\":1}}],[\"通过分析消费者行为和偏好\",{\"1\":{\"531\":1}}],[\"通过增强现实\",{\"1\":{\"531\":1}}],[\"通过社交媒体平台推广产品\",{\"1\":{\"531\":1}}],[\"通过收集用户行为数据来优化营销策略和产品设计\",{\"1\":{\"531\":1}}],[\"通过收集和分析用户行为数据\",{\"1\":{\"527\":1}}],[\"通过优化产品和服务设计来满足消费者的需求\",{\"1\":{\"531\":1}}],[\"通过两段叙事\",{\"1\":{\"530\":1}}],[\"通过关注客户来理解和创造价值\",{\"1\":{\"525\":1}}],[\"通过理解和操作公司的底层经济结构\",{\"1\":{\"525\":1}}],[\"通过非线性竞争\",{\"1\":{\"525\":1}}],[\"通过设计教育改革\",{\"1\":{\"524\":1}}],[\"通过留学如何来帮我创造价值和提升\",{\"1\":{\"523\":1}}],[\"通过深入的用户研究和多元设计方法\",{\"1\":{\"522\":1}}],[\"通过深入的用户研究和市场调研\",{\"1\":{\"522\":1}}],[\"通过捐赠\",{\"1\":{\"520\":1}}],[\"通过这种方式\",{\"1\":{\"525\":1}}],[\"通过这篇文章\",{\"1\":{\"519\":1}}],[\"通过这几个简单例子\",{\"1\":{\"510\":1}}],[\"通过服务化思维促进产品和服务的创新\",{\"1\":{\"519\":1}}],[\"通过使用数字孪生技术来保护设计数据\",{\"1\":{\"519\":1}}],[\"通过云设计和制造\",{\"1\":{\"519\":1}}],[\"通过实时数据决策来支持产品和服务的进化\",{\"1\":{\"519\":1}}],[\"通过数据驱动\",{\"1\":{\"519\":1}}],[\"通过数字艺术展与权益的结合也许就是一个新的可能\",{\"1\":{\"30\":1}}],[\"通过数字藏品\",{\"1\":{\"14\":1}}],[\"通过系统性地利用员工创造力进行决策制定\",{\"1\":{\"519\":1}}],[\"通过整合各类技术\",{\"1\":{\"517\":1}}],[\"通过将界面从屏幕移至耳朵\",{\"1\":{\"516\":1}}],[\"通过音频增强现实技术思考\",{\"1\":{\"504\":1}}],[\"通过摄影和游戏来寻找灵感\",{\"1\":{\"504\":1}}],[\"通过edbi来思考设计师的未来\",{\"0\":{\"510\":1},\"1\":{\"503\":1}}],[\"通过玩游戏思考留学\",{\"0\":{\"523\":1},\"1\":{\"502\":1}}],[\"通过跨学科的碰撞和团队协作\",{\"1\":{\"500\":1}}],[\"通过认识和区分不同的形状\",{\"1\":{\"81\":1}}],[\"通过访谈和调研我也得到了启发\",{\"1\":{\"80\":1}}],[\"通过在广州华德福学校和重庆国际儿童艺术节的学术交流\",{\"1\":{\"76\":1}}],[\"通过在不同地区举办活动\",{\"1\":{\"31\":1}}],[\"通过剖面的关系去设想造梦城的运作\",{\"1\":{\"60\":1}}],[\"通过虚幻的小说和真实的历史\",{\"1\":{\"53\":1}}],[\"通过材料的不同特性提出不同的设想\",{\"1\":{\"45\":1}}],[\"通过ip的\",{\"1\":{\"39\":1}}],[\"通过全区块链模式经济体系\",{\"1\":{\"39\":1}}],[\"通过全链上模式的经济体系和开放性系统玩法\",{\"1\":{\"33\":1}}],[\"通过前期创业经历的痛点分析\",{\"1\":{\"33\":1}}],[\"通过一个名为\",{\"1\":{\"23\":1,\"46\":1}}],[\"通过构建富有想象力的故事世界\",{\"1\":{\"22\":1}}],[\"通过账号数据分析\",{\"1\":{\"20\":1}}],[\"通过结合虚构的叙事与真实的过去\",{\"1\":{\"17\":1}}],[\"通过乌托邦式的畅想\",{\"1\":{\"17\":1}}],[\"通过游戏引擎\",{\"1\":{\"16\":1}}],[\"材料都是我经常要考虑的\",{\"1\":{\"92\":1}}],[\"唯一能做的就是遇到问题迅速和团队沟通解决问题\",{\"1\":{\"92\":1}}],[\"唯有一直往上攀爬\",{\"1\":{\"66\":1}}],[\"唯有踏入风暴\",{\"1\":{\"24\":1}}],[\"遇到沟通矛盾只能冷静处理再一步步解决\",{\"1\":{\"92\":1}}],[\"安全\",{\"1\":{\"92\":1}}],[\"落地材料的选择\",{\"1\":{\"92\":1}}],[\"汇报\",{\"1\":{\"91\":1}}],[\"舞美\",{\"1\":{\"91\":1}}],[\"舞美活动全案策划\",{\"0\":{\"91\":1}}],[\"舞美活动与展会项目经历\",{\"0\":{\"15\":1}}],[\"后面我也思考对于这类非常规体验游戏是否未来可能有更大的发展空间\",{\"1\":{\"520\":1}}],[\"后面又辞职做自媒体做了个人工作室\",{\"1\":{\"507\":1}}],[\"后续也欢迎大家来找我交流探讨\",{\"1\":{\"90\":1}}],[\"后来一场大灾难把我们分开了\",{\"1\":{\"46\":1}}],[\"增量与突破性思维\",{\"1\":{\"525\":1}}],[\"增强听众的体验\",{\"1\":{\"532\":1}}],[\"增强他们的适应能力和解决问题的能力\",{\"1\":{\"520\":1}}],[\"增强现实\",{\"1\":{\"84\":1}}],[\"增加儿童与计算机的互动内容\",{\"1\":{\"90\":1}}],[\"反面思考\",{\"1\":{\"523\":1}}],[\"反观有时候教授就喜欢看你的实验过程\",{\"1\":{\"509\":1}}],[\"反馈反思\",{\"0\":{\"87\":1}}],[\"反思\",{\"0\":{\"42\":1}}],[\"技术术语\",{\"1\":{\"535\":1}}],[\"技术\",{\"1\":{\"531\":1}}],[\"技术与经济激励\",{\"1\":{\"525\":1}}],[\"技术和创新策略也在被提出\",{\"1\":{\"519\":1}}],[\"技术和大数据\",{\"1\":{\"80\":1}}],[\"技术趋势\",{\"1\":{\"516\":1}}],[\"技术挑战\",{\"0\":{\"516\":1}}],[\"技术架构\",{\"0\":{\"514\":1}}],[\"技术手段转换成更深入的话题\",{\"1\":{\"511\":1}}],[\"技术点来说\",{\"1\":{\"509\":1}}],[\"技术更新迭代太快了\",{\"1\":{\"507\":1}}],[\"技术触点和测试\",{\"0\":{\"85\":1}}],[\"技术增强了他们的体验\",{\"1\":{\"84\":1}}],[\"学府\",{\"1\":{\"524\":1}}],[\"学习其他领域的基础知识\",{\"1\":{\"535\":1}}],[\"学习成果与参与度提升\",{\"1\":{\"528\":1}}],[\"学习进度与反馈收集\",{\"1\":{\"528\":1}}],[\"学习路径改进\",{\"1\":{\"528\":1}}],[\"学习组织\",{\"1\":{\"519\":1}}],[\"学习形状可以帮助儿童发展空间认知和思维能力\",{\"1\":{\"81\":1}}],[\"学术关键词5\",{\"1\":{\"519\":1}}],[\"学术关键词4\",{\"1\":{\"519\":1}}],[\"学术关键词3\",{\"1\":{\"519\":1}}],[\"学术关键词2\",{\"1\":{\"519\":1}}],[\"学术关键词1\",{\"1\":{\"519\":1}}],[\"学校的设计学院也是闻名国际\",{\"1\":{\"523\":1}}],[\"学校的校友圈与当下的发展是否可以产生更好的联系呢\",{\"1\":{\"523\":1}}],[\"学校是否有一些校企合作可以有更落地的项目实践\",{\"1\":{\"523\":1}}],[\"学校对我而言既是学习的地方\",{\"1\":{\"523\":1}}],[\"学校就是香港理工大学\",{\"1\":{\"508\":1}}],[\"学校或者其他教育机构\",{\"1\":{\"83\":1}}],[\"学会销售自己的设计\",{\"1\":{\"534\":1}}],[\"学会如何去面对各种紧急事件\",{\"1\":{\"91\":1}}],[\"学会独立和客户沟通\",{\"1\":{\"91\":1}}],[\"学生\",{\"1\":{\"83\":1}}],[\"涉及到复杂的社会技术系统设计\",{\"1\":{\"519\":1}}],[\"涉及的利益相关者主要是平台方\",{\"1\":{\"83\":1}}],[\"涉及复杂的生产和分销流程\",{\"1\":{\"35\":1}}],[\"服务\",{\"1\":{\"522\":1}}],[\"服务化等趋势进行调整和优化\",{\"1\":{\"519\":1}}],[\"服务化产品\",{\"1\":{\"519\":1}}],[\"服务层面去看待设计\",{\"1\":{\"92\":1}}],[\"服务系统\",{\"0\":{\"83\":1}}],[\"服务设计\",{\"1\":{\"4\":1,\"510\":1}}],[\"平面和立体相互转换的概念\",{\"1\":{\"82\":1}}],[\"平台是负责提供教育游戏工具的一方\",{\"1\":{\"83\":1}}],[\"平台核心是提供ip孵化机制\",{\"1\":{\"33\":1}}],[\"平台化的尝试\",{\"0\":{\"31\":1}}],[\"平台因为是ugc类型\",{\"1\":{\"23\":1}}],[\"混合现实游戏环境中的最佳体验源于物理活动\",{\"1\":{\"81\":1}}],[\"混沌错乱\",{\"1\":{\"23\":1}}],[\"根据文章中的信息\",{\"1\":{\"539\":1}}],[\"根据用户或客户的特定需求和偏好定制内容\",{\"1\":{\"539\":1}}],[\"根据对不舒适体验的简单调研\",{\"1\":{\"520\":1}}],[\"根据反思\",{\"1\":{\"90\":1}}],[\"根据\",{\"1\":{\"81\":1,\"525\":1}}],[\"根据设计的理念去寻找合适的场地\",{\"1\":{\"45\":1}}],[\"现在需要更多的奉献精神\",{\"1\":{\"538\":1}}],[\"现在的构思是最便宜的\",{\"1\":{\"538\":1}}],[\"现在的关键是影响\",{\"1\":{\"538\":1}}],[\"现在浏览互联网\",{\"1\":{\"538\":1}}],[\"现在灯亮下才发现这是许多食物残留的垃圾\",{\"1\":{\"46\":1}}],[\"现场就是最好的老师\",{\"1\":{\"93\":1}}],[\"现场工作人员\",{\"1\":{\"87\":1}}],[\"现有的研究表明\",{\"1\":{\"82\":1}}],[\"现实世界与虚拟世界的融合\",{\"1\":{\"81\":1}}],[\"兰佩\",{\"1\":{\"81\":1}}],[\"马蒂亚斯\",{\"1\":{\"81\":1}}],[\"马克\",{\"1\":{\"81\":1}}],[\"朗海因里希\",{\"1\":{\"81\":1}}],[\"辛斯克\",{\"1\":{\"81\":1}}],[\"史蒂夫\",{\"1\":{\"81\":1}}],[\"应避免隐藏信息或使内容过于复杂\",{\"1\":{\"532\":1}}],[\"应考虑到系统1的快速\",{\"1\":{\"532\":1}}],[\"应该去识别用户需要的至关信息\",{\"1\":{\"521\":1}}],[\"应该营造一个开放的环境\",{\"1\":{\"82\":1}}],[\"应用程序的核心\",{\"1\":{\"516\":1}}],[\"应用程序具有魔力\",{\"1\":{\"81\":1}}],[\"应邀参加上海第二届小红书官方社区相亲节\",{\"1\":{\"9\":1}}],[\"眼镜\",{\"1\":{\"516\":1}}],[\"眼睛需要像观看不同距离的物体一样进行调整\",{\"1\":{\"81\":1}}],[\"眼前是一座小岛\",{\"1\":{\"46\":1}}],[\"计算机生成的图像显示在屏幕上\",{\"1\":{\"81\":1}}],[\"家长必须积极管理孩子接触\",{\"1\":{\"81\":1}}],[\"家庭与儿童\",{\"1\":{\"43\":1,\"74\":1}}],[\"要说\",{\"1\":{\"535\":1}}],[\"要求总结并得出结论\",{\"1\":{\"523\":1}}],[\"要求用户将视线集中在一个固定点上\",{\"1\":{\"81\":1}}],[\"要去想怎么防止不好的事情发生而不是先下定不应该产生某事\",{\"1\":{\"521\":1}}],[\"要怎么做这些事\",{\"1\":{\"521\":1}}],[\"要知道\",{\"1\":{\"509\":1}}],[\"要解决这个问题\",{\"1\":{\"81\":1}}],[\"要不我们让船长再带我们再去一次岛屿吧\",{\"1\":{\"46\":1}}],[\"担心\",{\"1\":{\"81\":1}}],[\"电子邮件\",{\"1\":{\"531\":1}}],[\"电子邮件等\",{\"1\":{\"531\":1}}],[\"电子邮件生成器\",{\"1\":{\"505\":1,\"539\":1}}],[\"电子商务平台的购物体验\",{\"1\":{\"528\":1}}],[\"电影\",{\"1\":{\"517\":1}}],[\"电影和漫画书的出现也引发了类似的担忧\",{\"1\":{\"81\":1}}],[\"电视\",{\"1\":{\"81\":1}}],[\"玩家一号\",{\"1\":{\"81\":1}}],[\"玩到天黑了\",{\"1\":{\"46\":1}}],[\"像素推手\",{\"1\":{\"538\":1}}],[\"像\",{\"1\":{\"81\":1}}],[\"像一部即时默片\",{\"1\":{\"73\":1}}],[\"逃避现实和分散注意力\",{\"1\":{\"81\":1}}],[\"逃离\",{\"0\":{\"61\":1}}],[\"岁以下儿童使用\",{\"1\":{\"81\":1}}],[\"岁以下儿童不宜使用\",{\"1\":{\"81\":3}}],[\"索尼\",{\"1\":{\"81\":1}}],[\"索伦森\",{\"1\":{\"48\":1}}],[\"v2\",{\"1\":{\"393\":1}}],[\"vhdl\",{\"1\":{\"363\":1}}],[\"vlpfn\",{\"1\":{\"491\":1}}],[\"vlprompt\",{\"1\":{\"491\":4}}],[\"vllm\",{\"1\":{\"376\":1}}],[\"vllms\",{\"0\":{\"123\":1},\"1\":{\"123\":2,\"376\":3}}],[\"vlm\",{\"0\":{\"355\":1},\"1\":{\"355\":2}}],[\"vs\",{\"1\":{\"198\":1,\"319\":1,\"353\":1,\"387\":1,\"400\":1,\"410\":1,\"441\":1,\"454\":1,\"486\":2}}],[\"vummanthala\",{\"1\":{\"391\":1}}],[\"vul\",{\"1\":{\"413\":1}}],[\"vulnerabilities\",{\"0\":{\"374\":1},\"1\":{\"374\":2,\"409\":1,\"413\":3,\"434\":1}}],[\"vulnerability\",{\"0\":{\"413\":1},\"1\":{\"340\":1,\"374\":1,\"413\":3,\"488\":2,\"496\":1}}],[\"vulnerable\",{\"1\":{\"282\":1,\"374\":1,\"409\":1,\"446\":1}}],[\"vulliémoz\",{\"1\":{\"189\":1}}],[\"vuppalapati\",{\"1\":{\"265\":1}}],[\"vuppala\",{\"1\":{\"190\":1}}],[\"vqa\",{\"0\":{\"185\":1,\"430\":1},\"1\":{\"185\":1,\"319\":2,\"430\":2}}],[\"v0\",{\"1\":{\"133\":1,\"331\":1}}],[\"vetted\",{\"1\":{\"396\":1}}],[\"vector\",{\"1\":{\"334\":2,\"390\":1,\"438\":1}}],[\"vectors\",{\"0\":{\"199\":1},\"1\":{\"199\":1,\"270\":1,\"334\":1}}],[\"vectara\",{\"1\":{\"325\":1}}],[\"vehicular\",{\"1\":{\"232\":1}}],[\"vehicle\",{\"0\":{\"297\":1},\"1\":{\"186\":1,\"297\":2,\"407\":2}}],[\"vehicles\",{\"0\":{\"150\":1,\"221\":1,\"297\":1},\"1\":{\"150\":1,\"221\":4,\"272\":1,\"297\":1,\"298\":1,\"468\":1}}],[\"velocity\",{\"1\":{\"224\":1}}],[\"vedya\",{\"1\":{\"148\":1}}],[\"venkata\",{\"1\":{\"196\":1,\"265\":1}}],[\"venkatesh\",{\"1\":{\"121\":1}}],[\"venaik\",{\"1\":{\"172\":1,\"370\":1}}],[\"venuti\",{\"1\":{\"120\":1}}],[\"verbing\",{\"0\":{\"497\":1}}],[\"verbal\",{\"1\":{\"124\":1,\"307\":1,\"407\":1}}],[\"verma\",{\"1\":{\"490\":1}}],[\"veronica\",{\"1\":{\"416\":1}}],[\"vergari\",{\"1\":{\"203\":1}}],[\"verilog\",{\"1\":{\"363\":1}}],[\"verica\",{\"1\":{\"167\":1}}],[\"verifier\",{\"1\":{\"490\":1}}],[\"verified\",{\"1\":{\"463\":1,\"494\":1}}],[\"verifiable\",{\"1\":{\"428\":1}}],[\"verification\",{\"0\":{\"101\":1},\"1\":{\"101\":2,\"242\":1,\"390\":1}}],[\"verifying\",{\"1\":{\"140\":1,\"326\":1,\"477\":1}}],[\"verify\",{\"0\":{\"494\":1},\"1\":{\"119\":1,\"326\":1,\"403\":1,\"487\":1}}],[\"vertical\",{\"1\":{\"151\":1,\"348\":1}}],[\"vernero\",{\"1\":{\"149\":1}}],[\"versatility\",{\"1\":{\"323\":1}}],[\"versatile\",{\"1\":{\"119\":1,\"205\":1,\"345\":1,\"359\":1,\"377\":1}}],[\"verstegen\",{\"1\":{\"297\":1}}],[\"versus\",{\"1\":{\"215\":1,\"251\":1,\"291\":1,\"423\":1,\"476\":1}}],[\"versions\",{\"1\":{\"234\":1,\"374\":1,\"422\":1,\"494\":1}}],[\"version\",{\"1\":{\"139\":1,\"234\":2,\"403\":1,\"444\":1}}],[\"vered\",{\"1\":{\"133\":1,\"331\":1}}],[\"vera\",{\"1\":{\"126\":1}}],[\"very\",{\"1\":{\"96\":1,\"97\":1,\"126\":1,\"132\":1,\"169\":1,\"179\":1,\"240\":1,\"249\":1,\"256\":1,\"284\":2,\"312\":1,\"330\":1,\"366\":1,\"374\":1,\"391\":1,\"395\":1,\"402\":2,\"409\":1,\"427\":1,\"455\":1,\"482\":1}}],[\"voltage\",{\"1\":{\"284\":1}}],[\"volunteers\",{\"1\":{\"265\":1}}],[\"voluntary\",{\"0\":{\"150\":1},\"1\":{\"150\":2}}],[\"volume\",{\"1\":{\"147\":1,\"238\":1,\"415\":1}}],[\"vocal\",{\"0\":{\"232\":1}}],[\"vocabulary\",{\"0\":{\"190\":1},\"1\":{\"127\":1,\"190\":3,\"349\":1,\"384\":1,\"442\":1,\"460\":1,\"485\":1}}],[\"vogel\",{\"1\":{\"209\":1}}],[\"voigt\",{\"1\":{\"187\":1,\"203\":1}}],[\"voicebot\",{\"1\":{\"232\":3}}],[\"voicepilot\",{\"0\":{\"184\":1,\"380\":1},\"1\":{\"184\":1,\"380\":1}}],[\"voice\",{\"1\":{\"163\":4,\"201\":1,\"232\":1,\"282\":1}}],[\"voices\",{\"1\":{\"117\":1}}],[\"von\",{\"1\":{\"187\":1}}],[\"vonasch\",{\"1\":{\"107\":1}}],[\"vote\",{\"1\":{\"391\":1}}],[\"voter\",{\"1\":{\"234\":1}}],[\"voters\",{\"1\":{\"234\":1,\"248\":1}}],[\"voted\",{\"1\":{\"176\":1}}],[\"voting\",{\"0\":{\"176\":1,\"234\":1},\"1\":{\"176\":2,\"234\":1,\"494\":1}}],[\"v\",{\"1\":{\"112\":1,\"190\":1,\"319\":4,\"333\":1}}],[\"vylomova\",{\"1\":{\"365\":1}}],[\"vy\",{\"1\":{\"101\":1}}],[\"vagrant\",{\"1\":{\"395\":1}}],[\"vahid\",{\"1\":{\"334\":1}}],[\"vaccines\",{\"1\":{\"241\":1}}],[\"vaccine\",{\"1\":{\"241\":1}}],[\"vanilla\",{\"1\":{\"434\":1,\"455\":1,\"494\":1}}],[\"vanishing\",{\"1\":{\"260\":1}}],[\"van\",{\"1\":{\"278\":1,\"385\":1,\"399\":1}}],[\"vanc\",{\"1\":{\"240\":1}}],[\"vangelis\",{\"1\":{\"117\":1}}],[\"vaa\",{\"1\":{\"234\":1}}],[\"vasudeva\",{\"1\":{\"325\":1}}],[\"vascular\",{\"1\":{\"295\":1}}],[\"vasoactivity\",{\"0\":{\"295\":1}}],[\"vast\",{\"1\":{\"213\":1,\"314\":1,\"390\":1,\"445\":1}}],[\"vaswani2017attention\",{\"1\":{\"159\":1}}],[\"vazquez\",{\"1\":{\"164\":1}}],[\"varshney\",{\"1\":{\"344\":1}}],[\"varshini\",{\"1\":{\"150\":1}}],[\"varma\",{\"1\":{\"325\":1}}],[\"vary\",{\"1\":{\"272\":1,\"304\":1}}],[\"varying\",{\"0\":{\"198\":1,\"387\":1},\"1\":{\"131\":1,\"141\":1,\"198\":1,\"202\":1,\"207\":1,\"209\":1,\"215\":1,\"228\":1,\"249\":1,\"279\":1,\"337\":1,\"387\":1,\"414\":2,\"425\":1,\"437\":1}}],[\"varepsilon$\",{\"1\":{\"187\":3}}],[\"varies\",{\"1\":{\"413\":1}}],[\"variety\",{\"1\":{\"155\":1,\"163\":1,\"181\":1,\"302\":1,\"345\":1,\"427\":1,\"431\":1,\"450\":1,\"467\":1,\"468\":1,\"471\":1,\"482\":1}}],[\"varied\",{\"1\":{\"146\":1,\"155\":1}}],[\"variance\",{\"1\":{\"197\":1,\"355\":1}}],[\"variant\",{\"1\":{\"188\":1,\"386\":1}}],[\"variants\",{\"1\":{\"187\":1,\"226\":1,\"368\":1,\"445\":1}}],[\"variable\",{\"1\":{\"121\":1}}],[\"variables\",{\"1\":{\"110\":1,\"255\":1,\"304\":1,\"321\":1,\"422\":1}}],[\"variability\",{\"1\":{\"120\":1,\"143\":1,\"300\":1}}],[\"variational\",{\"1\":{\"491\":1}}],[\"variations\",{\"1\":{\"163\":1,\"226\":1,\"257\":1,\"328\":1,\"437\":1,\"459\":1}}],[\"variation\",{\"0\":{\"165\":1},\"1\":{\"98\":1,\"204\":1,\"238\":1,\"260\":1}}],[\"various\",{\"1\":{\"97\":1,\"111\":1,\"115\":1,\"124\":1,\"128\":1,\"130\":1,\"133\":1,\"136\":1,\"146\":1,\"153\":1,\"165\":2,\"166\":1,\"172\":1,\"176\":1,\"190\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"219\":1,\"221\":1,\"222\":1,\"224\":1,\"226\":1,\"234\":1,\"240\":1,\"246\":1,\"254\":1,\"269\":1,\"290\":1,\"302\":1,\"312\":1,\"314\":1,\"316\":1,\"322\":1,\"323\":1,\"325\":1,\"328\":1,\"331\":1,\"333\":2,\"337\":1,\"339\":1,\"350\":1,\"356\":1,\"359\":1,\"368\":3,\"370\":1,\"372\":2,\"391\":1,\"393\":1,\"400\":2,\"401\":1,\"408\":1,\"413\":1,\"417\":1,\"418\":1,\"425\":1,\"441\":1,\"442\":1,\"449\":1,\"450\":1,\"458\":1,\"471\":1,\"472\":1,\"478\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":2,\"491\":1,\"495\":1,\"498\":2}}],[\"valentina\",{\"1\":{\"497\":1}}],[\"valeros\",{\"1\":{\"416\":1}}],[\"valerie\",{\"1\":{\"215\":1}}],[\"vali\",{\"1\":{\"333\":1}}],[\"valid\",{\"1\":{\"382\":1}}],[\"validity\",{\"1\":{\"223\":1,\"385\":1,\"492\":1}}],[\"validating\",{\"1\":{\"477\":1}}],[\"validation\",{\"0\":{\"186\":1},\"1\":{\"103\":1,\"130\":1,\"176\":1,\"186\":4,\"326\":3,\"388\":1,\"444\":1,\"480\":1}}],[\"validate\",{\"1\":{\"184\":1,\"238\":1,\"246\":1,\"309\":1,\"380\":1,\"384\":1,\"445\":1,\"447\":1}}],[\"validated\",{\"1\":{\"138\":1,\"149\":1,\"165\":1,\"196\":1,\"234\":1,\"404\":1}}],[\"value\",{\"1\":{\"197\":1,\"272\":1,\"304\":2,\"366\":1,\"391\":1,\"404\":1,\"454\":1}}],[\"values\",{\"1\":{\"131\":1,\"139\":1,\"206\":1,\"234\":1,\"237\":1,\"246\":1,\"334\":1,\"335\":1,\"350\":1,\"378\":1,\"469\":1}}],[\"valuable\",{\"1\":{\"97\":1,\"222\":1,\"236\":2,\"246\":1,\"282\":1,\"295\":1,\"309\":1,\"312\":1,\"345\":1,\"349\":1,\"413\":1,\"417\":1,\"439\":1,\"465\":1,\"466\":1}}],[\"vallefuoco\",{\"1\":{\"120\":1}}],[\"va\",{\"1\":{\"100\":3}}],[\"villata\",{\"1\":{\"315\":1}}],[\"villa\",{\"1\":{\"315\":1}}],[\"victor\",{\"1\":{\"435\":1}}],[\"victoria\",{\"1\":{\"340\":1}}],[\"victim\",{\"1\":{\"298\":1}}],[\"vicuna\",{\"1\":{\"313\":1}}],[\"vivado\",{\"1\":{\"363\":2}}],[\"vivaroutes\",{\"1\":{\"270\":2}}],[\"vivian\",{\"1\":{\"326\":1}}],[\"vivienne\",{\"1\":{\"303\":1}}],[\"vivek\",{\"1\":{\"97\":1,\"211\":1,\"312\":1}}],[\"vive\",{\"1\":{\"81\":1}}],[\"vikram\",{\"1\":{\"207\":1}}],[\"virus\",{\"1\":{\"488\":2}}],[\"virginia\",{\"1\":{\"256\":1}}],[\"virgüez\",{\"1\":{\"205\":1}}],[\"virtually\",{\"1\":{\"340\":1}}],[\"virtual\",{\"0\":{\"119\":1,\"120\":1,\"132\":1,\"165\":1,\"191\":1,\"195\":1,\"201\":1,\"274\":1,\"330\":1},\"1\":{\"119\":1,\"120\":4,\"127\":1,\"132\":1,\"157\":1,\"165\":2,\"186\":1,\"191\":2,\"195\":2,\"201\":1,\"204\":1,\"227\":1,\"239\":1,\"274\":2,\"278\":2,\"279\":1,\"304\":1,\"305\":2,\"330\":1,\"468\":1}}],[\"virtualhome\",{\"1\":{\"116\":1,\"118\":1}}],[\"vincent\",{\"1\":{\"342\":1,\"444\":1}}],[\"vineet\",{\"1\":{\"201\":1}}],[\"vinay\",{\"1\":{\"200\":1}}],[\"vidya\",{\"1\":{\"226\":1}}],[\"vidyapu\",{\"1\":{\"185\":1}}],[\"videos\",{\"0\":{\"116\":1,\"137\":1},\"1\":{\"116\":2,\"118\":2,\"137\":2,\"184\":1,\"200\":1,\"235\":4,\"376\":4,\"380\":1,\"393\":2,\"438\":4}}],[\"video\",{\"0\":{\"108\":1,\"122\":1,\"165\":1,\"194\":1,\"235\":1,\"273\":1,\"376\":1,\"393\":2},\"1\":{\"108\":2,\"116\":2,\"118\":5,\"122\":4,\"137\":1,\"165\":1,\"173\":1,\"194\":3,\"200\":2,\"219\":1,\"227\":1,\"235\":1,\"258\":1,\"265\":2,\"273\":3,\"319\":1,\"361\":1,\"376\":6,\"393\":4,\"401\":1,\"433\":1}}],[\"violations\",{\"1\":{\"434\":2}}],[\"viola\",{\"1\":{\"178\":1,\"259\":1}}],[\"vittorio\",{\"1\":{\"492\":1}}],[\"vital\",{\"1\":{\"223\":1,\"309\":3,\"470\":1}}],[\"vitica\",{\"1\":{\"144\":1}}],[\"vit\",{\"1\":{\"137\":2}}],[\"vis\",{\"0\":{\"217\":1}}],[\"visible\",{\"1\":{\"217\":1,\"304\":1}}],[\"visibility\",{\"1\":{\"186\":1,\"200\":1}}],[\"visitor\",{\"1\":{\"249\":1}}],[\"visitors\",{\"1\":{\"117\":1,\"249\":2}}],[\"visiting\",{\"1\":{\"173\":4}}],[\"vision\",{\"0\":{\"364\":1,\"444\":1},\"1\":{\"105\":2,\"123\":1,\"137\":1,\"157\":1,\"159\":1,\"169\":1,\"181\":1,\"219\":1,\"246\":1,\"277\":1,\"319\":2,\"355\":1,\"364\":1,\"393\":1,\"396\":1,\"399\":1,\"401\":1,\"430\":1,\"444\":2}}],[\"visuality\",{\"1\":{\"361\":1}}],[\"visualization\",{\"0\":{\"188\":1,\"217\":1,\"226\":1,\"229\":1,\"249\":1,\"250\":1,\"255\":1,\"280\":1},\"1\":{\"141\":6,\"143\":2,\"152\":3,\"182\":1,\"185\":1,\"189\":4,\"210\":1,\"213\":1,\"217\":5,\"226\":2,\"229\":3,\"249\":2,\"250\":2,\"252\":1,\"255\":1,\"270\":4,\"280\":3,\"426\":1}}],[\"visualizations\",{\"0\":{\"141\":1},\"1\":{\"104\":1,\"119\":1,\"140\":1,\"141\":2,\"146\":1,\"152\":1,\"188\":1,\"189\":1,\"213\":1,\"242\":1,\"249\":1,\"250\":1,\"252\":1,\"258\":1,\"270\":1,\"280\":2,\"301\":2,\"426\":1,\"433\":1}}],[\"visualizes\",{\"1\":{\"242\":1,\"361\":1}}],[\"visualized\",{\"1\":{\"181\":1,\"301\":1}}],[\"visualize\",{\"1\":{\"111\":1,\"210\":1,\"249\":1,\"258\":1,\"399\":1,\"433\":1}}],[\"visualizing\",{\"0\":{\"252\":1,\"270\":1,\"301\":1,\"426\":1},\"1\":{\"105\":1,\"111\":1,\"234\":1}}],[\"visualwebbench\",{\"0\":{\"345\":1}}],[\"visuals\",{\"1\":{\"252\":1,\"426\":1}}],[\"visually\",{\"1\":{\"140\":1,\"156\":2,\"176\":1,\"219\":3,\"226\":1,\"401\":3,\"444\":1}}],[\"visual\",{\"0\":{\"100\":1,\"111\":1,\"152\":1,\"189\":1,\"258\":1,\"277\":1,\"319\":1,\"361\":1,\"393\":1,\"430\":1,\"433\":1},\"1\":{\"100\":2,\"104\":1,\"108\":1,\"111\":3,\"118\":1,\"123\":2,\"126\":1,\"142\":1,\"151\":1,\"152\":1,\"176\":1,\"182\":3,\"185\":1,\"189\":1,\"191\":1,\"217\":2,\"219\":1,\"235\":2,\"252\":1,\"256\":1,\"257\":1,\"258\":1,\"270\":2,\"280\":1,\"296\":1,\"305\":1,\"319\":3,\"348\":1,\"355\":1,\"361\":3,\"362\":1,\"375\":1,\"376\":1,\"393\":4,\"399\":1,\"401\":1,\"426\":1,\"430\":2,\"433\":1,\"438\":1,\"444\":1}}],[\"viable\",{\"1\":{\"376\":1}}],[\"via\",{\"0\":{\"96\":1,\"103\":1,\"219\":1,\"224\":1,\"235\":1,\"255\":1,\"257\":1,\"260\":1,\"317\":1,\"360\":1,\"366\":1,\"396\":1,\"401\":1,\"403\":1,\"441\":1,\"442\":1,\"445\":1,\"478\":1,\"482\":1},\"1\":{\"99\":1,\"156\":1,\"159\":2,\"169\":1,\"173\":1,\"180\":1,\"213\":1,\"272\":1,\"282\":1,\"284\":1,\"305\":1,\"314\":1,\"359\":1,\"362\":1,\"363\":1,\"368\":1,\"381\":1,\"382\":1,\"389\":1,\"398\":1,\"409\":1,\"455\":2,\"477\":1,\"478\":1,\"496\":1}}],[\"views\",{\"0\":{\"400\":1},\"1\":{\"160\":1,\"182\":1,\"205\":1,\"213\":1,\"302\":1}}],[\"viewers\",{\"1\":{\"122\":1,\"200\":1,\"280\":1}}],[\"viewer\",{\"1\":{\"122\":1}}],[\"viewing\",{\"1\":{\"118\":1,\"252\":1,\"426\":1}}],[\"viewpoints\",{\"1\":{\"117\":1,\"167\":1}}],[\"view\",{\"0\":{\"270\":1,\"272\":1},\"1\":{\"81\":2,\"120\":1,\"122\":1,\"127\":1,\"182\":1,\"213\":1,\"249\":1,\"250\":1,\"270\":1,\"272\":5}}],[\"vriend\",{\"1\":{\"185\":1}}],[\"vr\",{\"0\":{\"203\":1,\"204\":1,\"227\":1,\"304\":1},\"1\":{\"81\":8,\"119\":4,\"120\":4,\"165\":4,\"191\":1,\"195\":2,\"201\":2,\"203\":4,\"204\":2,\"227\":1,\"239\":1,\"274\":5,\"304\":2,\"305\":5,\"531\":1}}],[\"vr对孩子安全吗\",{\"1\":{\"81\":1}}],[\"vr技术的潜在危险性\",{\"1\":{\"81\":1}}],[\"谷歌\",{\"1\":{\"81\":1}}],[\"三星\",{\"1\":{\"81\":1}}],[\"三战\",{\"1\":{\"46\":1}}],[\"绘画等\",{\"1\":{\"81\":1}}],[\"绘图\",{\"1\":{\"8\":1}}],[\"意识到潜在的偏差\",{\"1\":{\"532\":1}}],[\"意味着采用多样化的方法\",{\"1\":{\"80\":1}}],[\"意为游乐场\",{\"1\":{\"48\":1}}],[\"意为垃圾\",{\"1\":{\"48\":1}}],[\"倪昆老师谈到\",{\"1\":{\"80\":1}}],[\"进一步凸显了质疑现实的力量\",{\"1\":{\"533\":1}}],[\"进行的营销活动\",{\"1\":{\"531\":1}}],[\"进行实时分享\",{\"1\":{\"519\":1}}],[\"进行了采访\",{\"1\":{\"80\":1}}],[\"进入造梦城后\",{\"1\":{\"65\":1}}],[\"进入了一个死循环\",{\"1\":{\"42\":1}}],[\"进入了各种对ai绘画视频等等的探索\",{\"1\":{\"30\":1}}],[\"进入2023年\",{\"1\":{\"30\":1}}],[\"进入当下的城市状况做一个新的叙事性创想\",{\"1\":{\"23\":1}}],[\"操作前的孩子专注于对象现在的显示方式\",{\"1\":{\"79\":1}}],[\"守恒是理解即使外观发生变化\",{\"1\":{\"79\":1}}],[\"体现了技术进步对产品开发的影响\",{\"1\":{\"519\":1}}],[\"体积和数量守恒的实验\",{\"1\":{\"79\":1}}],[\"体验到qian所未有的冒险之旅\",{\"1\":{\"530\":1}}],[\"体验到前所未有的冒险之旅\",{\"1\":{\"74\":1}}],[\"体验更好\",{\"1\":{\"521\":1}}],[\"体验的两项早期尝试\",{\"1\":{\"512\":1}}],[\"体验社交\",{\"1\":{\"40\":1}}],[\"长度\",{\"1\":{\"79\":1}}],[\"保持批判性和创造性思维\",{\"1\":{\"518\":1}}],[\"保持对行业的探索同时\",{\"1\":{\"42\":1}}],[\"保存\",{\"1\":{\"79\":1}}],[\"子项可以将对象组织到基元集合中\",{\"1\":{\"79\":1}}],[\"子午莲\",{\"1\":{\"24\":1}}],[\"用更接地气的方式去沟通的时候\",{\"1\":{\"535\":1}}],[\"用直观的示意图替换复杂的流程\",{\"1\":{\"535\":1}}],[\"用故事代替术语\",{\"1\":{\"535\":1}}],[\"用文本或者加入一些抽象拼贴把它描绘出来\",{\"1\":{\"530\":1}}],[\"用数据来支持和验证设计决策\",{\"1\":{\"527\":1}}],[\"用艺术思维革新商业\",{\"0\":{\"533\":1},\"1\":{\"502\":1}}],[\"用户界面\",{\"1\":{\"531\":1}}],[\"用户满意度与粘性增强\",{\"1\":{\"528\":1}}],[\"用户行为分析自动化\",{\"1\":{\"529\":1}}],[\"用户行为分析\",{\"1\":{\"528\":1}}],[\"用户购买路径分析\",{\"1\":{\"528\":1}}],[\"用户反馈收集\",{\"1\":{\"528\":1}}],[\"用户测试\",{\"1\":{\"522\":1}}],[\"用户研究和市场调研\",{\"1\":{\"522\":1}}],[\"用户研究\",{\"1\":{\"522\":2}}],[\"用户交互和用户体验\",{\"1\":{\"522\":1}}],[\"用户模型\",{\"1\":{\"521\":1}}],[\"用户只是被你某一条笔记吸引或者是喜欢你的社区内容\",{\"1\":{\"521\":1}}],[\"用户可以干什么\",{\"1\":{\"521\":1}}],[\"用户可以通过耳机与设备进行对话\",{\"1\":{\"515\":1}}],[\"用户可以在听歌时看到歌词同步显示\",{\"1\":{\"514\":1}}],[\"用户可以将其连接到智能手机或其他电子设备上\",{\"1\":{\"514\":1}}],[\"用户心理感受和行为\",{\"1\":{\"521\":1}}],[\"用户心理\",{\"1\":{\"508\":1}}],[\"用户体验已死\",{\"1\":{\"538\":1}}],[\"用户体验至上\",{\"1\":{\"531\":1}}],[\"用户体验设计师的决策\",{\"1\":{\"532\":1}}],[\"用户体验设计\",{\"1\":{\"521\":1,\"522\":3}}],[\"用户体验设计中的决策心理学\",{\"0\":{\"532\":1},\"1\":{\"502\":1}}],[\"用户体验\",{\"1\":{\"516\":1,\"531\":2}}],[\"用户体验要素\",{\"1\":{\"502\":1}}],[\"用户旅程图\",{\"0\":{\"40\":1}}],[\"用于电子邮件生成器以优化内容\",{\"1\":{\"539\":1}}],[\"用于此处的电子邮件生成\",{\"1\":{\"539\":1}}],[\"用于帮助组织识别不同类型的创造力\",{\"1\":{\"524\":1}}],[\"用于娱乐和教育\",{\"1\":{\"81\":1}}],[\"用于描述婴儿\",{\"1\":{\"79\":1}}],[\"荣誉教授和荣誉科学院士的称号\",{\"1\":{\"79\":1}}],[\"荣誉奖项\",{\"0\":{\"7\":1}}],[\"瑞士人\",{\"1\":{\"79\":1}}],[\"皮亚杰的研究\",{\"1\":{\"79\":1}}],[\"皮亚杰更多地描述了术前阶段的局限性和他们无法完成的脑力任务\",{\"1\":{\"79\":1}}],[\"皮亚杰提出的四个阶段中的第二个阶段\",{\"1\":{\"79\":1}}],[\"皮亚杰\",{\"1\":{\"79\":1}}],[\"引导元素的安排\",{\"1\":{\"521\":1}}],[\"引领团队取得成果\",{\"1\":{\"518\":1}}],[\"引起了社会的极大关注\",{\"1\":{\"78\":1}}],[\"引出问题去思考\",{\"1\":{\"74\":1,\"530\":1}}],[\"明确\",{\"1\":{\"521\":1}}],[\"明确要求在课后服务中引入体育\",{\"1\":{\"78\":1}}],[\"明镜\",{\"1\":{\"73\":1}}],[\"出租车司机\",{\"1\":{\"533\":1}}],[\"出发\",{\"1\":{\"525\":1}}],[\"出台\",{\"1\":{\"78\":1}}],[\"出现了两波客户群体\",{\"1\":{\"21\":1}}],[\"出现了\",{\"1\":{\"21\":1}}],[\"阶段推行中小学生课外活动计划的通知\",{\"1\":{\"78\":1}}],[\"北京市教育委员会关于在义务教育2014年1月\",{\"1\":{\"78\":1}}],[\"掀起了我国steam教育的热潮\",{\"1\":{\"78\":1}}],[\"她巧妙地将这些元素融入了故事\",{\"1\":{\"511\":1}}],[\"她认为人类建造并遗弃的地方也有其美感\",{\"1\":{\"511\":1}}],[\"她的电子游戏如\",{\"1\":{\"511\":1}}],[\"她将艺术\",{\"1\":{\"78\":1}}],[\"她把这个想法带到了伦敦\",{\"1\":{\"48\":1}}],[\"教学过程中缺乏趣味性\",{\"1\":{\"78\":1}}],[\"教育应用程序的用户体验迭代\",{\"1\":{\"528\":1}}],[\"教育应注重培养创新能力和终身学习\",{\"1\":{\"519\":1}}],[\"教育和公益事业的品牌\",{\"1\":{\"520\":1}}],[\"教育机构\",{\"1\":{\"520\":1}}],[\"教育领域是否也有一些潜在的商业空间呢\",{\"1\":{\"520\":1}}],[\"教育不仅仅是形式上书本上的内容\",{\"1\":{\"43\":1}}],[\"教育\",{\"0\":{\"2\":1}}],[\"知识诅咒\",{\"0\":{\"535\":1},\"1\":{\"502\":1,\"535\":2}}],[\"知识的广度和深度仍存在一定的局限性\",{\"1\":{\"78\":1}}],[\"知识产权拥有者\",{\"1\":{\"40\":1}}],[\"谁来做\",{\"1\":{\"78\":1}}],[\"做什么和怎么做\",{\"1\":{\"78\":1}}],[\"做的内容和活动类策划\",{\"1\":{\"20\":1}}],[\"美陈\",{\"1\":{\"534\":1}}],[\"美陈设计\",{\"1\":{\"20\":1,\"21\":1}}],[\"美国弗吉尼亚理工大学教授雅克曼认为\",{\"1\":{\"78\":1}}],[\"过度消费导致资源浪费\",{\"1\":{\"531\":1}}],[\"过度消费\",{\"1\":{\"531\":1}}],[\"过渡性业务建模和技术建模等概念\",{\"1\":{\"525\":1}}],[\"过往很多儿童艺术教育还是停留在传统绘画\",{\"1\":{\"77\":1}}],[\"过去基本都是以浏览为主\",{\"1\":{\"20\":1}}],[\"到选择穿哪件衣服\",{\"1\":{\"532\":1}}],[\"到了结构层\",{\"1\":{\"521\":1}}],[\"到重庆参加国际儿童艺术节\",{\"1\":{\"77\":1}}],[\"到最高的塔顶去\",{\"1\":{\"66\":1}}],[\"z53dq\",{\"1\":{\"249\":1}}],[\"zdravko\",{\"1\":{\"238\":1}}],[\"zelong\",{\"1\":{\"485\":1}}],[\"zecheng\",{\"1\":{\"478\":1}}],[\"zeke\",{\"1\":{\"462\":1}}],[\"zephyr\",{\"1\":{\"404\":2}}],[\"zeyao\",{\"1\":{\"480\":1}}],[\"zeyi\",{\"1\":{\"313\":1}}],[\"zeyu\",{\"1\":{\"197\":1,\"224\":1,\"306\":1}}],[\"zezhong\",{\"1\":{\"243\":1}}],[\"zero\",{\"0\":{\"192\":1,\"266\":1,\"386\":1,\"388\":1,\"417\":1,\"444\":1,\"497\":1},\"1\":{\"192\":3,\"266\":1,\"323\":1,\"327\":1,\"349\":1,\"350\":1,\"355\":1,\"369\":1,\"372\":1,\"376\":1,\"382\":1,\"386\":3,\"388\":1,\"417\":1,\"442\":1,\"444\":1,\"456\":1,\"472\":1,\"481\":1,\"485\":2,\"497\":1}}],[\"zeng\",{\"1\":{\"173\":1,\"224\":1,\"260\":1,\"449\":1}}],[\"zui近各大美院的毕业展也开始准备举行了\",{\"1\":{\"530\":1}}],[\"zupan\",{\"1\":{\"301\":1}}],[\"zubizarreta\",{\"1\":{\"186\":1}}],[\"zulekha\",{\"1\":{\"184\":1,\"380\":1}}],[\"zahra\",{\"1\":{\"481\":1}}],[\"zaiwen\",{\"1\":{\"447\":1}}],[\"zainab\",{\"1\":{\"206\":1}}],[\"zachary\",{\"1\":{\"288\":1}}],[\"zackory\",{\"1\":{\"184\":1,\"380\":1}}],[\"zakariya\",{\"1\":{\"288\":1}}],[\"zaninello\",{\"1\":{\"315\":1}}],[\"zande\",{\"1\":{\"278\":1}}],[\"zancanaro\",{\"1\":{\"158\":1}}],[\"zafeirid\",{\"1\":{\"237\":1}}],[\"zarrin\",{\"1\":{\"176\":1}}],[\"zaria\",{\"1\":{\"155\":1}}],[\"zad\",{\"1\":{\"117\":1}}],[\"zou2023universal\",{\"1\":{\"313\":1}}],[\"zou\",{\"1\":{\"128\":1,\"428\":1}}],[\"zones\",{\"1\":{\"113\":1}}],[\"zoo\",{\"1\":{\"9\":2}}],[\"zichen\",{\"1\":{\"489\":1}}],[\"zijun\",{\"1\":{\"480\":1}}],[\"zijie\",{\"1\":{\"258\":1,\"433\":1}}],[\"zijian\",{\"1\":{\"222\":1}}],[\"zilong\",{\"1\":{\"455\":1}}],[\"zilin\",{\"1\":{\"251\":1,\"423\":1}}],[\"zibin\",{\"1\":{\"445\":1}}],[\"ziegele\",{\"1\":{\"400\":1}}],[\"ziems\",{\"1\":{\"180\":1}}],[\"ziyan\",{\"1\":{\"425\":1}}],[\"ziyang\",{\"1\":{\"377\":1}}],[\"ziyu\",{\"1\":{\"132\":1,\"164\":1,\"330\":1}}],[\"ziyue\",{\"1\":{\"108\":1}}],[\"ziqi\",{\"1\":{\"360\":1}}],[\"ziabari\",{\"1\":{\"461\":1}}],[\"ziaber\",{\"1\":{\"350\":1}}],[\"ziatdinov\",{\"1\":{\"112\":1}}],[\"zineb\",{\"1\":{\"350\":1}}],[\"zining\",{\"1\":{\"306\":1}}],[\"ziwei\",{\"1\":{\"283\":1}}],[\"zimo\",{\"1\":{\"192\":1}}],[\"zihao\",{\"1\":{\"366\":1}}],[\"zihan\",{\"1\":{\"170\":1,\"322\":1,\"455\":1,\"471\":1}}],[\"zihuan\",{\"1\":{\"138\":1}}],[\"zhijiang\",{\"1\":{\"483\":1}}],[\"zhivar\",{\"1\":{\"461\":1}}],[\"zhiliang\",{\"1\":{\"403\":1}}],[\"zhile\",{\"1\":{\"210\":1}}],[\"zhiqiang\",{\"1\":{\"394\":1}}],[\"zhiting\",{\"1\":{\"353\":1}}],[\"zhibo\",{\"1\":{\"322\":1}}],[\"zhiyu\",{\"1\":{\"405\":1}}],[\"zhiyuan\",{\"1\":{\"316\":1,\"411\":1}}],[\"zhiying\",{\"1\":{\"478\":1}}],[\"zhiyi\",{\"1\":{\"235\":1}}],[\"zhiyong\",{\"1\":{\"235\":1}}],[\"zhiwei\",{\"1\":{\"235\":1}}],[\"zhi\",{\"1\":{\"224\":1,\"428\":1}}],[\"zhihao\",{\"1\":{\"378\":1}}],[\"zhihang\",{\"1\":{\"378\":1}}],[\"zhihan\",{\"1\":{\"207\":1}}],[\"zhihui\",{\"1\":{\"223\":1}}],[\"zhicheng\",{\"1\":{\"124\":1,\"213\":1}}],[\"zhuo\",{\"1\":{\"529\":1}}],[\"zhuo推崇\",{\"1\":{\"527\":1}}],[\"zhucong\",{\"1\":{\"361\":1}}],[\"zhu\",{\"1\":{\"173\":1,\"257\":1,\"261\":1,\"349\":1,\"393\":1,\"395\":1,\"430\":1,\"487\":1,\"489\":1}}],[\"zhuang\",{\"1\":{\"141\":1,\"173\":1}}],[\"zhe\",{\"1\":{\"151\":1,\"219\":1,\"348\":1,\"401\":1}}],[\"zhen\",{\"1\":{\"353\":1,\"403\":1,\"437\":1}}],[\"zhensong\",{\"1\":{\"235\":1}}],[\"zhengxuan\",{\"1\":{\"428\":1}}],[\"zhengyang\",{\"1\":{\"425\":1}}],[\"zhengyuan\",{\"1\":{\"130\":1}}],[\"zhenghao\",{\"1\":{\"411\":1}}],[\"zhengwei\",{\"1\":{\"314\":1}}],[\"zheng\",{\"1\":{\"214\":1,\"241\":1,\"329\":1,\"377\":1,\"425\":2,\"445\":1}}],[\"zhenhao\",{\"1\":{\"150\":1}}],[\"zhonghao\",{\"1\":{\"261\":1}}],[\"zhong\",{\"1\":{\"150\":1}}],[\"zhouliang\",{\"1\":{\"377\":1}}],[\"zhou\",{\"1\":{\"103\":1,\"150\":2,\"173\":1,\"195\":1,\"218\":1,\"316\":1,\"328\":1,\"360\":1,\"377\":1,\"411\":1,\"471\":1,\"479\":1,\"480\":1,\"483\":1,\"494\":1,\"495\":1}}],[\"zhai\",{\"1\":{\"486\":1}}],[\"zhan\",{\"1\":{\"432\":1}}],[\"zhang\",{\"1\":{\"105\":1,\"132\":1,\"138\":3,\"151\":1,\"175\":1,\"176\":1,\"207\":1,\"210\":1,\"223\":1,\"224\":2,\"235\":1,\"245\":1,\"283\":1,\"295\":1,\"306\":1,\"316\":1,\"322\":1,\"324\":1,\"326\":1,\"330\":1,\"337\":1,\"342\":1,\"348\":1,\"359\":1,\"362\":1,\"377\":1,\"382\":1,\"385\":1,\"391\":4,\"392\":1,\"412\":1,\"414\":2,\"425\":1,\"428\":1,\"429\":1,\"437\":1,\"462\":1,\"465\":1,\"467\":1,\"471\":1,\"479\":2,\"480\":4,\"482\":1,\"485\":1,\"488\":2,\"489\":1,\"495\":1}}],[\"zhaoxin\",{\"1\":{\"479\":1}}],[\"zhao\",{\"1\":{\"138\":1,\"215\":1,\"224\":1,\"270\":1,\"275\":1,\"277\":1,\"316\":1,\"391\":1,\"425\":2,\"428\":1,\"432\":1,\"443\":1,\"480\":1}}],[\"z\",{\"1\":{\"74\":1,\"302\":1}}],[\"gcg\",{\"1\":{\"313\":2,\"409\":1}}],[\"gcg~\",{\"1\":{\"313\":1}}],[\"gsm8k\",{\"1\":{\"494\":2}}],[\"gsm\",{\"1\":{\"298\":1}}],[\"gdpr\",{\"1\":{\"254\":1}}],[\"ghouthi\",{\"1\":{\"444\":1}}],[\"ghosh\",{\"1\":{\"344\":1}}],[\"ghasemi\",{\"1\":{\"498\":1}}],[\"ghahroodi\",{\"1\":{\"333\":1}}],[\"ghanavati\",{\"1\":{\"254\":1}}],[\"ghulam\",{\"1\":{\"197\":1}}],[\"gningue\",{\"1\":{\"232\":1}}],[\"gnadlinger\",{\"1\":{\"129\":1}}],[\"görtler\",{\"1\":{\"210\":1}}],[\"gpu\",{\"1\":{\"462\":1}}],[\"gpus\",{\"1\":{\"462\":1,\"465\":1,\"479\":1}}],[\"gps\",{\"1\":{\"298\":2}}],[\"gpt2\",{\"1\":{\"381\":2}}],[\"gpt3\",{\"1\":{\"381\":2,\"461\":1}}],[\"gpt4\",{\"1\":{\"325\":1,\"381\":1}}],[\"gpt\",{\"0\":{\"228\":1},\"1\":{\"151\":1,\"161\":1,\"313\":1,\"324\":1,\"327\":1,\"334\":2,\"340\":1,\"345\":1,\"348\":1,\"351\":1,\"353\":1,\"354\":1,\"358\":3,\"365\":1,\"372\":1,\"391\":2,\"394\":1,\"404\":3,\"408\":1,\"409\":2,\"411\":1,\"412\":1,\"422\":4,\"424\":10,\"430\":1,\"442\":1,\"443\":1,\"447\":2,\"458\":2,\"460\":2,\"467\":1,\"471\":2,\"478\":1,\"481\":2,\"486\":4,\"492\":2,\"494\":1,\"495\":3,\"497\":5,\"512\":1}}],[\"gplab\",{\"1\":{\"4\":1}}],[\"gripon\",{\"1\":{\"444\":1}}],[\"griffin\",{\"1\":{\"427\":3}}],[\"grid\",{\"1\":{\"112\":1}}],[\"greener\",{\"0\":{\"465\":1}}],[\"greedy\",{\"1\":{\"318\":1,\"487\":1}}],[\"greiner\",{\"1\":{\"458\":1}}],[\"gregory\",{\"1\":{\"407\":1}}],[\"grey\",{\"1\":{\"253\":1}}],[\"greater\",{\"1\":{\"302\":1,\"304\":1,\"415\":1,\"466\":1,\"488\":1}}],[\"great\",{\"0\":{\"418\":1},\"1\":{\"224\":1,\"261\":1,\"275\":1,\"289\":1,\"369\":1,\"436\":1,\"437\":1}}],[\"greatly\",{\"1\":{\"212\":1,\"462\":1,\"467\":1}}],[\"grundy\",{\"1\":{\"154\":1,\"396\":1}}],[\"groth\",{\"1\":{\"388\":1}}],[\"groh\",{\"1\":{\"327\":1}}],[\"grows\",{\"1\":{\"349\":1}}],[\"grown\",{\"1\":{\"292\":1}}],[\"grow\",{\"1\":{\"126\":1,\"213\":1,\"479\":1}}],[\"growth\",{\"1\":{\"126\":1,\"210\":1,\"218\":1,\"253\":1,\"274\":4,\"428\":1,\"469\":1}}],[\"growing\",{\"1\":{\"119\":2,\"176\":1,\"203\":1,\"213\":1,\"237\":1,\"272\":1,\"385\":1,\"439\":1}}],[\"groundwork\",{\"1\":{\"159\":1,\"356\":1}}],[\"grounding\",{\"0\":{\"345\":1,\"494\":1},\"1\":{\"151\":2,\"275\":1,\"345\":2,\"348\":2}}],[\"grounded\",{\"0\":{\"151\":1,\"348\":1},\"1\":{\"167\":1,\"171\":1,\"197\":1,\"260\":1}}],[\"ground\",{\"1\":{\"132\":1,\"199\":1,\"246\":1,\"330\":1,\"390\":1,\"470\":1}}],[\"groundbreaking\",{\"1\":{\"123\":1}}],[\"groupings\",{\"1\":{\"368\":2}}],[\"group\",{\"0\":{\"230\":1},\"1\":{\"108\":1,\"132\":2,\"149\":1,\"155\":1,\"176\":1,\"178\":1,\"195\":1,\"196\":1,\"230\":1,\"243\":1,\"245\":1,\"269\":1,\"290\":5,\"291\":2,\"307\":1,\"330\":2,\"416\":1,\"462\":1,\"476\":2}}],[\"groups\",{\"1\":{\"105\":1,\"108\":1,\"130\":1,\"149\":1,\"160\":1,\"164\":1,\"173\":1,\"176\":1,\"189\":2,\"217\":1,\"237\":1,\"462\":1,\"468\":1}}],[\"grammars\",{\"1\":{\"490\":1}}],[\"graham\",{\"1\":{\"345\":1}}],[\"grace\",{\"1\":{\"231\":1}}],[\"grade\",{\"1\":{\"486\":1}}],[\"graded\",{\"1\":{\"363\":1}}],[\"gradually\",{\"1\":{\"418\":1}}],[\"graduate\",{\"1\":{\"172\":1,\"291\":1,\"307\":1,\"370\":1,\"476\":1}}],[\"gradient\",{\"1\":{\"224\":1,\"260\":1,\"305\":1,\"378\":2}}],[\"gradients\",{\"1\":{\"102\":1,\"213\":1,\"378\":1}}],[\"grayden\",{\"1\":{\"166\":1}}],[\"grained\",{\"1\":{\"141\":1,\"345\":1,\"368\":1,\"376\":1,\"430\":2}}],[\"granitzer\",{\"1\":{\"408\":1}}],[\"granholm\",{\"1\":{\"268\":1,\"452\":1}}],[\"granular\",{\"1\":{\"205\":1,\"479\":1}}],[\"granularity\",{\"1\":{\"135\":2,\"228\":2,\"392\":1,\"444\":1}}],[\"granted\",{\"1\":{\"128\":1}}],[\"grasp\",{\"1\":{\"124\":1,\"245\":1}}],[\"graphs\",{\"0\":{\"252\":1,\"314\":1,\"426\":1},\"1\":{\"252\":1,\"314\":1,\"426\":1}}],[\"graphical\",{\"1\":{\"222\":1,\"339\":1,\"361\":1}}],[\"graphics\",{\"1\":{\"165\":1}}],[\"graphite\",{\"1\":{\"207\":1}}],[\"graph\",{\"0\":{\"118\":1,\"382\":1,\"390\":1},\"1\":{\"118\":2,\"223\":2,\"252\":1,\"318\":1,\"382\":1,\"390\":2,\"399\":1,\"426\":1,\"432\":2}}],[\"gustavo\",{\"1\":{\"419\":1,\"420\":1}}],[\"gumus\",{\"1\":{\"408\":1}}],[\"gutenberg\",{\"1\":{\"369\":1}}],[\"guanlin\",{\"1\":{\"480\":1}}],[\"guande\",{\"1\":{\"275\":1}}],[\"guarantee\",{\"1\":{\"434\":1}}],[\"guaranteed\",{\"1\":{\"355\":2}}],[\"guarantees\",{\"1\":{\"344\":1,\"378\":1}}],[\"guardrails\",{\"1\":{\"211\":1,\"374\":1}}],[\"guimarães\",{\"1\":{\"533\":1}}],[\"guimarãesa\",{\"1\":{\"533\":1}}],[\"guinand\",{\"1\":{\"498\":1}}],[\"guixuan\",{\"1\":{\"224\":1}}],[\"gui\",{\"1\":{\"219\":1,\"222\":1,\"293\":1,\"401\":1}}],[\"guiding\",{\"1\":{\"160\":1,\"301\":1,\"316\":1,\"360\":1,\"447\":1}}],[\"guidance\",{\"0\":{\"160\":1},\"1\":{\"160\":4,\"197\":1,\"228\":1,\"233\":1,\"353\":1,\"462\":1}}],[\"guides\",{\"1\":{\"407\":1,\"449\":1,\"470\":1}}],[\"guideline\",{\"1\":{\"271\":1,\"457\":1}}],[\"guidelines\",{\"1\":{\"114\":1,\"117\":1,\"163\":1,\"184\":1,\"191\":1,\"197\":1,\"380\":1,\"447\":1,\"475\":1,\"492\":1}}],[\"guided\",{\"0\":{\"118\":1},\"1\":{\"118\":1,\"158\":4,\"173\":1,\"289\":1,\"462\":1}}],[\"guide\",{\"0\":{\"159\":1},\"1\":{\"110\":1,\"188\":1,\"252\":1,\"291\":1,\"321\":1,\"334\":1,\"364\":1,\"426\":1,\"476\":1}}],[\"gu\",{\"1\":{\"176\":1,\"353\":1}}],[\"guyue\",{\"1\":{\"173\":1}}],[\"guetschel\",{\"1\":{\"188\":1}}],[\"guenda\",{\"1\":{\"171\":1}}],[\"guests\",{\"1\":{\"105\":1}}],[\"guoqiang\",{\"1\":{\"488\":1}}],[\"guorui\",{\"1\":{\"377\":1}}],[\"guo\",{\"1\":{\"141\":1,\"231\":1,\"399\":1,\"402\":1,\"483\":1}}],[\"gunasekaran\",{\"1\":{\"108\":1}}],[\"gupta\",{\"1\":{\"99\":1,\"184\":1,\"380\":1}}],[\"goiri\",{\"1\":{\"465\":1}}],[\"going\",{\"1\":{\"135\":1}}],[\"gomez\",{\"1\":{\"419\":1,\"420\":1}}],[\"gorilla\",{\"1\":{\"326\":2}}],[\"gordon\",{\"1\":{\"122\":1}}],[\"gonzalez\",{\"1\":{\"315\":1,\"326\":1}}],[\"gonzález\",{\"1\":{\"282\":1}}],[\"gong\",{\"1\":{\"173\":1,\"438\":1}}],[\"godbole\",{\"1\":{\"266\":1}}],[\"government\",{\"1\":{\"252\":1,\"426\":1}}],[\"governance\",{\"1\":{\"205\":2,\"230\":1}}],[\"goex\",{\"0\":{\"326\":1},\"1\":{\"326\":2}}],[\"goebel\",{\"1\":{\"220\":1}}],[\"goel\",{\"1\":{\"172\":1,\"370\":1}}],[\"golden\",{\"1\":{\"470\":2}}],[\"goldberg\",{\"1\":{\"340\":1}}],[\"gold\",{\"1\":{\"194\":1}}],[\"goyal\",{\"1\":{\"172\":1,\"370\":1}}],[\"go\",{\"0\":{\"171\":1},\"1\":{\"301\":1,\"419\":1,\"420\":1,\"482\":1}}],[\"goog\",{\"1\":{\"539\":1}}],[\"google\",{\"0\":{\"128\":1},\"1\":{\"128\":1,\"184\":1,\"192\":1,\"257\":1,\"334\":1,\"380\":1,\"494\":1}}],[\"gool\",{\"1\":{\"399\":1}}],[\"goods\",{\"1\":{\"205\":1}}],[\"good\",{\"0\":{\"107\":1,\"236\":1,\"438\":1,\"450\":1},\"1\":{\"256\":1,\"334\":1,\"381\":1,\"469\":1}}],[\"goo\",{\"1\":{\"104\":1}}],[\"goals\",{\"0\":{\"216\":1},\"1\":{\"96\":5,\"98\":1,\"173\":1,\"214\":1,\"216\":2,\"222\":1,\"244\":1,\"275\":2,\"307\":1,\"399\":1}}],[\"goal\",{\"0\":{\"96\":1},\"1\":{\"96\":3,\"156\":1,\"182\":1,\"217\":1,\"265\":1,\"269\":1,\"326\":1,\"389\":1,\"395\":2,\"444\":1,\"465\":1}}],[\"glm\",{\"1\":{\"447\":1}}],[\"gloss\",{\"1\":{\"438\":1}}],[\"globe\",{\"1\":{\"164\":1}}],[\"global\",{\"1\":{\"131\":1,\"137\":1,\"260\":1,\"277\":1,\"314\":1,\"318\":2,\"390\":2,\"428\":1,\"462\":2,\"498\":1}}],[\"gligoric\",{\"1\":{\"241\":1}}],[\"glyphs\",{\"1\":{\"213\":1}}],[\"glasses\",{\"1\":{\"117\":1}}],[\"glaser\",{\"1\":{\"101\":1}}],[\"glean\",{\"1\":{\"236\":1}}],[\"gle\",{\"1\":{\"104\":1}}],[\"glen\",{\"1\":{\"99\":1}}],[\"giving\",{\"1\":{\"489\":1}}],[\"give\",{\"0\":{\"280\":1},\"1\":{\"169\":1,\"363\":1}}],[\"gives\",{\"1\":{\"153\":1,\"161\":1,\"354\":1}}],[\"given\",{\"1\":{\"96\":1,\"101\":1,\"130\":1,\"143\":1,\"151\":1,\"174\":1,\"178\":1,\"181\":1,\"196\":1,\"198\":1,\"200\":1,\"211\":1,\"217\":1,\"257\":1,\"280\":1,\"313\":1,\"348\":1,\"350\":2,\"355\":1,\"369\":1,\"371\":1,\"387\":1,\"395\":1,\"422\":2,\"432\":1,\"445\":1,\"449\":1,\"465\":1,\"478\":1}}],[\"gipplab\",{\"1\":{\"458\":1}}],[\"gipp\",{\"1\":{\"458\":1}}],[\"giessing\",{\"1\":{\"458\":1}}],[\"giulia\",{\"1\":{\"444\":1}}],[\"giuseppe\",{\"1\":{\"157\":1,\"338\":1,\"405\":1}}],[\"gilbert\",{\"1\":{\"304\":1}}],[\"gil\",{\"1\":{\"282\":1}}],[\"gibson\",{\"1\":{\"194\":1}}],[\"gioacchini\",{\"1\":{\"338\":1}}],[\"giorgia\",{\"1\":{\"292\":1}}],[\"gionnieve\",{\"1\":{\"161\":1,\"354\":1}}],[\"giovanni\",{\"1\":{\"98\":1,\"158\":1}}],[\"gianluca\",{\"1\":{\"368\":1}}],[\"giang\",{\"1\":{\"159\":1}}],[\"gianpaolo\",{\"1\":{\"120\":1}}],[\"gi\",{\"1\":{\"146\":1}}],[\"gig\",{\"0\":{\"135\":1,\"293\":1},\"1\":{\"135\":2,\"293\":9}}],[\"git\",{\"1\":{\"123\":1}}],[\"github\",{\"0\":{\"495\":1},\"1\":{\"123\":2,\"137\":1,\"159\":1,\"235\":1,\"258\":1,\"260\":1,\"316\":1,\"326\":1,\"337\":1,\"338\":1,\"350\":1,\"366\":1,\"375\":1,\"379\":1,\"381\":1,\"388\":1,\"391\":1,\"393\":1,\"398\":2,\"409\":1,\"427\":1,\"432\":1,\"433\":1,\"443\":1,\"456\":1,\"458\":1,\"482\":1,\"485\":1,\"487\":1,\"494\":1,\"495\":6}}],[\"gifts\",{\"0\":{\"107\":1}}],[\"gautam\",{\"1\":{\"395\":1}}],[\"gauthier\",{\"1\":{\"232\":1}}],[\"gawon\",{\"1\":{\"381\":2}}],[\"gaeun\",{\"1\":{\"356\":1}}],[\"gayla\",{\"1\":{\"281\":1}}],[\"garneau\",{\"1\":{\"328\":1}}],[\"garnered\",{\"1\":{\"226\":1,\"362\":1,\"472\":1}}],[\"garcía\",{\"1\":{\"282\":1,\"315\":1}}],[\"garcia\",{\"1\":{\"126\":2,\"416\":1}}],[\"garg\",{\"1\":{\"266\":1}}],[\"gating\",{\"1\":{\"432\":1}}],[\"gatto\",{\"1\":{\"431\":1}}],[\"gate\",{\"1\":{\"363\":1}}],[\"gat\",{\"0\":{\"223\":1},\"1\":{\"223\":2}}],[\"gathering\",{\"1\":{\"491\":1}}],[\"gather\",{\"1\":{\"133\":1,\"151\":1,\"331\":1,\"348\":1}}],[\"gathered\",{\"1\":{\"116\":1}}],[\"gaowen\",{\"1\":{\"442\":1}}],[\"gao\",{\"1\":{\"214\":1,\"224\":1,\"271\":1,\"290\":1,\"297\":1,\"353\":1,\"377\":1,\"457\":1,\"483\":1}}],[\"galinkin\",{\"1\":{\"344\":1}}],[\"gal\",{\"1\":{\"199\":1}}],[\"ganqu\",{\"1\":{\"411\":1}}],[\"gans\",{\"1\":{\"289\":2}}],[\"gan\",{\"1\":{\"151\":1,\"213\":1,\"348\":1,\"366\":1}}],[\"ganesh\",{\"1\":{\"137\":1}}],[\"gashteovski\",{\"1\":{\"338\":1}}],[\"gasca\",{\"1\":{\"147\":1}}],[\"gastronomic\",{\"1\":{\"142\":1}}],[\"gastrophysics\",{\"1\":{\"142\":1}}],[\"gabriella\",{\"1\":{\"196\":1}}],[\"gabriel\",{\"1\":{\"111\":1,\"156\":1,\"252\":1,\"426\":1}}],[\"gaze\",{\"0\":{\"116\":1,\"118\":1,\"185\":1,\"273\":1},\"1\":{\"108\":1,\"116\":8,\"118\":5,\"127\":1,\"185\":2,\"273\":6}}],[\"gains\",{\"1\":{\"183\":1,\"215\":2,\"386\":1,\"393\":1,\"441\":1,\"492\":1}}],[\"gained\",{\"1\":{\"169\":1,\"443\":1,\"445\":1}}],[\"gaining\",{\"1\":{\"152\":1}}],[\"gain\",{\"1\":{\"102\":1,\"159\":1,\"172\":1,\"191\":1,\"360\":1,\"370\":1,\"427\":1}}],[\"gambardella\",{\"1\":{\"292\":1}}],[\"gamified\",{\"1\":{\"133\":1,\"331\":1}}],[\"gaming\",{\"1\":{\"98\":1,\"120\":1,\"203\":1}}],[\"gameplay\",{\"0\":{\"203\":1,\"265\":1},\"1\":{\"98\":3,\"265\":2}}],[\"games\",{\"0\":{\"98\":1,\"204\":1},\"1\":{\"98\":4,\"204\":1,\"245\":1,\"265\":5,\"275\":1}}],[\"game\",{\"0\":{\"98\":1,\"245\":1,\"265\":1,\"304\":1,\"403\":1},\"1\":{\"98\":3,\"201\":1,\"205\":1,\"245\":4,\"265\":2,\"304\":1,\"403\":3}}],[\"gaps\",{\"0\":{\"122\":1},\"1\":{\"122\":5,\"215\":1,\"301\":1,\"345\":1,\"395\":1,\"458\":1}}],[\"gap\",{\"0\":{\"216\":1,\"492\":1},\"1\":{\"97\":1,\"133\":1,\"169\":1,\"175\":1,\"182\":1,\"212\":1,\"216\":1,\"266\":1,\"282\":1,\"312\":1,\"323\":1,\"328\":1,\"331\":1,\"353\":1,\"379\":1,\"408\":2,\"412\":1,\"418\":1,\"428\":1,\"430\":1,\"437\":2,\"438\":1,\"491\":2,\"496\":1}}],[\"g\",{\"1\":{\"96\":1,\"102\":1,\"124\":1,\"126\":1,\"128\":3,\"133\":1,\"151\":1,\"192\":1,\"198\":1,\"211\":1,\"220\":1,\"222\":1,\"224\":1,\"240\":1,\"241\":1,\"252\":1,\"268\":1,\"270\":1,\"301\":1,\"319\":1,\"326\":2,\"331\":1,\"348\":1,\"353\":1,\"382\":1,\"387\":1,\"390\":1,\"391\":2,\"394\":1,\"409\":4,\"417\":1,\"426\":1,\"427\":1,\"441\":1,\"442\":1,\"444\":1,\"452\":1,\"454\":1,\"455\":1,\"483\":1,\"494\":1}}],[\"gehring\",{\"1\":{\"441\":1}}],[\"gemma\",{\"1\":{\"369\":2,\"409\":1}}],[\"gemini\",{\"1\":{\"334\":1,\"345\":1,\"372\":1,\"418\":2,\"461\":1,\"467\":1,\"478\":1}}],[\"get\",{\"0\":{\"435\":1},\"1\":{\"402\":1,\"462\":1}}],[\"getting\",{\"1\":{\"279\":1,\"405\":1}}],[\"getis\",{\"1\":{\"146\":1}}],[\"gebreegziabher\",{\"1\":{\"271\":1,\"457\":1}}],[\"german\",{\"1\":{\"315\":1}}],[\"gerjets\",{\"1\":{\"273\":1}}],[\"gerard\",{\"1\":{\"233\":1}}],[\"gerd\",{\"1\":{\"112\":1}}],[\"ge\",{\"1\":{\"214\":1,\"377\":1,\"412\":1,\"429\":1,\"485\":1}}],[\"gelareh\",{\"1\":{\"204\":1}}],[\"gesture\",{\"0\":{\"149\":1,\"224\":1,\"235\":1,\"269\":1},\"1\":{\"149\":1,\"224\":2,\"235\":3,\"269\":1}}],[\"gestures\",{\"1\":{\"126\":1,\"149\":6,\"224\":5,\"235\":3,\"240\":1,\"269\":1}}],[\"geographic\",{\"1\":{\"394\":1,\"431\":1}}],[\"geographical\",{\"1\":{\"270\":1,\"417\":4}}],[\"georg\",{\"1\":{\"327\":1}}],[\"georgia\",{\"1\":{\"278\":1}}],[\"georgios\",{\"1\":{\"123\":1}}],[\"geometric\",{\"1\":{\"259\":1}}],[\"geological\",{\"1\":{\"249\":1}}],[\"geospatial\",{\"1\":{\"146\":2,\"272\":1}}],[\"geyu\",{\"1\":{\"130\":1}}],[\"geng\",{\"1\":{\"438\":1,\"479\":1}}],[\"genqrensemblerf\",{\"1\":{\"386\":2}}],[\"genqrensemble\",{\"0\":{\"386\":1},\"1\":{\"386\":2}}],[\"gen4ds\",{\"0\":{\"243\":1},\"1\":{\"243\":1}}],[\"genetic\",{\"1\":{\"220\":1}}],[\"generic\",{\"1\":{\"153\":1,\"180\":1,\"350\":1}}],[\"generalists\",{\"0\":{\"411\":1}}],[\"generalize\",{\"1\":{\"475\":1,\"497\":1}}],[\"generalized\",{\"1\":{\"139\":1}}],[\"generalizing\",{\"1\":{\"376\":1}}],[\"generalizability\",{\"1\":{\"488\":1}}],[\"generalizable\",{\"1\":{\"257\":1,\"352\":1,\"369\":1}}],[\"generalization\",{\"0\":{\"417\":1},\"1\":{\"255\":1,\"283\":1,\"417\":1,\"472\":1,\"478\":1,\"497\":1}}],[\"generally\",{\"1\":{\"203\":1,\"221\":1,\"339\":1,\"413\":1,\"450\":1}}],[\"general\",{\"0\":{\"471\":1},\"1\":{\"117\":1,\"125\":1,\"131\":1,\"135\":1,\"151\":1,\"176\":1,\"216\":1,\"228\":1,\"242\":1,\"257\":1,\"265\":1,\"287\":1,\"317\":1,\"323\":1,\"344\":1,\"345\":1,\"348\":1,\"378\":1,\"379\":2,\"384\":1,\"411\":1,\"418\":1,\"449\":2,\"454\":1,\"480\":1}}],[\"generators\",{\"0\":{\"481\":1},\"1\":{\"422\":1,\"539\":2}}],[\"generator\",{\"1\":{\"259\":1,\"349\":1,\"422\":1,\"485\":1}}],[\"generative\",{\"0\":{\"154\":1,\"212\":1,\"239\":1,\"243\":1,\"283\":1,\"289\":1,\"313\":1,\"386\":1},\"1\":{\"125\":1,\"154\":1,\"209\":1,\"212\":1,\"213\":2,\"229\":1,\"239\":2,\"243\":3,\"250\":1,\"260\":1,\"289\":1,\"313\":2,\"333\":1,\"339\":1,\"344\":1,\"358\":2,\"385\":1,\"407\":1,\"485\":4}}],[\"generations\",{\"1\":{\"482\":1}}],[\"generation\",{\"0\":{\"138\":1,\"169\":1,\"224\":1,\"235\":1,\"258\":1,\"266\":1,\"329\":1,\"335\":1,\"339\":1,\"349\":1,\"356\":1,\"360\":1,\"384\":1,\"392\":1,\"415\":1,\"422\":1,\"427\":1,\"433\":1,\"437\":1,\"441\":1},\"1\":{\"101\":2,\"110\":4,\"138\":1,\"169\":3,\"179\":1,\"184\":1,\"219\":2,\"222\":1,\"224\":1,\"235\":2,\"248\":1,\"257\":1,\"258\":3,\"264\":1,\"266\":1,\"282\":1,\"283\":1,\"289\":1,\"313\":1,\"315\":1,\"319\":1,\"321\":4,\"323\":1,\"329\":3,\"339\":2,\"342\":1,\"349\":1,\"358\":2,\"359\":2,\"360\":2,\"372\":1,\"375\":1,\"378\":1,\"379\":1,\"380\":1,\"382\":1,\"384\":1,\"385\":1,\"388\":1,\"389\":1,\"392\":2,\"396\":1,\"401\":2,\"402\":1,\"404\":1,\"405\":1,\"411\":1,\"415\":2,\"422\":3,\"427\":2,\"433\":3,\"437\":4,\"441\":1,\"448\":1,\"453\":1,\"454\":1,\"466\":1,\"467\":2,\"471\":1,\"482\":2,\"483\":1,\"485\":2,\"487\":1,\"495\":1}}],[\"generating\",{\"0\":{\"101\":1,\"222\":1},\"1\":{\"172\":3,\"188\":1,\"213\":1,\"222\":1,\"224\":1,\"251\":1,\"289\":1,\"319\":1,\"334\":1,\"335\":1,\"349\":1,\"353\":2,\"356\":1,\"360\":1,\"370\":3,\"375\":1,\"385\":1,\"392\":1,\"410\":1,\"413\":1,\"415\":1,\"416\":1,\"423\":1,\"441\":1,\"454\":1,\"467\":1,\"479\":1,\"481\":2,\"490\":1}}],[\"generates\",{\"1\":{\"124\":1,\"246\":1,\"319\":1,\"322\":1,\"329\":1,\"335\":2,\"386\":1,\"407\":1,\"410\":2,\"422\":1,\"490\":1}}],[\"generated\",{\"0\":{\"110\":1,\"213\":1,\"228\":1,\"321\":1,\"342\":1,\"352\":1,\"385\":1,\"410\":1,\"491\":1},\"1\":{\"101\":1,\"102\":1,\"110\":2,\"116\":1,\"118\":1,\"133\":2,\"154\":1,\"213\":3,\"256\":1,\"258\":1,\"266\":2,\"289\":1,\"305\":1,\"321\":2,\"324\":2,\"326\":3,\"329\":1,\"331\":2,\"335\":1,\"336\":1,\"339\":2,\"342\":2,\"349\":2,\"352\":2,\"353\":1,\"359\":1,\"368\":1,\"384\":2,\"385\":1,\"388\":1,\"410\":4,\"415\":1,\"422\":2,\"433\":1,\"437\":2,\"439\":4,\"442\":1,\"454\":1,\"466\":1,\"467\":1,\"477\":1,\"480\":1,\"491\":1,\"496\":1}}],[\"generate\",{\"0\":{\"481\":1},\"1\":{\"101\":2,\"121\":1,\"123\":1,\"142\":1,\"172\":1,\"205\":1,\"212\":1,\"213\":1,\"215\":1,\"218\":1,\"219\":1,\"235\":2,\"242\":1,\"258\":1,\"268\":1,\"288\":1,\"289\":2,\"313\":1,\"334\":2,\"335\":1,\"339\":1,\"342\":1,\"360\":1,\"363\":2,\"370\":1,\"374\":1,\"378\":1,\"381\":1,\"382\":1,\"386\":1,\"399\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"410\":1,\"415\":1,\"416\":1,\"422\":1,\"430\":2,\"433\":1,\"437\":1,\"442\":1,\"452\":1,\"458\":2,\"467\":1,\"481\":1,\"487\":2,\"489\":1,\"490\":1,\"496\":1}}],[\"generous\",{\"1\":{\"107\":2}}],[\"genermton\",{\"1\":{\"74\":1}}],[\"genuine\",{\"1\":{\"198\":3,\"266\":1,\"387\":3}}],[\"gently\",{\"1\":{\"158\":1}}],[\"genai\",{\"1\":{\"212\":8}}],[\"gena\",{\"1\":{\"149\":1}}],[\"gender\",{\"0\":{\"302\":1},\"1\":{\"106\":1,\"202\":1,\"302\":1,\"335\":1,\"394\":1,\"461\":1}}],[\"gear\",{\"1\":{\"81\":2}}],[\"ue\",{\"1\":{\"509\":1}}],[\"uq\",{\"1\":{\"467\":5}}],[\"uml\",{\"1\":{\"339\":1}}],[\"umap\",{\"0\":{\"188\":1},\"1\":{\"188\":7,\"301\":1}}],[\"uchicago\",{\"0\":{\"307\":1}}],[\"ucl\",{\"1\":{\"74\":1}}],[\"uk\",{\"1\":{\"302\":1}}],[\"udp\",{\"1\":{\"284\":1}}],[\"ultrainteract\",{\"1\":{\"411\":4}}],[\"ultra\",{\"0\":{\"415\":1},\"1\":{\"337\":1,\"418\":1}}],[\"ultralong\",{\"1\":{\"337\":1}}],[\"ultraeval\",{\"0\":{\"316\":1},\"1\":{\"316\":4}}],[\"ultimately\",{\"1\":{\"150\":1,\"171\":1,\"180\":1,\"223\":1,\"248\":1,\"386\":1,\"437\":1,\"470\":1}}],[\"ulrich\",{\"1\":{\"273\":1}}],[\"urbex\",{\"0\":{\"511\":1},\"1\":{\"504\":1,\"511\":1}}],[\"urbanization\",{\"1\":{\"298\":1}}],[\"urban\",{\"1\":{\"272\":5}}],[\"url\",{\"1\":{\"316\":1,\"458\":1}}],[\"uri\",{\"1\":{\"309\":1}}],[\"urgent\",{\"1\":{\"223\":1}}],[\"urgency\",{\"1\":{\"217\":1}}],[\"u\",{\"1\":{\"260\":2}}],[\"uxd和hcd在实践中往往过于关注个体的心理特征\",{\"1\":{\"522\":1}}],[\"uxd的目标是创建直观\",{\"1\":{\"522\":1}}],[\"uxd\",{\"1\":{\"522\":3}}],[\"ux设计和相关实践在反映整个社会群体的共享社会文化实践方面存在不足\",{\"1\":{\"522\":1}}],[\"ux\",{\"0\":{\"203\":1},\"1\":{\"203\":1,\"510\":1,\"516\":2,\"529\":2,\"531\":1,\"535\":1}}],[\"ufuk\",{\"1\":{\"195\":1}}],[\"ujjwal\",{\"1\":{\"172\":1,\"370\":1}}],[\"upto\",{\"1\":{\"386\":1}}],[\"upadhayay\",{\"1\":{\"334\":1}}],[\"updated\",{\"1\":{\"398\":1}}],[\"updates\",{\"1\":{\"260\":1,\"303\":1,\"333\":1,\"463\":1,\"479\":1}}],[\"updating\",{\"1\":{\"317\":1}}],[\"upper\",{\"0\":{\"308\":1},\"1\":{\"333\":1}}],[\"upholding\",{\"1\":{\"359\":1}}],[\"uphold\",{\"1\":{\"291\":1,\"396\":1,\"476\":1}}],[\"upcoming\",{\"1\":{\"268\":1,\"452\":1}}],[\"upstream\",{\"1\":{\"150\":1}}],[\"upon\",{\"1\":{\"142\":1,\"180\":1,\"183\":1,\"323\":1,\"393\":1,\"404\":1,\"428\":1,\"467\":1,\"488\":1}}],[\"up\",{\"0\":{\"251\":1,\"423\":1},\"1\":{\"126\":1,\"155\":2,\"170\":1,\"175\":1,\"228\":1,\"279\":1,\"284\":1,\"317\":1,\"327\":1,\"337\":1,\"366\":1,\"382\":1,\"386\":1,\"414\":1,\"427\":1,\"428\":2,\"434\":1,\"441\":1,\"460\":1,\"465\":1,\"466\":1,\"482\":1}}],[\"ubiquitous\",{\"1\":{\"125\":1,\"207\":1,\"253\":1,\"465\":1}}],[\"utebc\",{\"0\":{\"404\":1}}],[\"utility\",{\"1\":{\"146\":1,\"210\":1,\"215\":1,\"246\":1,\"270\":1,\"272\":1,\"281\":1,\"283\":2,\"287\":1,\"309\":1,\"338\":1,\"359\":2,\"374\":1,\"398\":2,\"427\":1,\"449\":1,\"490\":1}}],[\"utilization\",{\"1\":{\"182\":1,\"191\":1,\"333\":1,\"339\":1,\"362\":1,\"414\":1,\"460\":1,\"462\":1,\"477\":1}}],[\"utilizing\",{\"1\":{\"136\":1,\"142\":2,\"224\":1,\"246\":1,\"260\":1,\"349\":1,\"356\":1,\"377\":1,\"412\":1,\"432\":1,\"442\":1,\"445\":1,\"481\":1}}],[\"utilized\",{\"1\":{\"218\":1,\"259\":1,\"286\":1,\"381\":1,\"398\":1,\"474\":1}}],[\"utilizes\",{\"1\":{\"118\":1,\"284\":1,\"305\":1,\"324\":1,\"399\":1,\"453\":1,\"478\":1}}],[\"utilize\",{\"1\":{\"99\":2,\"116\":1,\"118\":1,\"184\":1,\"191\":1,\"212\":1,\"272\":1,\"308\":1,\"313\":1,\"329\":1,\"380\":1,\"402\":1,\"447\":1}}],[\"utilised\",{\"1\":{\"137\":1}}],[\"utilise\",{\"1\":{\"123\":1,\"294\":1}}],[\"utkarsh\",{\"1\":{\"112\":1,\"172\":1,\"370\":1}}],[\"ui\",{\"0\":{\"151\":2,\"257\":1,\"348\":2},\"1\":{\"110\":3,\"151\":10,\"257\":3,\"321\":3,\"348\":10,\"531\":1}}],[\"uis\",{\"0\":{\"110\":1,\"321\":1},\"1\":{\"110\":1,\"257\":1,\"321\":1}}],[\"unjustified\",{\"1\":{\"494\":1}}],[\"unprecedented\",{\"1\":{\"359\":1}}],[\"unpredictabilities\",{\"1\":{\"100\":1}}],[\"unpredictability\",{\"0\":{\"100\":1}}],[\"until\",{\"1\":{\"322\":1,\"454\":1}}],[\"untrained\",{\"1\":{\"283\":2}}],[\"untangling\",{\"0\":{\"125\":1}}],[\"untapped\",{\"1\":{\"98\":1,\"142\":1}}],[\"unmatched\",{\"1\":{\"253\":1}}],[\"unmasking\",{\"0\":{\"237\":1}}],[\"unveiling\",{\"0\":{\"233\":1,\"390\":1,\"424\":1}}],[\"un\",{\"0\":{\"232\":1}}],[\"undo\",{\"1\":{\"326\":1}}],[\"undesired\",{\"1\":{\"319\":1}}],[\"underwent\",{\"1\":{\"356\":1}}],[\"underexplored\",{\"1\":{\"297\":1}}],[\"underutilized\",{\"1\":{\"291\":1,\"476\":1}}],[\"undergoes\",{\"1\":{\"359\":1}}],[\"undergo\",{\"1\":{\"218\":1,\"374\":1}}],[\"undergraduate\",{\"1\":{\"172\":1,\"307\":1,\"370\":1}}],[\"underlines\",{\"1\":{\"201\":1,\"253\":1}}],[\"underlying\",{\"1\":{\"99\":1,\"167\":1,\"171\":1,\"204\":1,\"390\":2}}],[\"underrepresented\",{\"1\":{\"181\":1,\"394\":1}}],[\"underscoring\",{\"1\":{\"300\":1,\"404\":1}}],[\"underscore\",{\"1\":{\"239\":1,\"350\":1,\"365\":1,\"424\":1,\"480\":1}}],[\"underscored\",{\"1\":{\"164\":1}}],[\"underscores\",{\"1\":{\"103\":1,\"106\":1,\"141\":1,\"237\":1,\"328\":1,\"429\":1,\"477\":1,\"478\":1}}],[\"understudied\",{\"1\":{\"249\":1}}],[\"understandable\",{\"1\":{\"324\":1}}],[\"understandability\",{\"1\":{\"131\":1,\"339\":1}}],[\"understands\",{\"0\":{\"264\":1,\"448\":1}}],[\"understand\",{\"0\":{\"237\":1},\"1\":{\"110\":1,\"114\":1,\"128\":1,\"148\":1,\"155\":1,\"164\":1,\"166\":1,\"198\":1,\"204\":1,\"209\":1,\"213\":1,\"214\":1,\"219\":2,\"226\":1,\"242\":1,\"249\":1,\"255\":1,\"256\":1,\"264\":1,\"269\":1,\"271\":1,\"283\":1,\"292\":1,\"297\":1,\"308\":2,\"321\":1,\"329\":1,\"334\":1,\"376\":2,\"387\":1,\"401\":2,\"448\":1,\"457\":1,\"480\":1,\"491\":1}}],[\"understandings\",{\"1\":{\"286\":1,\"360\":1,\"474\":1}}],[\"understanding\",{\"0\":{\"123\":1,\"131\":1,\"151\":1,\"199\":1,\"257\":1,\"265\":1,\"345\":1,\"348\":1,\"379\":1,\"393\":1,\"416\":1},\"1\":{\"100\":1,\"110\":2,\"116\":1,\"118\":2,\"125\":1,\"151\":1,\"159\":1,\"178\":1,\"199\":3,\"200\":1,\"204\":1,\"212\":1,\"217\":2,\"226\":1,\"237\":1,\"244\":1,\"251\":1,\"257\":1,\"258\":1,\"259\":1,\"261\":1,\"270\":1,\"275\":2,\"283\":1,\"290\":2,\"297\":1,\"315\":1,\"321\":2,\"328\":1,\"337\":1,\"340\":1,\"345\":1,\"348\":1,\"376\":2,\"377\":1,\"379\":1,\"381\":1,\"393\":2,\"394\":1,\"398\":1,\"402\":1,\"412\":2,\"413\":1,\"416\":1,\"419\":1,\"420\":1,\"423\":1,\"424\":1,\"429\":1,\"433\":1,\"437\":1,\"454\":1,\"455\":1,\"467\":1,\"471\":1,\"472\":1,\"481\":1,\"486\":2,\"495\":1}}],[\"under\",{\"0\":{\"223\":1},\"1\":{\"96\":1,\"97\":1,\"100\":1,\"106\":1,\"117\":1,\"155\":1,\"160\":1,\"171\":1,\"176\":1,\"190\":1,\"223\":1,\"269\":1,\"274\":1,\"275\":1,\"301\":1,\"312\":1,\"338\":1,\"353\":1,\"355\":1,\"360\":1,\"372\":1,\"412\":1,\"431\":1,\"441\":1,\"455\":1,\"459\":1,\"465\":1,\"485\":2}}],[\"undirected\",{\"1\":{\"223\":1}}],[\"unbalanced\",{\"1\":{\"334\":1}}],[\"unblind\",{\"0\":{\"219\":1,\"401\":1}}],[\"unbiased\",{\"1\":{\"160\":3}}],[\"unravelling\",{\"1\":{\"390\":1}}],[\"unraveling\",{\"0\":{\"102\":1},\"1\":{\"424\":1}}],[\"unresolved\",{\"1\":{\"350\":1}}],[\"unrefined\",{\"1\":{\"207\":1}}],[\"unknown\",{\"0\":{\"489\":1},\"1\":{\"191\":1,\"488\":1}}],[\"unforgettable\",{\"1\":{\"243\":1}}],[\"unfortunately\",{\"1\":{\"188\":1,\"219\":1,\"401\":1}}],[\"unfaithful\",{\"1\":{\"483\":1}}],[\"unfavourable\",{\"1\":{\"158\":1}}],[\"unfamiliar\",{\"1\":{\"127\":1}}],[\"uncovers\",{\"1\":{\"466\":1}}],[\"uncover\",{\"1\":{\"213\":1,\"313\":1,\"340\":1,\"394\":1}}],[\"uncovered\",{\"1\":{\"212\":1,\"390\":1}}],[\"unclear\",{\"1\":{\"187\":1,\"209\":1,\"263\":1}}],[\"uncertainties\",{\"1\":{\"212\":1}}],[\"uncertainty\",{\"0\":{\"135\":1,\"226\":1,\"325\":1,\"467\":1},\"1\":{\"135\":3,\"226\":5,\"244\":1,\"288\":1,\"378\":1,\"467\":2}}],[\"uncertain\",{\"1\":{\"127\":1,\"226\":1,\"240\":1,\"429\":1}}],[\"unavoidable\",{\"1\":{\"488\":1}}],[\"unavoidably\",{\"1\":{\"378\":1}}],[\"unavailable\",{\"1\":{\"441\":1}}],[\"unaffected\",{\"1\":{\"340\":1}}],[\"unaware\",{\"1\":{\"265\":1}}],[\"unanswered\",{\"1\":{\"234\":1}}],[\"unable\",{\"1\":{\"184\":1,\"345\":1,\"376\":1,\"380\":1,\"392\":1}}],[\"unacquainted\",{\"1\":{\"108\":2}}],[\"unstable\",{\"1\":{\"355\":1}}],[\"unstructured\",{\"0\":{\"279\":1},\"1\":{\"182\":1,\"201\":1,\"279\":1}}],[\"unseen\",{\"1\":{\"283\":1,\"322\":1,\"352\":1,\"419\":1,\"420\":1,\"485\":1}}],[\"unsupported\",{\"1\":{\"291\":1,\"476\":1}}],[\"unsuccessful\",{\"1\":{\"260\":1}}],[\"unsuitable\",{\"1\":{\"194\":1}}],[\"unsafe\",{\"1\":{\"105\":1}}],[\"unnoticed\",{\"1\":{\"301\":1}}],[\"unnecessary\",{\"1\":{\"150\":1,\"306\":1,\"336\":1}}],[\"unnseo\",{\"1\":{\"121\":1}}],[\"unleashing\",{\"0\":{\"449\":1}}],[\"unlock\",{\"1\":{\"326\":1,\"495\":1}}],[\"unlocking\",{\"0\":{\"154\":1}}],[\"unlike\",{\"1\":{\"239\":1,\"322\":1,\"400\":1,\"418\":1,\"419\":1,\"420\":1,\"491\":1}}],[\"unlikely\",{\"1\":{\"96\":1}}],[\"unlabelled\",{\"1\":{\"137\":1}}],[\"unethical\",{\"1\":{\"418\":1,\"468\":1}}],[\"uneven\",{\"1\":{\"413\":1}}],[\"unevenly\",{\"1\":{\"132\":1,\"330\":1}}],[\"uneasy\",{\"1\":{\"355\":1}}],[\"unexplored\",{\"1\":{\"159\":1,\"167\":1,\"227\":1,\"350\":1,\"394\":1,\"417\":1}}],[\"unexpected\",{\"1\":{\"108\":1,\"286\":1,\"474\":1}}],[\"universally\",{\"1\":{\"328\":1,\"329\":1}}],[\"universal\",{\"0\":{\"313\":1},\"1\":{\"263\":1,\"313\":1,\"349\":1}}],[\"university\",{\"1\":{\"149\":1,\"172\":1,\"307\":1,\"308\":1,\"370\":1,\"388\":1}}],[\"unifying\",{\"1\":{\"224\":1}}],[\"unified\",{\"0\":{\"224\":1},\"1\":{\"224\":1,\"229\":1,\"316\":2,\"353\":2,\"385\":1,\"400\":1,\"411\":1,\"414\":1}}],[\"uniform\",{\"1\":{\"188\":1,\"197\":1,\"221\":1,\"425\":1}}],[\"union\",{\"1\":{\"181\":1}}],[\"unidentified\",{\"1\":{\"161\":1,\"354\":1}}],[\"unintended\",{\"1\":{\"128\":2,\"178\":1,\"211\":1}}],[\"uniqueness\",{\"1\":{\"251\":1,\"423\":1}}],[\"uniquely\",{\"1\":{\"125\":1,\"146\":1,\"377\":1}}],[\"unique\",{\"1\":{\"117\":1,\"166\":1,\"204\":1,\"207\":1,\"237\":1,\"244\":1,\"249\":1,\"257\":1,\"275\":1,\"345\":1,\"399\":1,\"409\":1,\"427\":1,\"442\":1,\"483\":1,\"485\":1}}],[\"united\",{\"1\":{\"328\":1}}],[\"units\",{\"1\":{\"249\":1,\"309\":1}}],[\"unit\",{\"1\":{\"74\":1,\"215\":1,\"441\":2}}],[\"unity\",{\"1\":{\"8\":1}}],[\"usually\",{\"1\":{\"289\":1,\"360\":1}}],[\"usual\",{\"1\":{\"227\":1}}],[\"us\",{\"1\":{\"155\":1,\"161\":1,\"174\":1,\"175\":2,\"179\":1,\"206\":1,\"226\":1,\"248\":1,\"340\":1,\"350\":1,\"354\":1,\"371\":1,\"381\":1,\"385\":1,\"399\":1,\"409\":1,\"411\":1,\"415\":1,\"482\":1}}],[\"uses\",{\"1\":{\"113\":1,\"116\":1,\"148\":1,\"159\":1,\"175\":1,\"203\":1,\"219\":1,\"234\":1,\"249\":1,\"268\":1,\"277\":1,\"308\":1,\"352\":1,\"353\":1,\"401\":1,\"452\":1,\"453\":1}}],[\"use\",{\"0\":{\"214\":1,\"236\":1,\"241\":1,\"286\":1,\"308\":1,\"395\":1,\"428\":1,\"474\":1},\"1\":{\"101\":1,\"114\":1,\"128\":1,\"135\":1,\"138\":1,\"153\":1,\"156\":3,\"161\":2,\"165\":1,\"171\":1,\"172\":2,\"178\":1,\"181\":1,\"184\":1,\"200\":1,\"207\":1,\"219\":1,\"224\":1,\"227\":1,\"230\":1,\"236\":1,\"239\":1,\"241\":5,\"255\":1,\"256\":2,\"261\":1,\"272\":5,\"273\":1,\"279\":1,\"286\":1,\"288\":1,\"289\":1,\"290\":1,\"302\":2,\"308\":4,\"316\":1,\"333\":1,\"334\":2,\"335\":2,\"338\":2,\"340\":1,\"344\":1,\"354\":2,\"363\":1,\"365\":1,\"368\":1,\"370\":2,\"374\":2,\"380\":1,\"381\":1,\"388\":1,\"391\":1,\"395\":5,\"396\":1,\"401\":1,\"403\":1,\"407\":1,\"409\":1,\"414\":1,\"422\":1,\"430\":2,\"434\":1,\"435\":1,\"441\":1,\"450\":1,\"454\":1,\"460\":1,\"461\":2,\"465\":2,\"474\":1,\"479\":1,\"481\":1,\"488\":1,\"490\":2,\"492\":1}}],[\"usefulness\",{\"1\":{\"172\":1,\"213\":1,\"219\":1,\"270\":1,\"309\":1,\"335\":1,\"370\":1,\"401\":1}}],[\"useful\",{\"0\":{\"335\":1},\"1\":{\"101\":1,\"126\":1,\"146\":1,\"153\":1,\"175\":1,\"360\":1,\"425\":1,\"466\":1,\"468\":1}}],[\"used\",{\"1\":{\"101\":1,\"103\":1,\"104\":1,\"106\":1,\"110\":1,\"111\":1,\"116\":1,\"123\":2,\"128\":3,\"139\":1,\"149\":1,\"154\":1,\"156\":1,\"173\":1,\"176\":1,\"178\":1,\"181\":2,\"187\":1,\"191\":2,\"199\":1,\"204\":1,\"214\":1,\"215\":2,\"224\":1,\"227\":1,\"231\":1,\"248\":1,\"255\":2,\"256\":2,\"272\":1,\"288\":1,\"289\":1,\"292\":1,\"304\":1,\"308\":2,\"309\":1,\"315\":2,\"321\":1,\"334\":1,\"335\":1,\"347\":1,\"350\":1,\"363\":3,\"381\":1,\"384\":1,\"386\":1,\"391\":1,\"395\":1,\"396\":1,\"403\":1,\"411\":1,\"417\":1,\"428\":1,\"438\":1,\"450\":1,\"461\":1,\"466\":1,\"471\":1,\"482\":1,\"486\":1,\"496\":1}}],[\"user\",{\"0\":{\"100\":1,\"114\":1,\"154\":1,\"158\":1,\"272\":1,\"479\":1},\"1\":{\"99\":2,\"100\":2,\"105\":1,\"110\":3,\"114\":3,\"119\":1,\"127\":2,\"128\":3,\"131\":1,\"138\":2,\"140\":1,\"146\":1,\"149\":1,\"151\":1,\"154\":5,\"159\":2,\"163\":1,\"166\":5,\"167\":2,\"176\":1,\"183\":1,\"185\":2,\"186\":1,\"191\":2,\"192\":1,\"198\":1,\"203\":3,\"210\":2,\"215\":1,\"218\":2,\"219\":1,\"222\":4,\"226\":1,\"229\":2,\"231\":1,\"234\":1,\"238\":11,\"242\":1,\"244\":2,\"258\":1,\"259\":3,\"260\":1,\"261\":1,\"264\":2,\"266\":1,\"268\":1,\"270\":1,\"271\":1,\"272\":1,\"278\":1,\"282\":3,\"288\":1,\"294\":1,\"295\":2,\"297\":2,\"316\":1,\"321\":3,\"324\":1,\"348\":1,\"361\":1,\"375\":3,\"386\":2,\"387\":1,\"392\":1,\"394\":1,\"396\":1,\"399\":2,\"401\":1,\"402\":1,\"417\":2,\"433\":1,\"435\":1,\"447\":1,\"448\":2,\"452\":1,\"457\":1,\"479\":10,\"480\":1,\"481\":3,\"485\":1,\"486\":1}}],[\"users\",{\"0\":{\"99\":1},\"1\":{\"99\":2,\"108\":1,\"110\":4,\"111\":5,\"122\":1,\"124\":3,\"127\":1,\"128\":3,\"131\":3,\"133\":2,\"135\":3,\"140\":2,\"141\":1,\"146\":1,\"149\":1,\"153\":1,\"158\":3,\"159\":3,\"167\":1,\"169\":1,\"173\":4,\"187\":2,\"189\":1,\"191\":4,\"192\":4,\"195\":5,\"200\":2,\"203\":1,\"209\":1,\"210\":2,\"212\":4,\"213\":2,\"215\":1,\"219\":3,\"227\":1,\"229\":1,\"234\":1,\"238\":4,\"242\":2,\"252\":1,\"258\":1,\"259\":1,\"266\":2,\"270\":1,\"272\":3,\"282\":1,\"296\":1,\"297\":4,\"321\":4,\"331\":2,\"334\":1,\"399\":1,\"401\":3,\"417\":3,\"426\":1,\"433\":1,\"435\":1,\"437\":1,\"445\":5,\"479\":1}}],[\"using\",{\"0\":{\"108\":1,\"140\":1,\"165\":1,\"171\":1,\"194\":1,\"204\":1,\"237\":1,\"256\":1,\"268\":1,\"296\":1,\"301\":1,\"305\":1,\"325\":1,\"358\":1,\"400\":1,\"407\":1,\"434\":1,\"452\":1,\"468\":1,\"481\":1,\"489\":1},\"1\":{\"96\":2,\"97\":2,\"99\":1,\"101\":1,\"103\":1,\"105\":1,\"106\":1,\"120\":2,\"125\":1,\"135\":1,\"137\":1,\"140\":1,\"146\":1,\"154\":3,\"160\":1,\"165\":3,\"171\":1,\"175\":1,\"184\":1,\"185\":2,\"186\":1,\"187\":1,\"188\":1,\"194\":1,\"195\":1,\"196\":2,\"197\":2,\"199\":2,\"204\":2,\"207\":2,\"210\":2,\"211\":1,\"212\":1,\"213\":1,\"215\":1,\"220\":2,\"228\":1,\"229\":1,\"232\":2,\"234\":1,\"237\":1,\"238\":1,\"248\":1,\"250\":1,\"251\":1,\"252\":1,\"256\":1,\"259\":1,\"266\":1,\"270\":1,\"271\":1,\"272\":2,\"273\":1,\"278\":1,\"281\":1,\"287\":1,\"289\":1,\"291\":1,\"301\":1,\"302\":1,\"305\":3,\"308\":1,\"309\":1,\"312\":2,\"317\":1,\"326\":1,\"327\":1,\"336\":1,\"340\":1,\"344\":1,\"350\":1,\"356\":1,\"363\":1,\"365\":1,\"372\":1,\"375\":1,\"380\":1,\"386\":2,\"388\":3,\"389\":1,\"390\":2,\"392\":1,\"394\":1,\"403\":1,\"404\":2,\"405\":1,\"407\":2,\"408\":1,\"410\":2,\"413\":2,\"415\":1,\"416\":1,\"419\":1,\"420\":1,\"422\":2,\"423\":1,\"426\":1,\"428\":2,\"431\":1,\"435\":1,\"438\":1,\"441\":1,\"442\":1,\"444\":1,\"447\":1,\"454\":1,\"457\":1,\"462\":1,\"468\":1,\"470\":1,\"471\":1,\"476\":1,\"479\":1,\"482\":1,\"485\":2,\"486\":1,\"487\":1,\"490\":2,\"492\":2,\"496\":1}}],[\"usability\",{\"1\":{\"140\":1,\"201\":1,\"210\":1,\"212\":1}}],[\"usable\",{\"1\":{\"131\":1}}],[\"usage\",{\"0\":{\"172\":1,\"370\":1,\"480\":1},\"1\":{\"124\":2,\"128\":2,\"172\":2,\"213\":1,\"237\":2,\"258\":1,\"278\":1,\"308\":2,\"360\":1,\"370\":2,\"428\":1,\"433\":1,\"461\":1,\"465\":1,\"469\":1,\"482\":1,\"496\":1}}],[\"usa\",{\"1\":{\"74\":1}}],[\"d+i+e\",{\"1\":{\"510\":1}}],[\"d+b\",{\"1\":{\"510\":1}}],[\"d=设计\",{\"1\":{\"510\":1}}],[\"dtv\",{\"1\":{\"494\":1}}],[\"djamila\",{\"1\":{\"460\":1}}],[\"dpo\",{\"1\":{\"453\":1}}],[\"dcl\",{\"1\":{\"301\":1}}],[\"dnn\",{\"1\":{\"284\":2}}],[\"dynamism\",{\"1\":{\"244\":1}}],[\"dynamically\",{\"1\":{\"142\":1,\"257\":1,\"264\":1,\"359\":1,\"378\":1,\"390\":1,\"398\":1,\"448\":1,\"489\":1}}],[\"dynamic\",{\"0\":{\"143\":1,\"218\":1,\"264\":1,\"448\":1},\"1\":{\"142\":1,\"143\":2,\"146\":1,\"186\":1,\"218\":2,\"269\":2,\"300\":2,\"404\":1,\"415\":1,\"429\":1,\"430\":1,\"460\":1}}],[\"dynamics\",{\"0\":{\"121\":1,\"290\":1},\"1\":{\"106\":1,\"133\":1,\"271\":1,\"290\":3,\"307\":1,\"331\":1,\"424\":2,\"457\":1}}],[\"dylan\",{\"1\":{\"195\":1}}],[\"díaz\",{\"1\":{\"186\":1}}],[\"dhole\",{\"1\":{\"386\":1}}],[\"dhruv\",{\"1\":{\"172\":1,\"370\":1}}],[\"dharshikgan\",{\"1\":{\"137\":1}}],[\"dobhal\",{\"1\":{\"490\":1}}],[\"doveh\",{\"1\":{\"454\":1}}],[\"dollars\",{\"1\":{\"443\":1}}],[\"dozens\",{\"1\":{\"443\":1}}],[\"dogs\",{\"1\":{\"375\":1}}],[\"dog\",{\"1\":{\"375\":2}}],[\"door\",{\"1\":{\"339\":1}}],[\"doratossadat\",{\"1\":{\"333\":1}}],[\"dorneich\",{\"1\":{\"304\":1}}],[\"dorit\",{\"1\":{\"292\":1}}],[\"dodda\",{\"1\":{\"265\":1}}],[\"doing\",{\"1\":{\"251\":1,\"423\":1}}],[\"dot\",{\"1\":{\"226\":1}}],[\"dotch\",{\"1\":{\"144\":1}}],[\"dominique\",{\"1\":{\"400\":1}}],[\"dominik\",{\"1\":{\"210\":1}}],[\"domains\",{\"1\":{\"136\":1,\"143\":1,\"163\":1,\"189\":1,\"238\":1,\"246\":1,\"258\":1,\"282\":1,\"297\":1,\"319\":1,\"328\":1,\"339\":1,\"345\":1,\"352\":1,\"372\":1,\"374\":1,\"381\":2,\"433\":1,\"460\":1,\"470\":1,\"478\":1,\"482\":1}}],[\"domain\",{\"0\":{\"315\":1,\"432\":1,\"466\":1},\"1\":{\"132\":1,\"135\":1,\"137\":1,\"138\":1,\"151\":1,\"154\":1,\"171\":1,\"176\":1,\"182\":1,\"189\":5,\"194\":1,\"223\":1,\"269\":1,\"305\":3,\"315\":5,\"319\":1,\"323\":1,\"328\":1,\"330\":1,\"336\":1,\"345\":1,\"348\":1,\"363\":1,\"379\":2,\"381\":4,\"382\":1,\"384\":1,\"398\":1,\"432\":5,\"437\":1,\"449\":1,\"458\":1,\"488\":1,\"490\":1}}],[\"doerr\",{\"1\":{\"182\":1}}],[\"doesn\",{\"1\":{\"419\":1,\"420\":1}}],[\"does\",{\"0\":{\"159\":1,\"328\":1},\"1\":{\"121\":1,\"135\":1,\"159\":1,\"173\":1,\"202\":1,\"322\":1,\"344\":1,\"393\":1,\"403\":1,\"442\":1,\"453\":1,\"454\":1,\"458\":1,\"461\":1,\"487\":1}}],[\"doc\",{\"0\":{\"347\":1},\"1\":{\"347\":1}}],[\"documentation\",{\"1\":{\"335\":1,\"356\":6}}],[\"documenting\",{\"1\":{\"291\":1,\"476\":1}}],[\"documented\",{\"1\":{\"205\":1,\"211\":1,\"338\":1}}],[\"documents\",{\"1\":{\"202\":1,\"329\":1,\"337\":1,\"339\":1,\"356\":1,\"386\":1,\"394\":1,\"402\":2,\"419\":1,\"420\":1,\"480\":1}}],[\"document\",{\"1\":{\"179\":1,\"402\":5,\"480\":1}}],[\"doctors\",{\"0\":{\"176\":1},\"1\":{\"176\":2}}],[\"done\",{\"1\":{\"497\":1}}],[\"donald\",{\"1\":{\"281\":1}}],[\"dongsheng\",{\"1\":{\"403\":1}}],[\"dongwei\",{\"1\":{\"385\":1}}],[\"dongwon\",{\"1\":{\"198\":1,\"387\":1}}],[\"dong\",{\"1\":{\"297\":1,\"427\":1}}],[\"donggang\",{\"1\":{\"259\":1}}],[\"donghwa\",{\"1\":{\"143\":1}}],[\"don\",{\"0\":{\"170\":1,\"494\":1}}],[\"doubted\",{\"1\":{\"160\":1}}],[\"double\",{\"0\":{\"106\":1}}],[\"downing\",{\"1\":{\"422\":1}}],[\"down\",{\"1\":{\"155\":1}}],[\"downstream\",{\"1\":{\"116\":1,\"159\":1,\"205\":1,\"241\":1,\"374\":1,\"449\":1,\"466\":1}}],[\"do\",{\"0\":{\"214\":1,\"327\":1,\"381\":1,\"394\":1,\"431\":1},\"1\":{\"113\":1,\"115\":1,\"132\":1,\"152\":1,\"170\":1,\"175\":1,\"187\":1,\"189\":1,\"196\":1,\"200\":1,\"214\":1,\"215\":2,\"248\":1,\"301\":1,\"330\":1,\"336\":1,\"337\":1,\"379\":1,\"385\":1,\"391\":1,\"409\":1,\"412\":1,\"418\":2,\"434\":1,\"462\":1}}],[\"dkl\",{\"1\":{\"112\":2}}],[\"d\",{\"0\":{\"232\":1},\"1\":{\"111\":1,\"122\":1,\"150\":1,\"428\":1}}],[\"duc\",{\"1\":{\"412\":1,\"435\":1}}],[\"dudek\",{\"1\":{\"407\":1}}],[\"dubbed\",{\"1\":{\"359\":1}}],[\"dual\",{\"1\":{\"349\":1,\"359\":1,\"419\":1,\"420\":1}}],[\"duanmu\",{\"1\":{\"414\":1}}],[\"duan\",{\"1\":{\"140\":1,\"337\":1,\"362\":1,\"414\":1}}],[\"dustin\",{\"1\":{\"231\":1}}],[\"duscher\",{\"1\":{\"112\":1}}],[\"durme\",{\"1\":{\"385\":1}}],[\"durmus\",{\"1\":{\"241\":1}}],[\"durability\",{\"1\":{\"306\":1}}],[\"duration\",{\"0\":{\"203\":1,\"265\":1},\"1\":{\"203\":1,\"237\":1,\"256\":1,\"265\":1}}],[\"during\",{\"0\":{\"165\":1,\"273\":1},\"1\":{\"108\":1,\"113\":1,\"120\":1,\"125\":1,\"126\":1,\"186\":1,\"207\":1,\"218\":1,\"224\":1,\"227\":1,\"228\":1,\"242\":1,\"255\":1,\"257\":1,\"265\":1,\"266\":2,\"273\":1,\"279\":2,\"282\":1,\"290\":2,\"308\":1,\"313\":2,\"314\":1,\"322\":1,\"329\":1,\"344\":1,\"358\":1,\"400\":1,\"419\":2,\"420\":2,\"441\":1,\"478\":1,\"496\":1}}],[\"duen\",{\"1\":{\"258\":1,\"433\":1}}],[\"duenser\",{\"1\":{\"244\":1}}],[\"due\",{\"1\":{\"106\":1,\"116\":1,\"121\":2,\"132\":1,\"146\":1,\"149\":1,\"188\":1,\"196\":2,\"203\":1,\"220\":1,\"238\":1,\"253\":1,\"260\":1,\"273\":1,\"281\":1,\"284\":1,\"286\":1,\"298\":1,\"306\":1,\"316\":1,\"323\":1,\"330\":1,\"333\":1,\"345\":1,\"355\":1,\"376\":1,\"378\":1,\"386\":1,\"391\":1,\"405\":1,\"408\":1,\"414\":1,\"415\":1,\"427\":2,\"435\":1,\"443\":1,\"445\":1,\"458\":1,\"462\":1,\"474\":1,\"475\":1,\"478\":1,\"482\":1,\"489\":2,\"498\":1}}],[\"du\",{\"1\":{\"104\":1,\"245\":1,\"377\":1,\"442\":1}}],[\"db2\",{\"1\":{\"103\":1}}],[\"db1\",{\"1\":{\"103\":1}}],[\"dlt\",{\"1\":{\"246\":1}}],[\"dl\",{\"1\":{\"101\":2,\"102\":1}}],[\"drug\",{\"1\":{\"358\":1}}],[\"drugs\",{\"1\":{\"334\":1}}],[\"drywall\",{\"1\":{\"201\":1}}],[\"drl\",{\"1\":{\"150\":1}}],[\"drinking\",{\"1\":{\"142\":4}}],[\"drinks\",{\"0\":{\"142\":1}}],[\"drivers\",{\"1\":{\"135\":1,\"186\":1}}],[\"driver\",{\"0\":{\"135\":1,\"150\":1},\"1\":{\"150\":5,\"186\":3,\"229\":1}}],[\"drive\",{\"1\":{\"132\":1,\"186\":1,\"330\":1,\"362\":1}}],[\"driven\",{\"0\":{\"108\":1,\"120\":1,\"141\":1,\"223\":1,\"284\":1,\"300\":1,\"314\":1,\"431\":1},\"1\":{\"108\":3,\"119\":1,\"174\":1,\"204\":1,\"216\":1,\"217\":1,\"223\":1,\"229\":1,\"235\":1,\"252\":1,\"277\":1,\"284\":1,\"300\":1,\"314\":1,\"349\":1,\"371\":1,\"398\":1,\"407\":1,\"426\":1,\"431\":3,\"458\":1,\"460\":1,\"519\":1,\"529\":2}}],[\"driving\",{\"0\":{\"362\":1},\"1\":{\"114\":1,\"135\":1,\"137\":1,\"186\":3,\"239\":1,\"270\":3,\"362\":5}}],[\"draft\",{\"1\":{\"487\":3}}],[\"drastic\",{\"1\":{\"482\":1}}],[\"dramatically\",{\"1\":{\"412\":1,\"435\":1}}],[\"drawbacks\",{\"1\":{\"313\":1}}],[\"drawn\",{\"1\":{\"269\":1,\"444\":1}}],[\"draw\",{\"1\":{\"144\":1,\"229\":1}}],[\"drawing\",{\"1\":{\"132\":1,\"143\":1,\"180\":1,\"244\":1,\"286\":1,\"330\":1,\"474\":1}}],[\"draws\",{\"1\":{\"129\":1}}],[\"dragon\",{\"1\":{\"347\":1}}],[\"dragnoise\",{\"1\":{\"260\":5}}],[\"dragdiffusion\",{\"1\":{\"260\":2}}],[\"drag\",{\"0\":{\"260\":1},\"1\":{\"99\":1,\"361\":1}}],[\"drone\",{\"1\":{\"181\":1}}],[\"drosos\",{\"1\":{\"122\":1}}],[\"drops\",{\"1\":{\"395\":2}}],[\"drop\",{\"1\":{\"99\":1,\"230\":1,\"361\":1,\"482\":1}}],[\"daksh\",{\"1\":{\"490\":1}}],[\"dakar\",{\"1\":{\"232\":1}}],[\"da\",{\"1\":{\"453\":1,\"489\":1}}],[\"dawei\",{\"1\":{\"395\":1,\"470\":1}}],[\"dave\",{\"1\":{\"391\":1}}],[\"davinci\",{\"1\":{\"492\":1}}],[\"davis\",{\"1\":{\"308\":2}}],[\"david\",{\"1\":{\"166\":1,\"215\":1,\"222\":1,\"253\":1,\"338\":1,\"364\":1,\"407\":1,\"468\":1,\"497\":1}}],[\"davide\",{\"1\":{\"120\":1,\"338\":1}}],[\"dadaamin\",{\"1\":{\"379\":1}}],[\"dada\",{\"1\":{\"379\":1}}],[\"dahua\",{\"1\":{\"337\":1,\"414\":1}}],[\"darlene\",{\"1\":{\"305\":1}}],[\"darlington\",{\"1\":{\"246\":1}}],[\"daragh\",{\"1\":{\"164\":1}}],[\"dagan\",{\"1\":{\"236\":1}}],[\"damian\",{\"1\":{\"350\":1}}],[\"damien\",{\"1\":{\"192\":1}}],[\"damage\",{\"1\":{\"326\":1}}],[\"damon\",{\"1\":{\"117\":1}}],[\"date\",{\"1\":{\"182\":1}}],[\"datastore\",{\"1\":{\"425\":1}}],[\"dataset\",{\"0\":{\"355\":1},\"1\":{\"116\":1,\"118\":1,\"133\":3,\"148\":1,\"151\":1,\"161\":1,\"190\":1,\"194\":1,\"199\":3,\"204\":1,\"234\":1,\"245\":1,\"249\":1,\"251\":1,\"272\":1,\"294\":1,\"301\":1,\"322\":1,\"328\":1,\"331\":3,\"336\":1,\"344\":3,\"348\":1,\"350\":1,\"354\":1,\"355\":6,\"356\":1,\"372\":1,\"381\":1,\"394\":1,\"395\":1,\"404\":2,\"411\":1,\"413\":1,\"423\":1,\"431\":1,\"444\":1,\"447\":4,\"453\":3,\"455\":2,\"459\":2,\"478\":1,\"481\":1,\"485\":1,\"488\":1,\"490\":1,\"491\":2,\"492\":1}}],[\"datasets\",{\"0\":{\"459\":1},\"1\":{\"103\":2,\"123\":2,\"133\":1,\"137\":2,\"139\":1,\"151\":1,\"190\":1,\"194\":1,\"213\":1,\"240\":1,\"249\":1,\"258\":1,\"272\":4,\"281\":1,\"301\":1,\"309\":1,\"314\":1,\"319\":1,\"322\":1,\"323\":1,\"327\":1,\"328\":1,\"331\":1,\"334\":1,\"337\":2,\"344\":1,\"347\":2,\"348\":1,\"368\":1,\"378\":1,\"379\":1,\"381\":3,\"382\":1,\"390\":1,\"400\":1,\"404\":1,\"405\":1,\"408\":1,\"412\":1,\"413\":1,\"417\":1,\"433\":1,\"435\":1,\"453\":1,\"455\":4,\"459\":2,\"460\":1,\"485\":2,\"490\":1,\"494\":2}}],[\"databases\",{\"1\":{\"447\":1}}],[\"database\",{\"0\":{\"126\":1},\"1\":{\"126\":3,\"264\":1,\"399\":1,\"447\":1,\"448\":1,\"479\":1,\"488\":1}}],[\"data\",{\"0\":{\"97\":1,\"120\":1,\"188\":1,\"189\":1,\"226\":1,\"242\":1,\"243\":1,\"253\":1,\"301\":1,\"312\":1,\"322\":1,\"402\":1,\"405\":1,\"424\":1,\"460\":1,\"480\":1,\"492\":1,\"496\":1},\"1\":{\"97\":6,\"98\":1,\"101\":5,\"111\":3,\"116\":1,\"118\":2,\"119\":4,\"120\":3,\"121\":1,\"125\":1,\"128\":1,\"136\":1,\"137\":2,\"139\":2,\"140\":1,\"143\":3,\"146\":2,\"152\":1,\"154\":2,\"159\":1,\"163\":1,\"170\":2,\"172\":1,\"178\":5,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"188\":1,\"189\":2,\"194\":1,\"195\":1,\"196\":2,\"203\":1,\"204\":1,\"207\":1,\"215\":1,\"217\":2,\"226\":3,\"227\":1,\"229\":1,\"231\":1,\"237\":1,\"240\":2,\"242\":6,\"243\":4,\"246\":1,\"249\":2,\"251\":1,\"252\":2,\"253\":4,\"255\":2,\"258\":2,\"272\":2,\"277\":1,\"278\":1,\"281\":2,\"287\":2,\"290\":2,\"291\":1,\"298\":1,\"301\":6,\"305\":2,\"306\":1,\"308\":2,\"312\":6,\"313\":1,\"315\":1,\"316\":1,\"319\":2,\"322\":1,\"323\":1,\"329\":2,\"333\":4,\"335\":1,\"340\":3,\"355\":3,\"368\":1,\"369\":1,\"370\":1,\"377\":2,\"379\":1,\"380\":1,\"388\":1,\"393\":1,\"396\":1,\"398\":1,\"402\":3,\"405\":1,\"407\":1,\"408\":5,\"411\":1,\"417\":1,\"419\":4,\"420\":4,\"423\":1,\"424\":3,\"425\":1,\"426\":2,\"433\":2,\"435\":3,\"442\":1,\"443\":2,\"446\":1,\"450\":2,\"453\":2,\"454\":1,\"460\":6,\"465\":2,\"469\":1,\"471\":3,\"475\":3,\"476\":1,\"477\":3,\"479\":1,\"480\":3,\"482\":2,\"485\":2,\"488\":1,\"491\":1,\"492\":3,\"496\":5,\"519\":1,\"527\":1,\"529\":2}}],[\"daily\",{\"1\":{\"160\":1,\"184\":1,\"214\":1,\"380\":1,\"479\":1}}],[\"daijin\",{\"1\":{\"98\":1}}],[\"danny\",{\"1\":{\"309\":1}}],[\"danqing\",{\"1\":{\"243\":1}}],[\"dangerous\",{\"1\":{\"201\":1,\"241\":2}}],[\"dancing\",{\"1\":{\"182\":1}}],[\"dancers\",{\"1\":{\"182\":3}}],[\"dance\",{\"0\":{\"100\":1,\"182\":1},\"1\":{\"182\":4}}],[\"danielle\",{\"1\":{\"197\":1}}],[\"daniel\",{\"1\":{\"152\":1,\"185\":1,\"209\":1,\"281\":1,\"300\":1,\"328\":1,\"385\":1,\"419\":1,\"420\":1,\"480\":1,\"490\":1}}],[\"dan\",{\"1\":{\"150\":1,\"241\":1,\"359\":1,\"445\":1,\"456\":1}}],[\"danaë\",{\"1\":{\"148\":1}}],[\"dastgheib\",{\"1\":{\"333\":1}}],[\"dashcams\",{\"1\":{\"272\":1}}],[\"dashboards\",{\"1\":{\"217\":1}}],[\"dashboard\",{\"0\":{\"156\":1},\"1\":{\"146\":1,\"156\":3}}],[\"das\",{\"1\":{\"215\":1,\"391\":1}}],[\"dass\",{\"1\":{\"137\":1}}],[\"dalmia\",{\"1\":{\"419\":1,\"420\":1}}],[\"dal\",{\"1\":{\"120\":1}}],[\"dall\",{\"1\":{\"101\":1}}],[\"dayuan\",{\"1\":{\"449\":1}}],[\"days\",{\"1\":{\"207\":1,\"218\":1}}],[\"day\",{\"0\":{\"293\":1},\"1\":{\"178\":2,\"214\":2,\"216\":1,\"410\":1,\"488\":1}}],[\"daylamani\",{\"1\":{\"117\":1}}],[\"daydream\",{\"1\":{\"81\":2}}],[\"di\",{\"1\":{\"453\":1}}],[\"dih\",{\"1\":{\"436\":1}}],[\"dip\",{\"1\":{\"412\":1}}],[\"dietrich\",{\"1\":{\"395\":1}}],[\"dieter\",{\"1\":{\"140\":1}}],[\"ding\",{\"1\":{\"377\":1,\"393\":1,\"411\":1,\"443\":1}}],[\"dining\",{\"1\":{\"142\":1,\"155\":2}}],[\"dickstein\",{\"1\":{\"389\":1}}],[\"dickens\",{\"1\":{\"369\":1}}],[\"dictionary\",{\"0\":{\"149\":1},\"1\":{\"149\":1,\"365\":2}}],[\"digits\",{\"1\":{\"454\":2}}],[\"digit\",{\"1\":{\"454\":1}}],[\"digitization\",{\"1\":{\"253\":1}}],[\"digitalization\",{\"1\":{\"296\":1}}],[\"digital\",{\"0\":{\"124\":1,\"178\":1,\"237\":1,\"246\":1,\"282\":1},\"1\":{\"101\":1,\"120\":3,\"124\":2,\"157\":1,\"161\":1,\"169\":1,\"178\":1,\"205\":1,\"206\":1,\"207\":1,\"224\":1,\"237\":3,\"243\":1,\"249\":1,\"275\":1,\"282\":4,\"287\":1,\"296\":2,\"354\":1,\"363\":1,\"519\":4}}],[\"dirk\",{\"1\":{\"237\":1}}],[\"directed\",{\"1\":{\"301\":2}}],[\"direct\",{\"1\":{\"224\":1,\"235\":1,\"246\":1,\"273\":1,\"297\":1,\"302\":1,\"375\":1,\"404\":1,\"495\":1}}],[\"directionality\",{\"1\":{\"301\":1}}],[\"directional\",{\"1\":{\"231\":1,\"301\":1}}],[\"direction\",{\"0\":{\"301\":1},\"1\":{\"216\":1,\"301\":2,\"325\":1,\"378\":1,\"469\":1}}],[\"directions\",{\"1\":{\"97\":1,\"169\":1,\"254\":1,\"306\":1,\"312\":1,\"339\":1,\"364\":1,\"429\":1,\"431\":1,\"470\":2,\"475\":1}}],[\"directly\",{\"1\":{\"127\":1,\"159\":1,\"355\":1,\"356\":1,\"362\":1,\"389\":1}}],[\"diyi\",{\"1\":{\"180\":1,\"428\":1}}],[\"divtod\",{\"0\":{\"449\":1},\"1\":{\"449\":2}}],[\"divulging\",{\"1\":{\"443\":1}}],[\"divyanshu\",{\"1\":{\"374\":1}}],[\"divergent\",{\"0\":{\"424\":1}}],[\"divergence\",{\"1\":{\"273\":1}}],[\"diverges\",{\"1\":{\"377\":1}}],[\"diverge\",{\"1\":{\"230\":1}}],[\"diverging\",{\"0\":{\"197\":1},\"1\":{\"197\":1}}],[\"diversifying\",{\"0\":{\"449\":1}}],[\"diversity\",{\"0\":{\"106\":1},\"1\":{\"106\":1,\"121\":1,\"133\":1,\"135\":1,\"167\":1,\"331\":1,\"352\":1,\"447\":1,\"449\":1,\"460\":1}}],[\"diverse\",{\"0\":{\"218\":1},\"1\":{\"106\":1,\"114\":1,\"130\":1,\"154\":1,\"195\":1,\"218\":2,\"228\":1,\"233\":2,\"246\":1,\"254\":1,\"258\":1,\"316\":1,\"317\":1,\"318\":1,\"328\":2,\"333\":1,\"350\":1,\"353\":2,\"360\":1,\"365\":1,\"391\":2,\"407\":1,\"408\":1,\"411\":2,\"431\":1,\"433\":1,\"442\":1,\"449\":3,\"478\":1}}],[\"divide\",{\"1\":{\"282\":1}}],[\"divided\",{\"1\":{\"149\":1,\"151\":1,\"160\":1,\"297\":1,\"348\":1,\"369\":1}}],[\"division\",{\"1\":{\"151\":2,\"348\":2}}],[\"dialectical\",{\"0\":{\"453\":1},\"1\":{\"453\":2}}],[\"dialects\",{\"1\":{\"434\":1}}],[\"dialect\",{\"1\":{\"434\":2}}],[\"dialogs\",{\"1\":{\"351\":1}}],[\"dialog\",{\"1\":{\"261\":1}}],[\"dialoguant\",{\"0\":{\"232\":1}}],[\"dialogues\",{\"1\":{\"211\":1,\"282\":1,\"399\":1,\"449\":3}}],[\"dialogue\",{\"0\":{\"432\":1,\"449\":1},\"1\":{\"136\":1,\"155\":1,\"264\":1,\"326\":1,\"418\":1,\"432\":5,\"448\":1,\"449\":4,\"492\":1}}],[\"dialogic\",{\"1\":{\"130\":1}}],[\"diameter\",{\"1\":{\"295\":1,\"305\":1}}],[\"diagrams\",{\"1\":{\"339\":3}}],[\"diagram\",{\"1\":{\"277\":2}}],[\"diagrammatic\",{\"1\":{\"277\":1}}],[\"diagnosis\",{\"1\":{\"475\":1}}],[\"diagnosing\",{\"1\":{\"445\":1}}],[\"diagnoses\",{\"1\":{\"466\":1}}],[\"diagnosed\",{\"1\":{\"120\":1}}],[\"diagnostics\",{\"1\":{\"379\":1}}],[\"diagnostic\",{\"1\":{\"253\":1}}],[\"diabetes\",{\"1\":{\"220\":1}}],[\"dianbo\",{\"1\":{\"478\":1}}],[\"dianhui\",{\"1\":{\"478\":1}}],[\"dianping\",{\"1\":{\"402\":1}}],[\"diana\",{\"1\":{\"220\":1}}],[\"dianti\",{\"1\":{\"120\":1}}],[\"diminish\",{\"1\":{\"461\":1,\"463\":1}}],[\"diminishes\",{\"1\":{\"359\":1}}],[\"diminishing\",{\"1\":{\"329\":1}}],[\"dima\",{\"1\":{\"117\":1}}],[\"dimbridge\",{\"1\":{\"111\":4}}],[\"dimension\",{\"0\":{\"281\":1},\"1\":{\"111\":1,\"250\":1,\"281\":4}}],[\"dimensional\",{\"0\":{\"188\":1,\"293\":1,\"301\":1},\"1\":{\"111\":1,\"126\":1,\"188\":1,\"189\":1,\"204\":1,\"234\":1,\"281\":2,\"293\":1,\"301\":2}}],[\"dimensionality\",{\"0\":{\"111\":1},\"1\":{\"111\":1,\"188\":1,\"301\":2,\"475\":1}}],[\"dimensions\",{\"1\":{\"74\":1,\"111\":1,\"204\":1,\"250\":1,\"269\":1,\"281\":1,\"317\":1,\"366\":2}}],[\"did\",{\"0\":{\"195\":1},\"1\":{\"107\":1,\"165\":1,\"265\":1,\"273\":1}}],[\"dixon\",{\"1\":{\"104\":1,\"114\":1}}],[\"dilemma\",{\"0\":{\"102\":1},\"1\":{\"102\":1,\"183\":1}}],[\"differing\",{\"1\":{\"269\":1}}],[\"differs\",{\"1\":{\"182\":1}}],[\"differ\",{\"1\":{\"163\":1,\"197\":1}}],[\"difference\",{\"1\":{\"202\":1,\"220\":1,\"230\":1,\"246\":1,\"335\":2,\"497\":1}}],[\"differences\",{\"0\":{\"302\":1},\"1\":{\"106\":1,\"197\":1,\"202\":1,\"230\":1,\"237\":1,\"246\":1,\"249\":2,\"302\":1,\"304\":1,\"378\":1,\"413\":1}}],[\"differently\",{\"1\":{\"249\":1}}],[\"differentiable\",{\"1\":{\"301\":1}}],[\"differentiate\",{\"1\":{\"237\":2,\"295\":1,\"410\":1}}],[\"differentiating\",{\"1\":{\"213\":1}}],[\"differentiation\",{\"1\":{\"204\":2}}],[\"differential\",{\"0\":{\"187\":1},\"1\":{\"139\":1,\"187\":5}}],[\"different\",{\"1\":{\"110\":1,\"123\":1,\"126\":2,\"130\":1,\"132\":1,\"133\":1,\"135\":1,\"137\":1,\"146\":2,\"149\":3,\"165\":2,\"186\":1,\"189\":1,\"190\":3,\"191\":1,\"195\":1,\"199\":1,\"204\":1,\"224\":2,\"228\":1,\"240\":1,\"245\":1,\"249\":2,\"250\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"275\":1,\"277\":1,\"279\":1,\"284\":1,\"290\":1,\"292\":1,\"297\":1,\"313\":1,\"316\":1,\"319\":1,\"321\":1,\"328\":2,\"330\":1,\"331\":1,\"333\":1,\"334\":2,\"335\":2,\"337\":1,\"339\":1,\"342\":1,\"353\":4,\"355\":1,\"363\":1,\"366\":1,\"374\":1,\"384\":1,\"390\":1,\"395\":1,\"403\":1,\"404\":1,\"409\":2,\"412\":1,\"413\":2,\"422\":1,\"425\":1,\"427\":1,\"432\":1,\"433\":1,\"436\":1,\"437\":1,\"442\":1,\"445\":1,\"453\":2,\"461\":1,\"466\":1,\"468\":2,\"481\":1,\"482\":1,\"485\":2,\"497\":1}}],[\"diffusion\",{\"0\":{\"224\":1,\"235\":1,\"256\":1,\"260\":1},\"1\":{\"101\":1,\"213\":3,\"224\":3,\"235\":1,\"256\":1,\"260\":3,\"349\":2}}],[\"difficulty\",{\"1\":{\"127\":1,\"259\":1,\"274\":1,\"284\":1,\"333\":1,\"355\":2}}],[\"difficulties\",{\"1\":{\"110\":1,\"252\":1,\"253\":1,\"321\":1,\"353\":1,\"382\":1,\"426\":1,\"495\":1}}],[\"difficult\",{\"1\":{\"101\":1,\"117\":1,\"127\":1,\"133\":1,\"206\":1,\"257\":1,\"284\":1,\"326\":1,\"331\":1,\"342\":1,\"355\":1,\"363\":1,\"375\":2,\"402\":1}}],[\"dishonesty\",{\"1\":{\"439\":1}}],[\"disguiser\",{\"0\":{\"403\":1},\"1\":{\"403\":2}}],[\"disguise\",{\"0\":{\"403\":1},\"1\":{\"403\":4}}],[\"dis\",{\"1\":{\"378\":1}}],[\"dissemination\",{\"1\":{\"439\":1}}],[\"disseminating\",{\"1\":{\"428\":1}}],[\"dissect\",{\"1\":{\"328\":1}}],[\"dissatisfaction\",{\"1\":{\"259\":1}}],[\"disfluencies\",{\"1\":{\"266\":1}}],[\"disfluency\",{\"0\":{\"266\":1}}],[\"disfluent\",{\"1\":{\"266\":1}}],[\"disambiguates\",{\"1\":{\"375\":1}}],[\"disapproval\",{\"1\":{\"241\":1}}],[\"disappoint\",{\"0\":{\"228\":1}}],[\"disagreement\",{\"1\":{\"238\":1}}],[\"disasters\",{\"1\":{\"223\":1,\"258\":1,\"433\":1}}],[\"disabling\",{\"1\":{\"166\":1}}],[\"disabilities\",{\"1\":{\"164\":1}}],[\"disability\",{\"0\":{\"164\":1},\"1\":{\"184\":1,\"284\":1,\"380\":1}}],[\"disruption\",{\"1\":{\"329\":1}}],[\"disruptive\",{\"1\":{\"253\":1}}],[\"disrupting\",{\"1\":{\"212\":1,\"242\":1}}],[\"disregards\",{\"1\":{\"196\":1}}],[\"dislike\",{\"1\":{\"198\":1,\"387\":1}}],[\"display\",{\"1\":{\"287\":1,\"298\":1,\"315\":1}}],[\"displays\",{\"1\":{\"139\":1,\"213\":1,\"497\":1}}],[\"disproportionately\",{\"1\":{\"181\":1,\"302\":1}}],[\"disparities\",{\"1\":{\"106\":1,\"365\":1,\"424\":1}}],[\"diseases\",{\"1\":{\"281\":1}}],[\"disease\",{\"0\":{\"121\":1},\"1\":{\"121\":2,\"220\":1}}],[\"disorder\",{\"0\":{\"120\":1},\"1\":{\"120\":1,\"261\":1}}],[\"distant\",{\"1\":{\"480\":1}}],[\"distances\",{\"1\":{\"182\":1}}],[\"distance\",{\"0\":{\"324\":1},\"1\":{\"139\":1,\"149\":1,\"191\":1,\"324\":2,\"417\":1,\"444\":1}}],[\"distort\",{\"1\":{\"340\":1}}],[\"distillation\",{\"1\":{\"455\":4}}],[\"distilling\",{\"0\":{\"436\":1,\"455\":1},\"1\":{\"356\":1}}],[\"distilled\",{\"1\":{\"213\":1}}],[\"distinction\",{\"0\":{\"241\":1},\"1\":{\"241\":3}}],[\"distinctive\",{\"1\":{\"174\":1,\"333\":1,\"371\":1,\"458\":1}}],[\"distinct\",{\"1\":{\"142\":1,\"146\":2,\"174\":1,\"185\":1,\"213\":1,\"237\":1,\"290\":1,\"297\":1,\"323\":1,\"371\":1,\"399\":1,\"424\":1,\"449\":1,\"477\":1}}],[\"distinguishes\",{\"1\":{\"324\":1}}],[\"distinguished\",{\"1\":{\"241\":1,\"356\":1,\"429\":1}}],[\"distinguishing\",{\"1\":{\"241\":1,\"319\":1,\"439\":1}}],[\"distinguish\",{\"1\":{\"101\":1,\"179\":1}}],[\"distractor\",{\"1\":{\"395\":2}}],[\"distractors\",{\"1\":{\"395\":2}}],[\"distracted\",{\"1\":{\"395\":1}}],[\"distrust\",{\"1\":{\"150\":1}}],[\"distribute\",{\"1\":{\"205\":1}}],[\"distributed\",{\"0\":{\"279\":1},\"1\":{\"108\":2,\"132\":1,\"172\":1,\"279\":3,\"286\":1,\"330\":1,\"370\":1,\"474\":1}}],[\"distribution\",{\"1\":{\"106\":1,\"313\":1,\"425\":1,\"437\":1,\"496\":1}}],[\"disturbances\",{\"1\":{\"150\":1}}],[\"discharge\",{\"0\":{\"356\":1},\"1\":{\"356\":3,\"379\":1}}],[\"discarded\",{\"1\":{\"306\":1}}],[\"discriminate\",{\"1\":{\"444\":2}}],[\"discriminating\",{\"1\":{\"385\":1,\"432\":1}}],[\"discriminative\",{\"1\":{\"223\":1,\"317\":2,\"385\":1}}],[\"discrimination\",{\"0\":{\"317\":1},\"1\":{\"196\":1,\"317\":1,\"385\":1}}],[\"discrepancy\",{\"1\":{\"322\":1,\"424\":1}}],[\"discrepancies\",{\"1\":{\"253\":1}}],[\"discrete\",{\"1\":{\"204\":2,\"313\":1,\"438\":1}}],[\"disciplinary\",{\"1\":{\"180\":1,\"398\":1}}],[\"discipline\",{\"1\":{\"142\":1}}],[\"disciplines\",{\"1\":{\"130\":1,\"132\":1,\"330\":1}}],[\"disconnection\",{\"1\":{\"230\":1}}],[\"discourse\",{\"1\":{\"157\":1,\"171\":1,\"211\":1,\"302\":1,\"359\":1,\"431\":1}}],[\"discovering\",{\"0\":{\"242\":1}}],[\"discovered\",{\"0\":{\"270\":1},\"1\":{\"228\":1,\"270\":2,\"350\":1}}],[\"discovery\",{\"0\":{\"211\":1},\"1\":{\"112\":2,\"146\":1,\"211\":3,\"242\":1}}],[\"discover\",{\"1\":{\"110\":1,\"270\":1,\"289\":1,\"319\":1,\"321\":1,\"322\":1}}],[\"discomfort\",{\"1\":{\"106\":1}}],[\"disclosure\",{\"1\":{\"108\":1,\"178\":1}}],[\"discussing\",{\"1\":{\"189\":1,\"306\":1}}],[\"discussions\",{\"0\":{\"400\":1},\"1\":{\"178\":1,\"307\":1,\"400\":1}}],[\"discussion\",{\"1\":{\"97\":1,\"100\":2,\"105\":1,\"114\":1,\"132\":2,\"172\":1,\"174\":1,\"181\":1,\"206\":1,\"217\":3,\"231\":1,\"243\":1,\"248\":1,\"297\":1,\"308\":1,\"312\":1,\"330\":2,\"370\":1,\"371\":1,\"400\":2}}],[\"discussed\",{\"1\":{\"148\":1,\"179\":1,\"196\":1,\"253\":1}}],[\"discusses\",{\"1\":{\"136\":1,\"153\":1,\"171\":1}}],[\"discuss\",{\"1\":{\"97\":1,\"101\":1,\"108\":1,\"110\":1,\"115\":1,\"121\":1,\"148\":1,\"169\":1,\"173\":1,\"200\":2,\"206\":1,\"211\":2,\"229\":1,\"231\":1,\"243\":2,\"249\":2,\"261\":1,\"268\":2,\"272\":1,\"277\":2,\"286\":1,\"288\":1,\"293\":1,\"294\":1,\"295\":1,\"306\":1,\"312\":1,\"313\":1,\"321\":1,\"410\":1,\"452\":2,\"474\":1}}],[\"discerning\",{\"1\":{\"424\":1,\"489\":1}}],[\"discern\",{\"1\":{\"96\":1,\"213\":1,\"329\":1,\"340\":1,\"355\":1}}],[\"de4\",{\"1\":{\"519\":2}}],[\"derivation\",{\"0\":{\"497\":1},\"1\":{\"497\":1}}],[\"derived\",{\"1\":{\"173\":1,\"189\":1,\"190\":1,\"195\":1,\"199\":1,\"281\":2,\"309\":1,\"339\":1,\"362\":1,\"365\":1,\"379\":1}}],[\"derive\",{\"1\":{\"156\":1,\"278\":1,\"411\":1,\"417\":1}}],[\"dehghani\",{\"1\":{\"461\":1}}],[\"deilamsalehy\",{\"1\":{\"376\":1}}],[\"deininger\",{\"1\":{\"273\":1}}],[\"deyuan\",{\"1\":{\"478\":1}}],[\"deyao\",{\"1\":{\"393\":1}}],[\"dey\",{\"1\":{\"297\":1}}],[\"deaths\",{\"1\":{\"298\":1}}],[\"dearth\",{\"1\":{\"293\":1}}],[\"dealing\",{\"1\":{\"241\":1,\"340\":1,\"471\":1}}],[\"deductive\",{\"1\":{\"245\":1}}],[\"dedicated\",{\"1\":{\"217\":1,\"469\":1}}],[\"debugger\",{\"1\":{\"361\":1}}],[\"debugging\",{\"0\":{\"104\":1},\"1\":{\"104\":2,\"172\":2,\"361\":1,\"370\":2}}],[\"debiased\",{\"1\":{\"336\":2}}],[\"debiasing\",{\"0\":{\"336\":1},\"1\":{\"336\":2}}],[\"debargha\",{\"1\":{\"297\":1}}],[\"debate\",{\"1\":{\"248\":1}}],[\"debates\",{\"1\":{\"243\":1}}],[\"debbah\",{\"1\":{\"239\":1}}],[\"deft\",{\"0\":{\"462\":1},\"1\":{\"462\":2}}],[\"defne\",{\"1\":{\"336\":1}}],[\"defu\",{\"1\":{\"318\":1}}],[\"defeat\",{\"1\":{\"455\":1}}],[\"defence\",{\"1\":{\"416\":1}}],[\"defense\",{\"0\":{\"403\":1},\"1\":{\"403\":6,\"453\":2}}],[\"defend\",{\"1\":{\"313\":1}}],[\"defective\",{\"1\":{\"355\":1}}],[\"defect\",{\"0\":{\"355\":1},\"1\":{\"355\":7}}],[\"defederate\",{\"1\":{\"230\":1}}],[\"defederation\",{\"1\":{\"230\":5}}],[\"deficiency\",{\"1\":{\"344\":1}}],[\"deficiencies\",{\"1\":{\"257\":1}}],[\"define\",{\"0\":{\"382\":1},\"1\":{\"344\":1,\"350\":1,\"382\":1,\"422\":1}}],[\"defined\",{\"1\":{\"149\":1,\"284\":1,\"353\":1,\"382\":1}}],[\"definitions\",{\"1\":{\"254\":1,\"271\":1,\"457\":1}}],[\"definition\",{\"1\":{\"153\":1,\"169\":2,\"174\":1,\"187\":1,\"287\":1,\"371\":1,\"382\":1,\"388\":1}}],[\"defining\",{\"0\":{\"149\":1},\"1\":{\"112\":1,\"132\":1,\"282\":1,\"292\":1,\"330\":1}}],[\"deng\",{\"1\":{\"411\":1,\"482\":1}}],[\"dense\",{\"1\":{\"347\":1}}],[\"densities\",{\"0\":{\"226\":1}}],[\"density\",{\"1\":{\"143\":1,\"207\":1,\"220\":1,\"273\":1}}],[\"denoising\",{\"1\":{\"260\":2}}],[\"dennis\",{\"1\":{\"215\":1}}],[\"degrade\",{\"1\":{\"207\":1}}],[\"degradation\",{\"1\":{\"207\":1,\"427\":1,\"469\":1}}],[\"degree\",{\"0\":{\"165\":1},\"1\":{\"165\":1,\"198\":1,\"246\":1,\"259\":1,\"362\":1,\"387\":1,\"402\":2,\"436\":1,\"497\":2}}],[\"degrees\",{\"1\":{\"141\":1,\"437\":1}}],[\"deemed\",{\"1\":{\"196\":1}}],[\"deepshikha\",{\"1\":{\"270\":1}}],[\"deeper\",{\"1\":{\"172\":1,\"175\":1,\"223\":1,\"360\":1,\"370\":1,\"398\":1,\"479\":1}}],[\"deep\",{\"0\":{\"101\":1,\"255\":1,\"289\":1},\"1\":{\"101\":1,\"102\":1,\"112\":1,\"125\":2,\"150\":1,\"188\":1,\"199\":1,\"223\":1,\"249\":2,\"253\":1,\"284\":1,\"355\":1,\"398\":1,\"400\":1}}],[\"dementia\",{\"1\":{\"475\":1}}],[\"demands\",{\"1\":{\"323\":1,\"379\":1,\"458\":1,\"460\":1}}],[\"demand\",{\"0\":{\"183\":1},\"1\":{\"183\":1,\"213\":1,\"252\":1,\"349\":1,\"426\":1,\"469\":1,\"478\":1,\"498\":1}}],[\"demanding\",{\"1\":{\"179\":1,\"498\":1}}],[\"demon\",{\"1\":{\"414\":1}}],[\"demonstrative\",{\"1\":{\"317\":1}}],[\"demonstration\",{\"1\":{\"140\":2,\"412\":1}}],[\"demonstrations\",{\"1\":{\"99\":1,\"140\":2,\"317\":2}}],[\"demonstrating\",{\"1\":{\"105\":1,\"143\":1,\"204\":1,\"214\":1,\"223\":1,\"237\":1,\"246\":1,\"273\":1,\"283\":1,\"306\":1,\"351\":1,\"356\":1,\"377\":1,\"424\":1,\"425\":1,\"447\":1,\"460\":1,\"470\":1}}],[\"demonstrates\",{\"1\":{\"127\":1,\"231\":1,\"242\":1,\"261\":1,\"284\":1,\"314\":1,\"361\":1,\"369\":1,\"419\":1,\"420\":1,\"422\":1,\"459\":1,\"478\":1,\"498\":1}}],[\"demonstrate\",{\"1\":{\"103\":1,\"111\":1,\"112\":1,\"113\":1,\"116\":1,\"124\":1,\"125\":1,\"137\":1,\"139\":1,\"150\":1,\"163\":1,\"186\":1,\"192\":1,\"195\":1,\"207\":2,\"213\":1,\"219\":1,\"250\":2,\"263\":1,\"280\":1,\"281\":1,\"284\":1,\"288\":1,\"291\":1,\"298\":1,\"301\":1,\"303\":1,\"317\":1,\"319\":1,\"322\":1,\"323\":1,\"328\":1,\"329\":1,\"335\":2,\"337\":1,\"344\":1,\"360\":1,\"374\":1,\"375\":1,\"376\":1,\"378\":1,\"382\":1,\"389\":1,\"390\":1,\"400\":1,\"401\":1,\"408\":1,\"409\":1,\"413\":1,\"415\":1,\"417\":1,\"418\":1,\"425\":2,\"432\":1,\"434\":1,\"435\":1,\"444\":1,\"445\":2,\"450\":1,\"454\":2,\"476\":1,\"477\":1,\"478\":1,\"483\":1,\"488\":1,\"494\":1,\"496\":1}}],[\"demonstrated\",{\"1\":{\"103\":1,\"119\":1,\"120\":1,\"132\":1,\"138\":1,\"142\":1,\"166\":1,\"201\":1,\"218\":2,\"251\":1,\"257\":1,\"318\":1,\"330\":1,\"355\":1,\"358\":1,\"363\":1,\"372\":1,\"376\":1,\"394\":1,\"398\":1,\"419\":1,\"420\":1,\"423\":1,\"458\":1,\"467\":1,\"483\":1}}],[\"demographic\",{\"1\":{\"277\":1,\"461\":1}}],[\"demo\",{\"1\":{\"219\":1,\"258\":1,\"361\":1,\"401\":1,\"433\":1}}],[\"demos\",{\"1\":{\"217\":1,\"235\":1}}],[\"democratizes\",{\"1\":{\"157\":1}}],[\"delayed\",{\"1\":{\"298\":1}}],[\"delays\",{\"0\":{\"226\":1},\"1\":{\"196\":1}}],[\"delegation\",{\"1\":{\"279\":1}}],[\"delegate\",{\"1\":{\"192\":1,\"326\":1}}],[\"delve\",{\"0\":{\"249\":1},\"1\":{\"249\":3}}],[\"delves\",{\"1\":{\"100\":1,\"394\":1}}],[\"delving\",{\"1\":{\"239\":1,\"328\":1}}],[\"deliberativeness\",{\"1\":{\"400\":1}}],[\"deliberative\",{\"1\":{\"400\":5}}],[\"deliberation\",{\"0\":{\"400\":1},\"1\":{\"400\":1}}],[\"delivering\",{\"1\":{\"399\":1}}],[\"delivered\",{\"1\":{\"261\":1}}],[\"delivers\",{\"1\":{\"188\":1,\"324\":1,\"389\":1}}],[\"delivery\",{\"0\":{\"303\":1},\"1\":{\"167\":1,\"223\":1,\"303\":3}}],[\"delineate\",{\"1\":{\"174\":1,\"371\":1}}],[\"depiction\",{\"1\":{\"359\":2}}],[\"depp\",{\"1\":{\"268\":1,\"452\":1}}],[\"depth\",{\"1\":{\"207\":1,\"266\":1,\"300\":1,\"317\":1,\"353\":1,\"411\":1}}],[\"depositing\",{\"1\":{\"192\":1}}],[\"deployable\",{\"1\":{\"361\":1}}],[\"deploy\",{\"1\":{\"261\":1,\"363\":1}}],[\"deploying\",{\"1\":{\"249\":1,\"300\":1,\"479\":1}}],[\"deployment\",{\"1\":{\"179\":1,\"210\":1,\"244\":1,\"249\":1,\"316\":1,\"344\":1,\"358\":1,\"361\":1,\"427\":1,\"437\":1,\"465\":1}}],[\"deployed\",{\"0\":{\"249\":1},\"1\":{\"135\":1,\"171\":1,\"179\":1,\"221\":1,\"381\":1,\"465\":1,\"479\":1}}],[\"depends\",{\"1\":{\"273\":1}}],[\"depending\",{\"1\":{\"195\":1,\"282\":1,\"425\":1,\"437\":1,\"462\":1,\"465\":1,\"466\":1}}],[\"dependon\",{\"1\":{\"190\":1}}],[\"depend\",{\"1\":{\"183\":2,\"221\":1,\"253\":1}}],[\"dependability\",{\"1\":{\"158\":2}}],[\"dependence\",{\"1\":{\"355\":1}}],[\"dependency\",{\"1\":{\"165\":1}}],[\"dependencies\",{\"1\":{\"137\":2,\"235\":1,\"360\":2}}],[\"dependent\",{\"1\":{\"165\":2,\"199\":2,\"254\":1,\"275\":1,\"304\":1,\"328\":1}}],[\"depended\",{\"1\":{\"135\":1}}],[\"departments\",{\"1\":{\"156\":1}}],[\"detommaso\",{\"1\":{\"368\":1}}],[\"detachable\",{\"0\":{\"306\":1}}],[\"detailing\",{\"1\":{\"334\":1}}],[\"detail\",{\"1\":{\"153\":1,\"196\":1,\"291\":1,\"295\":1,\"306\":1,\"307\":1,\"349\":1,\"476\":1}}],[\"details\",{\"1\":{\"131\":1,\"151\":1,\"209\":1,\"224\":1,\"235\":1,\"287\":1,\"316\":1,\"324\":1,\"348\":1,\"350\":1,\"424\":1,\"491\":1}}],[\"detailed\",{\"1\":{\"131\":1,\"151\":1,\"174\":1,\"182\":1,\"246\":1,\"271\":1,\"317\":1,\"324\":1,\"327\":1,\"337\":1,\"348\":1,\"353\":1,\"358\":1,\"371\":1,\"377\":1,\"399\":1,\"430\":1,\"442\":1,\"457\":1}}],[\"determining\",{\"1\":{\"350\":1}}],[\"determines\",{\"1\":{\"399\":1,\"410\":1}}],[\"determined\",{\"1\":{\"324\":1}}],[\"determine\",{\"1\":{\"234\":1,\"489\":1,\"491\":1}}],[\"detect2interact\",{\"0\":{\"430\":1},\"1\":{\"430\":2}}],[\"detectable\",{\"1\":{\"410\":1}}],[\"detects\",{\"1\":{\"240\":1,\"246\":1}}],[\"detectors\",{\"1\":{\"391\":3}}],[\"detector\",{\"0\":{\"325\":1},\"1\":{\"190\":1}}],[\"detecting\",{\"0\":{\"391\":1},\"1\":{\"176\":1,\"213\":1,\"298\":1,\"391\":1,\"439\":1,\"491\":1}}],[\"detection\",{\"0\":{\"296\":1,\"298\":1,\"388\":1,\"439\":1,\"460\":1,\"477\":1,\"491\":1},\"1\":{\"126\":1,\"127\":1,\"146\":1,\"176\":1,\"181\":1,\"198\":2,\"213\":1,\"237\":1,\"241\":1,\"265\":1,\"278\":1,\"296\":3,\"298\":4,\"309\":1,\"352\":2,\"387\":2,\"388\":1,\"391\":4,\"409\":2,\"410\":2,\"413\":1,\"430\":3,\"437\":1,\"439\":2,\"443\":1,\"460\":1,\"477\":2,\"486\":1,\"488\":1,\"491\":1}}],[\"detect\",{\"1\":{\"146\":1,\"181\":1,\"199\":1,\"257\":1,\"351\":1,\"391\":1}}],[\"detected\",{\"1\":{\"124\":1,\"127\":1,\"296\":1}}],[\"detrimental\",{\"1\":{\"211\":1,\"308\":1}}],[\"devoted\",{\"1\":{\"249\":1}}],[\"devin\",{\"1\":{\"281\":1}}],[\"deviation\",{\"1\":{\"437\":1}}],[\"deviations\",{\"1\":{\"182\":1,\"309\":2}}],[\"deviate\",{\"1\":{\"238\":1,\"298\":1,\"437\":1}}],[\"devised\",{\"1\":{\"488\":1}}],[\"devise\",{\"1\":{\"212\":1,\"245\":1}}],[\"devices\",{\"1\":{\"139\":1,\"190\":1,\"207\":1,\"210\":2,\"220\":1,\"246\":1,\"284\":2}}],[\"device\",{\"1\":{\"126\":2,\"149\":1,\"207\":1,\"210\":1,\"214\":1,\"306\":1}}],[\"develops\",{\"1\":{\"150\":1}}],[\"developed\",{\"1\":{\"117\":1,\"133\":1,\"142\":1,\"153\":1,\"155\":1,\"156\":1,\"160\":1,\"163\":1,\"171\":1,\"173\":1,\"174\":1,\"182\":3,\"189\":1,\"192\":1,\"210\":1,\"213\":1,\"214\":1,\"219\":1,\"231\":2,\"250\":1,\"264\":1,\"271\":1,\"274\":1,\"279\":1,\"331\":1,\"334\":1,\"350\":1,\"371\":1,\"398\":1,\"401\":1,\"425\":1,\"448\":1,\"457\":1}}],[\"developer\",{\"1\":{\"114\":1,\"128\":1,\"495\":1}}],[\"developers\",{\"1\":{\"106\":3,\"114\":1,\"128\":1,\"219\":1,\"254\":1,\"265\":1,\"361\":1,\"398\":2,\"401\":1}}],[\"developing\",{\"0\":{\"166\":1},\"1\":{\"114\":1,\"133\":1,\"154\":1,\"181\":1,\"184\":1,\"251\":1,\"305\":1,\"316\":1,\"331\":1,\"353\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"423\":1,\"481\":1,\"492\":1}}],[\"develop\",{\"1\":{\"100\":1,\"122\":1,\"125\":2,\"154\":3,\"156\":2,\"175\":1,\"176\":1,\"178\":1,\"207\":2,\"218\":1,\"227\":1,\"245\":1,\"248\":1,\"259\":1,\"292\":2,\"297\":1,\"353\":1,\"368\":1,\"400\":1}}],[\"developments\",{\"1\":{\"101\":1,\"253\":1}}],[\"development\",{\"0\":{\"126\":1,\"254\":1},\"1\":{\"98\":1,\"99\":1,\"106\":1,\"113\":1,\"117\":1,\"119\":1,\"126\":4,\"130\":1,\"157\":1,\"171\":1,\"180\":1,\"186\":1,\"211\":1,\"216\":1,\"218\":1,\"231\":2,\"243\":1,\"254\":3,\"265\":1,\"268\":1,\"272\":1,\"286\":1,\"287\":1,\"291\":1,\"292\":1,\"296\":1,\"315\":1,\"316\":1,\"317\":1,\"324\":1,\"329\":1,\"334\":1,\"341\":1,\"358\":1,\"361\":3,\"379\":1,\"398\":2,\"408\":1,\"415\":1,\"425\":1,\"427\":1,\"429\":1,\"452\":1,\"463\":1,\"474\":1,\"476\":1,\"482\":1}}],[\"declaration\",{\"1\":{\"453\":1}}],[\"decline\",{\"1\":{\"107\":1}}],[\"decentralized\",{\"1\":{\"230\":2}}],[\"deceptive\",{\"0\":{\"491\":1},\"1\":{\"213\":2,\"340\":1,\"491\":1}}],[\"decreases\",{\"1\":{\"302\":2}}],[\"decrease\",{\"1\":{\"223\":1,\"463\":1,\"471\":1,\"482\":1}}],[\"decreased\",{\"1\":{\"121\":1}}],[\"decreasing\",{\"1\":{\"188\":1}}],[\"decade\",{\"1\":{\"169\":1,\"243\":1,\"257\":1}}],[\"decades\",{\"1\":{\"153\":1,\"182\":1,\"239\":1,\"269\":1}}],[\"decouples\",{\"1\":{\"479\":1}}],[\"decoupled\",{\"0\":{\"235\":1},\"1\":{\"235\":1,\"485\":1}}],[\"decoupling\",{\"1\":{\"318\":1}}],[\"decodes\",{\"1\":{\"390\":1}}],[\"decoders\",{\"1\":{\"436\":1}}],[\"decoder\",{\"1\":{\"234\":2,\"395\":3,\"436\":1}}],[\"decode\",{\"1\":{\"207\":1}}],[\"decoding\",{\"0\":{\"463\":1,\"487\":1},\"1\":{\"207\":1,\"414\":1,\"462\":2,\"463\":6,\"487\":3}}],[\"decontextualization\",{\"1\":{\"165\":3}}],[\"decolonising\",{\"0\":{\"117\":1},\"1\":{\"117\":1}}],[\"decompose\",{\"1\":{\"317\":1}}],[\"decomposed\",{\"1\":{\"103\":1}}],[\"decomposing\",{\"0\":{\"317\":1}}],[\"decomposition\",{\"0\":{\"103\":1},\"1\":{\"103\":2,\"141\":1,\"471\":1,\"482\":1}}],[\"decided\",{\"1\":{\"298\":1}}],[\"deciding\",{\"1\":{\"96\":1}}],[\"decisions\",{\"1\":{\"107\":1,\"129\":1,\"135\":2,\"158\":1,\"159\":3,\"160\":1,\"176\":3,\"183\":1,\"196\":2,\"226\":2,\"245\":1,\"253\":1,\"254\":1,\"263\":1,\"318\":1,\"388\":1}}],[\"decision\",{\"0\":{\"135\":1,\"160\":1},\"1\":{\"106\":1,\"135\":2,\"146\":1,\"160\":6,\"171\":3,\"176\":3,\"217\":4,\"226\":2,\"245\":1,\"251\":2,\"253\":1,\"288\":1,\"305\":1,\"318\":1,\"356\":1,\"423\":2,\"429\":1,\"466\":2}}],[\"desai\",{\"1\":{\"422\":1}}],[\"desoldering\",{\"1\":{\"306\":1}}],[\"despite\",{\"1\":{\"204\":1,\"213\":1,\"215\":1,\"226\":1,\"230\":1,\"284\":1,\"304\":1,\"313\":1,\"319\":1,\"353\":1,\"376\":1,\"382\":1,\"396\":1,\"419\":1,\"420\":1,\"424\":1,\"427\":1,\"437\":1,\"458\":1,\"467\":1,\"478\":1,\"486\":1}}],[\"destination\",{\"1\":{\"191\":1,\"375\":1}}],[\"describe\",{\"1\":{\"235\":1,\"258\":1,\"326\":1,\"388\":1,\"413\":1,\"433\":1,\"490\":1}}],[\"described\",{\"1\":{\"164\":1,\"286\":1,\"388\":1,\"474\":1}}],[\"describes\",{\"1\":{\"150\":1,\"153\":1,\"161\":1,\"253\":1,\"325\":1,\"354\":1,\"375\":1}}],[\"describing\",{\"1\":{\"153\":1}}],[\"descriptive\",{\"1\":{\"250\":1,\"319\":1,\"333\":1}}],[\"description\",{\"1\":{\"151\":1,\"153\":1,\"251\":1,\"348\":1,\"355\":1,\"362\":1,\"363\":1,\"423\":1}}],[\"descriptions\",{\"1\":{\"123\":2,\"163\":1,\"172\":1,\"319\":1,\"370\":1,\"404\":1,\"413\":1,\"430\":1,\"490\":1}}],[\"descriptors\",{\"1\":{\"112\":1,\"442\":2}}],[\"desktop\",{\"0\":{\"119\":1},\"1\":{\"119\":2}}],[\"deserve\",{\"1\":{\"101\":1}}],[\"desirable\",{\"1\":{\"200\":2,\"303\":1,\"533\":1}}],[\"desired\",{\"1\":{\"99\":1,\"188\":1,\"317\":1,\"407\":1}}],[\"desires\",{\"0\":{\"500\":1},\"1\":{\"16\":1,\"520\":1}}],[\"designating\",{\"1\":{\"205\":1}}],[\"designated\",{\"1\":{\"128\":1,\"298\":1}}],[\"designer\",{\"0\":{\"399\":1},\"1\":{\"399\":1}}],[\"designers\",{\"1\":{\"149\":3,\"181\":2,\"218\":2,\"250\":1,\"265\":1}}],[\"designed\",{\"1\":{\"100\":1,\"117\":1,\"119\":1,\"120\":1,\"126\":1,\"139\":1,\"142\":1,\"146\":2,\"148\":1,\"150\":1,\"167\":1,\"171\":1,\"181\":1,\"199\":2,\"210\":1,\"219\":1,\"222\":1,\"224\":1,\"235\":1,\"243\":1,\"246\":1,\"248\":1,\"264\":1,\"266\":1,\"282\":1,\"287\":1,\"298\":1,\"300\":1,\"303\":1,\"306\":1,\"307\":1,\"318\":1,\"323\":1,\"324\":1,\"329\":1,\"336\":2,\"345\":2,\"361\":1,\"379\":1,\"393\":1,\"395\":1,\"396\":1,\"398\":1,\"401\":1,\"411\":1,\"447\":1,\"448\":1,\"467\":1,\"470\":1,\"477\":1,\"487\":1}}],[\"designing\",{\"0\":{\"171\":1,\"173\":1,\"181\":1,\"189\":1,\"192\":1,\"263\":1,\"272\":1},\"1\":{\"132\":1,\"144\":2,\"163\":1,\"171\":1,\"250\":1,\"272\":1,\"292\":1,\"330\":1,\"471\":1,\"475\":1}}],[\"designs\",{\"0\":{\"326\":1,\"335\":1},\"1\":{\"122\":1,\"135\":1,\"185\":1,\"196\":1,\"209\":1,\"236\":1,\"257\":4,\"280\":2,\"363\":1,\"399\":1,\"450\":1}}],[\"design\",{\"0\":{\"144\":1,\"156\":1,\"164\":1,\"185\":1,\"206\":1,\"236\":1,\"250\":1,\"292\":1,\"297\":1,\"399\":1,\"519\":1},\"1\":{\"74\":1,\"96\":1,\"98\":1,\"99\":3,\"105\":1,\"113\":7,\"114\":1,\"117\":3,\"122\":1,\"123\":1,\"125\":1,\"129\":2,\"140\":1,\"141\":1,\"144\":3,\"148\":2,\"153\":1,\"154\":1,\"156\":15,\"160\":2,\"164\":1,\"165\":1,\"166\":2,\"171\":3,\"173\":2,\"174\":1,\"181\":1,\"184\":1,\"185\":1,\"189\":3,\"191\":1,\"197\":2,\"206\":1,\"212\":1,\"214\":1,\"218\":2,\"222\":1,\"226\":1,\"227\":1,\"229\":1,\"231\":2,\"233\":1,\"235\":1,\"236\":3,\"244\":1,\"249\":2,\"250\":4,\"252\":2,\"257\":1,\"258\":1,\"263\":1,\"268\":1,\"269\":1,\"271\":1,\"275\":2,\"278\":1,\"291\":1,\"293\":1,\"295\":2,\"296\":4,\"297\":4,\"303\":1,\"306\":3,\"308\":2,\"314\":1,\"326\":1,\"339\":1,\"355\":1,\"358\":1,\"363\":2,\"371\":1,\"380\":1,\"399\":10,\"403\":1,\"409\":1,\"417\":1,\"426\":2,\"433\":1,\"445\":1,\"452\":1,\"457\":1,\"466\":1,\"470\":1,\"475\":2,\"476\":1,\"487\":1,\"490\":2,\"497\":1,\"502\":1,\"510\":2,\"519\":3,\"524\":1,\"527\":1,\"529\":4}}],[\"de\",{\"0\":{\"232\":1},\"1\":{\"96\":1,\"158\":1,\"232\":1,\"233\":1,\"256\":1,\"278\":1,\"280\":1,\"282\":1,\"315\":1,\"365\":1,\"414\":1,\"419\":2,\"420\":2,\"429\":1,\"492\":1}}],[\"3️⃣\",{\"1\":{\"527\":1,\"531\":1}}],[\"3h\",{\"0\":{\"453\":1},\"1\":{\"453\":1}}],[\"33\",{\"1\":{\"411\":1}}],[\"38\",{\"1\":{\"333\":1,\"466\":1}}],[\"300\",{\"1\":{\"377\":1,\"492\":1}}],[\"30630\",{\"1\":{\"307\":1}}],[\"30\",{\"0\":{\"267\":1,\"451\":1},\"1\":{\"366\":1}}],[\"30$\",{\"1\":{\"140\":1}}],[\"362\",{\"1\":{\"254\":1}}],[\"360\",{\"0\":{\"165\":1},\"1\":{\"122\":1,\"165\":1}}],[\"314\",{\"1\":{\"350\":1}}],[\"31\",{\"0\":{\"262\":1,\"440\":1},\"1\":{\"176\":1,\"360\":1}}],[\"32k\",{\"1\":{\"412\":1}}],[\"32k+\",{\"1\":{\"337\":1}}],[\"32\",{\"1\":{\"176\":1,\"335\":1,\"411\":1}}],[\"34\",{\"1\":{\"120\":1,\"166\":1,\"362\":1,\"395\":1}}],[\"37\",{\"1\":{\"103\":1,\"133\":1,\"154\":1,\"331\":1,\"395\":1,\"458\":1}}],[\"39\",{\"1\":{\"103\":1,\"204\":1,\"360\":1,\"482\":1}}],[\"3d\",{\"0\":{\"306\":1},\"1\":{\"81\":1,\"137\":3,\"246\":2,\"259\":6,\"306\":1,\"399\":3}}],[\"3d打印火热\",{\"1\":{\"78\":1}}],[\"3\",{\"0\":{\"351\":1,\"516\":1},\"1\":{\"74\":1,\"120\":1,\"156\":1,\"179\":1,\"184\":1,\"187\":1,\"188\":1,\"199\":1,\"210\":2,\"231\":1,\"248\":1,\"261\":1,\"269\":1,\"271\":1,\"313\":1,\"327\":1,\"328\":1,\"334\":2,\"345\":1,\"351\":1,\"353\":1,\"369\":2,\"372\":1,\"376\":1,\"380\":1,\"391\":2,\"404\":1,\"409\":1,\"411\":4,\"422\":2,\"424\":5,\"428\":1,\"431\":1,\"442\":1,\"443\":1,\"447\":3,\"453\":1,\"455\":1,\"457\":1,\"460\":2,\"462\":1,\"463\":1,\"481\":1,\"486\":4,\"487\":3,\"492\":2,\"495\":1,\"497\":2}}],[\"3月\",{\"1\":{\"9\":1}}],[\"罗伯特\",{\"1\":{\"74\":1}}],[\"亦可以是精神性的\",{\"1\":{\"74\":1,\"530\":1}}],[\"帮助他们找到非传统的解决方向\",{\"1\":{\"533\":1}}],[\"帮助他们分析业务\",{\"1\":{\"525\":1}}],[\"帮助用户避免因选择过多而导致的决策疲劳\",{\"1\":{\"532\":1}}],[\"帮助用户快速排解情绪\",{\"1\":{\"499\":1}}],[\"帮助旅行者做出明智的住宿选择\",{\"1\":{\"532\":1}}],[\"帮助建立品牌认知和吸引潜在客户\",{\"1\":{\"531\":1}}],[\"帮助设计师在快速变化的环境中导航\",{\"1\":{\"527\":1}}],[\"帮助孩子们圆梦\",{\"1\":{\"74\":1,\"530\":1}}],[\"帮助学生规划和形成从留学到职业发展的长期路线\",{\"1\":{\"4\":1}}],[\"帮助学生打造专属的个人ip作品集\",{\"1\":{\"4\":1}}],[\"内心的力量被放大\",{\"1\":{\"74\":1,\"530\":1}}],[\"内容销售涉及虚拟现实场景和在线课程的购买或租赁\",{\"1\":{\"520\":1}}],[\"内容参考\",{\"1\":{\"42\":1}}],[\"内容开发等一系列环节\",{\"1\":{\"36\":1}}],[\"小身体中的巨大力量被挖掘\",{\"1\":{\"74\":1}}],[\"小红书类转化率机制\",{\"1\":{\"521\":1}}],[\"小红书\",{\"1\":{\"511\":1}}],[\"小红书官方邀请了第二届上海小红书社区熟人节\",{\"1\":{\"26\":1}}],[\"小红书第二届上海社区熟人节\",{\"0\":{\"26\":1}}],[\"结账流程与商品展示优化\",{\"1\":{\"528\":1}}],[\"结构层\",{\"1\":{\"521\":1}}],[\"结果体现在业务的增长和发展上\",{\"1\":{\"525\":1}}],[\"结果\",{\"1\":{\"519\":1,\"524\":1,\"525\":1,\"539\":1}}],[\"结合设计师的直觉\",{\"1\":{\"527\":1}}],[\"结合ai技术提升服务质量和客户满意度\",{\"1\":{\"518\":1}}],[\"结合交互技术形成一个社会创新设计\",{\"1\":{\"92\":1}}],[\"结合让\",{\"1\":{\"79\":1}}],[\"结合新技术带来的有利条件\",{\"1\":{\"78\":1}}],[\"结论\",{\"0\":{\"74\":1},\"1\":{\"522\":1}}],[\"结尾留白给人想象\",{\"1\":{\"46\":1}}],[\"太阳将要升起\",{\"1\":{\"73\":1}}],[\"太阳升了起来\",{\"1\":{\"46\":1}}],[\"周围的人看到的是孩子们的身影\",{\"1\":{\"73\":1}}],[\"周年庆典活动策划\",{\"1\":{\"6\":1}}],[\"巨大力量的释放\",{\"1\":{\"73\":1}}],[\"巨大的金属扩音器底下是孩子们心中音乐种子的萌发\",{\"1\":{\"70\":1}}],[\"无奈不到一年\",{\"1\":{\"534\":1}}],[\"无论是个人还是团队\",{\"1\":{\"538\":1}}],[\"无论是消费设计还是数字营销\",{\"1\":{\"531\":1}}],[\"无论我们喜欢与否\",{\"1\":{\"538\":1}}],[\"无论如何\",{\"1\":{\"50\":1}}],[\"无法理性消费\",{\"1\":{\"531\":1}}],[\"无法跟踪转换\",{\"1\":{\"79\":1}}],[\"无畏\",{\"1\":{\"73\":1}}],[\"夜晚\",{\"1\":{\"73\":1}}],[\"会有非常多的可能性出现\",{\"1\":{\"521\":1}}],[\"会自主思考\",{\"1\":{\"509\":1}}],[\"会造成社交孤立\",{\"1\":{\"81\":1}}],[\"会是兴奋\",{\"1\":{\"73\":1}}],[\"会议\",{\"1\":{\"40\":1}}],[\"父母会是紧张\",{\"1\":{\"73\":1}}],[\"放映机偶尔闪过江边父母的神情写照\",{\"1\":{\"73\":1}}],[\"构想\",{\"1\":{\"533\":1}}],[\"构筑着他们心中的小小世界\",{\"1\":{\"72\":1}}],[\"构造属于自己的敲击乐器\",{\"1\":{\"70\":1}}],[\"构造一个针对z世代用户的开放式世界\",{\"1\":{\"33\":1}}],[\"面对建筑的废墟\",{\"1\":{\"72\":1}}],[\"4️⃣数据分析可以帮助识别设计中的瓶颈\",{\"1\":{\"527\":1}}],[\"4o的发布\",{\"1\":{\"512\":1}}],[\"4open\",{\"1\":{\"159\":1}}],[\"47\",{\"1\":{\"482\":1}}],[\"4$\",{\"1\":{\"462\":1}}],[\"48\",{\"1\":{\"458\":1}}],[\"45\",{\"1\":{\"408\":1}}],[\"42\",{\"1\":{\"408\":1,\"454\":2}}],[\"49\",{\"1\":{\"362\":1}}],[\"44\",{\"1\":{\"304\":1}}],[\"411\",{\"1\":{\"172\":1,\"370\":1}}],[\"404\",{\"1\":{\"540\":1}}],[\"40b\",{\"1\":{\"497\":1}}],[\"409\",{\"1\":{\"431\":1}}],[\"4096\",{\"1\":{\"207\":1}}],[\"40\",{\"1\":{\"181\":1,\"234\":1,\"302\":1}}],[\"400\",{\"1\":{\"159\":1,\"369\":1}}],[\"40年代的场景\",{\"1\":{\"72\":1}}],[\"460\",{\"1\":{\"155\":1}}],[\"4v\",{\"1\":{\"151\":1,\"345\":1,\"348\":1}}],[\"432\",{\"1\":{\"470\":1}}],[\"430\",{\"1\":{\"416\":1}}],[\"43\",{\"1\":{\"122\":1}}],[\"4至7岁的术前儿童进入直觉思维子阶段\",{\"1\":{\"79\":1}}],[\"4\",{\"0\":{\"456\":1,\"519\":1},\"1\":{\"74\":1,\"79\":1,\"126\":1,\"154\":1,\"161\":1,\"172\":1,\"219\":1,\"253\":1,\"269\":1,\"271\":1,\"313\":1,\"334\":1,\"337\":1,\"340\":1,\"353\":1,\"354\":1,\"358\":3,\"365\":3,\"370\":1,\"391\":3,\"393\":1,\"401\":1,\"404\":2,\"405\":1,\"408\":1,\"409\":2,\"412\":1,\"422\":2,\"424\":5,\"430\":1,\"443\":1,\"447\":1,\"456\":2,\"457\":1,\"458\":2,\"460\":1,\"462\":1,\"467\":1,\"471\":2,\"478\":1,\"481\":1,\"482\":1,\"486\":5,\"495\":2,\"497\":3,\"502\":1,\"510\":1,\"519\":7}}],[\"4d\",{\"1\":{\"8\":1,\"112\":1}}],[\"六帽子思考法则\",{\"0\":{\"523\":1},\"1\":{\"502\":1}}],[\"六种材料中只有建筑材料被运输到水底\",{\"1\":{\"72\":1}}],[\"六个灯塔散布在四周\",{\"1\":{\"58\":1}}],[\"绳子解开\",{\"1\":{\"72\":1}}],[\"蒸汽狂欢\",{\"0\":{\"71\":1}}],[\"演变进化的名字\",{\"1\":{\"74\":1}}],[\"演变历程的摘录\",{\"0\":{\"49\":1}}],[\"演出就此开始\",{\"1\":{\"70\":1}}],[\"站在这些巨大的扩音器内\",{\"1\":{\"70\":1}}],[\"固定之上\",{\"1\":{\"70\":1}}],[\"乐观主义者的帽子\",{\"1\":{\"523\":1}}],[\"乐团\",{\"0\":{\"70\":1}}],[\"乐土之梦\",{\"0\":{\"47\":1}}],[\"拼接\",{\"1\":{\"69\":1}}],[\"搭建\",{\"1\":{\"69\":1}}],[\"筒壁还附着着许多奇异的橡胶球让他们借力和缓冲\",{\"1\":{\"68\":1}}],[\"飞跃的梦\",{\"0\":{\"68\":1}}],[\"运用黄帽思维\",{\"1\":{\"523\":1}}],[\"运输塔往上运输这批废料\",{\"1\":{\"67\":1}}],[\"运营的思维在调动你的设计产出\",{\"1\":{\"537\":1}}],[\"运营逻辑也将发生变化\",{\"1\":{\"36\":1}}],[\"运营\",{\"1\":{\"21\":1,\"23\":1}}],[\"运营取得了显著的推广效果\",{\"1\":{\"14\":1}}],[\"乘坐着机械滑轮车\",{\"1\":{\"67\":1}}],[\"乘风\",{\"1\":{\"24\":1}}],[\"乘风破浪\",{\"1\":{\"24\":1}}],[\"乘风破浪ip合作\",{\"0\":{\"24\":1}}],[\"乘风破浪3\",{\"1\":{\"4\":1,\"23\":1,\"24\":5}}],[\"突破了洞口\",{\"1\":{\"66\":1}}],[\"突破传统动画形式的局限\",{\"1\":{\"16\":1}}],[\"塔\",{\"0\":{\"66\":1}}],[\"踏上了寻找之路后\",{\"1\":{\"65\":1}}],[\"抵达\",{\"0\":{\"65\":1}}],[\"暂别这个繁闹的都市中心\",{\"1\":{\"63\":1}}],[\"欢迎\",{\"1\":{\"521\":1}}],[\"欢迎来到车车灵感一分钟\",{\"1\":{\"511\":1}}],[\"欢庆之日\",{\"1\":{\"60\":1}}],[\"欢声笑语让这冷寂之地又有了新生\",{\"1\":{\"46\":1}}],[\"灯塔成为码头的一部分\",{\"1\":{\"62\":1}}],[\"灯火灿烂\",{\"1\":{\"59\":1}}],[\"灯光\",{\"1\":{\"6\":1,\"534\":1}}],[\"降下\",{\"1\":{\"59\":1}}],[\"围墙被收缩到水下\",{\"1\":{\"73\":1}}],[\"围墙被降下\",{\"1\":{\"53\":1}}],[\"围墙包裹着这片乐土\",{\"1\":{\"59\":1}}],[\"围墙\",{\"0\":{\"59\":1}}],[\"沉入水底的橡胶筒和岩石层\",{\"1\":{\"58\":1}}],[\"扭曲的岩石层穿插在塔和平台之间\",{\"1\":{\"58\":1}}],[\"两者将共同推动营销领域的创新\",{\"1\":{\"531\":1}}],[\"两者都在寻求以创新和有意义的方式创造\",{\"1\":{\"533\":1}}],[\"两者都强调用户体验\",{\"1\":{\"531\":1}}],[\"两者都依赖于现代技术\",{\"1\":{\"531\":1}}],[\"两座塔在江面上形成\",{\"1\":{\"58\":1}}],[\"两端叙事与历史的交错\",{\"1\":{\"45\":1}}],[\"秩序的形成\",{\"0\":{\"58\":1}}],[\"橡胶\",{\"1\":{\"57\":1}}],[\"橡胶一直是成为人们飞天和入海的某种载体伴随\",{\"1\":{\"57\":1}}],[\"纸材\",{\"1\":{\"57\":1}}],[\"纸材同样蕴藏着记忆\",{\"1\":{\"57\":1}}],[\"塑料城\",{\"0\":{\"69\":1}}],[\"塑料\",{\"1\":{\"57\":1}}],[\"塑料造就着孩子们童年的玩具梦\",{\"1\":{\"57\":1}}],[\"历史\",{\"1\":{\"530\":1}}],[\"历史的反思\",{\"1\":{\"57\":1}}],[\"历史与现实对比\",{\"0\":{\"52\":1}}],[\"金匠的面试真的是我觉得体验感最好的了\",{\"1\":{\"509\":1}}],[\"金属乐\",{\"1\":{\"70\":1}}],[\"金属\",{\"1\":{\"57\":1}}],[\"金属独特的声响已经可以埋下音乐的种子\",{\"1\":{\"57\":1}}],[\"金融等领域的价值必然存在\",{\"1\":{\"33\":1}}],[\"食材的美妙香味和色彩足以构成美妙的画面\",{\"1\":{\"57\":1}}],[\"废料达到之时\",{\"1\":{\"72\":1}}],[\"废料被传送到上层的不同地方\",{\"1\":{\"65\":1}}],[\"废弃的纸堆在机器中可以使用不同的试液去处理\",{\"1\":{\"69\":1}}],[\"废弃的塑料又被合成形状大小不一的塑料片\",{\"1\":{\"69\":1}}],[\"废弃的橡胶经过再造成为孩子的充气服\",{\"1\":{\"68\":1}}],[\"废弃材料的美妙\",{\"0\":{\"57\":1}}],[\"废物游乐场\",{\"1\":{\"48\":1}}],[\"感官和视觉效果\",{\"1\":{\"81\":1}}],[\"感叹着那份美好的丢失\",{\"1\":{\"56\":1}}],[\"感觉良好\",{\"1\":{\"50\":1}}],[\"厨师为了最美好的一道菜舍弃许多少有瑕疵的食材\",{\"1\":{\"56\":1}}],[\"工具\",{\"1\":{\"524\":1}}],[\"工业4\",{\"1\":{\"510\":1,\"519\":2}}],[\"工作和创新的未来可能确实需要一种新的语言\",{\"1\":{\"533\":1}}],[\"工作坊现场\",{\"0\":{\"86\":1}}],[\"工作室服务内容包括ux\",{\"1\":{\"4\":1}}],[\"工人麻木了机械化的工厂造件\",{\"1\":{\"56\":1}}],[\"工程师叹息着建筑的拆迁\",{\"1\":{\"56\":1}}],[\"诞生\",{\"0\":{\"55\":1}}],[\"重要性\",{\"1\":{\"521\":1}}],[\"重量\",{\"1\":{\"79\":1}}],[\"重生三部曲\",{\"0\":{\"54\":1}}],[\"重新调整设计优先级\",{\"1\":{\"527\":1}}],[\"重新去思考传统艺术教育的痛点和未来的机会点\",{\"1\":{\"77\":1}}],[\"重新起航\",{\"1\":{\"46\":1}}],[\"重新连接知识产权\",{\"1\":{\"39\":1}}],[\"重新链接ip\",{\"1\":{\"33\":1}}],[\"重新启航\",{\"1\":{\"24\":1}}],[\"自然语言处理\",{\"1\":{\"539\":1}}],[\"自动化\",{\"1\":{\"539\":1}}],[\"自我介绍\",{\"1\":{\"538\":1}}],[\"自毕业设计开始已经决定转行\",{\"1\":{\"534\":1}}],[\"自由无畏\",{\"1\":{\"53\":1}}],[\"自媒体创作起步\",{\"0\":{\"20\":1}}],[\"自媒体运营\",{\"0\":{\"14\":1}}],[\"隔绝和绽放的反差都代表着一种强大精神力量的回归\",{\"1\":{\"53\":1}}],[\"外部与内部\",{\"1\":{\"53\":1}}],[\"犹如绽放江面的灿烂之花\",{\"1\":{\"53\":1}}],[\"晚上\",{\"1\":{\"53\":1}}],[\"墙内是属于孩子们自由的岛屿\",{\"1\":{\"53\":1}}],[\"江边的大人看到空白围墙的阻挡\",{\"1\":{\"53\":1}}],[\"高管和商业战略家之间的混合体\",{\"1\":{\"525\":1}}],[\"高效\",{\"1\":{\"522\":1}}],[\"高效的结构优点应该是容纳成长\",{\"1\":{\"521\":1}}],[\"高层的塑料城的演化\",{\"1\":{\"58\":1}}],[\"高度规划的城市花园都是成人世界所赋予的\",{\"1\":{\"52\":1}}],[\"高级空间设计师\",{\"1\":{\"6\":1}}],[\"密集高耸的天际线中\",{\"1\":{\"52\":1}}],[\"没有人会对那些80\",{\"1\":{\"538\":1}}],[\"没有做好\",{\"1\":{\"534\":1}}],[\"没有什么用处\",{\"1\":{\"512\":1}}],[\"没有明确说明使用年龄\",{\"1\":{\"81\":1}}],[\"没有大人的看管\",{\"1\":{\"74\":1,\"530\":1}}],[\"没有地下隧道\",{\"1\":{\"52\":1}}],[\"没有一个岛是你们想去探索的吗\",{\"1\":{\"46\":1}}],[\"海心沙码头与广州塔码头可通过船来过渡\",{\"1\":{\"51\":1,\"52\":1}}],[\"海心沙与广州塔之间的水域多被作为珠江夜游经过的路线\",{\"1\":{\"51\":1,\"52\":1}}],[\"珠江新城就是城市的cbd\",{\"1\":{\"51\":1}}],[\"繁华的广州都市里\",{\"1\":{\"51\":1}}],[\"摩天大楼城市花园之下\",{\"1\":{\"50\":1}}],[\"试着可以寻求一个精神复生的乌托邦\",{\"1\":{\"50\":1}}],[\"试发了几个作品\",{\"1\":{\"20\":1}}],[\"精神却仍被延续\",{\"1\":{\"50\":1}}],[\"然而却被工程化的市场牵引\",{\"1\":{\"57\":1}}],[\"然而\",{\"1\":{\"50\":1,\"81\":1,\"512\":1,\"519\":2,\"524\":1,\"527\":1}}],[\"写了关于垃圾游乐场的文章\",{\"1\":{\"50\":1}}],[\"优化主题行\",{\"1\":{\"539\":1}}],[\"优化主题行和发送时间\",{\"1\":{\"539\":1}}],[\"优化邮件内容\",{\"1\":{\"539\":1}}],[\"优化用户体验和转化率\",{\"1\":{\"527\":1}}],[\"优化用户体验\",{\"1\":{\"527\":1}}],[\"优化特性功能的组合方式\",{\"1\":{\"521\":1}}],[\"优质转化\",{\"1\":{\"521\":1}}],[\"优先级也不同\",{\"1\":{\"50\":1}}],[\"优秀案例\",{\"1\":{\"7\":1}}],[\"儿童认为\",{\"1\":{\"81\":1}}],[\"儿童可以利用形状来识别和描述物体\",{\"1\":{\"81\":1}}],[\"儿童可以增强他们的空间感知\",{\"1\":{\"81\":1}}],[\"儿童和青少年的认知发展\",{\"1\":{\"79\":1}}],[\"儿童教育的多样化发展\",{\"1\":{\"78\":1}}],[\"儿童教育也需要面向时代未来\",{\"1\":{\"77\":1}}],[\"儿童机器人设计等课程\",{\"1\":{\"77\":1}}],[\"儿童在公共领域不合适\",{\"1\":{\"50\":1}}],[\"儿童友好城市\",{\"1\":{\"43\":5}}],[\"回应假设\",{\"1\":{\"500\":1}}],[\"回归问题本身\",{\"1\":{\"509\":1}}],[\"回归\",{\"0\":{\"72\":1}}],[\"回归到当下的场地\",{\"1\":{\"53\":1}}],[\"回归的心声和重生的设想\",{\"0\":{\"50\":1}}],[\"回望身后丝毫没有容身之地的城市\",{\"1\":{\"46\":1}}],[\"回望管道道中的东西竟然是刚刚从船中卸下的垃圾\",{\"1\":{\"46\":1}}],[\"今年刚好准备重新回归校园入读硕士\",{\"1\":{\"530\":1}}],[\"今年还新开了一个整合设计\",{\"1\":{\"510\":1}}],[\"今年我也非常荣幸拿到金匠拓展实践的无条件offer\",{\"1\":{\"509\":1}}],[\"今天的灵感一分钟就到这里了\",{\"1\":{\"511\":1}}],[\"今天通过摄影和游戏来寻找灵感\",{\"1\":{\"511\":1}}],[\"今天先从我个人角度聊聊留学申请这一年\",{\"1\":{\"506\":1}}],[\"今天\",{\"1\":{\"49\":1}}],[\"今晚就要开始了\",{\"1\":{\"46\":1}}],[\"个性化内容以及根据收件人的兴趣和行为进行细分\",{\"1\":{\"539\":1}}],[\"个性化\",{\"1\":{\"539\":2}}],[\"个性化营销\",{\"1\":{\"531\":1}}],[\"个性化推荐算法\",{\"1\":{\"528\":1}}],[\"个人对自己创新能力和创造力的信念程度\",{\"1\":{\"524\":1}}],[\"个人作品\",{\"1\":{\"9\":2}}],[\"个适合个人和专业用途的\",{\"1\":{\"505\":1}}],[\"个\",{\"1\":{\"49\":1}}],[\"个冒险乐园\",{\"1\":{\"49\":1}}],[\"近来读到一文分析探讨大脑的决策系统\",{\"1\":{\"532\":1}}],[\"近代最有名的儿童心理学家\",{\"1\":{\"79\":1}}],[\"近\",{\"1\":{\"49\":1}}],[\"世纪\",{\"1\":{\"49\":1}}],[\"世界拼图最全区域|湾区区块链\",{\"1\":{\"42\":1}}],[\"世界中还有许多未知的答案等待着采花者去追寻和探索\",{\"1\":{\"24\":1}}],[\"世界被垃圾堆满\",{\"1\":{\"23\":1}}],[\"取而代之的是带有固定设备的课后俱乐部\",{\"1\":{\"49\":1}}],[\"卡尼曼和阿莫斯\",{\"1\":{\"532\":1}}],[\"卡姆登委员会拆除了它的冒险游乐场\",{\"1\":{\"49\":1}}],[\"卡尔\",{\"1\":{\"48\":1}}],[\"骨折比精神破碎更好\",{\"1\":{\"49\":1}}],[\"艾伦夫人\",{\"1\":{\"49\":1}}],[\"艾伦夫人觉得垃圾游乐场值得一个更\",{\"1\":{\"48\":1}}],[\"破坏和犯罪\",{\"1\":{\"49\":1}}],[\"破坏和犯罪之间的区别并不明显\",{\"1\":{\"49\":1}}],[\"诺丁山冒险游乐场\",{\"1\":{\"48\":1}}],[\"讨论并同意垃圾游乐场的新名词\",{\"1\":{\"48\":1}}],[\"赫特伍德的艾伦夫人和乔治佩普勒爵士在午餐时间会面\",{\"1\":{\"48\":1}}],[\"赫特伍德的艾伦夫人从英格兰访问了emdrup\",{\"1\":{\"48\":1}}],[\"英国有\",{\"1\":{\"49\":1}}],[\"英国最初的冒险游乐场被称为\",{\"1\":{\"48\":1}}],[\"英国伦敦飞跃乌托邦艺术展\",{\"1\":{\"29\":1}}],[\"或更精细的类词词典\",{\"1\":{\"521\":1}}],[\"或者两种\",{\"1\":{\"533\":1}}],[\"或者利用新技术发展例如\",{\"1\":{\"531\":1}}],[\"或者找到颠覆现有市场格局的新机会\",{\"1\":{\"525\":1}}],[\"或者在训练模拟环境中提供音效指导\",{\"1\":{\"517\":1}}],[\"或者在观看电影时听到环绕音效\",{\"1\":{\"514\":1}}],[\"或者热爱探索城市的人\",{\"1\":{\"511\":1}}],[\"或者应该说复合型人才\",{\"1\":{\"510\":1}}],[\"或者就是很结构化\",{\"1\":{\"509\":1}}],[\"或凝固成形或柔软成片\",{\"1\":{\"69\":1}}],[\"或\",{\"1\":{\"48\":2}}],[\"或许该思考如何应对ai带来的市场变化\",{\"1\":{\"518\":1}}],[\"或许是时代所变让他难以生存\",{\"1\":{\"74\":1}}],[\"或许在某个时刻认出父母地半张面孔\",{\"1\":{\"73\":1}}],[\"或许也只有孩子们知道\",{\"1\":{\"46\":1}}],[\"或许对于还在废都里的大人也是\",{\"1\":{\"46\":1}}],[\"或许当有一天不再需要和用户说明区块链是什么\",{\"1\":{\"42\":1}}],[\"jhuma\",{\"1\":{\"460\":1}}],[\"jäntti\",{\"1\":{\"239\":1}}],[\"jifan\",{\"1\":{\"480\":1}}],[\"jihyun\",{\"1\":{\"391\":1}}],[\"jiye\",{\"1\":{\"356\":1}}],[\"jibin\",{\"1\":{\"283\":1}}],[\"ji\",{\"1\":{\"236\":1,\"428\":1}}],[\"jieping\",{\"1\":{\"378\":1}}],[\"jie\",{\"1\":{\"224\":1,\"271\":1,\"316\":1,\"377\":1,\"378\":1,\"457\":1,\"480\":1,\"496\":1}}],[\"jilani\",{\"1\":{\"197\":1}}],[\"jiaxuan\",{\"1\":{\"462\":1}}],[\"jiaxiang\",{\"1\":{\"361\":1}}],[\"jiaxin\",{\"1\":{\"323\":1}}],[\"jiao\",{\"1\":{\"443\":1}}],[\"jiahui\",{\"1\":{\"483\":1}}],[\"jiaheng\",{\"1\":{\"377\":1}}],[\"jiahao\",{\"1\":{\"218\":1}}],[\"jiawei\",{\"1\":{\"324\":1}}],[\"jialuo\",{\"1\":{\"447\":1}}],[\"jialou\",{\"1\":{\"430\":1}}],[\"jialin\",{\"1\":{\"317\":1}}],[\"jiale\",{\"1\":{\"174\":1,\"218\":1,\"371\":1}}],[\"jiajie\",{\"1\":{\"316\":1}}],[\"jiajing\",{\"1\":{\"138\":1}}],[\"jiasheng\",{\"1\":{\"306\":1}}],[\"jia\",{\"1\":{\"223\":1,\"246\":1,\"259\":1,\"271\":1,\"286\":1,\"329\":1,\"411\":1,\"438\":1,\"457\":1,\"474\":1}}],[\"jianfeng\",{\"1\":{\"491\":1}}],[\"jianyi\",{\"1\":{\"360\":1}}],[\"jianyu\",{\"1\":{\"223\":1}}],[\"jian\",{\"1\":{\"214\":1,\"393\":1}}],[\"jiangfei\",{\"1\":{\"414\":1}}],[\"jiangfan\",{\"1\":{\"257\":1}}],[\"jiangtao\",{\"1\":{\"173\":1}}],[\"jiang\",{\"1\":{\"133\":1,\"331\":1,\"385\":1,\"404\":1,\"410\":1,\"425\":1,\"453\":1}}],[\"jiayuan\",{\"1\":{\"453\":1}}],[\"jiayun\",{\"1\":{\"322\":1}}],[\"jiayang\",{\"1\":{\"174\":1,\"218\":1,\"371\":1}}],[\"jiaying\",{\"1\":{\"300\":1}}],[\"jiayi\",{\"1\":{\"141\":1,\"173\":1}}],[\"jiacheng\",{\"1\":{\"141\":1}}],[\"jiafei\",{\"1\":{\"140\":1}}],[\"jinpz\",{\"1\":{\"494\":1}}],[\"jinchang\",{\"1\":{\"480\":1}}],[\"jinwei\",{\"1\":{\"462\":1}}],[\"jinwook\",{\"1\":{\"143\":1,\"191\":1}}],[\"jingzhe\",{\"1\":{\"447\":1}}],[\"jingyu\",{\"1\":{\"385\":1}}],[\"jingbo\",{\"1\":{\"322\":1,\"455\":1,\"470\":1}}],[\"jing\",{\"1\":{\"270\":1,\"480\":1}}],[\"jinbin\",{\"1\":{\"213\":1}}],[\"jinmook\",{\"1\":{\"210\":1}}],[\"jinsil\",{\"1\":{\"156\":1,\"266\":1}}],[\"jin\",{\"1\":{\"128\":1,\"494\":1}}],[\"jinkyung\",{\"1\":{\"97\":1,\"211\":1,\"312\":1}}],[\"jahan\",{\"1\":{\"460\":1}}],[\"jaggi\",{\"1\":{\"456\":1}}],[\"jayawardena\",{\"1\":{\"384\":1}}],[\"jaehoon\",{\"1\":{\"389\":1}}],[\"jaeheon\",{\"1\":{\"274\":1}}],[\"jaewook\",{\"1\":{\"105\":1}}],[\"jacopo\",{\"1\":{\"390\":1,\"466\":1}}],[\"jacobson\",{\"1\":{\"268\":1,\"452\":1}}],[\"jack\",{\"1\":{\"325\":1}}],[\"jacqueline\",{\"1\":{\"178\":1}}],[\"jasper和copy\",{\"1\":{\"539\":1}}],[\"jasper等提供了多种功能和价格选项\",{\"1\":{\"539\":1}}],[\"jasper\",{\"1\":{\"539\":1}}],[\"jascha\",{\"1\":{\"389\":1}}],[\"jason\",{\"1\":{\"268\":1,\"452\":1}}],[\"jasmeet\",{\"1\":{\"176\":1}}],[\"javadpour\",{\"1\":{\"304\":1}}],[\"javascript\",{\"1\":{\"231\":1,\"287\":1}}],[\"javier\",{\"1\":{\"126\":1}}],[\"james\",{\"1\":{\"221\":1,\"236\":1,\"308\":1,\"428\":1,\"456\":1}}],[\"jamshid\",{\"1\":{\"169\":1}}],[\"jaan\",{\"1\":{\"216\":1}}],[\"january\",{\"1\":{\"428\":1}}],[\"janice\",{\"1\":{\"391\":1}}],[\"janeiro\",{\"1\":{\"282\":1}}],[\"jansen\",{\"1\":{\"217\":1}}],[\"jan\",{\"1\":{\"203\":1,\"537\":1}}],[\"jang\",{\"1\":{\"191\":1,\"212\":1,\"290\":1,\"325\":1}}],[\"jangra\",{\"1\":{\"169\":1}}],[\"janavi\",{\"1\":{\"184\":1,\"380\":1}}],[\"jared\",{\"1\":{\"175\":1}}],[\"jatin\",{\"1\":{\"172\":1,\"370\":1}}],[\"jatowt\",{\"1\":{\"169\":1}}],[\"jail\",{\"1\":{\"344\":1}}],[\"jailbreaks\",{\"1\":{\"418\":1}}],[\"jailbreak\",{\"0\":{\"418\":1},\"1\":{\"313\":1,\"374\":1,\"403\":1,\"409\":1,\"418\":5}}],[\"jailbreaking\",{\"0\":{\"313\":1,\"409\":1},\"1\":{\"313\":1,\"374\":1,\"409\":3}}],[\"jain\",{\"1\":{\"156\":1}}],[\"jaime\",{\"1\":{\"126\":1}}],[\"jauregui\",{\"1\":{\"147\":1}}],[\"j\",{\"1\":{\"114\":2,\"199\":1,\"233\":1,\"244\":1,\"258\":1,\"261\":1,\"282\":1,\"398\":2,\"433\":1}}],[\"jovan\",{\"1\":{\"465\":1}}],[\"joty\",{\"1\":{\"443\":1}}],[\"joachimiak\",{\"1\":{\"398\":1}}],[\"joanna\",{\"1\":{\"268\":1,\"452\":1}}],[\"joon\",{\"1\":{\"356\":1}}],[\"joel\",{\"1\":{\"323\":1}}],[\"journals\",{\"1\":{\"428\":1}}],[\"journal\",{\"1\":{\"268\":1,\"452\":1}}],[\"journaling\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"268\":3,\"452\":3}}],[\"journey\",{\"1\":{\"266\":1}}],[\"job\",{\"1\":{\"263\":1}}],[\"join\",{\"1\":{\"243\":1}}],[\"jointly\",{\"1\":{\"366\":1,\"390\":1}}],[\"joint\",{\"0\":{\"103\":1,\"246\":1},\"1\":{\"103\":2,\"224\":1,\"246\":2,\"252\":1,\"426\":1}}],[\"johana\",{\"1\":{\"315\":1}}],[\"johansson\",{\"1\":{\"302\":1}}],[\"johanne\",{\"1\":{\"296\":1}}],[\"johann\",{\"1\":{\"233\":1}}],[\"john\",{\"1\":{\"48\":1,\"150\":1,\"154\":1,\"156\":1,\"166\":2,\"180\":1,\"228\":1,\"396\":1,\"533\":1}}],[\"jochen\",{\"1\":{\"210\":1}}],[\"jorge\",{\"1\":{\"194\":1,\"272\":1}}],[\"joão\",{\"1\":{\"194\":1}}],[\"josé\",{\"1\":{\"282\":1}}],[\"joshi\",{\"1\":{\"209\":1,\"308\":1}}],[\"joshua\",{\"1\":{\"147\":1,\"176\":1}}],[\"josep\",{\"1\":{\"465\":1}}],[\"joseph\",{\"1\":{\"207\":1,\"326\":1,\"431\":1,\"454\":1}}],[\"jose\",{\"1\":{\"315\":1}}],[\"josef\",{\"1\":{\"253\":1}}],[\"joseba\",{\"1\":{\"186\":1}}],[\"jonathan\",{\"1\":{\"194\":1,\"287\":1,\"294\":1,\"302\":1,\"304\":1}}],[\"jonas\",{\"1\":{\"146\":1,\"441\":1}}],[\"jon\",{\"1\":{\"105\":1}}],[\"julka\",{\"1\":{\"408\":1}}],[\"julius\",{\"1\":{\"216\":1}}],[\"julian\",{\"1\":{\"126\":1}}],[\"jui\",{\"1\":{\"376\":1}}],[\"jurisdictional\",{\"1\":{\"254\":1}}],[\"jurafsky\",{\"1\":{\"241\":1}}],[\"judy\",{\"1\":{\"302\":1}}],[\"judith\",{\"1\":{\"253\":1}}],[\"judge\",{\"1\":{\"342\":1,\"409\":1}}],[\"judging\",{\"1\":{\"251\":1,\"423\":1}}],[\"judgment\",{\"1\":{\"202\":1}}],[\"judgments\",{\"1\":{\"197\":1,\"432\":1,\"481\":1}}],[\"jupyter\",{\"1\":{\"231\":2}}],[\"jupyterlab\",{\"0\":{\"231\":1},\"1\":{\"110\":1,\"231\":1,\"321\":1}}],[\"jump\",{\"1\":{\"222\":1}}],[\"ju\",{\"1\":{\"198\":1,\"272\":1,\"387\":1}}],[\"juntao\",{\"1\":{\"485\":1}}],[\"junda\",{\"1\":{\"364\":1}}],[\"junpeng\",{\"1\":{\"345\":1}}],[\"junyu\",{\"1\":{\"242\":1}}],[\"junjie\",{\"1\":{\"219\":1,\"360\":1,\"401\":1}}],[\"jungcheol\",{\"1\":{\"274\":1}}],[\"jung\",{\"1\":{\"143\":1,\"356\":1}}],[\"junichi\",{\"1\":{\"142\":1}}],[\"jun\",{\"1\":{\"140\":1,\"219\":1,\"271\":1,\"286\":1,\"356\":1,\"401\":1,\"438\":1,\"450\":1,\"457\":1,\"474\":1,\"479\":1}}],[\"junkology\",{\"1\":{\"48\":1}}],[\"juanzi\",{\"1\":{\"480\":1}}],[\"juan\",{\"1\":{\"126\":1}}],[\"justine\",{\"1\":{\"244\":1}}],[\"justice\",{\"0\":{\"147\":1},\"1\":{\"147\":1,\"148\":1}}],[\"justa\",{\"1\":{\"192\":1}}],[\"just\",{\"0\":{\"395\":1},\"1\":{\"100\":1,\"192\":1,\"220\":1,\"236\":1,\"239\":1,\"302\":1,\"368\":1,\"460\":1}}],[\"jeopardizes\",{\"1\":{\"443\":1}}],[\"jeongmi\",{\"1\":{\"191\":1}}],[\"jeon\",{\"1\":{\"143\":1,\"274\":1}}],[\"jeremy\",{\"1\":{\"268\":1,\"452\":1}}],[\"jerry\",{\"1\":{\"207\":1}}],[\"jesudara\",{\"1\":{\"246\":1}}],[\"jessie\",{\"1\":{\"184\":1,\"380\":1}}],[\"jessica\",{\"1\":{\"131\":1}}],[\"jens\",{\"1\":{\"238\":1,\"379\":1}}],[\"jennifer\",{\"1\":{\"132\":1,\"330\":1}}],[\"jen\",{\"1\":{\"111\":1}}],[\"jeffrey\",{\"1\":{\"110\":1,\"151\":1,\"210\":1,\"269\":1,\"321\":1,\"348\":1,\"389\":1}}],[\"jean\",{\"1\":{\"79\":1,\"300\":1}}],[\"jeweler\",{\"1\":{\"42\":1}}],[\"游乐场的回归抗议问题的现象存在\",{\"1\":{\"74\":1}}],[\"游乐场这个词\",{\"1\":{\"48\":1}}],[\"游戏等娱乐领域也正在探索audio\",{\"1\":{\"517\":1}}],[\"游戏\",{\"1\":{\"508\":1}}],[\"游戏的商业思考\",{\"0\":{\"520\":1},\"1\":{\"502\":1}}],[\"游戏规则容易被打破\",{\"1\":{\"89\":1}}],[\"游戏及游乐场百科全书\",{\"1\":{\"74\":1}}],[\"游戏史\",{\"1\":{\"74\":1}}],[\"游戏工作者sherriff将自己的生命奉献给了垃圾游戏的哲学\",{\"1\":{\"50\":1}}],[\"游戏世界帮助用户体验虚拟世界\",{\"1\":{\"39\":1}}],[\"游戏融为一体\",{\"1\":{\"33\":1}}],[\"游戏引擎\",{\"1\":{\"8\":1}}],[\"游戏设计师中村育美通过名为\",{\"1\":{\"511\":1}}],[\"游戏设计\",{\"1\":{\"4\":1}}],[\"建立全球设计教育联盟\",{\"1\":{\"524\":1}}],[\"建立好成功标准\",{\"1\":{\"521\":1}}],[\"建造新的塑料楼梯等等\",{\"1\":{\"69\":1}}],[\"建筑设计\",{\"1\":{\"81\":1}}],[\"建筑材料\",{\"1\":{\"57\":1}}],[\"建筑材料藏着故事的沉淀\",{\"1\":{\"57\":1}}],[\"建筑\",{\"1\":{\"48\":1}}],[\"建模软件\",{\"1\":{\"8\":1}}],[\"l40\",{\"1\":{\"427\":1}}],[\"lyu\",{\"1\":{\"417\":1}}],[\"ly\",{\"1\":{\"398\":1}}],[\"lympouridis\",{\"1\":{\"117\":1}}],[\"lcd\",{\"1\":{\"298\":1}}],[\"løvlie\",{\"1\":{\"296\":1}}],[\"lstm\",{\"1\":{\"246\":1}}],[\"lma\",{\"1\":{\"364\":3}}],[\"lm\",{\"1\":{\"175\":1,\"392\":1,\"455\":2}}],[\"lms\",{\"1\":{\"175\":7,\"381\":4,\"455\":1}}],[\"l\",{\"1\":{\"147\":1,\"176\":1,\"251\":1,\"337\":1,\"392\":1,\"398\":1,\"423\":1,\"456\":1,\"468\":1,\"488\":1}}],[\"llava\",{\"1\":{\"319\":1}}],[\"llama\",{\"1\":{\"313\":1,\"334\":1,\"340\":1,\"365\":1,\"369\":2,\"374\":1,\"391\":1,\"409\":1,\"418\":1,\"427\":1,\"447\":1,\"461\":1,\"481\":1,\"492\":1,\"497\":1}}],[\"llama2\",{\"1\":{\"258\":1,\"335\":2,\"394\":1,\"433\":1,\"456\":1,\"463\":1,\"466\":2,\"492\":1}}],[\"llach\",{\"1\":{\"300\":1}}],[\"ll\",{\"0\":{\"122\":1}}],[\"llm\",{\"0\":{\"110\":1,\"132\":1,\"161\":1,\"172\":1,\"174\":1,\"198\":1,\"219\":1,\"258\":2,\"264\":1,\"268\":1,\"271\":1,\"286\":2,\"310\":1,\"315\":1,\"318\":1,\"321\":1,\"325\":1,\"326\":1,\"330\":1,\"333\":1,\"335\":1,\"338\":1,\"343\":1,\"344\":1,\"347\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"360\":1,\"361\":1,\"364\":1,\"365\":1,\"366\":1,\"370\":1,\"371\":1,\"374\":1,\"376\":1,\"377\":1,\"382\":1,\"386\":1,\"387\":1,\"388\":1,\"391\":1,\"392\":1,\"398\":1,\"399\":1,\"401\":1,\"402\":1,\"403\":1,\"410\":1,\"411\":1,\"413\":1,\"414\":1,\"415\":1,\"418\":1,\"422\":1,\"427\":1,\"429\":1,\"433\":2,\"437\":1,\"439\":1,\"441\":1,\"442\":1,\"444\":1,\"445\":1,\"448\":1,\"452\":1,\"455\":1,\"457\":1,\"462\":1,\"465\":1,\"474\":2,\"477\":1,\"478\":1,\"479\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"491\":1,\"494\":1,\"495\":1},\"1\":{\"104\":1,\"110\":2,\"114\":2,\"124\":1,\"132\":3,\"133\":3,\"138\":1,\"161\":3,\"172\":3,\"174\":3,\"198\":1,\"201\":1,\"215\":2,\"219\":1,\"228\":2,\"242\":2,\"245\":2,\"251\":1,\"252\":8,\"253\":1,\"258\":6,\"271\":4,\"275\":3,\"286\":2,\"291\":2,\"308\":1,\"314\":1,\"318\":1,\"321\":2,\"323\":1,\"324\":1,\"326\":5,\"330\":3,\"331\":3,\"333\":1,\"335\":1,\"337\":1,\"338\":2,\"341\":1,\"342\":1,\"343\":1,\"344\":6,\"347\":2,\"350\":2,\"353\":2,\"354\":3,\"355\":2,\"356\":3,\"360\":1,\"361\":1,\"364\":1,\"365\":2,\"366\":1,\"368\":1,\"369\":1,\"370\":3,\"371\":3,\"374\":3,\"376\":1,\"377\":8,\"382\":1,\"384\":4,\"387\":1,\"391\":4,\"392\":2,\"393\":2,\"401\":1,\"402\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":2,\"410\":6,\"412\":1,\"414\":1,\"415\":1,\"416\":2,\"419\":3,\"420\":3,\"422\":3,\"423\":1,\"425\":2,\"426\":8,\"428\":4,\"432\":1,\"433\":6,\"434\":4,\"437\":2,\"439\":2,\"441\":1,\"442\":2,\"445\":1,\"446\":1,\"447\":2,\"450\":1,\"453\":3,\"455\":3,\"456\":1,\"457\":4,\"458\":3,\"459\":1,\"460\":2,\"462\":1,\"465\":3,\"466\":1,\"467\":2,\"468\":1,\"474\":2,\"475\":1,\"476\":2,\"477\":1,\"478\":2,\"479\":3,\"480\":1,\"482\":2,\"485\":1,\"486\":1,\"487\":1,\"489\":1,\"490\":1,\"491\":2,\"492\":1,\"494\":2,\"495\":2}}],[\"llmsense\",{\"0\":{\"475\":1},\"1\":{\"475\":1}}],[\"llmsanitize\",{\"0\":{\"443\":1},\"1\":{\"443\":2}}],[\"llms\",{\"0\":{\"97\":1,\"133\":1,\"151\":1,\"161\":1,\"184\":1,\"251\":1,\"252\":1,\"291\":1,\"312\":1,\"313\":1,\"314\":1,\"316\":1,\"317\":1,\"319\":1,\"322\":1,\"323\":1,\"324\":1,\"327\":1,\"328\":1,\"329\":1,\"331\":1,\"334\":1,\"336\":1,\"337\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"345\":1,\"348\":1,\"354\":1,\"358\":1,\"359\":1,\"362\":1,\"363\":1,\"368\":1,\"369\":1,\"372\":1,\"375\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"384\":1,\"385\":1,\"389\":1,\"390\":1,\"391\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"400\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"412\":1,\"416\":1,\"417\":1,\"419\":1,\"420\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"428\":1,\"430\":1,\"431\":1,\"432\":1,\"434\":1,\"435\":2,\"436\":1,\"438\":1,\"443\":1,\"446\":1,\"447\":1,\"449\":1,\"450\":1,\"453\":1,\"454\":1,\"456\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":2,\"471\":1,\"472\":2,\"475\":1,\"476\":1,\"480\":1,\"481\":1,\"488\":1,\"489\":1,\"490\":2,\"492\":1,\"496\":1,\"497\":1,\"498\":1},\"1\":{\"97\":6,\"110\":4,\"114\":1,\"130\":3,\"132\":2,\"133\":6,\"136\":5,\"138\":3,\"151\":1,\"154\":2,\"161\":3,\"172\":6,\"184\":4,\"198\":1,\"215\":6,\"228\":1,\"242\":3,\"245\":5,\"251\":4,\"252\":2,\"258\":1,\"261\":5,\"264\":1,\"268\":3,\"275\":1,\"283\":3,\"286\":1,\"291\":1,\"308\":11,\"312\":6,\"313\":6,\"314\":2,\"315\":3,\"316\":2,\"317\":5,\"318\":1,\"319\":6,\"321\":4,\"323\":3,\"324\":3,\"325\":2,\"326\":3,\"328\":7,\"330\":2,\"331\":6,\"333\":5,\"334\":8,\"335\":1,\"336\":1,\"337\":5,\"338\":1,\"339\":2,\"340\":6,\"341\":1,\"342\":4,\"343\":1,\"344\":2,\"348\":1,\"350\":2,\"353\":2,\"354\":3,\"359\":4,\"360\":3,\"362\":4,\"363\":3,\"364\":1,\"365\":2,\"366\":1,\"368\":2,\"369\":2,\"370\":6,\"372\":3,\"374\":4,\"375\":1,\"377\":2,\"378\":1,\"379\":7,\"380\":4,\"381\":5,\"382\":3,\"385\":3,\"387\":1,\"388\":1,\"389\":3,\"390\":5,\"391\":7,\"392\":1,\"394\":6,\"396\":1,\"398\":2,\"404\":3,\"405\":2,\"408\":1,\"409\":1,\"410\":3,\"411\":1,\"412\":7,\"413\":9,\"414\":5,\"417\":7,\"418\":2,\"419\":2,\"420\":2,\"422\":1,\"423\":4,\"424\":3,\"425\":3,\"426\":2,\"427\":3,\"428\":3,\"429\":2,\"431\":4,\"432\":2,\"433\":1,\"434\":1,\"435\":2,\"436\":1,\"437\":8,\"438\":3,\"439\":6,\"441\":2,\"443\":5,\"445\":1,\"447\":3,\"448\":1,\"449\":2,\"452\":3,\"453\":6,\"455\":1,\"456\":2,\"458\":5,\"459\":2,\"461\":8,\"462\":1,\"463\":2,\"465\":2,\"466\":4,\"467\":3,\"468\":1,\"469\":1,\"470\":5,\"471\":5,\"472\":5,\"474\":1,\"475\":3,\"476\":1,\"478\":1,\"479\":8,\"480\":2,\"481\":2,\"482\":7,\"483\":2,\"485\":2,\"486\":1,\"487\":1,\"489\":2,\"490\":4,\"491\":1,\"492\":3,\"494\":1,\"495\":4,\"496\":5,\"498\":3}}],[\"luq\",{\"0\":{\"467\":1},\"1\":{\"467\":4}}],[\"lunch\",{\"0\":{\"463\":1}}],[\"luxury\",{\"1\":{\"399\":1}}],[\"lukas\",{\"1\":{\"444\":1}}],[\"lukes\",{\"1\":{\"249\":1}}],[\"lukoff\",{\"1\":{\"147\":1}}],[\"luc\",{\"1\":{\"399\":1}}],[\"luca\",{\"1\":{\"292\":1,\"338\":1}}],[\"lucas\",{\"1\":{\"104\":1,\"114\":1}}],[\"lucie\",{\"1\":{\"336\":1}}],[\"lucio\",{\"1\":{\"292\":1}}],[\"lucia\",{\"1\":{\"241\":1}}],[\"lua\",{\"1\":{\"212\":1}}],[\"lu\",{\"1\":{\"173\":1,\"242\":1,\"324\":1,\"391\":1,\"414\":1,\"489\":1,\"491\":1}}],[\"lupfer\",{\"1\":{\"156\":1}}],[\"luise\",{\"1\":{\"187\":1}}],[\"luis\",{\"1\":{\"148\":1}}],[\"lueck\",{\"1\":{\"147\":1}}],[\"luen\",{\"1\":{\"113\":1,\"115\":1}}],[\"luoxuan\",{\"1\":{\"242\":1}}],[\"luo\",{\"1\":{\"141\":1,\"316\":1,\"342\":1,\"353\":1,\"377\":1,\"483\":1}}],[\"luísa\",{\"1\":{\"96\":1}}],[\"lrp\",{\"1\":{\"102\":1}}],[\"lal\",{\"1\":{\"434\":1}}],[\"lasal\",{\"1\":{\"384\":1}}],[\"lastly\",{\"1\":{\"336\":1}}],[\"last\",{\"1\":{\"153\":1,\"169\":1,\"179\":1,\"182\":1,\"243\":1}}],[\"lawrence\",{\"1\":{\"338\":1}}],[\"lags\",{\"1\":{\"333\":1}}],[\"lavelli\",{\"1\":{\"315\":1}}],[\"la\",{\"1\":{\"315\":1}}],[\"laouar\",{\"1\":{\"288\":1}}],[\"ladder\",{\"1\":{\"277\":2}}],[\"lama\",{\"1\":{\"369\":1}}],[\"lam\",{\"1\":{\"257\":1,\"345\":1}}],[\"lambert\",{\"1\":{\"200\":1}}],[\"lacey\",{\"1\":{\"244\":1}}],[\"lacked\",{\"1\":{\"333\":1}}],[\"lacking\",{\"1\":{\"183\":1,\"324\":1}}],[\"lack\",{\"0\":{\"133\":1,\"331\":1},\"1\":{\"101\":1,\"121\":1,\"125\":1,\"133\":1,\"158\":1,\"166\":1,\"175\":1,\"206\":1,\"212\":1,\"219\":1,\"266\":1,\"286\":2,\"318\":1,\"322\":1,\"328\":1,\"331\":1,\"345\":1,\"353\":1,\"391\":1,\"401\":1,\"410\":1,\"428\":1,\"443\":1,\"447\":1,\"467\":1,\"474\":2}}],[\"laak\",{\"1\":{\"216\":1}}],[\"laboni\",{\"1\":{\"422\":1}}],[\"laboriously\",{\"1\":{\"375\":1}}],[\"labor\",{\"1\":{\"293\":1,\"300\":1,\"486\":1}}],[\"laboratories\",{\"1\":{\"252\":1,\"426\":1}}],[\"laboratory\",{\"1\":{\"236\":1}}],[\"labs\",{\"1\":{\"334\":1,\"486\":1}}],[\"lab\",{\"0\":{\"486\":1},\"1\":{\"203\":1,\"388\":1,\"486\":1}}],[\"labellers\",{\"1\":{\"388\":1}}],[\"label\",{\"0\":{\"317\":1},\"1\":{\"238\":1,\"317\":3,\"378\":1,\"412\":3,\"435\":1,\"439\":1,\"444\":2,\"455\":2,\"459\":1,\"460\":3}}],[\"labeled\",{\"1\":{\"137\":2,\"161\":1,\"354\":1,\"442\":1}}],[\"labels\",{\"1\":{\"99\":1,\"294\":2,\"378\":1,\"412\":1,\"442\":1,\"488\":1}}],[\"labeling\",{\"1\":{\"97\":2,\"137\":2,\"312\":2,\"408\":1}}],[\"lauro\",{\"1\":{\"533\":1}}],[\"laura\",{\"1\":{\"189\":1,\"249\":1,\"436\":1}}],[\"lauscher\",{\"1\":{\"395\":1}}],[\"launched\",{\"1\":{\"179\":1,\"196\":1}}],[\"lakis\",{\"1\":{\"176\":1}}],[\"lakkaraju2022rethinking\",{\"1\":{\"159\":1}}],[\"latif\",{\"1\":{\"486\":1}}],[\"latter\",{\"1\":{\"160\":1,\"196\":1,\"382\":1}}],[\"late\",{\"1\":{\"268\":1,\"347\":1,\"452\":1}}],[\"later\",{\"1\":{\"256\":1,\"395\":1}}],[\"lateral\",{\"0\":{\"404\":1},\"1\":{\"186\":2,\"404\":1}}],[\"latency\",{\"1\":{\"210\":1,\"389\":1,\"427\":1,\"436\":1,\"462\":1,\"465\":1,\"469\":1}}],[\"latent\",{\"0\":{\"256\":1,\"289\":1,\"390\":1},\"1\":{\"111\":1,\"213\":1,\"234\":2,\"235\":2,\"260\":3,\"270\":1,\"289\":2,\"390\":5}}],[\"latest\",{\"1\":{\"74\":1,\"124\":1,\"213\":1,\"243\":1,\"313\":1,\"337\":1}}],[\"layton\",{\"1\":{\"396\":1}}],[\"layout\",{\"1\":{\"375\":1}}],[\"lays\",{\"1\":{\"159\":1,\"356\":1,\"428\":1}}],[\"layers\",{\"1\":{\"178\":1,\"199\":1,\"255\":1,\"366\":2,\"390\":1,\"398\":1,\"424\":1,\"439\":1,\"479\":2}}],[\"layer\",{\"0\":{\"366\":1},\"1\":{\"146\":1,\"255\":1,\"366\":3}}],[\"layered\",{\"1\":{\"139\":1}}],[\"larson\",{\"1\":{\"102\":1}}],[\"larger\",{\"0\":{\"441\":1},\"1\":{\"335\":1,\"382\":2,\"441\":3}}],[\"largest\",{\"1\":{\"250\":1,\"315\":1,\"428\":1,\"463\":1}}],[\"largely\",{\"1\":{\"142\":1,\"167\":1,\"272\":1,\"394\":1,\"412\":1}}],[\"large\",{\"0\":{\"97\":1,\"102\":1,\"114\":1,\"136\":1,\"138\":1,\"180\":1,\"201\":1,\"215\":1,\"242\":1,\"245\":1,\"250\":1,\"251\":1,\"261\":1,\"272\":1,\"308\":1,\"312\":1,\"353\":1,\"369\":1,\"377\":1,\"394\":1,\"408\":1,\"415\":1,\"423\":1,\"429\":1,\"446\":1,\"458\":1,\"498\":1},\"1\":{\"97\":1,\"101\":1,\"110\":1,\"113\":1,\"114\":1,\"123\":1,\"124\":1,\"127\":1,\"130\":1,\"132\":1,\"133\":1,\"136\":1,\"137\":1,\"138\":1,\"151\":1,\"154\":1,\"169\":1,\"172\":2,\"174\":1,\"180\":1,\"184\":1,\"198\":1,\"201\":1,\"215\":1,\"216\":1,\"228\":1,\"238\":1,\"242\":1,\"245\":1,\"250\":2,\"251\":1,\"252\":1,\"258\":1,\"259\":1,\"261\":2,\"264\":1,\"268\":1,\"270\":1,\"272\":1,\"275\":1,\"283\":1,\"286\":2,\"291\":1,\"296\":1,\"308\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":2,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":2,\"353\":1,\"355\":1,\"356\":1,\"359\":1,\"360\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":2,\"371\":1,\"372\":1,\"374\":1,\"376\":2,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":3,\"396\":1,\"398\":1,\"399\":1,\"402\":1,\"403\":7,\"405\":1,\"407\":2,\"410\":1,\"411\":2,\"412\":1,\"414\":1,\"415\":3,\"416\":1,\"417\":2,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":2,\"429\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":2,\"435\":2,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":2,\"445\":1,\"446\":1,\"447\":2,\"448\":1,\"450\":4,\"452\":1,\"453\":1,\"455\":1,\"458\":1,\"459\":1,\"460\":2,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"471\":2,\"472\":1,\"474\":2,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":2,\"494\":1,\"495\":1,\"496\":1,\"497\":2,\"498\":1}}],[\"langford\",{\"1\":{\"365\":1}}],[\"languages\",{\"0\":{\"408\":1},\"1\":{\"124\":1,\"222\":1,\"315\":3,\"327\":2,\"328\":1,\"333\":1,\"334\":4,\"363\":1,\"365\":1,\"377\":1,\"408\":3,\"419\":7,\"420\":7,\"434\":1}}],[\"language\",{\"0\":{\"97\":1,\"102\":1,\"114\":1,\"136\":1,\"138\":1,\"141\":1,\"175\":1,\"180\":1,\"201\":1,\"215\":1,\"242\":1,\"245\":1,\"251\":1,\"259\":1,\"261\":1,\"275\":1,\"280\":1,\"308\":1,\"312\":1,\"333\":1,\"334\":1,\"347\":1,\"353\":1,\"365\":1,\"369\":1,\"372\":1,\"377\":1,\"379\":1,\"381\":1,\"394\":1,\"408\":1,\"423\":1,\"429\":1,\"438\":1,\"444\":1,\"446\":1,\"458\":1,\"497\":1,\"498\":1},\"1\":{\"110\":1,\"114\":1,\"123\":2,\"124\":2,\"127\":1,\"130\":4,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"138\":1,\"141\":6,\"151\":1,\"153\":2,\"154\":1,\"163\":1,\"169\":2,\"172\":2,\"174\":1,\"175\":1,\"180\":1,\"184\":1,\"198\":1,\"199\":1,\"201\":1,\"215\":1,\"216\":1,\"228\":3,\"232\":2,\"240\":1,\"241\":2,\"242\":3,\"245\":1,\"251\":1,\"252\":2,\"258\":1,\"259\":1,\"261\":2,\"264\":1,\"266\":2,\"268\":1,\"275\":3,\"280\":1,\"282\":3,\"283\":1,\"286\":2,\"291\":1,\"308\":1,\"313\":1,\"314\":2,\"315\":4,\"316\":1,\"317\":2,\"318\":1,\"319\":3,\"321\":1,\"322\":1,\"323\":2,\"324\":1,\"325\":1,\"326\":1,\"328\":3,\"329\":1,\"330\":1,\"331\":1,\"333\":4,\"334\":3,\"336\":1,\"337\":1,\"338\":1,\"339\":2,\"340\":3,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"353\":1,\"355\":2,\"356\":1,\"359\":5,\"360\":1,\"362\":3,\"363\":1,\"364\":1,\"365\":5,\"366\":1,\"368\":1,\"369\":1,\"370\":2,\"371\":1,\"372\":5,\"374\":1,\"375\":3,\"376\":1,\"377\":4,\"378\":1,\"379\":2,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":2,\"393\":1,\"394\":2,\"395\":3,\"396\":2,\"398\":1,\"399\":3,\"402\":2,\"403\":1,\"405\":1,\"407\":3,\"410\":1,\"411\":1,\"412\":1,\"414\":1,\"415\":1,\"416\":3,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":2,\"425\":1,\"426\":2,\"427\":1,\"428\":1,\"429\":1,\"431\":1,\"432\":2,\"433\":1,\"434\":1,\"435\":2,\"437\":2,\"438\":5,\"439\":1,\"441\":1,\"442\":2,\"443\":1,\"444\":2,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":2,\"450\":3,\"452\":1,\"453\":1,\"454\":4,\"455\":4,\"458\":2,\"459\":2,\"460\":1,\"461\":2,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"471\":2,\"472\":2,\"474\":2,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"483\":1,\"485\":4,\"486\":2,\"487\":1,\"488\":2,\"489\":1,\"490\":4,\"491\":1,\"492\":2,\"494\":1,\"495\":1,\"496\":3,\"497\":6,\"498\":1,\"539\":1}}],[\"landrum\",{\"1\":{\"261\":1}}],[\"land\",{\"1\":{\"256\":1}}],[\"landmark\",{\"1\":{\"246\":1}}],[\"landmarks\",{\"1\":{\"246\":1,\"407\":2}}],[\"landscapes\",{\"1\":{\"408\":1}}],[\"landscape\",{\"0\":{\"169\":1},\"1\":{\"100\":1,\"151\":1,\"174\":1,\"211\":1,\"212\":1,\"348\":1,\"364\":1,\"371\":1,\"398\":1,\"482\":1}}],[\"lan\",{\"1\":{\"108\":1,\"243\":1,\"295\":1,\"429\":1}}],[\"lok\",{\"1\":{\"430\":1}}],[\"lots\",{\"1\":{\"450\":1}}],[\"lotfi\",{\"1\":{\"407\":1}}],[\"lotte\",{\"1\":{\"347\":1}}],[\"louise\",{\"1\":{\"396\":1}}],[\"louie\",{\"1\":{\"296\":1}}],[\"lou\",{\"1\":{\"391\":1}}],[\"lo\",{\"1\":{\"364\":1}}],[\"loyalty\",{\"1\":{\"232\":1}}],[\"lockdown\",{\"1\":{\"265\":1}}],[\"locomotion\",{\"0\":{\"173\":1},\"1\":{\"173\":2}}],[\"locate\",{\"1\":{\"413\":1}}],[\"located\",{\"1\":{\"184\":1,\"380\":1}}],[\"location\",{\"1\":{\"214\":1,\"237\":2,\"268\":1,\"298\":1,\"319\":1,\"375\":1,\"394\":1,\"425\":1,\"431\":1,\"452\":1,\"495\":1}}],[\"locations\",{\"1\":{\"112\":1,\"220\":1,\"399\":1}}],[\"localization\",{\"1\":{\"319\":1,\"430\":1}}],[\"localize\",{\"0\":{\"319\":1,\"445\":1},\"1\":{\"445\":2}}],[\"localized\",{\"0\":{\"220\":1},\"1\":{\"146\":1,\"220\":1}}],[\"localizing\",{\"0\":{\"430\":1},\"1\":{\"105\":1,\"445\":1}}],[\"local\",{\"1\":{\"112\":2,\"131\":1,\"137\":2,\"146\":2,\"173\":1,\"289\":1,\"301\":1,\"390\":2,\"435\":3}}],[\"lose\",{\"1\":{\"461\":1}}],[\"lost\",{\"1\":{\"166\":1}}],[\"losses\",{\"1\":{\"445\":1,\"456\":1}}],[\"loss\",{\"1\":{\"112\":1,\"220\":2,\"224\":1,\"301\":4,\"309\":1,\"313\":2,\"347\":1,\"410\":1,\"435\":1,\"438\":1,\"439\":1,\"468\":1}}],[\"logconfiglocalizer\",{\"1\":{\"445\":2}}],[\"logprob\",{\"1\":{\"409\":1}}],[\"logprobs\",{\"1\":{\"409\":2}}],[\"logs\",{\"0\":{\"445\":1},\"1\":{\"249\":1,\"339\":1,\"409\":1,\"445\":4}}],[\"log\",{\"1\":{\"210\":1,\"378\":2,\"445\":1}}],[\"logits\",{\"1\":{\"483\":1}}],[\"logit\",{\"1\":{\"196\":1}}],[\"login\",{\"0\":{\"192\":1},\"1\":{\"192\":3}}],[\"logical\",{\"0\":{\"161\":1,\"354\":1},\"1\":{\"161\":1,\"354\":1,\"372\":1,\"399\":1,\"411\":1,\"488\":1,\"494\":1}}],[\"logic\",{\"0\":{\"100\":1,\"111\":1,\"322\":1},\"1\":{\"111\":1,\"153\":1,\"322\":1,\"333\":1,\"475\":1}}],[\"logging\",{\"1\":{\"128\":2}}],[\"loads\",{\"1\":{\"295\":2}}],[\"load\",{\"0\":{\"185\":1,\"295\":1},\"1\":{\"128\":1,\"185\":2,\"195\":1}}],[\"lowest\",{\"1\":{\"313\":2,\"369\":1}}],[\"lowers\",{\"0\":{\"209\":1},\"1\":{\"209\":1}}],[\"lower\",{\"1\":{\"96\":1,\"102\":1,\"103\":1,\"173\":1,\"183\":1,\"191\":1,\"226\":1,\"228\":1,\"257\":1,\"304\":2,\"308\":1,\"322\":1,\"333\":1,\"334\":1,\"486\":1}}],[\"low\",{\"0\":{\"365\":1,\"405\":1,\"408\":1,\"471\":1},\"1\":{\"114\":1,\"135\":1,\"165\":1,\"178\":2,\"186\":1,\"189\":2,\"201\":1,\"219\":1,\"224\":2,\"277\":1,\"278\":1,\"281\":1,\"284\":3,\"334\":2,\"345\":1,\"350\":1,\"365\":4,\"381\":1,\"391\":1,\"401\":1,\"405\":1,\"408\":3,\"432\":1,\"434\":1,\"436\":1,\"460\":1,\"462\":1,\"469\":1,\"475\":2,\"479\":1}}],[\"look\",{\"0\":{\"159\":1},\"1\":{\"265\":1}}],[\"looking\",{\"1\":{\"100\":1,\"265\":1}}],[\"loops\",{\"1\":{\"229\":1}}],[\"loop\",{\"0\":{\"112\":1,\"408\":1},\"1\":{\"112\":1,\"186\":1,\"215\":1,\"218\":1,\"229\":1,\"231\":1,\"408\":2}}],[\"loose\",{\"1\":{\"105\":1}}],[\"lonely\",{\"1\":{\"237\":1}}],[\"loneliness\",{\"0\":{\"237\":2},\"1\":{\"237\":11,\"282\":1}}],[\"longstanding\",{\"1\":{\"477\":1}}],[\"longzhi\",{\"1\":{\"430\":1}}],[\"longbench\",{\"1\":{\"337\":1}}],[\"longer\",{\"0\":{\"209\":1},\"1\":{\"194\":1,\"203\":1,\"209\":2,\"256\":1,\"376\":1,\"405\":1,\"467\":1,\"479\":2}}],[\"longitudinal\",{\"0\":{\"126\":1,\"135\":1},\"1\":{\"126\":1,\"135\":2}}],[\"long\",{\"0\":{\"308\":1,\"337\":1,\"341\":1,\"376\":1,\"412\":2,\"467\":1,\"479\":1},\"1\":{\"104\":2,\"137\":1,\"196\":1,\"235\":1,\"246\":1,\"249\":1,\"283\":1,\"295\":1,\"307\":1,\"308\":1,\"317\":1,\"337\":8,\"376\":7,\"389\":1,\"402\":1,\"412\":9,\"425\":3,\"466\":1,\"467\":3,\"471\":1,\"475\":3,\"479\":1}}],[\"london\",{\"1\":{\"74\":1}}],[\"lp\",{\"1\":{\"96\":3}}],[\"leco\",{\"1\":{\"483\":1}}],[\"lecture\",{\"0\":{\"273\":1}}],[\"lexicon\",{\"1\":{\"497\":1}}],[\"lexical\",{\"1\":{\"460\":1,\"461\":1,\"497\":2}}],[\"lexy\",{\"1\":{\"308\":1}}],[\"lepp\",{\"1\":{\"428\":1}}],[\"lepri\",{\"1\":{\"158\":1,\"390\":1}}],[\"letters\",{\"1\":{\"379\":1}}],[\"letting\",{\"1\":{\"362\":2}}],[\"letian\",{\"1\":{\"322\":1,\"455\":1}}],[\"leticia\",{\"1\":{\"261\":1}}],[\"le\",{\"1\":{\"329\":1,\"483\":1}}],[\"lerner\",{\"1\":{\"322\":1}}],[\"leracy\",{\"1\":{\"74\":1}}],[\"left\",{\"1\":{\"319\":1}}],[\"leval\",{\"0\":{\"337\":1},\"1\":{\"337\":4}}],[\"levkowitz\",{\"1\":{\"305\":1}}],[\"lever\",{\"1\":{\"239\":1}}],[\"leveraged\",{\"1\":{\"172\":1,\"270\":1,\"363\":1,\"370\":1,\"479\":1}}],[\"leverages\",{\"1\":{\"133\":1,\"137\":1,\"141\":1,\"180\":1,\"190\":1,\"259\":1,\"331\":1,\"386\":1,\"434\":1,\"435\":1,\"489\":1,\"495\":1}}],[\"leverage\",{\"1\":{\"97\":1,\"123\":1,\"130\":1,\"138\":1,\"151\":1,\"207\":1,\"312\":1,\"348\":1,\"349\":1,\"375\":1,\"409\":1,\"414\":1,\"417\":1,\"447\":1,\"471\":1,\"481\":1,\"486\":1,\"494\":1}}],[\"leveraging\",{\"0\":{\"97\":1,\"142\":1,\"246\":1,\"288\":1,\"312\":1,\"408\":1},\"1\":{\"97\":1,\"111\":1,\"120\":1,\"124\":1,\"131\":1,\"136\":1,\"137\":1,\"157\":1,\"234\":1,\"244\":1,\"260\":1,\"291\":1,\"303\":1,\"312\":1,\"322\":1,\"323\":1,\"328\":1,\"335\":1,\"358\":1,\"365\":1,\"396\":1,\"407\":1,\"408\":1,\"459\":1,\"470\":1,\"472\":1,\"475\":1,\"476\":1}}],[\"levels\",{\"0\":{\"228\":1},\"1\":{\"107\":1,\"114\":1,\"135\":1,\"170\":1,\"186\":1,\"202\":1,\"209\":1,\"221\":1,\"228\":3,\"237\":1,\"277\":2,\"278\":1,\"279\":1,\"295\":1,\"303\":1,\"333\":1,\"428\":1,\"443\":1}}],[\"level\",{\"0\":{\"308\":1,\"347\":1,\"475\":1},\"1\":{\"101\":1,\"104\":2,\"108\":1,\"114\":2,\"132\":1,\"133\":1,\"141\":1,\"166\":1,\"172\":1,\"183\":1,\"184\":2,\"187\":1,\"188\":1,\"221\":2,\"224\":6,\"227\":1,\"228\":2,\"230\":1,\"260\":1,\"261\":1,\"270\":1,\"283\":1,\"308\":2,\"330\":1,\"331\":1,\"339\":1,\"347\":1,\"358\":1,\"369\":1,\"370\":1,\"380\":2,\"381\":3,\"389\":1,\"398\":1,\"407\":1,\"427\":1,\"428\":3,\"434\":1,\"438\":3,\"446\":2,\"465\":1,\"475\":7,\"479\":2,\"495\":1,\"496\":1}}],[\"leshem\",{\"1\":{\"454\":1}}],[\"lester\",{\"1\":{\"389\":1}}],[\"leste\",{\"1\":{\"365\":1}}],[\"lesions\",{\"1\":{\"238\":1}}],[\"lesion\",{\"0\":{\"238\":1}}],[\"lessen\",{\"1\":{\"265\":1}}],[\"lessening\",{\"1\":{\"143\":1}}],[\"less\",{\"0\":{\"302\":1},\"1\":{\"107\":1,\"155\":1,\"158\":1,\"172\":1,\"207\":1,\"221\":1,\"233\":1,\"246\":3,\"254\":1,\"263\":1,\"266\":1,\"301\":1,\"302\":1,\"322\":1,\"335\":1,\"370\":1,\"411\":1,\"415\":1,\"470\":1}}],[\"leonie\",{\"1\":{\"497\":1}}],[\"leonid\",{\"1\":{\"454\":1}}],[\"leo\",{\"1\":{\"227\":1,\"365\":1}}],[\"lei\",{\"1\":{\"314\":1,\"447\":1,\"491\":1}}],[\"leitch\",{\"1\":{\"291\":1,\"476\":1}}],[\"leibler\",{\"1\":{\"273\":1}}],[\"leisure\",{\"1\":{\"214\":1}}],[\"leif\",{\"1\":{\"183\":1}}],[\"led\",{\"1\":{\"174\":1,\"194\":1,\"298\":1,\"329\":1,\"338\":1,\"350\":1,\"371\":1}}],[\"legacy\",{\"0\":{\"460\":1},\"1\":{\"460\":1,\"488\":1}}],[\"legal\",{\"0\":{\"384\":1},\"1\":{\"171\":1,\"213\":1,\"253\":1,\"384\":2,\"498\":1}}],[\"legepladsen\",{\"1\":{\"48\":1}}],[\"leni\",{\"1\":{\"243\":1}}],[\"lena\",{\"1\":{\"181\":1,\"400\":1}}],[\"lending\",{\"1\":{\"158\":1}}],[\"lengthy\",{\"1\":{\"359\":1}}],[\"lengths\",{\"1\":{\"190\":1,\"209\":1,\"249\":1,\"337\":1,\"389\":2,\"428\":1,\"469\":1}}],[\"length\",{\"0\":{\"337\":1,\"479\":1},\"1\":{\"114\":1,\"203\":1,\"209\":4,\"234\":1,\"235\":1,\"301\":2,\"337\":3,\"341\":1,\"382\":1,\"389\":1,\"412\":2,\"415\":1,\"469\":1}}],[\"leung\",{\"1\":{\"110\":1,\"321\":1}}],[\"lebrun\",{\"1\":{\"107\":1}}],[\"leetcode\",{\"1\":{\"342\":1,\"411\":1}}],[\"lee\",{\"1\":{\"105\":1,\"146\":1,\"150\":1,\"157\":1,\"191\":1,\"198\":2,\"210\":1,\"258\":1,\"274\":2,\"355\":1,\"375\":1,\"387\":2,\"389\":1,\"433\":1,\"436\":1}}],[\"leaf\",{\"1\":{\"462\":1}}],[\"leaving\",{\"1\":{\"437\":1}}],[\"leaking\",{\"1\":{\"435\":1}}],[\"leakage\",{\"1\":{\"374\":1,\"435\":1,\"482\":1}}],[\"leans\",{\"1\":{\"424\":1}}],[\"lean\",{\"1\":{\"322\":1}}],[\"least\",{\"1\":{\"204\":1,\"408\":1,\"428\":1,\"443\":1,\"467\":1}}],[\"learnability\",{\"1\":{\"389\":1}}],[\"learnable\",{\"1\":{\"362\":1,\"376\":1,\"389\":1}}],[\"learns\",{\"1\":{\"257\":1,\"449\":1}}],[\"learnt\",{\"1\":{\"199\":1}}],[\"learned\",{\"1\":{\"188\":1,\"257\":1,\"269\":2,\"289\":1}}],[\"learners\",{\"1\":{\"122\":1,\"125\":2,\"273\":3,\"363\":2}}],[\"learner\",{\"0\":{\"122\":1},\"1\":{\"125\":1}}],[\"learn\",{\"0\":{\"322\":1,\"403\":1,\"470\":1},\"1\":{\"137\":1,\"164\":1,\"222\":1,\"235\":1,\"243\":1,\"313\":1,\"322\":1,\"355\":1,\"360\":2,\"374\":1,\"378\":1,\"402\":1,\"435\":1,\"449\":1,\"470\":2,\"483\":1}}],[\"learning\",{\"0\":{\"101\":1,\"148\":1,\"190\":1,\"196\":1,\"210\":1,\"216\":1,\"255\":1,\"273\":1,\"294\":2,\"305\":1,\"313\":1,\"317\":1,\"319\":1,\"362\":1,\"408\":1,\"412\":1,\"483\":1,\"485\":1,\"486\":1},\"1\":{\"97\":1,\"99\":1,\"101\":2,\"102\":1,\"104\":1,\"110\":2,\"112\":1,\"116\":1,\"118\":1,\"121\":1,\"122\":3,\"130\":4,\"137\":4,\"148\":2,\"150\":1,\"158\":1,\"159\":1,\"160\":1,\"165\":8,\"169\":2,\"172\":2,\"180\":1,\"183\":2,\"190\":1,\"196\":2,\"199\":1,\"204\":1,\"210\":1,\"212\":1,\"216\":5,\"219\":1,\"223\":1,\"228\":2,\"231\":1,\"237\":3,\"259\":1,\"273\":5,\"290\":2,\"291\":2,\"292\":1,\"294\":3,\"300\":1,\"303\":1,\"305\":1,\"307\":2,\"308\":5,\"309\":2,\"312\":1,\"317\":1,\"321\":2,\"322\":1,\"355\":1,\"362\":3,\"370\":2,\"372\":1,\"388\":1,\"389\":2,\"398\":1,\"400\":1,\"401\":1,\"403\":1,\"404\":1,\"405\":1,\"408\":2,\"409\":1,\"411\":4,\"412\":1,\"432\":2,\"435\":2,\"442\":1,\"455\":1,\"460\":1,\"470\":1,\"475\":2,\"476\":2,\"479\":1,\"482\":1,\"483\":2,\"486\":1,\"489\":1,\"539\":1}}],[\"leaderboard\",{\"0\":{\"482\":1},\"1\":{\"325\":1,\"482\":1}}],[\"leader\",{\"1\":{\"245\":3}}],[\"leaders\",{\"1\":{\"245\":2}}],[\"leadership\",{\"0\":{\"245\":1},\"1\":{\"245\":4}}],[\"leads\",{\"1\":{\"166\":1,\"204\":1,\"318\":1,\"322\":1,\"350\":1,\"381\":1,\"405\":1,\"411\":1,\"453\":1}}],[\"leading\",{\"0\":{\"409\":1},\"1\":{\"107\":1,\"223\":1,\"226\":1,\"242\":1,\"263\":1,\"283\":1,\"306\":1,\"307\":1,\"355\":1,\"358\":1,\"374\":1,\"384\":1,\"418\":1,\"482\":1,\"488\":1,\"490\":1}}],[\"lead\",{\"1\":{\"102\":1,\"108\":1,\"124\":1,\"125\":1,\"135\":1,\"171\":1,\"215\":1,\"268\":1,\"294\":1,\"319\":1,\"335\":1,\"342\":1,\"452\":1,\"453\":1,\"482\":1}}],[\"licenses\",{\"1\":{\"498\":1}}],[\"liconbench\",{\"1\":{\"412\":2}}],[\"lippmann\",{\"1\":{\"496\":1}}],[\"lijie\",{\"1\":{\"453\":1}}],[\"lioi\",{\"1\":{\"444\":1}}],[\"lionel\",{\"1\":{\"194\":1}}],[\"lifan\",{\"1\":{\"411\":1}}],[\"life\",{\"0\":{\"141\":1},\"1\":{\"105\":2,\"132\":1,\"160\":1,\"166\":1,\"180\":1,\"192\":1,\"214\":1,\"272\":1,\"328\":1,\"330\":1}}],[\"lifestyle\",{\"0\":{\"27\":1}}],[\"liret\",{\"1\":{\"384\":1}}],[\"lillian\",{\"1\":{\"359\":1}}],[\"lizhi\",{\"1\":{\"324\":1,\"392\":1}}],[\"lies\",{\"1\":{\"260\":1,\"281\":1,\"282\":1,\"360\":1,\"362\":1}}],[\"lihua\",{\"1\":{\"260\":1}}],[\"lian\",{\"1\":{\"318\":1}}],[\"liang\",{\"1\":{\"192\":1,\"246\":1,\"378\":1,\"425\":1,\"428\":1,\"432\":1,\"479\":1}}],[\"liao\",{\"1\":{\"207\":1,\"313\":1}}],[\"library\",{\"0\":{\"205\":1,\"353\":1,\"443\":1},\"1\":{\"205\":1,\"258\":2,\"343\":1,\"353\":2,\"433\":2,\"443\":1}}],[\"libriphrase\",{\"1\":{\"190\":1}}],[\"list\",{\"1\":{\"269\":1}}],[\"lists\",{\"1\":{\"253\":1,\"410\":1}}],[\"listing\",{\"1\":{\"151\":1,\"348\":1}}],[\"lisa\",{\"1\":{\"178\":1,\"199\":1}}],[\"living\",{\"1\":{\"166\":1,\"184\":2,\"298\":1,\"380\":2}}],[\"lively\",{\"1\":{\"235\":1}}],[\"lives\",{\"1\":{\"148\":1,\"298\":1}}],[\"live\",{\"1\":{\"117\":1,\"173\":1,\"399\":1}}],[\"liquids\",{\"1\":{\"142\":2}}],[\"liquid\",{\"1\":{\"142\":1}}],[\"liwei\",{\"1\":{\"133\":1,\"331\":1}}],[\"literary\",{\"1\":{\"333\":1}}],[\"literature\",{\"1\":{\"136\":1,\"171\":2,\"174\":1,\"196\":1,\"244\":1,\"278\":1,\"291\":1,\"292\":1,\"362\":1,\"371\":1,\"385\":1,\"400\":1,\"429\":1,\"476\":1,\"492\":2}}],[\"literacy\",{\"1\":{\"125\":1,\"249\":1}}],[\"little\",{\"1\":{\"128\":1,\"181\":1,\"182\":1,\"189\":1,\"214\":1,\"289\":1,\"391\":2,\"427\":1,\"497\":2}}],[\"li\",{\"1\":{\"111\":1,\"128\":1,\"133\":1,\"164\":1,\"167\":1,\"173\":1,\"174\":2,\"218\":3,\"223\":1,\"235\":1,\"252\":1,\"269\":1,\"271\":1,\"286\":2,\"295\":2,\"306\":1,\"314\":1,\"328\":1,\"331\":1,\"342\":1,\"345\":1,\"359\":1,\"361\":1,\"371\":2,\"403\":1,\"412\":1,\"414\":1,\"426\":1,\"430\":2,\"437\":1,\"443\":1,\"445\":2,\"447\":2,\"453\":1,\"456\":1,\"457\":1,\"470\":1,\"471\":1,\"474\":2,\"478\":2,\"480\":4,\"485\":1,\"494\":1}}],[\"lightweight\",{\"0\":{\"316\":1},\"1\":{\"316\":2,\"376\":1,\"434\":1,\"529\":1}}],[\"lighting\",{\"1\":{\"274\":1}}],[\"lightness\",{\"1\":{\"197\":2}}],[\"lighthouse\",{\"1\":{\"178\":1}}],[\"lightfoot\",{\"1\":{\"156\":1}}],[\"light\",{\"1\":{\"106\":1,\"133\":1,\"214\":1,\"216\":1,\"227\":1,\"256\":1,\"331\":1,\"424\":1,\"437\":1,\"458\":1,\"498\":1}}],[\"lightroom\",{\"1\":{\"8\":1}}],[\"lidar\",{\"1\":{\"105\":1,\"362\":1}}],[\"likelihood\",{\"1\":{\"251\":1,\"378\":2,\"423\":1}}],[\"likely\",{\"1\":{\"107\":3,\"263\":1,\"268\":1,\"452\":1,\"490\":1}}],[\"like\",{\"0\":{\"264\":1,\"448\":1},\"1\":{\"101\":1,\"126\":1,\"127\":1,\"136\":1,\"180\":1,\"182\":1,\"188\":1,\"198\":1,\"207\":2,\"211\":1,\"213\":2,\"218\":3,\"224\":1,\"226\":1,\"228\":1,\"248\":1,\"251\":1,\"263\":3,\"264\":1,\"266\":1,\"284\":1,\"309\":1,\"319\":1,\"325\":1,\"335\":1,\"350\":1,\"361\":1,\"374\":1,\"382\":1,\"387\":1,\"391\":1,\"412\":1,\"414\":1,\"423\":1,\"428\":1,\"430\":1,\"431\":1,\"434\":1,\"438\":1,\"442\":2,\"443\":1,\"448\":1,\"453\":1,\"460\":1,\"463\":1,\"482\":1,\"491\":2,\"496\":1}}],[\"limeng\",{\"1\":{\"491\":1}}],[\"lim\",{\"1\":{\"108\":1,\"131\":1,\"161\":1,\"274\":1,\"354\":1,\"436\":1}}],[\"limb\",{\"1\":{\"103\":1}}],[\"limit\",{\"1\":{\"343\":1,\"449\":1}}],[\"limitation\",{\"1\":{\"264\":1,\"266\":1,\"341\":1,\"369\":1,\"376\":1,\"443\":1,\"448\":1,\"454\":1}}],[\"limitations\",{\"0\":{\"369\":1},\"1\":{\"100\":1,\"125\":1,\"136\":1,\"140\":1,\"226\":1,\"234\":1,\"238\":1,\"253\":1,\"284\":2,\"306\":1,\"318\":1,\"325\":1,\"337\":1,\"345\":1,\"410\":1,\"413\":1,\"415\":1,\"417\":1,\"458\":1,\"460\":1,\"467\":1,\"482\":1,\"483\":1,\"489\":1}}],[\"limiting\",{\"1\":{\"171\":1,\"490\":1}}],[\"limits\",{\"1\":{\"113\":1,\"209\":1,\"272\":2}}],[\"limited\",{\"1\":{\"100\":1,\"103\":1,\"108\":1,\"123\":1,\"174\":1,\"178\":1,\"203\":1,\"210\":1,\"224\":1,\"228\":1,\"238\":1,\"271\":1,\"273\":1,\"284\":1,\"326\":1,\"371\":1,\"379\":1,\"381\":1,\"391\":1,\"408\":1,\"413\":1,\"434\":1,\"442\":1,\"447\":1,\"457\":1,\"467\":1,\"469\":1,\"472\":1,\"475\":1,\"482\":1}}],[\"lim+ed\",{\"1\":{\"74\":1}}],[\"linqi\",{\"1\":{\"483\":1}}],[\"linjian\",{\"1\":{\"479\":1}}],[\"linda\",{\"0\":{\"251\":1,\"423\":1}}],[\"lina\",{\"1\":{\"239\":1}}],[\"lines\",{\"1\":{\"434\":1}}],[\"line\",{\"1\":{\"228\":1,\"418\":1,\"465\":1,\"495\":1}}],[\"linear\",{\"0\":{\"96\":1},\"1\":{\"96\":1,\"131\":2,\"223\":1,\"246\":1}}],[\"lingming\",{\"1\":{\"482\":1}}],[\"lingrui\",{\"1\":{\"463\":1}}],[\"lingual\",{\"0\":{\"419\":1,\"420\":1},\"1\":{\"419\":1,\"420\":1}}],[\"linguistically\",{\"1\":{\"226\":1}}],[\"linguistics\",{\"0\":{\"352\":1},\"1\":{\"205\":1}}],[\"linguistic\",{\"0\":{\"461\":1},\"1\":{\"163\":1,\"340\":2,\"362\":1,\"408\":2,\"438\":1,\"449\":1,\"450\":2,\"461\":7}}],[\"ling\",{\"1\":{\"116\":1,\"118\":1}}],[\"lin\",{\"1\":{\"103\":1,\"130\":1,\"166\":1,\"235\":1,\"319\":1,\"329\":1,\"337\":1,\"345\":1,\"360\":1,\"411\":1,\"414\":1,\"432\":1,\"437\":1,\"438\":1,\"462\":1}}],[\"links\",{\"1\":{\"255\":1}}],[\"linkedin发布了一份关于2024年蕞抢手技能的报告\",{\"1\":{\"518\":1}}],[\"linked\",{\"1\":{\"237\":1}}],[\"linking\",{\"1\":{\"156\":1}}],[\"link\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":2,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":2,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":2,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":2,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"liu2021understanding\",{\"1\":{\"159\":1}}],[\"liu\",{\"1\":{\"74\":1,\"105\":1,\"112\":1,\"114\":2,\"124\":1,\"130\":1,\"141\":1,\"142\":1,\"147\":1,\"173\":2,\"213\":1,\"219\":1,\"224\":1,\"242\":1,\"260\":1,\"316\":1,\"318\":1,\"324\":2,\"328\":1,\"345\":1,\"349\":1,\"353\":1,\"377\":1,\"392\":1,\"401\":1,\"402\":1,\"403\":1,\"411\":2,\"425\":1,\"428\":1,\"437\":2,\"438\":1,\"442\":1,\"450\":1,\"463\":1,\"467\":1,\"472\":2,\"478\":1,\"487\":1}}],[\"li恩利共同创作\",{\"1\":{\"27\":1}}],[\"冒险\",{\"1\":{\"74\":3,\"530\":2}}],[\"冒险创造的重夺\",{\"1\":{\"53\":1}}],[\"冒险游乐场\",{\"1\":{\"48\":2}}],[\"冒险精神被也渐渐丢失\",{\"1\":{\"45\":1}}],[\"印象深刻\",{\"1\":{\"48\":1}}],[\"梦想和想象成为现实的垃圾操场\",{\"1\":{\"48\":1}}],[\"梦之舟\",{\"1\":{\"23\":1,\"46\":1}}],[\"他的艺术方法不仅改变了酒店业\",{\"1\":{\"533\":1}}],[\"他的认知发展理论成为了这个学科的典范\",{\"1\":{\"79\":1}}],[\"他也不会说出这些想法\",{\"1\":{\"509\":1}}],[\"他当时告诉我未来可以介入一些神经系统的测试\",{\"1\":{\"509\":1}}],[\"他进行了许多关于数量\",{\"1\":{\"79\":1}}],[\"他曾到过许多国家讲学\",{\"1\":{\"79\":1}}],[\"他表示\",{\"1\":{\"50\":1}}],[\"他刚刚完成了一个收集布里斯托尔和格洛斯特冒险游乐场记忆的项目\",{\"1\":{\"50\":1}}],[\"他说\",{\"1\":{\"50\":1}}],[\"他最初的想法开始了冒险游乐场运动\",{\"1\":{\"48\":1}}],[\"他想象出\",{\"1\":{\"48\":1}}],[\"他们需要理解数据背后的意义\",{\"1\":{\"527\":1}}],[\"他们具备深入理解技术\",{\"1\":{\"525\":1}}],[\"他们共同塑造未来的系统并确保其可持续性\",{\"1\":{\"519\":1}}],[\"他们对于美的事物确实非常有见解\",{\"1\":{\"509\":1}}],[\"他们独特的想象力在之前的手工拼贴中就已经表现出来\",{\"1\":{\"88\":1}}],[\"他们经常会伸手去触摸现实世界的环境\",{\"1\":{\"87\":1}}],[\"他们会变得非常快乐和惊叹\",{\"1\":{\"87\":1}}],[\"他们正在发育的大脑可能更容易受到这些影响\",{\"1\":{\"81\":1}}],[\"他们为艺术打下坚实的基础\",{\"1\":{\"80\":1}}],[\"他们也无法理解逆向变换会将材料恢复到原始状态\",{\"1\":{\"79\":1}}],[\"他们也面临数字商业创新\",{\"1\":{\"28\":1}}],[\"他们就可以进入分类和保护的能力\",{\"1\":{\"79\":1}}],[\"他们就是这部电影的主导者\",{\"1\":{\"73\":1}}],[\"他们又重回那种自由无畏的精神状态\",{\"1\":{\"74\":1,\"530\":1}}],[\"他们的内心对于这样一些东西或许和成人心中所感并不相同\",{\"1\":{\"74\":1}}],[\"他们的创造和疯狂自成影片\",{\"1\":{\"73\":1}}],[\"他们被冲刺进入到缓冲室\",{\"1\":{\"72\":1}}],[\"他们被带进各种看不懂的容器中\",{\"1\":{\"46\":1}}],[\"他们重新调配着这些色彩\",{\"1\":{\"71\":1}}],[\"他们尝试着钻孔\",{\"1\":{\"69\":1}}],[\"他们尝试着水底的探索\",{\"1\":{\"68\":1}}],[\"他们把它们带到塑料城中\",{\"1\":{\"69\":1}}],[\"他们可以为他们的自由天地喷上颜色\",{\"1\":{\"71\":1}}],[\"他们可以疯狂地在墙上挖洞\",{\"1\":{\"69\":1}}],[\"他们可以尝试去操控达到不同的体验\",{\"1\":{\"68\":1}}],[\"他们可以梦想\",{\"1\":{\"50\":1}}],[\"他们坐在装配过的橡胶圈随着气球一起升上天空\",{\"1\":{\"68\":1}}],[\"他们坐上了机械缆车穿梭\",{\"1\":{\"65\":1}}],[\"他们将成为创造新故事的主角\",{\"1\":{\"84\":1}}],[\"他们将会使用一个塑料底座将这些\",{\"1\":{\"70\":1}}],[\"他们将会被橡胶绳绑上一个动力装置\",{\"1\":{\"68\":1}}],[\"他们将跟着着六艘船去到城市的各个废料集中营里\",{\"1\":{\"63\":1}}],[\"他们带着废料来到这个城市中心的水面上重生\",{\"1\":{\"65\":1}}],[\"他们带着孩子走了进去\",{\"1\":{\"46\":1}}],[\"他们只能在江边观望着这里\",{\"1\":{\"59\":1}}],[\"他们想到了充满梦想和创造的孩子们\",{\"1\":{\"57\":1}}],[\"他们希望看到这些废物的重塑\",{\"1\":{\"57\":1}}],[\"他们希望研究这些污染的物质来寻求能量混乱的根源\",{\"1\":{\"23\":1}}],[\"他们不仅理解客户的需要\",{\"1\":{\"525\":1}}],[\"他们不忍心看着废弃材料的丢弃\",{\"1\":{\"57\":1}}],[\"他们不欢迎在街上\",{\"1\":{\"50\":1}}],[\"他们看到这些材料不断被浪费\",{\"1\":{\"56\":1}}],[\"他们诉说着自己的心声\",{\"1\":{\"56\":1}}],[\"他们生活\",{\"1\":{\"50\":1}}],[\"他们展开\",{\"1\":{\"50\":1}}],[\"他们天生就会长大\",{\"1\":{\"50\":1}}],[\"他们之前心中对于原来城市的不安都瞬间全无\",{\"1\":{\"46\":1}}],[\"他们喜欢变废为宝这种魔法\",{\"1\":{\"46\":1}}],[\"他们在战争年代的无惧向前的创作精神\",{\"1\":{\"43\":1}}],[\"他们通过不断努力将\",{\"1\":{\"23\":1}}],[\"西奥多\",{\"1\":{\"48\":1}}],[\"起源\",{\"0\":{\"47\":1}}],[\"起航\",{\"0\":{\"63\":1},\"1\":{\"46\":1}}],[\"装置\",{\"1\":{\"534\":1}}],[\"装置设计\",{\"1\":{\"5\":1}}],[\"装载完了垃圾\",{\"1\":{\"46\":1}}],[\"什么东西一旦被创新反而给用户增加困扰\",{\"1\":{\"521\":1}}],[\"什么是该创新打破传统的\",{\"1\":{\"521\":1}}],[\"什么是你感兴趣的\",{\"1\":{\"31\":1}}],[\"什么内容去满足\",{\"1\":{\"521\":1}}],[\"什么样的学习叫做有价值\",{\"1\":{\"509\":1}}],[\"什么昨天\",{\"1\":{\"46\":1}}],[\"昨天你不是带着我们去了好多岛吗\",{\"1\":{\"46\":1}}],[\"啊\",{\"1\":{\"46\":1}}],[\"说到这里\",{\"1\":{\"510\":1}}],[\"说到底就是这几年的专业越来越倾向于建立跨学科环境\",{\"1\":{\"510\":1}}],[\"说实话我大学很向往英国的教育\",{\"1\":{\"508\":1}}],[\"说\",{\"1\":{\"46\":1}}],[\"想想你已经收集的知识和见解\",{\"1\":{\"523\":1}}],[\"想想这里比那里都恐怖\",{\"1\":{\"46\":1}}],[\"想到之前的一门技术\",{\"1\":{\"512\":1}}],[\"想到我留学申请中对于专业选择的研究\",{\"1\":{\"510\":1}}],[\"想象\",{\"1\":{\"50\":1}}],[\"想下船去看看\",{\"1\":{\"46\":1}}],[\"想停留的可以下船\",{\"1\":{\"46\":1}}],[\"第一次工作跨界\",{\"1\":{\"534\":1}}],[\"第一个冒险游乐场于1943年在第二次世界大战期间在丹麦emdrup开放\",{\"1\":{\"48\":1}}],[\"第一部分就像前面说的是一个虚构的小说\",{\"1\":{\"43\":1}}],[\"第二天黎明\",{\"1\":{\"46\":1}}],[\"累到直接睡倒在码头地上\",{\"1\":{\"46\":1}}],[\"早已带着那些没有下船的孩子继续前往其他小岛清理各个部分的垃圾\",{\"1\":{\"46\":1}}],[\"轮回\",{\"1\":{\"46\":1}}],[\"城市中被束缚的自由越多\",{\"1\":{\"74\":1}}],[\"城市繁华尽收眼下\",{\"1\":{\"51\":1}}],[\"城市信息\",{\"0\":{\"51\":1}}],[\"城市对年轻人越来越敌视\",{\"1\":{\"50\":1}}],[\"城市的这场垃圾战争究竟是摧毁了孩子还是给孩子带来重生\",{\"1\":{\"46\":1}}],[\"城市公共设计里我们需要去思考哪些方面\",{\"1\":{\"43\":1}}],[\"究竟是一个起点还是一个终点\",{\"1\":{\"46\":1}}],[\"似乎一切都在分崩离析\",{\"1\":{\"538\":1}}],[\"似乎一种神秘之物汇集在林中\",{\"1\":{\"46\":1}}],[\"似乎047把孩子带到一片属于他们未来的重生净土\",{\"1\":{\"46\":1}}],[\"挂在苍天大树上的鲸鱼\",{\"1\":{\"46\":1}}],[\"盛会\",{\"1\":{\"46\":1}}],[\"害怕有一天这些岛也会成为一个垃圾堆无法挽回\",{\"1\":{\"46\":1}}],[\"唉\",{\"1\":{\"46\":1}}],[\"哇我想永远留下来\",{\"1\":{\"46\":1}}],[\"你在用户测试中发现了那些看似反常的行为\",{\"1\":{\"535\":1}}],[\"你也可以从销售思维你理解你做设计的优劣势\",{\"1\":{\"533\":1}}],[\"你便是一个销售的角色\",{\"1\":{\"533\":1}}],[\"你是以一个\",{\"1\":{\"530\":1}}],[\"你是谁\",{\"1\":{\"508\":1}}],[\"你写了一个虚构小说\",{\"1\":{\"530\":1}}],[\"你要运用批判性判断\",{\"1\":{\"523\":1}}],[\"你会强调你的想法可能带来的好处和附加值\",{\"1\":{\"523\":1}}],[\"你会以最积极的眼光看待问题\",{\"1\":{\"523\":1}}],[\"你会探索一系列的想法和可能的前进方向\",{\"1\":{\"523\":1}}],[\"你会制定议程\",{\"1\":{\"523\":1}}],[\"你会专注于控制自己的思维和管理决策过程\",{\"1\":{\"523\":1}}],[\"你的专业知识\",{\"1\":{\"535\":1}}],[\"你的组织框架\",{\"1\":{\"521\":1}}],[\"你的迭代方式\",{\"1\":{\"509\":1}}],[\"你知道什么该做什么不该做才不会永远在循环和发展没有一个定性的目标\",{\"1\":{\"521\":1}}],[\"你如何去挑战自己\",{\"1\":{\"509\":1}}],[\"你们赶紧回家吧\",{\"1\":{\"46\":1}}],[\"你们小孩子要去哪啊\",{\"1\":{\"46\":1}}],[\"你们自己做的选择\",{\"1\":{\"46\":1}}],[\"你们真的不下船吗\",{\"1\":{\"46\":1}}],[\"你们还有无限的潜能去发掘\",{\"1\":{\"46\":1}}],[\"你们刚好就要赶上喽\",{\"1\":{\"46\":1}}],[\"你可以表达自己的感受\",{\"1\":{\"523\":1}}],[\"你可以参与各种项目\",{\"1\":{\"40\":1}}],[\"你可以参与各种由知名ip方组织的活动\",{\"1\":{\"40\":1}}],[\"你可以使用你的nft购买真实世界的商品和服务\",{\"1\":{\"40\":1}}],[\"你可以建立属于自己的虚拟世界\",{\"1\":{\"40\":1}}],[\"每天\",{\"1\":{\"532\":1}}],[\"每天载着孩子的船穿梭于这片迷雾中的水域\",{\"1\":{\"46\":1}}],[\"每一个部分的内容都各不相同沟通失误也难免会出现\",{\"1\":{\"92\":1}}],[\"每一次活动都需要和许多不同的人接触\",{\"1\":{\"92\":1}}],[\"每年的盛会\",{\"1\":{\"46\":1}}],[\"每个人除了精通自己的专业\",{\"1\":{\"535\":1}}],[\"每个人精通之处不一样\",{\"1\":{\"46\":1}}],[\"每个平台确实都有自己的管理方式\",{\"1\":{\"521\":1}}],[\"每个国家地区留学各有优劣\",{\"1\":{\"508\":1}}],[\"每个岛会提供自己的一些材料去重新搭建一些盛会的场景\",{\"1\":{\"46\":1}}],[\"每个用户都可以在这里建造\",{\"1\":{\"40\":1}}],[\"每个创作者的特性可以被放大\",{\"1\":{\"27\":1}}],[\"每个作品系列都是一个独特的ip\",{\"1\":{\"12\":1}}],[\"靠着我们造的船相互联系交流\",{\"1\":{\"46\":1}}],[\"奇形怪状\",{\"1\":{\"46\":1}}],[\"奇怪的是\",{\"1\":{\"46\":1}}],[\"果实和以前城市里看到的一点都不同\",{\"1\":{\"46\":1}}],[\"真不可思议\",{\"1\":{\"46\":1}}],[\"真的是百果丰盛\",{\"1\":{\"46\":1}}],[\"真浪数字艺术展\",{\"0\":{\"29\":1}}],[\"真浪数字展\",{\"1\":{\"9\":1}}],[\"老人带着孩子乘着自己建造的船来到另外一个岛中\",{\"1\":{\"46\":1}}],[\"老人们带着孩子坐上升降梯来到岛上\",{\"1\":{\"46\":1}}],[\"老头们唯一没有带孩子们去的地方就是一个暗藏在水下的秘密基地\",{\"1\":{\"23\":1}}],[\"倒变成一股力量涌在心头\",{\"1\":{\"46\":1}}],[\"仿佛真的是做梦\",{\"1\":{\"46\":1}}],[\"仿佛一个巨型地下工厂般\",{\"1\":{\"46\":1}}],[\"能自动化创建\",{\"1\":{\"539\":1}}],[\"能不能得到各评委老师的认同我在前期是无法预知的\",{\"1\":{\"534\":1}}],[\"能够提升设计效果\",{\"1\":{\"527\":1}}],[\"能否安全落地需要不断和执行人员\",{\"1\":{\"92\":1}}],[\"能长出许多果实和蔬菜\",{\"1\":{\"46\":1}}],[\"能分享一下您的创作灵感吗\",{\"1\":{\"43\":1}}],[\"跟你们介绍一下\",{\"1\":{\"46\":1}}],[\"魔法品\",{\"1\":{\"46\":1}}],[\"魔法机器\",{\"1\":{\"46\":1}}],[\"屋里的那些孩子开心地奔跑着传递从\",{\"1\":{\"46\":1}}],[\"气泡池和搅拌机\",{\"1\":{\"46\":1}}],[\"穿梭和运送\",{\"0\":{\"67\":1}}],[\"穿过壁上点着蜡烛的小道\",{\"1\":{\"46\":1}}],[\"穿插一个叙事性的设想\",{\"1\":{\"45\":1}}],[\"走吧孩子们\",{\"1\":{\"46\":1}}],[\"天空无比温暖\",{\"1\":{\"73\":1}}],[\"天啊\",{\"1\":{\"46\":1}}],[\"天生喜欢拿着雪橇和特制的滑雪板\",{\"1\":{\"23\":1}}],[\"又如何进步\",{\"1\":{\"533\":1}}],[\"又是临近一年一度的毕业季\",{\"1\":{\"530\":1}}],[\"又并不给儿童太多规则上的限制\",{\"1\":{\"83\":1}}],[\"又要考虑到新接入的虚拟环境的新颖互动性\",{\"1\":{\"81\":1}}],[\"又或是惊讶\",{\"1\":{\"73\":1}}],[\"又到最后渐渐衰落\",{\"1\":{\"49\":1}}],[\"又鸣起它的船笛\",{\"1\":{\"46\":1}}],[\"又看到了重造的希望\",{\"1\":{\"72\":1}}],[\"又看到了\",{\"1\":{\"46\":1}}],[\"又看到了早上离开的那个\",{\"1\":{\"46\":1}}],[\"又大片大片地吵了起来\",{\"1\":{\"46\":1}}],[\"又一批孩子搭上这艘\",{\"1\":{\"46\":1}}],[\"顺手拨下了石壁上的铁渣\",{\"1\":{\"46\":1}}],[\"迎接着这一群他们也不知道来及哪个世纪的孩子\",{\"1\":{\"46\":1}}],[\"清理一部分的垃圾后就要重新开船了\",{\"1\":{\"46\":1}}],[\"船把孩子们下船了\",{\"1\":{\"65\":1}}],[\"船\",{\"0\":{\"62\":1}}],[\"船笛声再响起\",{\"1\":{\"46\":1}}],[\"船长停下船\",{\"1\":{\"46\":1}}],[\"船长船长\",{\"1\":{\"46\":1}}],[\"船长带着这些从不下船的孩子重新穿过了这片迷雾\",{\"1\":{\"46\":1}}],[\"船长问\",{\"1\":{\"46\":1}}],[\"船开进了一个山洞\",{\"1\":{\"46\":1}}],[\"船来了\",{\"1\":{\"46\":1,\"63\":1}}],[\"整合多种不舒适的体验\",{\"1\":{\"520\":1}}],[\"整合\",{\"1\":{\"510\":1}}],[\"整个项目通过服务设计\",{\"1\":{\"499\":1}}],[\"整个项目最显著的观察是\",{\"1\":{\"88\":1}}],[\"整个屋子充斥着各种冒出的蒸汽\",{\"1\":{\"46\":1}}],[\"整个铁箱中的垃圾被倾倒入石洞中\",{\"1\":{\"46\":1}}],[\"整片浓密的树林把岛给包围起来\",{\"1\":{\"46\":1}}],[\"整体数据还不错\",{\"1\":{\"20\":1}}],[\"迷雾退散\",{\"1\":{\"46\":1}}],[\"水中的迷雾很大\",{\"1\":{\"46\":1}}],[\"换来的就是这些垃圾怪的堆积\",{\"1\":{\"46\":1}}],[\"垃圾操场是最丑的\",{\"1\":{\"50\":1}}],[\"垃圾\",{\"1\":{\"48\":1,\"74\":1}}],[\"垃圾游乐场和冒险游乐场渐渐消失\",{\"1\":{\"50\":1}}],[\"垃圾游乐场\",{\"1\":{\"48\":3}}],[\"垃圾城\",{\"1\":{\"46\":1}}],[\"垃圾就会被自动归类\",{\"1\":{\"46\":1}}],[\"垃圾送上船后\",{\"1\":{\"46\":1}}],[\"垃圾变成的肥料竟有这么大的魔力\",{\"1\":{\"46\":1}}],[\"垃圾的大量不规范焚烧引起巨大污染\",{\"1\":{\"46\":1}}],[\"垃圾堆中一些动植物的遗体早已不复原貌\",{\"1\":{\"23\":1}}],[\"幕\",{\"0\":{\"73\":1},\"1\":{\"46\":1}}],[\"序\",{\"0\":{\"46\":1}}],[\"属于孩子们的最美星星\",{\"1\":{\"45\":1}}],[\"更需要具备跨学科的视角\",{\"1\":{\"535\":1}}],[\"更以人为本的产品和服务\",{\"1\":{\"532\":1}}],[\"更重要的也是平台\",{\"1\":{\"523\":1}}],[\"更多地考虑社会文化背景\",{\"1\":{\"522\":1}}],[\"更多让孩子参与的活动等等\",{\"1\":{\"43\":1}}],[\"更有效率\",{\"1\":{\"521\":1}}],[\"更要考虑整体效率和用户体验\",{\"1\":{\"519\":1}}],[\"更环境化\",{\"1\":{\"516\":1}}],[\"更是孩子们内心精神世界的疯狂\",{\"1\":{\"73\":1}}],[\"更是一种内心世界的创造和圆梦\",{\"1\":{\"45\":1}}],[\"更神奇地\",{\"1\":{\"46\":1}}],[\"叙事的推动通过三部曲来进行\",{\"1\":{\"45\":1}}],[\"承担社会责任\",{\"1\":{\"519\":1}}],[\"承载过去的精神而结合到当下的背景\",{\"1\":{\"45\":1}}],[\"承包商和供应商联系\",{\"1\":{\"6\":1}}],[\"提高电子邮件质量\",{\"1\":{\"539\":1}}],[\"提高邮件质量和针对性\",{\"1\":{\"539\":1}}],[\"提高利润\",{\"1\":{\"531\":1}}],[\"提高学生们的自信心和勇气\",{\"1\":{\"524\":1}}],[\"提高产品的易用性\",{\"1\":{\"522\":1}}],[\"提高公众对贫困\",{\"1\":{\"520\":1}}],[\"提高沉浸感\",{\"1\":{\"517\":1}}],[\"提升打开率和参与度\",{\"1\":{\"539\":1}}],[\"提升产品吸引力和用户体验\",{\"1\":{\"531\":1}}],[\"提升满意度\",{\"1\":{\"531\":1}}],[\"提升销售转化率\",{\"1\":{\"518\":1}}],[\"提升团队整体协作效率\",{\"1\":{\"518\":1}}],[\"提供清晰\",{\"1\":{\"532\":1}}],[\"提供了一种应对策略\",{\"1\":{\"527\":1}}],[\"提供足够细节来保持决策的方向\",{\"1\":{\"521\":1}}],[\"提供用户去哪儿\",{\"1\":{\"521\":1}}],[\"提供用户做某事\",{\"1\":{\"521\":1}}],[\"提供实时价值\",{\"1\":{\"519\":1}}],[\"提供一种沉浸式的体验\",{\"1\":{\"512\":1}}],[\"提供设计概念\",{\"1\":{\"6\":1}}],[\"提出一个假想去回应\",{\"1\":{\"53\":1}}],[\"提出疑问\",{\"1\":{\"45\":1}}],[\"追溯到1943年垃圾的诞生以及到冒险游乐场的演变历程\",{\"1\":{\"45\":1}}],[\"该模式由美术专业毕业生\",{\"1\":{\"533\":1}}],[\"该模型有助于我们更有效地理解用户行为和设计\",{\"1\":{\"532\":1}}],[\"该工具强调文化在形成个体和群体创新行为规范中的核心地位\",{\"1\":{\"524\":1}}],[\"该章节讨论了在industry\",{\"1\":{\"519\":1}}],[\"该框架仍然是很初级的阶段\",{\"1\":{\"90\":1}}],[\"该年龄段的孩子非常难控制\",{\"1\":{\"89\":1}}],[\"该机制既有一定的引导性\",{\"1\":{\"83\":1}}],[\"该机构优先重视传统的艺术课程\",{\"1\":{\"80\":1}}],[\"该\",{\"1\":{\"81\":1}}],[\"该项目来自我于广州美术学院2019年的个人毕业设计项目\",{\"1\":{\"44\":1}}],[\"该商业计划书只是我基于当时市场做的一份初步构建\",{\"1\":{\"42\":1}}],[\"爬树与翻墙|phantasisland\",{\"1\":{\"43\":1,\"74\":1}}],[\"看着很全能\",{\"1\":{\"533\":1}}],[\"看电影或进行其他活动时\",{\"1\":{\"512\":1}}],[\"看我如何挑战传统建筑\",{\"1\":{\"43\":1,\"74\":1,\"502\":1}}],[\"看到了自由和无畏\",{\"1\":{\"72\":1}}],[\"看到许多概念的大火又没落\",{\"1\":{\"30\":1}}],[\"看到ai时代下\",{\"1\":{\"29\":1}}],[\"毕业逆袭\",{\"1\":{\"43\":1,\"74\":1,\"502\":1}}],[\"毕业设计\",{\"1\":{\"17\":1}}],[\"聊聊建筑转行\",{\"0\":{\"534\":1},\"1\":{\"43\":1,\"74\":1,\"502\":1}}],[\"孩子又会怎么想呢\",{\"1\":{\"73\":1}}],[\"孩子的心灵是完全满足的\",{\"1\":{\"50\":1}}],[\"孩子\",{\"1\":{\"46\":1}}],[\"孩子也需要一些更能激发内心潜能的教育\",{\"1\":{\"43\":1}}],[\"孩子们对增强现实内容的兴奋程度比我预期的更高\",{\"1\":{\"88\":1}}],[\"孩子们得到自由\",{\"1\":{\"74\":1,\"530\":1}}],[\"孩子们更需要接触自然的产物\",{\"1\":{\"74\":1}}],[\"孩子们虽然在逃离\",{\"1\":{\"73\":1}}],[\"孩子们凝视着这些废弃的建筑碎片\",{\"1\":{\"72\":1}}],[\"孩子们似乎又回到了20世纪3\",{\"1\":{\"72\":1}}],[\"孩子们就是这个实验室的主导者\",{\"1\":{\"71\":1}}],[\"孩子们使用机器寻找着废弃金属里的美妙声音\",{\"1\":{\"70\":1}}],[\"孩子们操控着机器\",{\"1\":{\"69\":1}}],[\"孩子们驾驶着飞行的运输小车来帮忙装载和运输这些废料到他们想去的地方\",{\"1\":{\"67\":1}}],[\"孩子们爬上了塔\",{\"1\":{\"67\":1}}],[\"孩子们站在平台遥望远方的城市\",{\"1\":{\"66\":1}}],[\"孩子们可以跟随着这个上下循环的运输塔来到水下的世界\",{\"1\":{\"72\":1}}],[\"孩子们可以登上塔顶去去观望\",{\"1\":{\"62\":1}}],[\"孩子们可以自己搭建属于自己的天地\",{\"1\":{\"45\":1}}],[\"孩子们来到码头等待着这六艘船的到来\",{\"1\":{\"62\":1}}],[\"孩子们暂离繁华都市\",{\"1\":{\"53\":1}}],[\"孩子们需要能够承担风险\",{\"1\":{\"50\":1}}],[\"孩子们在某些游戏步骤中很难管理\",{\"1\":{\"90\":1}}],[\"孩子们在疯狂创造这一场香味盛宴\",{\"1\":{\"71\":1}}],[\"孩子们在这里的岩石大地上奔跑欢呼\",{\"1\":{\"65\":1}}],[\"孩子们在这里茁壮成长\",{\"1\":{\"50\":1}}],[\"孩子们在瓦砾中玩耍成为重建的有趄赴力隐喻\",{\"1\":{\"49\":1}}],[\"孩子们在城市中的新乐土会是一种怎样的状态\",{\"1\":{\"45\":1}}],[\"孩子们喜欢到处玩\",{\"1\":{\"48\":1}}],[\"孩子们清醒了\",{\"1\":{\"46\":1}}],[\"孩子们道别船长后\",{\"1\":{\"46\":1}}],[\"孩子们的疯狂世界的展现\",{\"1\":{\"60\":1}}],[\"孩子们的疯狂内心世界谁又得知\",{\"1\":{\"45\":1}}],[\"孩子们的欢声笑语遍布这些奇怪形态的东西\",{\"1\":{\"46\":1}}],[\"孩子们听到这番话\",{\"1\":{\"46\":1}}],[\"孩子们被眼前这一景象惊住了\",{\"1\":{\"46\":1}}],[\"孩子们吓坏了\",{\"1\":{\"46\":1}}],[\"孩子们还沉浸在为这些不可思议的事惊讶的情绪中\",{\"1\":{\"46\":1}}],[\"孩子们\",{\"1\":{\"46\":1,\"87\":1}}],[\"孩子们呼喊着\",{\"1\":{\"46\":1}}],[\"孩子们从前那种自由无畏的创造\",{\"1\":{\"45\":1}}],[\"孩子们逃离残酷的现实在这片梦幻乐土中自由创造\",{\"1\":{\"23\":1,\"46\":1}}],[\"孩子们拥有巨大的潜力\",{\"1\":{\"17\":1}}],[\"本文讨论了人工智能在电子邮件生成中的应用\",{\"1\":{\"539\":1}}],[\"本文的观点是工业4\",{\"1\":{\"519\":1}}],[\"本能和情绪化的系统\",{\"1\":{\"532\":1}}],[\"本能感受\",{\"1\":{\"523\":1}}],[\"本身是对于自然的一种态度\",{\"1\":{\"74\":1}}],[\"本设计基于历史背景下的经历和垃圾\",{\"1\":{\"74\":1}}],[\"本次展览的主题是\",{\"1\":{\"43\":1}}],[\"本来8月4日\",{\"1\":{\"26\":1}}],[\"作用环节\",{\"1\":{\"531\":1}}],[\"作者gennaro\",{\"1\":{\"525\":1}}],[\"作者\",{\"1\":{\"522\":1}}],[\"作者指出非技术能力\",{\"1\":{\"519\":1}}],[\"作者提到了如何在设计初期理解和管理不确定性\",{\"1\":{\"519\":1}}],[\"作者们强调了网络安全管理的重要性\",{\"1\":{\"519\":1}}],[\"作品集或案例研究也是如此\",{\"1\":{\"538\":1}}],[\"作品集灵感｜探索城市废墟和游戏创作🎴\",{\"1\":{\"511\":1}}],[\"作品集也是一种ip的打造\",{\"1\":{\"508\":1}}],[\"作品里分为三部曲\",{\"1\":{\"43\":1}}],[\"作为一个多次经历跨界的设计师\",{\"1\":{\"537\":1}}],[\"作为用户体验设计师\",{\"1\":{\"532\":1}}],[\"作为新教育模式的可能性\",{\"1\":{\"519\":1}}],[\"作为数字艺术家荣幸受邀得到小红书\",{\"1\":{\"507\":1}}],[\"作为数字艺术板块的主理人共创作品\",{\"1\":{\"26\":1}}],[\"作为本科毕业项目\",{\"1\":{\"76\":1}}],[\"作为本次创作的切入主题\",{\"1\":{\"24\":1}}],[\"作为在大湾区工作的人\",{\"1\":{\"42\":1}}],[\"作为视觉艺术家被邀请到成都春熙路的大屏展示\",{\"1\":{\"29\":1}}],[\"作为中国社交媒体平台中比较有审美调性的平台\",{\"1\":{\"20\":1}}],[\"作为工作室的创始人和负责人\",{\"1\":{\"15\":1}}],[\"您的作品与本次展览主题有哪些内在关联性\",{\"1\":{\"43\":1}}],[\"您的作品中也描绘了一座属于儿童的\",{\"1\":{\"43\":1}}],[\"您是怎样构思作品中的场景的呢\",{\"1\":{\"43\":1}}],[\"您非常细致地描绘了各种细节\",{\"1\":{\"43\":1}}],[\"您还为这件作品编写了一个故事\",{\"1\":{\"43\":1}}],[\"才能更好地与工程师\",{\"1\":{\"535\":1}}],[\"才能适应动态管理环境\",{\"1\":{\"518\":1}}],[\"才能找到最真的自我\",{\"1\":{\"24\":1}}],[\"才开启了这个设想的创作\",{\"1\":{\"43\":1}}],[\"偶然发现了这段有趣的历史\",{\"1\":{\"43\":1}}],[\"yq就出现了\",{\"1\":{\"534\":1}}],[\"yvonne\",{\"1\":{\"217\":1}}],[\"yuhuai\",{\"1\":{\"494\":1}}],[\"yuhan\",{\"1\":{\"141\":1}}],[\"yujia\",{\"1\":{\"470\":1}}],[\"yuji\",{\"1\":{\"459\":1}}],[\"yujun\",{\"1\":{\"450\":1}}],[\"yukai\",{\"1\":{\"449\":1}}],[\"yuki\",{\"1\":{\"264\":1,\"448\":1}}],[\"yukino\",{\"1\":{\"160\":1}}],[\"yuxin\",{\"1\":{\"445\":1}}],[\"yuxuan\",{\"1\":{\"167\":1,\"483\":1}}],[\"yulei\",{\"1\":{\"430\":1}}],[\"yulia\",{\"1\":{\"133\":1,\"331\":1}}],[\"yuzhou\",{\"1\":{\"410\":1}}],[\"yuzhu\",{\"1\":{\"359\":1}}],[\"yusen\",{\"1\":{\"391\":1}}],[\"yuyang\",{\"1\":{\"377\":1}}],[\"yutian\",{\"1\":{\"359\":1}}],[\"yutan\",{\"1\":{\"154\":1}}],[\"yucheng\",{\"1\":{\"495\":1}}],[\"yuchen\",{\"1\":{\"345\":1}}],[\"yuchuan\",{\"1\":{\"471\":1}}],[\"yuchu\",{\"1\":{\"141\":1}}],[\"yubo\",{\"1\":{\"293\":1}}],[\"yuanzhi\",{\"1\":{\"345\":1}}],[\"yuanqian\",{\"1\":{\"316\":1}}],[\"yuanchun\",{\"1\":{\"222\":1}}],[\"yuan\",{\"1\":{\"141\":1,\"184\":1,\"212\":1,\"361\":2,\"377\":2,\"380\":1,\"394\":1,\"411\":1,\"462\":1,\"479\":1}}],[\"yu\",{\"1\":{\"133\":1,\"138\":1,\"141\":1,\"222\":1,\"319\":1,\"331\":1,\"377\":1,\"472\":1,\"480\":1,\"488\":1,\"489\":1,\"495\":1}}],[\"yuejie\",{\"1\":{\"427\":1}}],[\"yuekai\",{\"1\":{\"219\":1,\"401\":1}}],[\"yue\",{\"1\":{\"132\":1,\"324\":1,\"330\":1,\"345\":1,\"378\":1,\"412\":1}}],[\"yuexi\",{\"1\":{\"124\":1}}],[\"yunze\",{\"1\":{\"497\":1}}],[\"yunha\",{\"1\":{\"356\":1}}],[\"yunhai\",{\"1\":{\"259\":1}}],[\"yuning\",{\"1\":{\"300\":1}}],[\"yunsheng\",{\"1\":{\"218\":1}}],[\"yun\",{\"1\":{\"108\":1,\"142\":1,\"419\":1,\"420\":1}}],[\"yejie\",{\"1\":{\"449\":1}}],[\"yejin\",{\"1\":{\"133\":1,\"331\":1}}],[\"yew\",{\"1\":{\"417\":1}}],[\"yes\",{\"1\":{\"368\":1,\"466\":1}}],[\"year\",{\"1\":{\"307\":1}}],[\"years\",{\"1\":{\"126\":2,\"179\":1,\"210\":1,\"292\":1,\"342\":1,\"443\":1}}],[\"ye\",{\"1\":{\"175\":1,\"218\":1,\"270\":1,\"322\":1,\"378\":1}}],[\"yen\",{\"1\":{\"116\":1,\"118\":1}}],[\"yet\",{\"0\":{\"466\":1},\"1\":{\"97\":1,\"133\":2,\"151\":1,\"164\":1,\"167\":1,\"204\":1,\"211\":1,\"227\":1,\"261\":1,\"278\":1,\"291\":1,\"304\":1,\"308\":1,\"312\":1,\"328\":2,\"331\":2,\"340\":1,\"348\":1,\"359\":1,\"363\":1,\"372\":1,\"412\":1,\"415\":1,\"432\":1,\"443\":1,\"449\":1,\"476\":1,\"483\":1,\"485\":1,\"488\":1,\"491\":1}}],[\"yadong\",{\"1\":{\"429\":1}}],[\"yadollah\",{\"1\":{\"404\":1}}],[\"yaghoobzadeh\",{\"1\":{\"404\":1}}],[\"yassir\",{\"1\":{\"444\":1}}],[\"yasheng\",{\"1\":{\"318\":1}}],[\"yasmin\",{\"1\":{\"148\":1}}],[\"yarosh\",{\"1\":{\"227\":1}}],[\"yarin\",{\"1\":{\"199\":1}}],[\"yanshen\",{\"1\":{\"491\":1}}],[\"yanming\",{\"1\":{\"488\":1}}],[\"yankai\",{\"1\":{\"411\":1}}],[\"yanrong\",{\"1\":{\"295\":1}}],[\"yan\",{\"1\":{\"176\":1,\"223\":1,\"306\":1,\"349\":1,\"350\":1,\"429\":1}}],[\"yangyang\",{\"1\":{\"324\":1,\"392\":1}}],[\"yang\",{\"0\":{\"167\":1},\"1\":{\"98\":1,\"108\":1,\"113\":1,\"115\":1,\"119\":1,\"142\":2,\"150\":1,\"151\":1,\"160\":1,\"167\":1,\"176\":1,\"180\":1,\"222\":1,\"235\":1,\"243\":1,\"246\":1,\"260\":1,\"270\":1,\"323\":1,\"348\":1,\"349\":1,\"350\":1,\"359\":1,\"360\":1,\"402\":1,\"425\":2,\"428\":1,\"430\":1,\"432\":2,\"437\":2,\"447\":1,\"453\":1,\"472\":1,\"480\":1,\"496\":1}}],[\"yaxin\",{\"1\":{\"173\":1}}],[\"yamaki\",{\"1\":{\"459\":1}}],[\"yaman\",{\"1\":{\"289\":1}}],[\"yamashita\",{\"1\":{\"160\":1}}],[\"yamaoka\",{\"1\":{\"142\":1}}],[\"yalong\",{\"1\":{\"119\":1}}],[\"yaohui\",{\"1\":{\"428\":1}}],[\"yao\",{\"1\":{\"116\":1,\"118\":1,\"132\":1,\"330\":1,\"455\":1,\"462\":1,\"480\":1,\"483\":1}}],[\"yiwei\",{\"1\":{\"463\":1}}],[\"yichen\",{\"1\":{\"445\":1}}],[\"yichun\",{\"1\":{\"277\":1}}],[\"yiping\",{\"1\":{\"403\":1}}],[\"yizhu\",{\"1\":{\"402\":1}}],[\"yizhe\",{\"1\":{\"349\":1}}],[\"yilun\",{\"1\":{\"391\":1}}],[\"yiqun\",{\"1\":{\"362\":1}}],[\"yifei\",{\"1\":{\"260\":1}}],[\"yifan\",{\"1\":{\"218\":1,\"345\":1,\"402\":1}}],[\"yiran\",{\"1\":{\"257\":1}}],[\"yihan\",{\"1\":{\"242\":1}}],[\"yihang\",{\"1\":{\"173\":1}}],[\"yixuan\",{\"1\":{\"132\":1,\"330\":1,\"438\":1}}],[\"yinlin\",{\"1\":{\"482\":1}}],[\"yintong\",{\"1\":{\"445\":1}}],[\"yinfei\",{\"1\":{\"151\":1,\"348\":1}}],[\"yin\",{\"0\":{\"167\":1},\"1\":{\"130\":1,\"167\":1,\"283\":1,\"317\":1,\"391\":1,\"413\":1}}],[\"yingqiang\",{\"1\":{\"485\":1}}],[\"yingjun\",{\"1\":{\"442\":1}}],[\"yinghui\",{\"1\":{\"361\":1}}],[\"yingchaojie\",{\"1\":{\"242\":1}}],[\"ying\",{\"1\":{\"128\":1,\"133\":1,\"331\":1}}],[\"yielded\",{\"1\":{\"487\":1}}],[\"yielding\",{\"1\":{\"249\":1,\"467\":1,\"491\":1}}],[\"yields\",{\"1\":{\"183\":1,\"323\":1,\"372\":1,\"469\":1}}],[\"yield\",{\"1\":{\"121\":1,\"324\":1,\"368\":2,\"441\":1}}],[\"yi\",{\"1\":{\"113\":1,\"115\":1,\"170\":1,\"173\":1,\"295\":1,\"353\":1,\"394\":1}}],[\"yolov8+gpt\",{\"1\":{\"486\":1}}],[\"yolov8\",{\"1\":{\"486\":1}}],[\"yoshikazu\",{\"1\":{\"459\":1}}],[\"yoshimasa\",{\"1\":{\"415\":1}}],[\"yossi\",{\"1\":{\"441\":1}}],[\"yoichi\",{\"1\":{\"415\":1}}],[\"yoav\",{\"1\":{\"340\":1}}],[\"yoon\",{\"1\":{\"274\":1}}],[\"yoonsuck\",{\"1\":{\"156\":1}}],[\"yongfeng\",{\"1\":{\"485\":1}}],[\"yongbin\",{\"1\":{\"471\":1}}],[\"yongqi\",{\"1\":{\"470\":1}}],[\"yongqiang\",{\"1\":{\"324\":1}}],[\"yong\",{\"1\":{\"227\":1,\"328\":1,\"479\":1,\"487\":1}}],[\"yongtao\",{\"1\":{\"112\":1}}],[\"youdi\",{\"1\":{\"314\":1}}],[\"your\",{\"0\":{\"260\":1,\"275\":1,\"333\":1}}],[\"yourselves\",{\"0\":{\"445\":1}}],[\"yours\",{\"1\":{\"9\":1}}],[\"you\",{\"0\":{\"192\":1},\"1\":{\"151\":1,\"243\":1,\"348\":1,\"453\":1,\"462\":1}}],[\"youtu\",{\"1\":{\"219\":1,\"258\":1,\"401\":1,\"433\":1}}],[\"youtube\",{\"0\":{\"200\":1},\"1\":{\"122\":1,\"200\":1}}],[\"youth\",{\"0\":{\"148\":1},\"1\":{\"148\":8}}],[\"younger\",{\"1\":{\"221\":1}}],[\"young\",{\"1\":{\"133\":1,\"331\":1,\"356\":1}}],[\"york\",{\"1\":{\"74\":1}}],[\"y\",{\"1\":{\"43\":3,\"131\":2,\"159\":1,\"281\":3,\"428\":1,\"435\":1}}],[\"文化被视为创新行为规范形成的中心\",{\"1\":{\"524\":1}}],[\"文化\",{\"1\":{\"524\":1}}],[\"文中还讨论了如何从利润导向的企业转变为\",{\"1\":{\"519\":1}}],[\"文中提到的垃圾游乐场在渐渐消失\",{\"1\":{\"74\":1}}],[\"文章涉及的领域\",{\"1\":{\"539\":1}}],[\"文章列举了一些顶级的ai电子邮件生成器\",{\"1\":{\"539\":1}}],[\"文章指出教育方式可能也需要适应变化\",{\"1\":{\"519\":1}}],[\"文章指出随着技术进步和互联网的普及\",{\"1\":{\"519\":1}}],[\"文章总结了这些解决方案的应用效果\",{\"1\":{\"519\":1}}],[\"文章还提到设计过程需要更加迭代决策和评估\",{\"1\":{\"519\":1}}],[\"文章还提到了诸如业务到消费者\",{\"1\":{\"519\":1}}],[\"文章还讨论了如何在设计中处理不确定性和复杂性\",{\"1\":{\"519\":1}}],[\"文章讨论了数字通信中的\",{\"1\":{\"519\":1}}],[\"文章提出组织需要从利润导向型企业转变为学习型组织\",{\"1\":{\"519\":1}}],[\"文章强调了在产品开发和制造中融入设计思维的重要性\",{\"1\":{\"519\":1}}],[\"文件\",{\"1\":{\"42\":1}}],[\"文本讲述了2199年世界被垃圾堆满\",{\"1\":{\"23\":1}}],[\"硬件再往大层面拓展\",{\"1\":{\"509\":1}}],[\"硬件也可以大致分为两种\",{\"1\":{\"509\":1}}],[\"硬件的\",{\"1\":{\"42\":1}}],[\"硬件装置的运用\",{\"1\":{\"16\":1}}],[\"科技类项目科学素养类产品进校服务应运而生\",{\"1\":{\"78\":1}}],[\"科技等等各类话题仍然不断\",{\"1\":{\"30\":1}}],[\"科研实力全扫描\",{\"1\":{\"42\":1}}],[\"上面还有\",{\"1\":{\"46\":1}}],[\"上船了\",{\"1\":{\"46\":1}}],[\"上船吧\",{\"1\":{\"46\":1}}],[\"上\",{\"1\":{\"42\":1}}],[\"上海喜马拉雅博物馆\",{\"1\":{\"4\":1}}],[\"粤港澳大湾区区块链政策\",{\"1\":{\"42\":1}}],[\"粤港澳大湾区区块链产业图谱\",{\"1\":{\"42\":1}}],[\"粤港澳已成为区块链发展风水宝地\",{\"1\":{\"42\":1}}],[\"相互融合的过程\",{\"1\":{\"531\":1}}],[\"相同点\",{\"1\":{\"531\":1}}],[\"相对于本科的学习\",{\"1\":{\"92\":1}}],[\"相关阅读phantasisland\",{\"1\":{\"530\":1}}],[\"相关阅读\",{\"1\":{\"42\":1,\"43\":1,\"74\":1}}],[\"相比同履历的设计背景同学\",{\"1\":{\"534\":1}}],[\"相比\",{\"1\":{\"34\":1}}],[\"对商业模式和产品经济学的理解和贡献与创造力和同理心同样重要\",{\"1\":{\"538\":1}}],[\"对商业工程的定义\",{\"1\":{\"525\":1}}],[\"对业务扩展有深刻的理解\",{\"1\":{\"525\":1}}],[\"对我启发甚大\",{\"1\":{\"530\":1}}],[\"对我而言\",{\"1\":{\"523\":1}}],[\"对我来说\",{\"1\":{\"50\":1,\"507\":1}}],[\"对自己划分一个基础定位\",{\"1\":{\"510\":1}}],[\"对呀\",{\"1\":{\"46\":2}}],[\"对了\",{\"1\":{\"46\":1}}],[\"对待区块链就如当下的网上购物一般成为生活的一部分\",{\"1\":{\"42\":1}}],[\"对于这种项目如何把握\",{\"1\":{\"530\":1}}],[\"对于激发创新至关重要\",{\"1\":{\"524\":1}}],[\"对于具有社会意义的项目\",{\"1\":{\"520\":1}}],[\"对于管理者的决策能力和领导力十分考验\",{\"1\":{\"518\":1}}],[\"对于未知的事物有强烈的好奇心\",{\"1\":{\"509\":1}}],[\"对于未来aigc时代艺术赋能的探索仍有许多路要走\",{\"1\":{\"30\":1}}],[\"对于垃圾游乐场的\",{\"1\":{\"74\":1}}],[\"对于场地\",{\"1\":{\"52\":1}}],[\"对于非中产阶级的孩子们有一种谨慎态度\",{\"1\":{\"50\":1}}],[\"对于他们来说\",{\"1\":{\"46\":1}}],[\"对于海外的web3生态还不够了解\",{\"1\":{\"37\":1}}],[\"对于创作\",{\"1\":{\"31\":1}}],[\"对于很多转专业的同学来说\",{\"1\":{\"31\":1}}],[\"对于大众如何去认识这串代码又是一个难题\",{\"1\":{\"31\":1}}],[\"对于传统艺术机构来说\",{\"1\":{\"28\":1}}],[\"难怪许多人都在重新审视自己的职业选择和行业未来\",{\"1\":{\"538\":1}}],[\"难以随意踏上的片区这是一片只有船才能走上的地方\",{\"1\":{\"52\":1}}],[\"难的是告诉用户为什么使用区块链钱包\",{\"1\":{\"42\":1}}],[\"难得可贵的一场活动确实给大家带来许多欢乐\",{\"1\":{\"26\":1}}],[\"告诉用户怎么用钱包十分简单\",{\"1\":{\"42\":1}}],[\"许多ai电子邮件生成器如rytr\",{\"1\":{\"539\":1}}],[\"许多家长\",{\"1\":{\"81\":1}}],[\"许多问题无法解决\",{\"1\":{\"74\":1}}],[\"许多地方当局以此为借口撤回对冒险游乐场的支持\",{\"1\":{\"49\":1}}],[\"许多孩子变得不守规矩和反社会\",{\"1\":{\"49\":1}}],[\"许多孩子在摇晃的船上迷糊地睡着了\",{\"1\":{\"46\":1}}],[\"许多资源被清光\",{\"1\":{\"46\":1}}],[\"许多品牌想要借此来赋能\",{\"1\":{\"42\":1}}],[\"许多创作者因为难以负担长期运营的成本\",{\"1\":{\"23\":1}}],[\"新闻等等都是你的理论依据\",{\"1\":{\"530\":1}}],[\"新闻网站的内容推荐\",{\"1\":{\"528\":1}}],[\"新加坡学生在创意思考上表现出色\",{\"1\":{\"524\":1}}],[\"新加坡教育部长陈春声先生积极推动设计思维的实践\",{\"1\":{\"524\":1}}],[\"新加坡在2022年的pisa评估中脱颖而出\",{\"1\":{\"524\":1}}],[\"新的市场条件要求我们从纯粹以客户为中心的方法转向更加以商业为中心的方法\",{\"1\":{\"538\":1}}],[\"新的一天开始了\",{\"1\":{\"73\":1}}],[\"新的孩子们也很快跑到人群中与他们玩耍起来\",{\"1\":{\"46\":1}}],[\"新乐器\",{\"1\":{\"70\":1}}],[\"新用户被认为韭菜\",{\"1\":{\"42\":1}}],[\"新技术不断出现\",{\"1\":{\"30\":1}}],[\"合作以及物理系统\",{\"1\":{\"519\":1}}],[\"合作以及新兴技术\",{\"1\":{\"80\":1}}],[\"合作机构的资料分析swot\",{\"0\":{\"80\":1}}],[\"合作和实现梦想的可能性\",{\"1\":{\"40\":1}}],[\"合作艺术家\",{\"1\":{\"4\":1}}],[\"开发独特的产品和服务\",{\"1\":{\"510\":1}}],[\"开发者\",{\"1\":{\"40\":1}}],[\"开放系统玩法来吸引用户\",{\"1\":{\"39\":1}}],[\"有几个新的\",{\"1\":{\"538\":1}}],[\"有效的沟通比专业知识本身更重要\",{\"1\":{\"535\":1}}],[\"有时候\",{\"1\":{\"535\":1}}],[\"有时候就会收获良多\",{\"1\":{\"533\":1}}],[\"有时候应该将自己跳脱出来\",{\"1\":{\"533\":1}}],[\"有时我们还会把一些我们制造的东西带给船长\",{\"1\":{\"46\":1}}],[\"有望将个人成长\",{\"1\":{\"520\":1}}],[\"有些成为品牌主理人\",{\"1\":{\"509\":1}}],[\"有些成为艺术家\",{\"1\":{\"509\":1}}],[\"有客观理性的逻辑去找到技术点所需要的工具\",{\"1\":{\"509\":1}}],[\"有逻辑性的推理\",{\"1\":{\"509\":1}}],[\"有跨学科环境的发展平台\",{\"1\":{\"507\":1}}],[\"有城市的中心高层地标\",{\"1\":{\"51\":1}}],[\"有曾经风光的亚运场地\",{\"1\":{\"51\":1}}],[\"有红色铜块覆盖的小山包\",{\"1\":{\"46\":1}}],[\"有黏土捏的屋子\",{\"1\":{\"46\":1}}],[\"有糖果做的屋子\",{\"1\":{\"46\":1}}],[\"有人担心\",{\"1\":{\"81\":1}}],[\"有人种植果实\",{\"1\":{\"46\":1}}],[\"有人说\",{\"1\":{\"42\":1}}],[\"有个五岁的孩子问道\",{\"1\":{\"46\":1}}],[\"有下船的孩子吗\",{\"1\":{\"46\":1}}],[\"有用垃圾为原料生产水果的百果岛\",{\"1\":{\"43\":1}}],[\"有机会成为团队共创的一部分\",{\"1\":{\"40\":1}}],[\"有部分艺术爱好者\",{\"1\":{\"28\":1}}],[\"限量版商品或者独特的体验\",{\"1\":{\"40\":1}}],[\"限时任务等\",{\"1\":{\"40\":1}}],[\"非同质化代币\",{\"1\":{\"40\":1}}],[\"ntunlp\",{\"1\":{\"443\":1}}],[\"ntinou\",{\"1\":{\"123\":1}}],[\"nvidia\",{\"1\":{\"427\":1}}],[\"njected\",{\"1\":{\"392\":1}}],[\"ndcg\",{\"1\":{\"386\":2,\"458\":1}}],[\"ndeye\",{\"1\":{\"232\":2}}],[\"nkisi\",{\"1\":{\"384\":1}}],[\"nlg\",{\"1\":{\"437\":1}}],[\"nli\",{\"1\":{\"372\":1}}],[\"nlis\",{\"1\":{\"242\":1}}],[\"nl\",{\"1\":{\"339\":2}}],[\"nlu\",{\"1\":{\"232\":1,\"340\":1}}],[\"nlp4gov\",{\"0\":{\"205\":1},\"1\":{\"205\":2}}],[\"nlp\",{\"0\":{\"241\":1,\"404\":1,\"460\":1},\"1\":{\"141\":1,\"205\":1,\"241\":1,\"323\":1,\"359\":1,\"369\":1,\"372\":1,\"391\":1,\"408\":1,\"417\":1,\"443\":1,\"459\":1,\"460\":1,\"467\":1,\"485\":1,\"496\":2,\"539\":1}}],[\"ng\",{\"1\":{\"342\":1}}],[\"ngai\",{\"1\":{\"138\":2}}],[\"nguyen\",{\"1\":{\"101\":1,\"159\":2}}],[\"n\",{\"1\":{\"135\":1,\"166\":3}}],[\"nnes\",{\"1\":{\"124\":6}}],[\"nishimura\",{\"1\":{\"415\":1}}],[\"ni\",{\"1\":{\"413\":1}}],[\"ning\",{\"1\":{\"411\":1}}],[\"nina\",{\"1\":{\"182\":1}}],[\"nirmalie\",{\"1\":{\"384\":1}}],[\"niu\",{\"1\":{\"342\":1}}],[\"nitsa\",{\"1\":{\"253\":1}}],[\"nitsche\",{\"1\":{\"248\":1}}],[\"nigel\",{\"1\":{\"467\":1}}],[\"nigar\",{\"1\":{\"249\":1}}],[\"nights\",{\"1\":{\"194\":1}}],[\"nikhil\",{\"1\":{\"350\":1,\"407\":1}}],[\"nikhita\",{\"1\":{\"209\":1}}],[\"nikos\",{\"1\":{\"468\":1}}],[\"nikoo\",{\"1\":{\"304\":1}}],[\"nikolas\",{\"1\":{\"164\":1}}],[\"niklas\",{\"1\":{\"203\":1,\"229\":1}}],[\"nikita\",{\"1\":{\"469\":1}}],[\"niki\",{\"1\":{\"123\":1}}],[\"nicholas\",{\"1\":{\"268\":1,\"452\":1}}],[\"nichols\",{\"1\":{\"110\":1,\"151\":1,\"321\":1,\"348\":1}}],[\"nicola\",{\"1\":{\"446\":1}}],[\"nicolas\",{\"1\":{\"328\":1,\"409\":1}}],[\"nicolini\",{\"1\":{\"390\":1}}],[\"nicole\",{\"1\":{\"217\":1}}],[\"nicolson\",{\"1\":{\"199\":1}}],[\"nic\",{\"1\":{\"156\":1}}],[\"nickyfot\",{\"1\":{\"123\":1}}],[\"nucleus\",{\"1\":{\"487\":1}}],[\"nuclear\",{\"0\":{\"223\":1},\"1\":{\"238\":1}}],[\"nuan\",{\"1\":{\"461\":1}}],[\"nuanced\",{\"1\":{\"184\":1,\"237\":1,\"278\":1,\"286\":1,\"380\":1,\"412\":1,\"430\":1,\"474\":1}}],[\"nuances\",{\"0\":{\"237\":1},\"1\":{\"124\":1,\"328\":1,\"333\":1,\"416\":2}}],[\"nuance\",{\"1\":{\"97\":1,\"312\":1}}],[\"nurullah\",{\"1\":{\"408\":1}}],[\"nudrat\",{\"1\":{\"369\":1}}],[\"nudge\",{\"1\":{\"209\":1}}],[\"nuñez\",{\"1\":{\"187\":1}}],[\"numeric\",{\"1\":{\"468\":1}}],[\"numerical\",{\"0\":{\"454\":1},\"1\":{\"150\":1,\"324\":1,\"454\":1}}],[\"numerically\",{\"1\":{\"124\":1}}],[\"numerologic\",{\"0\":{\"454\":1},\"1\":{\"454\":3}}],[\"numerous\",{\"1\":{\"174\":1,\"183\":1,\"243\":1,\"304\":1,\"364\":1,\"371\":1,\"469\":1,\"491\":1}}],[\"numbers\",{\"1\":{\"156\":1,\"253\":1,\"454\":2}}],[\"number\",{\"0\":{\"454\":1},\"1\":{\"112\":1,\"131\":1,\"155\":1,\"234\":2,\"254\":1,\"257\":1,\"298\":2,\"315\":1,\"350\":1,\"415\":1,\"436\":1,\"454\":5,\"471\":1,\"496\":1}}],[\"n=84\",{\"1\":{\"273\":1}}],[\"n=213\",{\"1\":{\"215\":1}}],[\"n=1992\",{\"1\":{\"302\":1}}],[\"n=18\",{\"1\":{\"212\":1}}],[\"n=136\",{\"1\":{\"102\":1}}],[\"n=419\",{\"1\":{\"198\":1,\"387\":1}}],[\"n=40\",{\"1\":{\"102\":1}}],[\"nabil\",{\"1\":{\"460\":1}}],[\"naganuma\",{\"1\":{\"459\":1}}],[\"nagireddy\",{\"1\":{\"215\":1}}],[\"nausheen\",{\"1\":{\"434\":1}}],[\"na\",{\"1\":{\"389\":1}}],[\"nawaz\",{\"1\":{\"284\":1}}],[\"nacenta\",{\"1\":{\"277\":1}}],[\"nassim\",{\"1\":{\"239\":1}}],[\"nascent\",{\"1\":{\"178\":1}}],[\"naimul\",{\"1\":{\"229\":1}}],[\"naima\",{\"1\":{\"206\":1}}],[\"nahar\",{\"1\":{\"198\":1,\"387\":1}}],[\"nadendla\",{\"1\":{\"196\":1}}],[\"nazar\",{\"1\":{\"163\":1}}],[\"naomi\",{\"1\":{\"160\":1}}],[\"navin\",{\"1\":{\"265\":1}}],[\"navigating\",{\"0\":{\"169\":1,\"257\":1},\"1\":{\"407\":1,\"458\":1}}],[\"navigation\",{\"0\":{\"119\":1,\"407\":1},\"1\":{\"119\":2,\"257\":1,\"269\":1,\"407\":2}}],[\"navigate\",{\"1\":{\"100\":1,\"173\":1,\"257\":5,\"289\":1,\"314\":1,\"453\":1,\"498\":1}}],[\"navarro\",{\"1\":{\"148\":1}}],[\"namikoshi\",{\"1\":{\"468\":1}}],[\"nam\",{\"1\":{\"183\":1}}],[\"namvarpour\",{\"1\":{\"136\":1}}],[\"named\",{\"0\":{\"323\":1},\"1\":{\"126\":1,\"173\":1,\"259\":1,\"313\":1,\"323\":1,\"344\":1,\"350\":1,\"361\":1,\"382\":1,\"443\":1,\"447\":1,\"455\":1,\"459\":1,\"491\":1}}],[\"namely\",{\"1\":{\"123\":1,\"149\":1,\"167\":1,\"231\":1,\"315\":1,\"334\":1,\"378\":1,\"424\":1,\"444\":1}}],[\"nan\",{\"1\":{\"224\":1,\"391\":1}}],[\"nancy\",{\"1\":{\"130\":1}}],[\"nanometer\",{\"1\":{\"112\":1}}],[\"nano\",{\"1\":{\"112\":1}}],[\"nataliia\",{\"1\":{\"408\":1}}],[\"natasha\",{\"1\":{\"396\":1}}],[\"nathaniel\",{\"1\":{\"385\":1}}],[\"nathan\",{\"1\":{\"230\":1,\"335\":1}}],[\"nationally\",{\"1\":{\"302\":1}}],[\"national\",{\"1\":{\"205\":1,\"221\":1,\"396\":1}}],[\"native\",{\"0\":{\"124\":1},\"1\":{\"124\":3,\"365\":3}}],[\"natural\",{\"0\":{\"141\":1,\"259\":1,\"372\":1},\"1\":{\"123\":1,\"141\":6,\"151\":1,\"163\":1,\"169\":1,\"207\":3,\"228\":2,\"232\":1,\"240\":1,\"242\":2,\"252\":1,\"261\":1,\"266\":1,\"275\":1,\"282\":1,\"314\":1,\"315\":1,\"339\":1,\"340\":1,\"348\":1,\"359\":3,\"372\":3,\"375\":3,\"392\":1,\"394\":1,\"399\":1,\"403\":1,\"424\":1,\"426\":1,\"435\":1,\"437\":1,\"442\":1,\"450\":1,\"454\":1,\"455\":1,\"458\":1,\"459\":1,\"472\":1,\"485\":2,\"486\":1,\"488\":1,\"490\":2,\"496\":2,\"497\":2,\"539\":1}}],[\"naturally\",{\"1\":{\"100\":1,\"184\":1,\"214\":1,\"380\":1,\"391\":1,\"427\":1,\"470\":1}}],[\"nature\",{\"1\":{\"108\":1,\"171\":1,\"223\":1,\"238\":1,\"253\":1,\"286\":1,\"290\":1,\"300\":1,\"301\":1,\"304\":1,\"333\":1,\"391\":1,\"424\":1,\"428\":2,\"429\":1,\"460\":1,\"474\":1}}],[\"naraki\",{\"1\":{\"459\":1}}],[\"narayan\",{\"1\":{\"319\":1}}],[\"narrow\",{\"1\":{\"338\":1,\"418\":1,\"491\":1}}],[\"narratives\",{\"1\":{\"117\":1}}],[\"narumi\",{\"1\":{\"108\":1,\"165\":1}}],[\"nenghai\",{\"1\":{\"488\":1}}],[\"nemecek\",{\"1\":{\"410\":1}}],[\"neopronouns\",{\"1\":{\"395\":1}}],[\"neoliberalism\",{\"1\":{\"293\":1}}],[\"neither\",{\"1\":{\"390\":1}}],[\"neighbor\",{\"1\":{\"274\":1}}],[\"nec\",{\"1\":{\"338\":1}}],[\"necessitate\",{\"1\":{\"467\":1}}],[\"necessitates\",{\"1\":{\"417\":1,\"424\":1,\"429\":1}}],[\"necessitating\",{\"1\":{\"186\":1,\"259\":1,\"333\":1,\"475\":1}}],[\"necessity\",{\"1\":{\"333\":1,\"445\":1}}],[\"necessarily\",{\"1\":{\"243\":1}}],[\"necessary\",{\"1\":{\"100\":1,\"243\":1,\"264\":1,\"446\":1,\"447\":1,\"448\":1}}],[\"ner\",{\"0\":{\"459\":1},\"1\":{\"323\":4,\"459\":4}}],[\"never\",{\"1\":{\"316\":1}}],[\"nevertheless\",{\"1\":{\"158\":1,\"263\":1,\"359\":1}}],[\"nepal\",{\"1\":{\"268\":1,\"452\":1}}],[\"neef\",{\"1\":{\"232\":1}}],[\"needing\",{\"1\":{\"140\":1,\"483\":1}}],[\"needleman\",{\"1\":{\"139\":2}}],[\"needed\",{\"1\":{\"132\":1,\"204\":1,\"235\":1,\"237\":1,\"272\":1,\"330\":1}}],[\"needs\",{\"1\":{\"114\":1,\"154\":1,\"159\":1,\"164\":1,\"166\":1,\"181\":1,\"219\":1,\"228\":1,\"231\":1,\"261\":1,\"277\":1,\"280\":1,\"379\":1,\"401\":1,\"410\":1,\"417\":1,\"470\":1,\"481\":1,\"485\":1}}],[\"need\",{\"0\":{\"114\":1},\"1\":{\"97\":1,\"106\":1,\"114\":1,\"119\":1,\"152\":1,\"176\":1,\"192\":1,\"201\":1,\"210\":1,\"215\":1,\"219\":1,\"222\":1,\"223\":1,\"237\":1,\"240\":1,\"246\":1,\"254\":2,\"259\":1,\"263\":1,\"303\":1,\"312\":1,\"349\":1,\"381\":1,\"401\":1,\"407\":1,\"413\":1,\"434\":1,\"460\":1,\"466\":1,\"471\":1,\"481\":1,\"483\":1,\"491\":1}}],[\"nearby\",{\"1\":{\"301\":1}}],[\"nearly\",{\"1\":{\"257\":1,\"409\":1}}],[\"near\",{\"0\":{\"194\":1},\"1\":{\"223\":1,\"313\":1,\"408\":1}}],[\"nelli\",{\"1\":{\"176\":1}}],[\"neda\",{\"1\":{\"176\":1,\"492\":1}}],[\"next\",{\"0\":{\"239\":1,\"279\":1,\"417\":2},\"1\":{\"164\":1,\"228\":1,\"417\":5,\"430\":1}}],[\"neubig\",{\"1\":{\"345\":1}}],[\"neutral\",{\"1\":{\"167\":1}}],[\"neutrality\",{\"0\":{\"167\":1},\"1\":{\"167\":1}}],[\"neuper\",{\"1\":{\"153\":1}}],[\"neuro\",{\"0\":{\"284\":1},\"1\":{\"284\":1}}],[\"neurogenetic\",{\"1\":{\"281\":1}}],[\"neuroscience\",{\"1\":{\"223\":2}}],[\"neuroscientific\",{\"1\":{\"204\":1}}],[\"neuroergonomics\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"neuron\",{\"1\":{\"202\":1}}],[\"neurological\",{\"1\":{\"166\":1,\"202\":1}}],[\"neurologists\",{\"1\":{\"126\":1}}],[\"neurodivergent\",{\"1\":{\"144\":6}}],[\"neurally\",{\"0\":{\"389\":1},\"1\":{\"389\":2}}],[\"neural\",{\"0\":{\"118\":1,\"183\":1,\"257\":1},\"1\":{\"103\":1,\"118\":1,\"137\":1,\"183\":2,\"188\":2,\"199\":1,\"203\":1,\"223\":1,\"246\":1,\"255\":2,\"257\":1,\"284\":1,\"309\":1,\"351\":1,\"389\":1,\"460\":1}}],[\"neglecting\",{\"1\":{\"439\":1}}],[\"negrini\",{\"1\":{\"292\":1}}],[\"negrut\",{\"1\":{\"150\":1}}],[\"negotiation\",{\"0\":{\"279\":1},\"1\":{\"279\":2}}],[\"negar\",{\"1\":{\"176\":1}}],[\"negative\",{\"1\":{\"167\":1,\"263\":1,\"302\":1,\"304\":2,\"340\":2,\"347\":1,\"369\":1,\"432\":4,\"444\":3,\"467\":1}}],[\"negatively\",{\"1\":{\"102\":1,\"128\":1,\"344\":1}}],[\"netflix的recommendation算法快速向用户展示基于其观看历史的选项\",{\"1\":{\"532\":1}}],[\"net\",{\"1\":{\"103\":3,\"260\":2}}],[\"networking\",{\"1\":{\"284\":1}}],[\"networked\",{\"0\":{\"157\":1},\"1\":{\"157\":1,\"206\":1}}],[\"networks\",{\"0\":{\"143\":1},\"1\":{\"137\":1,\"143\":3,\"188\":1,\"203\":1,\"230\":1,\"246\":2,\"289\":2,\"309\":1,\"398\":1,\"460\":1}}],[\"network\",{\"0\":{\"103\":1,\"118\":1,\"138\":1,\"189\":1,\"223\":1},\"1\":{\"103\":2,\"118\":1,\"138\":2,\"143\":2,\"188\":1,\"189\":1,\"199\":1,\"223\":5,\"235\":1,\"255\":1,\"284\":1,\"351\":1,\"477\":1}}],[\"newly\",{\"1\":{\"411\":1}}],[\"news\",{\"0\":{\"155\":1,\"336\":1,\"491\":1},\"1\":{\"155\":1,\"282\":3,\"336\":2,\"491\":5}}],[\"new\",{\"0\":{\"141\":1,\"353\":1},\"1\":{\"74\":1,\"96\":5,\"104\":1,\"105\":1,\"107\":1,\"113\":2,\"148\":1,\"151\":1,\"156\":1,\"170\":1,\"175\":1,\"179\":4,\"182\":1,\"194\":1,\"199\":1,\"203\":1,\"204\":1,\"215\":1,\"218\":2,\"222\":1,\"224\":1,\"239\":1,\"243\":2,\"244\":1,\"249\":1,\"256\":1,\"257\":2,\"258\":1,\"261\":1,\"265\":1,\"268\":3,\"270\":3,\"292\":1,\"296\":1,\"297\":1,\"303\":1,\"305\":1,\"315\":2,\"333\":1,\"334\":1,\"335\":1,\"338\":1,\"344\":1,\"348\":1,\"353\":2,\"361\":1,\"363\":1,\"369\":2,\"376\":2,\"392\":1,\"398\":1,\"399\":1,\"402\":1,\"424\":1,\"433\":1,\"436\":1,\"442\":1,\"443\":2,\"445\":1,\"452\":3,\"454\":1,\"456\":1,\"468\":1,\"470\":1,\"481\":1,\"491\":1,\"492\":1,\"495\":1}}],[\"nomi\",{\"1\":{\"398\":1}}],[\"noah\",{\"1\":{\"389\":1,\"458\":1}}],[\"nouri\",{\"1\":{\"333\":1}}],[\"noura\",{\"1\":{\"248\":1}}],[\"noppapon\",{\"1\":{\"326\":1}}],[\"noble\",{\"1\":{\"199\":1}}],[\"noises\",{\"1\":{\"224\":2}}],[\"noise\",{\"0\":{\"260\":1,\"378\":1},\"1\":{\"188\":1,\"224\":2,\"260\":1,\"318\":1,\"378\":3,\"459\":1}}],[\"noisy\",{\"1\":{\"96\":1,\"224\":1,\"240\":2,\"329\":1,\"378\":2}}],[\"nor\",{\"1\":{\"390\":1}}],[\"noreils\",{\"1\":{\"179\":1}}],[\"norm\",{\"1\":{\"303\":1}}],[\"normative\",{\"1\":{\"303\":1}}],[\"normativity\",{\"1\":{\"303\":1}}],[\"normalized\",{\"1\":{\"284\":1}}],[\"norman\",{\"1\":{\"135\":1}}],[\"norms\",{\"0\":{\"303\":1},\"1\":{\"133\":1,\"155\":1,\"230\":1,\"286\":1,\"303\":1,\"307\":1,\"309\":1,\"331\":1,\"474\":1}}],[\"north\",{\"1\":{\"119\":1}}],[\"novice\",{\"1\":{\"192\":2,\"363\":2}}],[\"novices\",{\"0\":{\"228\":1},\"1\":{\"110\":2,\"228\":1,\"321\":2}}],[\"novick\",{\"1\":{\"175\":1}}],[\"novelty\",{\"1\":{\"244\":1,\"282\":1,\"362\":1}}],[\"novel\",{\"1\":{\"96\":2,\"101\":2,\"110\":1,\"116\":1,\"119\":1,\"126\":3,\"137\":2,\"138\":1,\"139\":1,\"143\":1,\"150\":1,\"154\":1,\"174\":1,\"188\":1,\"189\":2,\"196\":3,\"212\":1,\"213\":1,\"218\":1,\"229\":1,\"235\":1,\"240\":1,\"245\":1,\"248\":2,\"255\":1,\"259\":2,\"261\":1,\"264\":1,\"266\":1,\"269\":1,\"272\":3,\"284\":1,\"296\":1,\"298\":1,\"309\":1,\"314\":1,\"318\":1,\"321\":1,\"339\":1,\"344\":1,\"349\":1,\"355\":1,\"360\":1,\"365\":1,\"368\":1,\"371\":1,\"379\":1,\"389\":1,\"392\":1,\"396\":1,\"411\":1,\"414\":1,\"415\":1,\"417\":1,\"418\":1,\"427\":1,\"434\":1,\"438\":1,\"448\":1,\"449\":1,\"450\":1,\"453\":1,\"455\":1,\"459\":2,\"467\":1,\"468\":1,\"469\":1,\"479\":1,\"488\":1,\"489\":1,\"491\":1,\"495\":1}}],[\"no\",{\"0\":{\"278\":2},\"1\":{\"106\":2,\"107\":1,\"108\":1,\"139\":1,\"155\":1,\"160\":1,\"186\":1,\"189\":1,\"203\":1,\"220\":1,\"230\":1,\"263\":1,\"335\":1,\"344\":1,\"350\":1,\"368\":1,\"379\":1,\"395\":1,\"427\":1,\"466\":1,\"469\":1}}],[\"now\",{\"0\":{\"418\":1},\"1\":{\"105\":1,\"279\":1,\"316\":1,\"326\":1,\"361\":1}}],[\"notification\",{\"1\":{\"298\":1}}],[\"noticeable\",{\"1\":{\"220\":1,\"245\":1,\"266\":1}}],[\"notice\",{\"0\":{\"195\":1}}],[\"notion提供模板和指南\",{\"1\":{\"532\":1}}],[\"notions\",{\"1\":{\"196\":1}}],[\"notion\",{\"1\":{\"125\":1,\"156\":1,\"187\":1,\"489\":1}}],[\"notoriously\",{\"1\":{\"183\":1,\"326\":1}}],[\"notably\",{\"1\":{\"163\":1,\"303\":1,\"314\":1,\"317\":1,\"336\":1,\"408\":1,\"411\":1,\"424\":1,\"439\":1,\"442\":1,\"485\":1}}],[\"notable\",{\"1\":{\"133\":1,\"216\":1,\"243\":1,\"323\":1,\"331\":1,\"339\":1,\"344\":1,\"412\":1}}],[\"notes\",{\"1\":{\"356\":4}}],[\"note\",{\"0\":{\"335\":1,\"356\":1},\"1\":{\"250\":1,\"335\":6,\"461\":1}}],[\"noted\",{\"1\":{\"166\":1}}],[\"notebook\",{\"1\":{\"119\":1,\"231\":1}}],[\"notebooks\",{\"0\":{\"110\":1,\"119\":1,\"170\":1,\"321\":1},\"1\":{\"104\":1,\"110\":2,\"119\":2,\"170\":3,\"231\":1,\"258\":1,\"321\":2,\"433\":1}}],[\"noteworthy\",{\"1\":{\"100\":1,\"151\":1,\"185\":1,\"348\":1,\"424\":1}}],[\"not\",{\"0\":{\"159\":1,\"195\":1,\"236\":1,\"329\":1},\"1\":{\"98\":1,\"100\":1,\"107\":1,\"112\":1,\"114\":1,\"121\":2,\"124\":1,\"128\":2,\"132\":1,\"139\":1,\"151\":1,\"153\":1,\"154\":1,\"158\":1,\"159\":1,\"165\":1,\"166\":1,\"169\":1,\"170\":1,\"173\":1,\"175\":1,\"189\":2,\"191\":1,\"196\":1,\"204\":2,\"215\":3,\"219\":1,\"220\":1,\"230\":1,\"231\":1,\"234\":1,\"236\":1,\"240\":1,\"241\":2,\"243\":1,\"248\":1,\"253\":2,\"256\":1,\"261\":1,\"266\":1,\"269\":1,\"270\":1,\"272\":1,\"273\":1,\"278\":1,\"282\":1,\"284\":1,\"301\":1,\"304\":1,\"315\":1,\"317\":1,\"318\":1,\"322\":1,\"327\":1,\"330\":1,\"335\":2,\"336\":1,\"337\":1,\"344\":2,\"348\":1,\"353\":1,\"356\":1,\"358\":1,\"368\":1,\"374\":1,\"376\":1,\"379\":1,\"381\":1,\"382\":1,\"385\":1,\"389\":2,\"391\":1,\"393\":1,\"395\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"403\":1,\"408\":1,\"409\":2,\"412\":1,\"422\":1,\"424\":1,\"432\":1,\"434\":1,\"435\":1,\"442\":1,\"443\":3,\"453\":1,\"454\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"467\":1,\"477\":1,\"482\":1,\"487\":1,\"491\":1,\"495\":1,\"540\":1}}],[\"nonfactual\",{\"1\":{\"467\":2}}],[\"noname057\",{\"1\":{\"416\":1}}],[\"none\",{\"1\":{\"336\":1}}],[\"nonlinear\",{\"1\":{\"235\":1}}],[\"nonuniform\",{\"0\":{\"143\":1},\"1\":{\"143\":2}}],[\"noncognitive\",{\"1\":{\"130\":1}}],[\"nonverbal\",{\"1\":{\"108\":1,\"307\":1}}],[\"non\",{\"0\":{\"124\":1,\"196\":1,\"233\":1,\"284\":1,\"400\":1},\"1\":{\"40\":1,\"120\":1,\"124\":2,\"166\":1,\"188\":1,\"194\":1,\"223\":1,\"233\":6,\"284\":2,\"333\":2,\"350\":2,\"379\":1,\"396\":1,\"399\":1,\"400\":2,\"410\":1,\"427\":1,\"434\":1,\"435\":1,\"454\":1,\"497\":3}}],[\"ny\",{\"1\":{\"74\":1}}],[\"nft下的新模式\",{\"0\":{\"36\":1}}],[\"nft的市场冷却期了\",{\"1\":{\"28\":1}}],[\"nft\",{\"1\":{\"14\":1,\"36\":1}}],[\"锻造城市\",{\"1\":{\"40\":1}}],[\"群岛区域\",{\"1\":{\"40\":1}}],[\"比例模型制作到最终落地\",{\"1\":{\"92\":1}}],[\"比如当设计师深陷用户行为的迷宫\",{\"1\":{\"535\":1}}],[\"比如当下已经有的分娩疼痛体验\",{\"1\":{\"520\":1}}],[\"比如我有许多机会可以和客户打交道\",{\"1\":{\"534\":1}}],[\"比如我的项目moodflow\",{\"1\":{\"92\":1}}],[\"比如舞台\",{\"1\":{\"534\":1}}],[\"比如相关专业与我现在的和将来的联系\",{\"1\":{\"523\":1}}],[\"比如数字营销策略\",{\"1\":{\"510\":1}}],[\"比如数字化在文化和艺术领域的运用\",{\"1\":{\"80\":1}}],[\"比如\",{\"1\":{\"510\":1,\"512\":1,\"516\":1,\"535\":1}}],[\"比如unity\",{\"1\":{\"509\":1}}],[\"比如c4d\",{\"1\":{\"509\":1}}],[\"比如xxx设计师\",{\"1\":{\"508\":1}}],[\"比如场地的尺寸限制\",{\"1\":{\"92\":1}}],[\"比如上海web3\",{\"1\":{\"29\":1}}],[\"比赛\",{\"1\":{\"40\":1}}],[\"怪兽广场\",{\"1\":{\"40\":1}}],[\"怪物诞生了\",{\"1\":{\"23\":1}}],[\"怪物\",{\"1\":{\"23\":4}}],[\"怪物工厂\",{\"1\":{\"14\":1,\"22\":1,\"39\":1}}],[\"如rytr\",{\"1\":{\"539\":1}}],[\"如今的数字产品很少是以线性\",{\"1\":{\"538\":1}}],[\"如今的留学\",{\"1\":{\"508\":1}}],[\"如invest决策或solve复杂的数学问题\",{\"1\":{\"532\":1}}],[\"如刷牙或浏览社交媒体\",{\"1\":{\"532\":1}}],[\"如买商品或发送重要email\",{\"1\":{\"532\":1}}],[\"如搜索引擎\",{\"1\":{\"531\":1}}],[\"如网站\",{\"1\":{\"531\":1}}],[\"如互联网\",{\"1\":{\"531\":1}}],[\"如资金流动\",{\"1\":{\"525\":1}}],[\"如收入模式\",{\"1\":{\"525\":1}}],[\"如社区参与式设计\",{\"1\":{\"522\":1}}],[\"如抖音\",{\"1\":{\"521\":1}}],[\"如创新思维\",{\"1\":{\"519\":1}}],[\"如课程长度可能缩短以保证知识不过时\",{\"1\":{\"519\":1}}],[\"如人\",{\"1\":{\"519\":1}}],[\"如物联网\",{\"1\":{\"519\":1}}],[\"如通过模型驱动框架进行人机交互设计\",{\"1\":{\"519\":1}}],[\"如生产效率\",{\"1\":{\"519\":1}}],[\"如在设计参数的选择和确定上引入灵活性\",{\"1\":{\"519\":1}}],[\"如数字孪生的应用\",{\"1\":{\"519\":1}}],[\"如沟通\",{\"1\":{\"518\":1}}],[\"如增强现实的环境声音和互动元素\",{\"1\":{\"517\":1}}],[\"如耳机\",{\"1\":{\"517\":1}}],[\"如智能设备或环保材料的应用\",{\"1\":{\"510\":1}}],[\"如我们常常提到的用户体验\",{\"1\":{\"510\":1}}],[\"如何让用户像逛公园一样轻松导航你的产品\",{\"1\":{\"535\":1}}],[\"如何让用户感觉到科技的便利而非侵入\",{\"1\":{\"517\":1}}],[\"如何将决策心理学应用于用户体验设计\",{\"1\":{\"532\":1}}],[\"如何建立并维护这个框架是一个挑战\",{\"1\":{\"519\":1}}],[\"如何利用数字化技术和人机交互解决de4\",{\"1\":{\"519\":1}}],[\"如何确保研究结果的客观性和准确性\",{\"1\":{\"518\":1}}],[\"如何设计用户界面和交互模式\",{\"1\":{\"517\":1}}],[\"如何适配不同的环境和场景\",{\"1\":{\"516\":1}}],[\"如何看待城市建筑和人类历史的\",{\"1\":{\"511\":1}}],[\"如何能从游戏交互体验中理解和感知这种来自废弃空间的情感\",{\"1\":{\"511\":1}}],[\"如何被记住\",{\"1\":{\"508\":1}}],[\"如何借助废弃材料等媒介\",{\"1\":{\"17\":1}}],[\"如从视觉\",{\"1\":{\"89\":1}}],[\"如几何\",{\"1\":{\"81\":1}}],[\"如区分圆形饼干和方形巧克力\",{\"1\":{\"81\":1}}],[\"如光必选\",{\"1\":{\"78\":1}}],[\"如果他或她不能有效地解释机器学习如何改善用户体验\",{\"1\":{\"535\":1}}],[\"如果最后一步\",{\"1\":{\"534\":1}}],[\"如果可以充分利用好平台资源也是一种价值实现\",{\"1\":{\"523\":1}}],[\"如果教授没有认真去阅览你的作品集或者只是简单地走过场\",{\"1\":{\"509\":1}}],[\"如果你是一名设计师\",{\"1\":{\"538\":1}}],[\"如果你是一个摄影爱好者\",{\"1\":{\"511\":1}}],[\"如果你是从设计产品的角度出发\",{\"1\":{\"509\":1}}],[\"如果你想对于艺术\",{\"1\":{\"509\":1}}],[\"如果你喜欢填鸭式教学\",{\"1\":{\"509\":1}}],[\"如果你们想离开那也只能等待下一班船来接送你们了\",{\"1\":{\"46\":1}}],[\"如果我们能借由某种手段去帮助孩子们又能让他们更自由去创造\",{\"1\":{\"74\":1,\"530\":1}}],[\"如图形成三者链接的系统\",{\"1\":{\"39\":1}}],[\"当我们过于沉浸在专业领域\",{\"1\":{\"535\":1}}],[\"当事人\",{\"1\":{\"530\":1}}],[\"当地的教育专家\",{\"1\":{\"524\":1}}],[\"当你和客户汇报方案时\",{\"1\":{\"533\":1}}],[\"当你停下来的时候\",{\"1\":{\"533\":1}}],[\"当你进行这种思考时\",{\"1\":{\"523\":1}}],[\"当你\",{\"1\":{\"523\":1}}],[\"当你或你的团队处于蓝帽模式时\",{\"1\":{\"523\":1}}],[\"当用户穿过城市时\",{\"1\":{\"512\":1}}],[\"当孩子们看到ar模型并听到音效时\",{\"1\":{\"87\":1}}],[\"当孩子真的完全独立地去创造和冒险\",{\"1\":{\"73\":1}}],[\"当然能做好这一点肯定也很重要\",{\"1\":{\"534\":1}}],[\"当然如果有时间有精力\",{\"1\":{\"509\":1}}],[\"当然\",{\"1\":{\"508\":1,\"512\":1}}],[\"当然这也是一个培养的方式\",{\"1\":{\"77\":1}}],[\"当然我们还有小孩子擅长演奏音乐\",{\"1\":{\"46\":1}}],[\"当上层的塔门打开\",{\"1\":{\"72\":1}}],[\"当他们身上的装置被触发\",{\"1\":{\"72\":1}}],[\"当他们凝结起来\",{\"1\":{\"43\":1}}],[\"当塔门被打开\",{\"1\":{\"67\":1}}],[\"当乘上这艘被称之为\",{\"1\":{\"46\":1}}],[\"当下从2023开始\",{\"1\":{\"536\":1}}],[\"当下的学历贬值\",{\"1\":{\"523\":1}}],[\"当下的我需要通过留学来提升自我吗\",{\"1\":{\"523\":1}}],[\"当下我确实遇到一些瓶颈\",{\"1\":{\"523\":1}}],[\"当下我们不一定需要那样恶劣的环境\",{\"1\":{\"43\":1}}],[\"当下时代的快速发展个更迭\",{\"1\":{\"77\":1}}],[\"当下市场模式和未来商业结构初想\",{\"0\":{\"38\":1}}],[\"当这个画面构想完成后那么我才根据这个画面来还原整个三维的世界结构\",{\"1\":{\"43\":1}}],[\"当时有头部的建筑景观公司找到我\",{\"1\":{\"534\":1}}],[\"当时做了一个开放性的思辨设计项目\",{\"1\":{\"534\":1}}],[\"当时为了解决过度生产的问题\",{\"1\":{\"531\":1}}],[\"当时在导师的指导下学习了\",{\"1\":{\"530\":1}}],[\"当时在平台上发售了一部分数字艺术ip作品\",{\"1\":{\"23\":1}}],[\"当时的第一志愿专业是met\",{\"1\":{\"508\":1}}],[\"当时的变现模式已经难以继续支撑\",{\"1\":{\"30\":1}}],[\"当时的作品以乘风破浪的姐姐\",{\"1\":{\"24\":1}}],[\"当时参加完2023广州设计周的合作后\",{\"1\":{\"29\":1}}],[\"当时非常感谢真浪数字艺术和ezvr团队的这次艺术联合活动\",{\"1\":{\"29\":1}}],[\"当时因为都是我一个人承包了设计\",{\"1\":{\"21\":1}}],[\"当时也是因为小红书平台我拿到了一些资源\",{\"1\":{\"21\":1}}],[\"当时我对于儿童教育的认知还并不完善\",{\"1\":{\"77\":1}}],[\"当时我参加的主要是国内的市场\",{\"1\":{\"37\":1}}],[\"当时我正在公司工作\",{\"1\":{\"20\":1}}],[\"当时我还在公司就职\",{\"1\":{\"20\":1}}],[\"案例调研\",{\"0\":{\"37\":1}}],[\"变现效率更高\",{\"1\":{\"36\":1}}],[\"变现和转型\",{\"0\":{\"21\":1}}],[\"链条更短\",{\"1\":{\"36\":1}}],[\"商业和艺术的世界并没有那么截然不同\",{\"1\":{\"533\":1}}],[\"商业世界是一个复杂的系统\",{\"1\":{\"525\":1}}],[\"商业模式中\",{\"1\":{\"533\":1}}],[\"商业模式理解\",{\"1\":{\"525\":1}}],[\"商业模式可能包括订阅服务\",{\"1\":{\"520\":1}}],[\"商业建模作为基础\",{\"1\":{\"525\":1}}],[\"商业工程的核心是基于业务建模的实验\",{\"1\":{\"525\":1}}],[\"商业工程师关注技术的经济影响\",{\"1\":{\"525\":1}}],[\"商业工程师利用业务建模进行实验\",{\"1\":{\"525\":1}}],[\"商业工程师理解如何分发技术\",{\"1\":{\"525\":1}}],[\"商业工程师结合了设计思维和商业建模\",{\"1\":{\"525\":1}}],[\"商业工程师借鉴了设计思维\",{\"1\":{\"525\":1}}],[\"商业工程师可以发现潜在的改进点\",{\"1\":{\"525\":1}}],[\"商业工程师运用逆向工程的思路\",{\"1\":{\"525\":1}}],[\"商业工程师是介于企业家\",{\"1\":{\"525\":1}}],[\"商业工程师概念的思考\",{\"0\":{\"525\":1},\"1\":{\"502\":1}}],[\"商业架构可能倾向于平台化运营\",{\"1\":{\"520\":1}}],[\"商业架构以及商业模式中的作用\",{\"1\":{\"520\":1}}],[\"商业价值体现在多个维度\",{\"1\":{\"520\":1}}],[\"商业价值计算\",{\"1\":{\"36\":1}}],[\"商业与创新结合\",{\"1\":{\"510\":1}}],[\"商业咨询等等\",{\"1\":{\"510\":1}}],[\"商业策略等\",{\"1\":{\"508\":1}}],[\"商业画布和盈利点\",{\"0\":{\"41\":1}}],[\"商业目标和系统地图\",{\"0\":{\"39\":1}}],[\"商业化\",{\"1\":{\"36\":1}}],[\"旧ip的nft化具备一定的门槛\",{\"1\":{\"36\":1}}],[\"泛娱乐新生态产业链的概念已经出现\",{\"1\":{\"36\":1}}],[\"强调的是信息的传播和与消费者的互动\",{\"1\":{\"531\":1}}],[\"强调的是设计方法和技术的创新\",{\"1\":{\"519\":1}}],[\"强调自下而上的创新和指数级增长机会\",{\"1\":{\"525\":1}}],[\"强调理解用户需求\",{\"1\":{\"524\":1}}],[\"强调了设计思维与服务导向的重要性\",{\"1\":{\"519\":1}}],[\"强调数据在设计决策中的作用\",{\"1\":{\"519\":1}}],[\"强调创造力\",{\"1\":{\"80\":1}}],[\"强行把web2的产品套进web3概念很多时候也不一定行得通\",{\"1\":{\"42\":1}}],[\"强者恒强\",{\"1\":{\"35\":1}}],[\"强化个人的设计风格\",{\"1\":{\"21\":1}}],[\"成年人每天大约要做出35\",{\"1\":{\"532\":1}}],[\"成为产品的创造者\",{\"1\":{\"538\":1}}],[\"成为全球在创意思考方面表现最出色的教育体系\",{\"1\":{\"524\":1}}],[\"成为战略规划的核心\",{\"1\":{\"519\":1}}],[\"成为节目合作的联合创作者\",{\"1\":{\"24\":1}}],[\"成员们增加了6\",{\"1\":{\"518\":1}}],[\"成立了六种集中营去收集着废弃的食物\",{\"1\":{\"57\":1}}],[\"成功的新ip很少见\",{\"1\":{\"35\":1}}],[\"只是现在需要付出更多的努力才能获得\",{\"1\":{\"538\":1}}],[\"只是前期经历的质疑和混乱是每个时代都存在的\",{\"1\":{\"30\":1}}],[\"只有64\",{\"1\":{\"524\":1}}],[\"只有少数几个拥有更好时机的主要ip才有可能取得成功\",{\"1\":{\"35\":1}}],[\"只能不断提升人际管理能力和战略思维\",{\"1\":{\"518\":1}}],[\"占据58\",{\"1\":{\"34\":1}}],[\"授权商品的零售额超过了3000亿美元\",{\"1\":{\"34\":1}}],[\"授权收入以及授权品牌公司数量\",{\"1\":{\"34\":1}}],[\"授权ip数量\",{\"1\":{\"34\":1}}],[\"中的虚拟体验可能会如此吸引人\",{\"1\":{\"81\":1}}],[\"中更加得到在城市街头中从未得到的自由\",{\"1\":{\"74\":1}}],[\"中间打通着一片船经过的水域\",{\"1\":{\"58\":1}}],[\"中产出的\",{\"1\":{\"46\":1}}],[\"中\",{\"1\":{\"43\":1,\"81\":1}}],[\"中您描绘了一个巨大的乌托邦城市\",{\"1\":{\"43\":1}}],[\"中国在2019年的全球授权商品零售销售额份额不到5\",{\"1\":{\"34\":1}}],[\"中国的ip授权市场仍然处于新兴阶段\",{\"1\":{\"34\":1}}],[\"中国的授权商品零售市场超过了1000亿元人民币\",{\"1\":{\"34\":1}}],[\"中担任参展艺术家\",{\"1\":{\"9\":1}}],[\"来提高电子邮件的打开率和参与度\",{\"1\":{\"539\":1}}],[\"来识别其最根本\",{\"1\":{\"525\":1}}],[\"来思考在人工智能飞速发展的今天\",{\"1\":{\"500\":1}}],[\"来自自然的声音或许为这片乐土再添生机\",{\"1\":{\"70\":1}}],[\"来自不同身份的诉说\",{\"0\":{\"56\":1}}],[\"来到了这片造梦天地\",{\"1\":{\"67\":1}}],[\"来到了各个不同的平台\",{\"1\":{\"65\":1}}],[\"来到这里的\",{\"1\":{\"23\":1}}],[\"来改变产品推广\",{\"1\":{\"33\":1}}],[\"很幸运也很感谢当时带我毕业设计的老师\",{\"1\":{\"534\":1}}],[\"很多面试其实是让人感觉有压力的\",{\"1\":{\"509\":1}}],[\"很多儿童机构已经开设了少儿编程\",{\"1\":{\"77\":1}}],[\"很多教育组织都在迭代教育理念\",{\"1\":{\"77\":1}}],[\"很多圈内人已经退出\",{\"1\":{\"29\":1}}],[\"很明显\",{\"1\":{\"50\":1}}],[\"很快又陷入了寒冬\",{\"1\":{\"33\":1}}],[\"入驻\",{\"1\":{\"521\":1}}],[\"入局者纷纷退场\",{\"1\":{\"33\":1}}],[\"入选\",{\"1\":{\"9\":1}}],[\"前往下一个地方\",{\"1\":{\"46\":1}}],[\"前言\",{\"0\":{\"33\":1,\"45\":1}}],[\"前瞻性设计决策\",{\"1\":{\"529\":1}}],[\"前瞻性\",{\"1\":{\"28\":1}}],[\"留学读研如果还都是纸上谈兵的构想就没意思了\",{\"1\":{\"523\":1}}],[\"留学学校\",{\"1\":{\"523\":1}}],[\"留学地区选择\",{\"1\":{\"523\":1}}],[\"留学思考\",{\"0\":{\"503\":1}}],[\"留学作品集并不只是一份项目集合的册子\",{\"1\":{\"31\":1}}],[\"留学项目合作\",{\"1\":{\"4\":1}}],[\"策略的思维方式\",{\"1\":{\"31\":1}}],[\"策划\",{\"1\":{\"23\":1}}],[\"策划等事项\",{\"1\":{\"21\":1}}],[\"区块链技术确实对创作者产权来说是一项很伟大的发明\",{\"1\":{\"31\":1}}],[\"区块链技术的越发成熟\",{\"1\":{\"26\":1}}],[\"搬运都对内容创作者造成了很大的伤害又无法受到保护\",{\"1\":{\"31\":1}}],[\"抄袭\",{\"1\":{\"31\":1}}],[\"其次\",{\"1\":{\"519\":2,\"520\":1}}],[\"其他的孩子会被带去哪里呢\",{\"1\":{\"46\":1}}],[\"其实会花费很多时间\",{\"1\":{\"533\":1}}],[\"其实香港理工大学的几个专业都是如此的概念\",{\"1\":{\"510\":1}}],[\"其实这是一种很正向的交流\",{\"1\":{\"509\":1}}],[\"其实咱们应该把目光看的更远些\",{\"1\":{\"509\":1}}],[\"其实我学到的都是挺多元化的\",{\"1\":{\"508\":1}}],[\"其实更像一个魔法屋\",{\"1\":{\"46\":1}}],[\"其实在数字艺术领域的这些经历也并非我大学专业技能所传授的\",{\"1\":{\"31\":1}}],[\"其实都让创作者没法真正去投入到内容当中\",{\"1\":{\"31\":1}}],[\"其中的灵感来源于遗忘的城市风景\",{\"1\":{\"511\":1}}],[\"其中最开始\",{\"1\":{\"43\":1}}],[\"其中monsterisland\",{\"1\":{\"14\":1}}],[\"其中包括暑期举行的内部大型团队工作坊等\",{\"1\":{\"4\":1}}],[\"流量密码\",{\"1\":{\"31\":1}}],[\"让男性可以站在女性视角去理解分娩的痛苦\",{\"1\":{\"520\":1}}],[\"让ai负责流程化和重复性的任务\",{\"1\":{\"518\":1}}],[\"让用户了解你的结构\",{\"1\":{\"521\":1}}],[\"让用户看到的\",{\"1\":{\"521\":1}}],[\"让用户定期获得新的体验内容\",{\"1\":{\"520\":1}}],[\"让用户在运动时不会错过关键信息\",{\"1\":{\"517\":1}}],[\"让用户能够在听音乐\",{\"1\":{\"512\":1}}],[\"让玩家感受到与现实世界的共鸣\",{\"1\":{\"511\":1}}],[\"让玩家在互动过程中体验不舒适的身体感受\",{\"1\":{\"500\":1}}],[\"让每个参与项目的孩子都能更直观地体验到ar世界\",{\"1\":{\"90\":1}}],[\"让增强现实内容能够提升儿童的学习和创造力\",{\"1\":{\"82\":1}}],[\"让\",{\"1\":{\"79\":2}}],[\"让我很快地在职场成长\",{\"1\":{\"93\":1}}],[\"让我挑战到设计师以外的角色\",{\"1\":{\"92\":1}}],[\"让我们回到当下\",{\"1\":{\"50\":1}}],[\"让我利用这个商业模式较好的打开了市场\",{\"1\":{\"31\":1}}],[\"让梦想和想象成为现实\",{\"1\":{\"50\":1}}],[\"让他捎给你们\",{\"1\":{\"46\":1}}],[\"让他们发自内心去融入到城市生活中\",{\"1\":{\"43\":1}}],[\"让他们在无拘无束的环境中发现自我\",{\"1\":{\"17\":1}}],[\"让人神志昏迷\",{\"1\":{\"46\":1}}],[\"让孩子未来如何更好地融入社会\",{\"1\":{\"43\":1}}],[\"让许多有着一夜暴富的心态的投机者入场炒作\",{\"1\":{\"33\":1}}],[\"未来ux的适应性框架\",{\"1\":{\"529\":1}}],[\"未来再去结合不同的技术\",{\"1\":{\"508\":1}}],[\"未来我也希望这个ip可以集合到更多人\",{\"1\":{\"507\":1}}],[\"未来如果可以结合ar眼镜加入这个项目\",{\"1\":{\"90\":1}}],[\"未来思考\",{\"0\":{\"90\":1}}],[\"未来城市\",{\"1\":{\"40\":1}}],[\"未来的旅程不仅仅是关于答案\",{\"1\":{\"533\":1}}],[\"未来的智慧城市依赖于互联网\",{\"1\":{\"80\":1}}],[\"未来的新数字内容\",{\"1\":{\"36\":1}}],[\"未来的区块链对于艺术\",{\"1\":{\"33\":1}}],[\"未来将会添加更多盈利和推广方法\",{\"1\":{\"31\":1}}],[\"未知的历险之中又往往有许多星光在指引前行\",{\"1\":{\"24\":1}}],[\"各个团队辛苦了很久\",{\"1\":{\"30\":1}}],[\"产生深远影响\",{\"1\":{\"519\":1}}],[\"产生新的科技解决方案\",{\"1\":{\"510\":1}}],[\"产生许多爆炸的信息过后aigc这个词又出来了\",{\"1\":{\"30\":1}}],[\"产品经理甚至市场营销人员对话\",{\"1\":{\"535\":1}}],[\"产品\",{\"1\":{\"525\":1}}],[\"产品品牌视觉的规范性及全局标准\",{\"1\":{\"521\":1}}],[\"产品和服务的数字化共享\",{\"1\":{\"519\":1}}],[\"产品和消费者之间的关系\",{\"1\":{\"39\":1}}],[\"产品和消费者的关系\",{\"1\":{\"33\":1}}],[\"产品生态系统设计的概念\",{\"1\":{\"519\":1}}],[\"产品部分帮助用户链接到现实世界\",{\"1\":{\"39\":1}}],[\"产品设计\",{\"1\":{\"4\":1}}],[\"下回可以再和大家分析这本书的内容\",{\"1\":{\"530\":1}}],[\"下层完成在相邻上层完成前\",{\"1\":{\"521\":1}}],[\"下面层决定上面层\",{\"1\":{\"521\":1}}],[\"下面是当时参加活动的两个作品\",{\"1\":{\"26\":1}}],[\"下图\",{\"1\":{\"29\":1}}],[\"基础目标\",{\"1\":{\"521\":1}}],[\"基础背景研究\",{\"0\":{\"78\":1}}],[\"基于初步的思考\",{\"1\":{\"83\":1}}],[\"基于儿童教育理论和技术运用提出的初步思考\",{\"0\":{\"81\":1}}],[\"基于前面已有的拼贴画面\",{\"1\":{\"43\":1}}],[\"基于构建的叙事世界\",{\"1\":{\"22\":1}}],[\"基本我都是围绕两个ip故事在创作和发布平台中\",{\"1\":{\"29\":1}}],[\"希望尽一点微薄之力给25\",{\"1\":{\"506\":1}}],[\"希望你们喜欢这里\",{\"1\":{\"46\":1}}],[\"希望未来大家认识的不仅仅是我\",{\"1\":{\"31\":1}}],[\"希望让大众能够客观认识和看待\",{\"1\":{\"29\":1}}],[\"希望在未来的数字时代\",{\"1\":{\"22\":1}}],[\"虚拟人or真实人照片\",{\"1\":{\"521\":1}}],[\"虚拟现实和儿童的研究是有限的\",{\"1\":{\"82\":1}}],[\"虚拟内容\",{\"1\":{\"81\":1}}],[\"虚拟服装\",{\"1\":{\"29\":1}}],[\"虚幻场景等等\",{\"1\":{\"29\":1}}],[\"虚实在不断变换\",{\"1\":{\"23\":1}}],[\"潮流\",{\"1\":{\"29\":1}}],[\"展示了空间音频和基于位置的寻路通知的强大功能\",{\"1\":{\"512\":1}}],[\"展示了这种爱好如何融入她的职业生涯\",{\"1\":{\"511\":1}}],[\"展会\",{\"1\":{\"91\":1}}],[\"展览不仅仅局限于画作外观观看\",{\"1\":{\"29\":1}}],[\"展翼\",{\"1\":{\"24\":1}}],[\"那么团队可能无法完全理解和接纳这个想法\",{\"1\":{\"535\":1}}],[\"那么多少岁的儿童适合创造力教育呢\",{\"1\":{\"78\":1}}],[\"那这个模型就散了\",{\"1\":{\"534\":1}}],[\"那这时候你需要的材料就更多了\",{\"1\":{\"509\":1}}],[\"那消费设计是什么呢\",{\"1\":{\"531\":1}}],[\"那不舒适的体验在娱乐\",{\"1\":{\"520\":1}}],[\"那我们在作品集里是不是也可以有这样一种思维去打造项目呢\",{\"1\":{\"510\":1}}],[\"那我觉得这所学校还是非常适合你去探索和实验的\",{\"1\":{\"509\":1}}],[\"那肯定一点价值都没有\",{\"1\":{\"509\":1}}],[\"那说了这么多\",{\"1\":{\"509\":1}}],[\"那具体艺术院校确实就读体验方面\",{\"1\":{\"509\":1}}],[\"那在结果层面来说\",{\"1\":{\"509\":1}}],[\"那就是回到最开始我说的\",{\"1\":{\"509\":1}}],[\"那你要想的是一个策略\",{\"1\":{\"509\":1}}],[\"那可能排名就是一个首要重点\",{\"1\":{\"508\":1}}],[\"那可是大自然的音乐\",{\"1\":{\"46\":1}}],[\"那为什么会选香港\",{\"1\":{\"508\":1}}],[\"那好吧\",{\"1\":{\"46\":1}}],[\"那艘船其他垃圾都会运到不同的岛上吗\",{\"1\":{\"46\":1}}],[\"那也需要建房子\",{\"1\":{\"46\":1}}],[\"那时候我们的课题需要寻找一些关于儿童教育的历史文献\",{\"1\":{\"43\":1}}],[\"那数字资产的价值才能真正凸显\",{\"1\":{\"42\":1}}],[\"那除了和nft\",{\"1\":{\"31\":1}}],[\"那个时候我也做了一个决定\",{\"1\":{\"30\":1}}],[\"那对于数字艺术家们该如何去选择未来发展的道路能呢\",{\"1\":{\"28\":1}}],[\"那段时间发的一些案例都得到比较好的反响\",{\"1\":{\"21\":1}}],[\"媒体的艺术盛会\",{\"1\":{\"28\":1}}],[\"爱好者\",{\"1\":{\"28\":1}}],[\"权威性\",{\"1\":{\"28\":1}}],[\"代表了我们与数字信息交互方式的重大转变\",{\"1\":{\"516\":1}}],[\"代表了我不同阶段的成长与探索\",{\"1\":{\"12\":1}}],[\"代表性\",{\"1\":{\"28\":1}}],[\"因此在后期做ar相关的儿童产品时候\",{\"1\":{\"89\":1}}],[\"因此它们的组织可能一次按形状\",{\"1\":{\"79\":1}}],[\"因此\",{\"1\":{\"36\":1,\"78\":1,\"535\":1}}],[\"因其丰富性\",{\"1\":{\"28\":1}}],[\"因为现实情况要混乱得多\",{\"1\":{\"538\":1}}],[\"因为现有技术和原型测试的限制\",{\"1\":{\"90\":1}}],[\"因为美院学生在当时对于一些审美要求的行业还是挺吃香\",{\"1\":{\"534\":1}}],[\"因为本身我选择了一个冒险的开题方式\",{\"1\":{\"534\":1}}],[\"因为涉及到数据的安全性和隐私保护\",{\"1\":{\"519\":1}}],[\"因为它们影响着全球设计生态系统中的每个人\",{\"1\":{\"538\":1}}],[\"因为它鼓励人们从不同的角度思考问题\",{\"1\":{\"533\":1}}],[\"因为它影响产品的功能\",{\"1\":{\"525\":1}}],[\"因为它允许用户专注于手头的任务\",{\"1\":{\"515\":1}}],[\"因为它涉及到社会基本层面\",{\"1\":{\"80\":1}}],[\"因为我们的收件箱和信息流中充满了令人震惊的头条新闻\",{\"1\":{\"538\":1}}],[\"因为我本身雅思并没有过关嘛\",{\"1\":{\"509\":1}}],[\"因为我找不到一个切入点去继续深化\",{\"1\":{\"43\":1}}],[\"因为这个行业\",{\"1\":{\"91\":1}}],[\"因为这短短一年多间\",{\"1\":{\"30\":1}}],[\"因为他害怕\",{\"1\":{\"46\":1}}],[\"因为很多时候线上办公\",{\"1\":{\"26\":1}}],[\"因为covid19的原因\",{\"1\":{\"26\":1}}],[\"因为在当时各种规范仍未完善\",{\"1\":{\"23\":1}}],[\"囊括了国内最具艺术才华\",{\"1\":{\"28\":1}}],[\"迄今已成功举办十五届博览会\",{\"1\":{\"28\":1}}],[\"全球范围内\",{\"1\":{\"34\":1}}],[\"全球创作者联盟\",{\"1\":{\"9\":1}}],[\"全称\",{\"1\":{\"28\":1}}],[\"全数字化的内容也在向虚拟世界迈进一步\",{\"1\":{\"27\":1}}],[\"同样\",{\"1\":{\"533\":1}}],[\"同样被当作垃圾运上船体前往某个地方\",{\"1\":{\"23\":1}}],[\"同比增长80\",{\"1\":{\"518\":1}}],[\"同时通过自动化和学习过程提高效率\",{\"1\":{\"539\":1}}],[\"同时需要能够用非技术人员的语言解释复杂的技术概念\",{\"1\":{\"535\":1}}],[\"同时在适当的时候坚持自己的观点\",{\"1\":{\"534\":1}}],[\"同时保持创新性\",{\"1\":{\"527\":1}}],[\"同时辛苦准备的申请最后都可能泡汤\",{\"1\":{\"523\":1}}],[\"同时用户的不同文化背景也可能对一个信息图产生不同的联想\",{\"1\":{\"521\":1}}],[\"同时也需保持对用户需求的敏锐洞察\",{\"1\":{\"527\":1}}],[\"同时也提升了品牌形象和社会责任感\",{\"1\":{\"520\":1}}],[\"同时也参加了一些活动展览\",{\"1\":{\"29\":1}}],[\"同时企业也需要转变商业模式\",{\"1\":{\"519\":1}}],[\"同时利用智能决策支持系统进行连续的决策制定和评估\",{\"1\":{\"519\":1}}],[\"同时\",{\"1\":{\"519\":2,\"520\":1,\"524\":1}}],[\"同时提升人类工作者的决策能力\",{\"1\":{\"518\":1}}],[\"同时感受到音效和环境声音\",{\"1\":{\"512\":1}}],[\"同时这也是一个人设的打造\",{\"1\":{\"510\":1}}],[\"同时这也是培养你对未来的思考\",{\"1\":{\"31\":1}}],[\"同时未来我也希望在多媒体行业有更多发展和找到更多人去往未来新领域去冲刺\",{\"1\":{\"507\":1}}],[\"同时借由废弃材料的媒介去发挥\",{\"1\":{\"74\":1,\"530\":1}}],[\"同时一些再造后的残渣也可以被一起向下运输\",{\"1\":{\"72\":1}}],[\"同时空间计算\",{\"1\":{\"31\":1}}],[\"同时web2生态中许多\",{\"1\":{\"31\":1}}],[\"同时对于大众来说\",{\"1\":{\"29\":1}}],[\"同时我们也是使用nft门票进行售卖希望给观展的用户留有区块链上的数字资产\",{\"1\":{\"29\":1}}],[\"同时虚拟人的使用也打破了杂志真人拍摄的传统\",{\"1\":{\"27\":1}}],[\"甚至\",{\"1\":{\"538\":1}}],[\"甚至是广狭义的拓展\",{\"1\":{\"521\":1}}],[\"甚至涉及到视觉版面内容\",{\"1\":{\"521\":1}}],[\"甚至也不包括提到的字母\",{\"1\":{\"510\":1}}],[\"甚至这些专业可能还与法律\",{\"1\":{\"510\":1}}],[\"甚至教授有时候都比你说得多\",{\"1\":{\"509\":1}}],[\"甚至怎么去组装\",{\"1\":{\"509\":1}}],[\"甚至企业合作\",{\"1\":{\"508\":1}}],[\"甚至对我来说设计之外的技能更加珍贵\",{\"1\":{\"508\":1}}],[\"甚至现在还加入了steam的概念\",{\"1\":{\"77\":1}}],[\"甚至被淘汰的东西\",{\"1\":{\"74\":1}}],[\"甚至疯狂得去攀附陡峭的岩石去登高\",{\"1\":{\"67\":1}}],[\"甚至经常使用一些奇怪的术语\",{\"1\":{\"46\":1}}],[\"甚至再创\",{\"1\":{\"27\":1}}],[\"甚至有一些准备毕业的同学也会询问各种问题\",{\"1\":{\"20\":1}}],[\"将会转向为更加系统化\",{\"1\":{\"537\":1}}],[\"将99部连接到google\",{\"1\":{\"533\":1}}],[\"将艺术融入技术进步不仅仅是为了美学\",{\"1\":{\"533\":1}}],[\"将客户置于业务战略的核心\",{\"1\":{\"525\":1}}],[\"将自己的特点挖掘\",{\"1\":{\"507\":1}}],[\"将游戏娱乐与人机交互相结合\",{\"1\":{\"500\":1}}],[\"将二者结合起来\",{\"1\":{\"81\":1}}],[\"将其视为独特的玩具\",{\"1\":{\"81\":1}}],[\"将他们收集在城市的某个地方\",{\"1\":{\"57\":1}}],[\"将迎来发展热潮\",{\"1\":{\"36\":1}}],[\"将消费\",{\"1\":{\"33\":1}}],[\"将恩利的虚拟人与我的数字艺术场景共创融合\",{\"1\":{\"27\":1}}],[\"将积分系统版本升级以及联合线上线下销售方式的推出\",{\"1\":{\"23\":1}}],[\"都是非常认可\",{\"1\":{\"537\":1}}],[\"都是在一点点为电影中的元宇宙世界做桥梁\",{\"1\":{\"26\":1}}],[\"都推辞了\",{\"1\":{\"534\":1}}],[\"都将更加注重用户体验\",{\"1\":{\"531\":1}}],[\"都有相关的项目经历\",{\"1\":{\"509\":1}}],[\"都停止发售了\",{\"1\":{\"23\":1}}],[\"此处用于优化电子邮件的主题行\",{\"1\":{\"539\":1}}],[\"此时\",{\"1\":{\"537\":1}}],[\"此时的\",{\"1\":{\"26\":1}}],[\"此外\",{\"1\":{\"23\":1,\"81\":1,\"516\":1,\"519\":7,\"520\":1,\"532\":1}}],[\"透过手机电脑去观看世界的种种\",{\"1\":{\"26\":1}}],[\"虽说是有种all\",{\"1\":{\"508\":1}}],[\"虽说世界混沌\",{\"1\":{\"23\":1}}],[\"虽不是真正的起源时间\",{\"1\":{\"30\":1}}],[\"虽然脱离了建筑景观行业\",{\"1\":{\"534\":1}}],[\"虽然毕业后算转行\",{\"1\":{\"507\":1}}],[\"虽然国内nft混乱的现状可能让许多人还无法相信和重新接受\",{\"1\":{\"30\":1}}],[\"虽然我当时的作品都是3d渲染而成的\",{\"1\":{\"28\":1}}],[\"虽然空间小了\",{\"1\":{\"26\":1}}],[\"共享经济是基于资源分享的经济社会体系\",{\"1\":{\"519\":1}}],[\"共创活动\",{\"1\":{\"40\":1}}],[\"共有35个创意空间\",{\"1\":{\"26\":1}}],[\"共同推进产品的开发和应用\",{\"1\":{\"520\":1}}],[\"共同探索孩子们的成长路径\",{\"1\":{\"17\":1}}],[\"共同制作数字艺术场景\",{\"1\":{\"9\":1}}],[\"等等\",{\"1\":{\"538\":1}}],[\"等公司正在拥抱\",{\"1\":{\"533\":1}}],[\"等人物倡导的概念\",{\"1\":{\"533\":1}}],[\"等人的观点\",{\"1\":{\"81\":1}}],[\"等人\",{\"1\":{\"81\":1}}],[\"等词\",{\"1\":{\"29\":1}}],[\"等待风来\",{\"1\":{\"24\":1}}],[\"等系列作品\",{\"1\":{\"22\":1}}],[\"领域\",{\"1\":{\"537\":1}}],[\"领导者正在努力应对他们的未来\",{\"1\":{\"538\":1}}],[\"领导者和员工将面临更多新挑战\",{\"1\":{\"518\":1}}],[\"领导力面临的挑战是如何在ai辅助下保持团队凝聚力\",{\"1\":{\"518\":1}}],[\"领导力\",{\"1\":{\"518\":1}}],[\"领导力和问题解决能力\",{\"1\":{\"518\":1}}],[\"领导并建立健全系统的现场协调流程\",{\"1\":{\"6\":1}}],[\"领略得失\",{\"1\":{\"24\":1}}],[\"万千风景的途中也该偶尔放慢脚步去自我思考\",{\"1\":{\"24\":1}}],[\"万元人民币\",{\"1\":{\"6\":1}}],[\"幻想和美好就在心中\",{\"1\":{\"24\":1}}],[\"坚持自我\",{\"1\":{\"24\":1}}],[\"四幅作品的四个主题代表了乘风破浪的心路经历\",{\"1\":{\"24\":1}}],[\"花映内心\",{\"1\":{\"24\":1}}],[\"花是梦的起点\",{\"1\":{\"24\":1}}],[\"去看待\",{\"1\":{\"534\":1}}],[\"去构建开放\",{\"1\":{\"518\":1}}],[\"去培养具有实际应用能力的工程师和设计师\",{\"1\":{\"510\":1}}],[\"去挖掘潜力\",{\"1\":{\"77\":1}}],[\"去再次构筑这片天地\",{\"1\":{\"69\":1}}],[\"去走入这些被遗弃物品的世界\",{\"1\":{\"63\":1}}],[\"去找到交叉点中最擅长的那部分\",{\"1\":{\"31\":1}}],[\"去大胆地享受和创造\",{\"1\":{\"24\":1}}],[\"去创造\",{\"1\":{\"24\":1,\"46\":1}}],[\"去描绘\",{\"1\":{\"24\":1}}],[\"去发展数字艺术ip板块\",{\"1\":{\"21\":1}}],[\"蒲公英\",{\"1\":{\"24\":1}}],[\"右图左一索伦森\",{\"1\":{\"48\":1}}],[\"右下\",{\"1\":{\"24\":1}}],[\"右上\",{\"1\":{\"24\":1}}],[\"紫苑花\",{\"1\":{\"24\":1}}],[\"左图艾伦夫人\",{\"1\":{\"48\":1}}],[\"左下\",{\"1\":{\"24\":1}}],[\"左上\",{\"1\":{\"24\":1}}],[\"野蘑菇\",{\"1\":{\"24\":1}}],[\"浴火中想要突破重生\",{\"1\":{\"24\":1}}],[\"浴火\",{\"1\":{\"24\":1}}],[\"可用性评估等\",{\"1\":{\"522\":1}}],[\"可持续的购物体验\",{\"1\":{\"531\":1}}],[\"可持续发展\",{\"1\":{\"531\":1}}],[\"可持续性设计等多个方面产生了深远影响\",{\"1\":{\"519\":1}}],[\"可持续设计\",{\"1\":{\"519\":1}}],[\"可能导致低转化率和客户流失\",{\"1\":{\"539\":1}}],[\"可能一张硕士文凭难以让我有飞跃性的提升\",{\"1\":{\"523\":1}}],[\"可能性非常多\",{\"1\":{\"510\":1}}],[\"可能是像服装\",{\"1\":{\"509\":1}}],[\"可能也是让我在当时自媒体创业前期能够扛得住的一部分原因\",{\"1\":{\"508\":1}}],[\"可重复使用的垃圾等\",{\"1\":{\"48\":1}}],[\"可是却一点也不让人觉得温暖\",{\"1\":{\"46\":1}}],[\"可是\",{\"1\":{\"46\":1}}],[\"可以打开更多方向\",{\"1\":{\"534\":1}}],[\"可以通过艺术思维来构建独特的品牌形象\",{\"1\":{\"533\":1}}],[\"可以增强设计的决策影响力\",{\"1\":{\"527\":1}}],[\"可以增强设计的影响力\",{\"1\":{\"527\":1}}],[\"可以帮助推动设计领域向更商业化方向发展\",{\"1\":{\"537\":1}}],[\"可以帮助创造出更有影响力的设计解决方案\",{\"1\":{\"525\":1}}],[\"可以帮助设计师从更广泛的角度考虑问题\",{\"1\":{\"522\":1}}],[\"可以采取公益模式\",{\"1\":{\"520\":1}}],[\"可以利用ai技术拓宽研究领域\",{\"1\":{\"518\":1}}],[\"可以着力提升ar技术的表现力\",{\"1\":{\"89\":1}}],[\"可以去走进不同的光怪陆离世界等等\",{\"1\":{\"45\":1}}],[\"可以聊聊关于\",{\"1\":{\"43\":1}}],[\"可以快速获得曝光被大众认知\",{\"1\":{\"23\":1}}],[\"可复制性太强\",{\"1\":{\"28\":1}}],[\"可谓是\",{\"1\":{\"23\":1}}],[\"官方数字藏品的创作邀请\",{\"1\":{\"23\":1}}],[\"经济学家和设计师开始训练人们成为顺从的消费者\",{\"1\":{\"531\":1}}],[\"经营者和用户的目标目的\",{\"1\":{\"521\":1}}],[\"经过这一层\",{\"1\":{\"523\":1}}],[\"经过一些了解我们也达成共识希望做一次工作坊实验来赋能儿童教育\",{\"1\":{\"80\":1}}],[\"经过访谈\",{\"1\":{\"23\":1}}],[\"经历\",{\"1\":{\"23\":1}}],[\"失败\",{\"1\":{\"23\":1}}],[\"市场也进入了冷却\",{\"1\":{\"23\":1}}],[\"市场营销为一体\",{\"1\":{\"23\":1}}],[\"收获\",{\"0\":{\"92\":1}}],[\"收藏家对于实体产品十分感兴趣\",{\"1\":{\"28\":1}}],[\"收藏家现在可以清楚地评估自己当前的积分情况\",{\"1\":{\"23\":1}}],[\"收集会员卡的过程非常冗长\",{\"1\":{\"23\":1}}],[\"发散思考\",{\"1\":{\"531\":1}}],[\"发起一个由13个机构组成的全球设计教育联盟\",{\"1\":{\"524\":1}}],[\"发起者竟是这些被人们抛弃丢弃的垃圾品\",{\"1\":{\"46\":1}}],[\"发现数字营销与消费设计都是现代营销策略中的两个关键组成部分\",{\"1\":{\"531\":1}}],[\"发现问题\",{\"1\":{\"524\":1,\"539\":1}}],[\"发现很多最后还不如新的ip势能来得猛\",{\"1\":{\"42\":1}}],[\"发现一些收藏家认为独特版本更新的速度不能跟上购买的步伐\",{\"1\":{\"23\":1}}],[\"发布\",{\"1\":{\"78\":1}}],[\"发布活动供用户选择的区域\",{\"1\":{\"40\":1}}],[\"发布一些自己设计的商业案例\",{\"1\":{\"21\":1}}],[\"并将信息分解成可消化的块\",{\"1\":{\"532\":1}}],[\"并将ar和创意发展的教育系统打造成更完整的游戏\",{\"1\":{\"90\":1}}],[\"并根据情况提供信息\",{\"1\":{\"532\":1}}],[\"并设计以抵消它们\",{\"1\":{\"532\":1}}],[\"并设计出能够适应这些变化的产品和服务\",{\"1\":{\"525\":1}}],[\"并结合消费设计理念\",{\"1\":{\"531\":1}}],[\"并结合数字化\",{\"1\":{\"519\":1}}],[\"并提供了它们的功能和定价信息\",{\"1\":{\"539\":1}}],[\"并提高设计项目的成功率\",{\"1\":{\"527\":1}}],[\"并提出了几个关键的解决方案\",{\"1\":{\"519\":1}}],[\"并提出了未来艺术与科技融合教育的可能性\",{\"1\":{\"76\":1}}],[\"并从客户的反馈中获得价值\",{\"1\":{\"525\":1}}],[\"并准确解释你为什么有顾虑\",{\"1\":{\"523\":1}}],[\"并确保设计能够满足不同用户的需求\",{\"1\":{\"522\":1}}],[\"并强调教育应该培养未来的工作力\",{\"1\":{\"519\":1}}],[\"并融入社会利益\",{\"1\":{\"519\":1}}],[\"并探讨在线教育\",{\"1\":{\"519\":1}}],[\"并在行业中保持竞争力\",{\"1\":{\"527\":1}}],[\"并在服务化产品设计\",{\"1\":{\"519\":1}}],[\"并在现场对我的作品进行解读和采访\",{\"1\":{\"43\":1}}],[\"并可能通过订阅费实现价值捕获\",{\"1\":{\"519\":1}}],[\"并协调物理流\",{\"1\":{\"519\":1}}],[\"并安全地集成机器人于生产线\",{\"1\":{\"519\":1}}],[\"并通过实验来优化业务模型\",{\"1\":{\"525\":1}}],[\"并通过对非技术能力的要求来适应数字化工作场所的变化\",{\"1\":{\"519\":1}}],[\"并通过数字双生\",{\"1\":{\"519\":1}}],[\"并通过未来产品来补充教育游戏\",{\"1\":{\"88\":1}}],[\"并利用大数据\",{\"1\":{\"519\":1}}],[\"并利用多种媒体\",{\"1\":{\"31\":1}}],[\"并表明音频\",{\"1\":{\"512\":1}}],[\"并创造出富有细节和沉浸感的虚拟世界\",{\"1\":{\"511\":1}}],[\"并不是非得是用上才能表达这个理念\",{\"1\":{\"510\":1}}],[\"并不断推出新的特别课程\",{\"1\":{\"80\":1}}],[\"并思考未来的商业形式\",{\"1\":{\"500\":1}}],[\"并反思人类欲望过度所带来的影响\",{\"1\":{\"500\":1}}],[\"并更积极地参与教育游戏\",{\"1\":{\"90\":1}}],[\"并与主办方倪昆老师\",{\"1\":{\"80\":1}}],[\"并于2006年提出了steam教育的概念\",{\"1\":{\"78\":1}}],[\"并达到\",{\"1\":{\"48\":1}}],[\"并对\",{\"1\":{\"48\":1}}],[\"并且更少受到设备外形的限制\",{\"1\":{\"516\":1}}],[\"并且可以成为创造游戏的灵感来源\",{\"1\":{\"511\":1}}],[\"并且不是唯一\",{\"1\":{\"43\":1}}],[\"并且和藏家进行沟通发布理念甚至共创\",{\"1\":{\"23\":1}}],[\"并没有那么繁杂的约束\",{\"1\":{\"43\":1}}],[\"并获得了ip的曝光和线上合作机会\",{\"1\":{\"31\":1}}],[\"并作为lifestyle\",{\"1\":{\"27\":1}}],[\"并作为\",{\"1\":{\"9\":1}}],[\"并作为空间负责人参与数字艺术共创\",{\"1\":{\"9\":1}}],[\"所包含的信息详细度\",{\"1\":{\"521\":1}}],[\"所有这些指标都显示出了全面增长\",{\"1\":{\"34\":1}}],[\"所有作品都需要自己管理和运营\",{\"1\":{\"23\":1}}],[\"所以从开题到最后的成果\",{\"1\":{\"534\":1}}],[\"所以平时有同学来问我们这个问题的时候\",{\"1\":{\"509\":1}}],[\"所以选择了交互这个大类的留学专业\",{\"1\":{\"507\":1}}],[\"所以也锻炼我快速从概念到执行的能力\",{\"1\":{\"92\":1}}],[\"所以也让我思考了实体和数字产品的关系\",{\"1\":{\"28\":1}}],[\"所以\",{\"1\":{\"31\":1,\"510\":1}}],[\"所以被命名为雪橇熊\",{\"1\":{\"23\":1}}],[\"所以在这个节点上\",{\"1\":{\"21\":1}}],[\"所以我基本抓住这一点\",{\"1\":{\"534\":1}}],[\"所以我基于此提出了一个利用抽象与具象\",{\"1\":{\"82\":1}}],[\"所以我先用两个不同角度来和大家分享\",{\"1\":{\"509\":1}}],[\"所以我对于留学专业的选择也一定是交叉多元\",{\"1\":{\"508\":1}}],[\"所以我也将年龄群体对准在这个阶段的儿童\",{\"1\":{\"79\":1}}],[\"所以我也希望可以在此方向去深入探索\",{\"1\":{\"78\":1}}],[\"所以我也希望将这部分思考慢慢传授给大家\",{\"1\":{\"31\":1}}],[\"所以我用了一副线稿的剖面图来开始形成我的整个故事逻辑\",{\"1\":{\"43\":1}}],[\"所以我需要重新思考未来商业结构\",{\"1\":{\"30\":1}}],[\"所以我认为数字化产品的确是非常有价值的\",{\"1\":{\"26\":1}}],[\"所以我想试着把这个风格结合进我的账号内容中\",{\"1\":{\"20\":1}}],[\"所以我打算深度分析我的一些过往作品\",{\"1\":{\"20\":1}}],[\"熊的特征非常明显\",{\"1\":{\"23\":1}}],[\"尾巴是松鼠和浣熊的结合体\",{\"1\":{\"23\":1}}],[\"它限制了创新的潜力\",{\"1\":{\"535\":1}}],[\"它是关于询问而不是解决\",{\"1\":{\"533\":1}}],[\"它是我的作品中最美好的\",{\"1\":{\"50\":1}}],[\"它消耗的能量较多\",{\"1\":{\"532\":1}}],[\"它自动\",{\"1\":{\"532\":1}}],[\"它帮助我们理解人们在现实世界中的行为\",{\"1\":{\"532\":1}}],[\"它帮助识别不同类型的创造力\",{\"1\":{\"524\":1}}],[\"它包括一系列旨在操纵消费者购买决策的策略和方法\",{\"1\":{\"531\":1}}],[\"它并非意味着ai取代设计师的工作\",{\"1\":{\"527\":1}}],[\"它结合了对业务建模的深刻理解以加速实验\",{\"1\":{\"525\":1}}],[\"它不仅关注产品设计\",{\"1\":{\"522\":1}}],[\"它关注的是产品的用户如何与产品互动\",{\"1\":{\"522\":1}}],[\"它通过集成设计信息和模型\",{\"1\":{\"519\":1}}],[\"它通过众包和数字平台驱动创新\",{\"1\":{\"519\":1}}],[\"它还强调可持续性和设计的循环经济思维\",{\"1\":{\"519\":1}}],[\"它反映了现代设计中的数据分析和应用\",{\"1\":{\"519\":1}}],[\"它涉及到动态的产品单元和用户互动\",{\"1\":{\"519\":1}}],[\"它能够为观众或玩家带来更丰富的感官体验\",{\"1\":{\"517\":1}}],[\"它有可能让技术感觉更少侵入性\",{\"1\":{\"516\":1}}],[\"它有熊一样的头\",{\"1\":{\"23\":1}}],[\"它太稀疏了\",{\"1\":{\"512\":1}}],[\"它会产生压倒性的刺耳声音\",{\"1\":{\"512\":1}}],[\"它展示了使用空间音频和基于位置的警报来增强现实的潜力\",{\"1\":{\"512\":1}}],[\"它将音频信息与现实环境相结合\",{\"1\":{\"512\":1}}],[\"它将被打开\",{\"1\":{\"59\":1}}],[\"它应该是有客观依据\",{\"1\":{\"509\":1}}],[\"它强调多重角色扮演身份\",{\"1\":{\"84\":1}}],[\"它绽放的不仅仅是与高楼媲美的靓丽\",{\"1\":{\"73\":1}}],[\"它被许多怪异的东西附着拼撞组合\",{\"1\":{\"66\":1}}],[\"它产生着这种隔离\",{\"1\":{\"59\":1}}],[\"它的城市发展多元性也不会落后\",{\"1\":{\"43\":1}}],[\"它可以涵盖很多当下需要我们去思考和关注的地方\",{\"1\":{\"43\":1}}],[\"它仍然保持着积极的增长态势\",{\"1\":{\"34\":1}}],[\"它们能够快速生成高质量内容\",{\"1\":{\"539\":1}}],[\"它们能变成我想要的形状吗\",{\"1\":{\"87\":1}}],[\"它们还能进行a\",{\"1\":{\"539\":1}}],[\"它们在推动销售和品牌建设方面发挥着重要作用\",{\"1\":{\"531\":1}}],[\"它们在生长\",{\"1\":{\"24\":1}}],[\"它们都强调用户的需求和体验\",{\"1\":{\"522\":1}}],[\"它们包括无法分心\",{\"1\":{\"79\":1}}],[\"它们身上是许多城市故事的残留\",{\"1\":{\"72\":1}}],[\"它们是自然的味道\",{\"1\":{\"71\":1}}],[\"它们两者有着直接的联系但并不是需要我去完全还原\",{\"1\":{\"43\":1}}],[\"它们将会是新光明能量的助力者\",{\"1\":{\"23\":1}}],[\"故事讲述和用户想象力的融合\",{\"1\":{\"81\":1}}],[\"故事中\",{\"1\":{\"23\":1}}],[\"故事背景\",{\"1\":{\"23\":1}}],[\"于是也联合机构开展了这个工作坊\",{\"1\":{\"77\":1}}],[\"于是\",{\"1\":{\"58\":1}}],[\"于是被停止\",{\"1\":{\"46\":1}}],[\"于是我也去调研了一些有关案例来探索web3模式下的商业可能性\",{\"1\":{\"37\":1}}],[\"于是准备自己创办工作室承接项目\",{\"1\":{\"21\":1}}],[\"于2022年3月至5月间以数字ip形式在nftcn平台独家发行\",{\"1\":{\"23\":1}}],[\"光能量注入实验室的各个机器里\",{\"1\":{\"23\":1}}],[\"但团队的反应却像是听天书\",{\"1\":{\"535\":1}}],[\"但还是和空间设计相关\",{\"1\":{\"534\":1}}],[\"但他现在开始质疑世界对记忆的依赖\",{\"1\":{\"533\":1}}],[\"但数据的融入为设计带来了新的机遇\",{\"1\":{\"527\":1}}],[\"但商业工程师更进一步\",{\"1\":{\"525\":1}}],[\"但学生对自身创造力的信心却不足\",{\"1\":{\"524\":1}}],[\"但在自信心方面存在不足\",{\"1\":{\"524\":1}}],[\"但根据数据\",{\"1\":{\"524\":1}}],[\"但它们的焦点和方法有所不同\",{\"1\":{\"522\":1}}],[\"但实际上那里并没有人\",{\"1\":{\"533\":1}}],[\"但实际上\",{\"1\":{\"522\":1}}],[\"但实现起来也面临一些挑战\",{\"1\":{\"516\":1}}],[\"但其实施需要解决软件和硬件的集成问题\",{\"1\":{\"519\":1}}],[\"但另一面思考却也可以加强跨职能团队协作\",{\"1\":{\"518\":1}}],[\"但同时也提供了新的服务模式\",{\"1\":{\"518\":1}}],[\"但同时强调早期培养儿童的创造力和抽象思维\",{\"1\":{\"80\":1}}],[\"但软技能如沟通\",{\"1\":{\"518\":1}}],[\"但要实现这一目标\",{\"1\":{\"516\":1}}],[\"但我认为这仍是一个机会\",{\"1\":{\"537\":1}}],[\"但我依然选择在这个时候多去争取不同地区和学校专业面试和录取的机会\",{\"1\":{\"508\":1}}],[\"但我也经常告诉同学们\",{\"1\":{\"31\":1}}],[\"但目前对于我来说\",{\"1\":{\"508\":1}}],[\"但也是和空间设计相关\",{\"1\":{\"507\":1}}],[\"但也只有大家的尝试才能让行业未来良性发展起来\",{\"1\":{\"30\":1}}],[\"但对我已经工作了几年的人来说确实是这样\",{\"1\":{\"507\":1}}],[\"但建议年幼儿童不要使用该产品\",{\"1\":{\"81\":1}}],[\"但不能以一致的方式进行\",{\"1\":{\"79\":1}}],[\"但这波悲观情绪只是一种噪音\",{\"1\":{\"538\":1}}],[\"但这些历史也表明这样一个地方就像是孩子们的精神乌托邦\",{\"1\":{\"74\":1}}],[\"但这个船长仍然孜孜不倦地每天重复着开往港口又离开\",{\"1\":{\"46\":1}}],[\"但或许一种逃离背后也藏着这样的思考\",{\"1\":{\"73\":1}}],[\"但最后我还是选择了极致地去表达作品\",{\"1\":{\"43\":1}}],[\"但保持学习探索\",{\"1\":{\"31\":1}}],[\"但是当时还是处在一个阶层难以突破\",{\"1\":{\"523\":1}}],[\"但是全球也不乏有许多优秀的艺术学院\",{\"1\":{\"509\":1}}],[\"但是实际上我们在制作作品集的时候\",{\"1\":{\"509\":1}}],[\"但是我们可以通过设计师的思维来给予观众反思的空间\",{\"1\":{\"530\":1}}],[\"但是我们还是集合了不同的团队合作方努力做了一次尝试\",{\"1\":{\"29\":1}}],[\"但是我们还是希望透过一些联动实体艺术和数字艺术的方式来尝试售卖\",{\"1\":{\"28\":1}}],[\"但是我还是想通过更体系化的学习帮助在该领域深造\",{\"1\":{\"507\":1}}],[\"但是只要你不断保持对新事物的思考\",{\"1\":{\"507\":1}}],[\"但是这种畅想也是一种对孩子们内心世界的回应\",{\"1\":{\"74\":1,\"530\":1}}],[\"但是同时孩子们被越加过度保护\",{\"1\":{\"74\":1}}],[\"但是船长不会带他们过来的\",{\"1\":{\"46\":1}}],[\"但是精神仍然不可缺失\",{\"1\":{\"43\":1}}],[\"但是通过参加这次展览\",{\"1\":{\"43\":1}}],[\"但是它是触发内心感性的一面\",{\"1\":{\"43\":1}}],[\"但是过度金融化的模式把真正的价值掩盖\",{\"1\":{\"33\":1}}],[\"但是nft与当下web2的社交媒体其实是有一定隔阂的\",{\"1\":{\"31\":1}}],[\"但是探索者是必须存在的\",{\"1\":{\"30\":1}}],[\"但是在金史密斯学院拓展实践项目的面试中就感受到他们独特的个性在\",{\"1\":{\"509\":1}}],[\"但是在他建造的游乐场里\",{\"1\":{\"48\":1}}],[\"但是在ai时代许多不太懂的观众会对作品生成的工具产生疑问\",{\"1\":{\"28\":1}}],[\"但是在当时大家出行困难的时期\",{\"1\":{\"26\":1}}],[\"但是也确实让一些内容创作者得到良性的发展机会\",{\"1\":{\"23\":1}}],[\"但是也通过各种平台快速的到曝光\",{\"1\":{\"23\":1}}],[\"但是\",{\"1\":{\"23\":1,\"31\":1,\"77\":1}}],[\"但是基因可以被提取可以进化\",{\"1\":{\"23\":1}}],[\"但没有熊的脚\",{\"1\":{\"23\":1}}],[\"但有黑暗就有光明\",{\"1\":{\"23\":1}}],[\"生意\",{\"1\":{\"534\":1}}],[\"生命周期管理以及供应链效率\",{\"1\":{\"519\":1}}],[\"生日花\",{\"1\":{\"24\":1}}],[\"生灵无法死而复生\",{\"1\":{\"23\":1}}],[\"生活时尚\",{\"1\":{\"9\":1}}],[\"生活美学与创意\",{\"1\":{\"5\":1}}],[\"以适应不同用户的需求\",{\"1\":{\"539\":1}}],[\"以适应快速变化的市场环境和技术发展\",{\"1\":{\"519\":1}}],[\"以上都是关于过程的案例研究感兴趣\",{\"1\":{\"538\":1}}],[\"以一个策略\",{\"1\":{\"537\":1}}],[\"以数据驱动的策略\",{\"1\":{\"537\":1}}],[\"以往的设计创新可能更多地依赖于个人技能和创意\",{\"1\":{\"537\":1}}],[\"以前有时候我做的工作很杂\",{\"1\":{\"533\":1}}],[\"以前就是一个岛\",{\"1\":{\"46\":1}}],[\"以防止用户做出非知情决策\",{\"1\":{\"532\":1}}],[\"以下是一些将决策心理学应用于设计的策略\",{\"1\":{\"532\":1}}],[\"以下正文\",{\"1\":{\"46\":1}}],[\"以支持决策\",{\"1\":{\"527\":1}}],[\"以客户为中心\",{\"1\":{\"525\":2}}],[\"以构建封闭的反馈循环\",{\"1\":{\"525\":1}}],[\"以实现更高效的营销效果\",{\"1\":{\"531\":1}}],[\"以实现系统化的方法\",{\"1\":{\"524\":1}}],[\"以实现动态集成制造创新\",{\"1\":{\"519\":1}}],[\"以提升文化中的创新行为规范\",{\"1\":{\"524\":1}}],[\"以提升团队凝聚力和员工的心理承受力\",{\"1\":{\"520\":1}}],[\"以重塑设计教育\",{\"1\":{\"524\":1}}],[\"以重新的定义方式呈现出来\",{\"1\":{\"23\":1}}],[\"以我亲身经历为例子\",{\"1\":{\"523\":1}}],[\"以解决当前存在的\",{\"1\":{\"522\":1}}],[\"以解决玩家们在购买早期独特版时遇到的缓慢和困难的问题\",{\"1\":{\"23\":1}}],[\"以人为中心的设计\",{\"1\":{\"522\":2}}],[\"以信息为导向\",{\"1\":{\"521\":1}}],[\"以任务为导向\",{\"1\":{\"521\":1}}],[\"以历史记录来说\",{\"1\":{\"521\":1}}],[\"以社会利益为导向\",{\"1\":{\"519\":1}}],[\"以保持职业竞争力\",{\"1\":{\"518\":1}}],[\"以便从多个角度审视设计\",{\"1\":{\"532\":1}}],[\"以便更好地将设计与商业目标相结合\",{\"1\":{\"525\":1}}],[\"以便在正确的时间提供正确的信息\",{\"1\":{\"516\":1}}],[\"以便音频可以与周围环境融合\",{\"1\":{\"516\":1}}],[\"以避免干扰或噪音\",{\"1\":{\"516\":1}}],[\"以确保设备之间的兼容性和内容的无缝传输\",{\"1\":{\"516\":1}}],[\"以后有机会会再分享\",{\"1\":{\"507\":1}}],[\"以用户为中心的产品设计\",{\"1\":{\"502\":1}}],[\"以建立更新的概念\",{\"1\":{\"90\":1}}],[\"以至于用户更喜欢虚拟世界而不是现实世界\",{\"1\":{\"81\":1}}],[\"以至于他们能在这里被释放内心\",{\"1\":{\"74\":1}}],[\"以激发儿童对艺术的兴趣\",{\"1\":{\"80\":1}}],[\"以两段叙事文本结合做出一个乌托邦式的畅想来进行讨论\",{\"1\":{\"74\":1}}],[\"以废弃材料为媒介作为切入\",{\"1\":{\"45\":1}}],[\"以增强购买欲望\",{\"1\":{\"23\":1}}],[\"以及设计师\",{\"1\":{\"538\":1}}],[\"以及可以稍后了解的信息\",{\"1\":{\"532\":1}}],[\"以及新技术如何影响成本结构和用户体验\",{\"1\":{\"525\":1}}],[\"以及它如何与其他业务元素相互作用\",{\"1\":{\"525\":1}}],[\"以及使用创意系统画布工具\",{\"1\":{\"524\":1}}],[\"以及你可以从哪里获得这些信息\",{\"1\":{\"523\":1}}],[\"以及用户在使用过程中的整体体验\",{\"1\":{\"522\":1}}],[\"以及到模块\",{\"1\":{\"521\":1}}],[\"以及应用生物测量设备进行用户体验分析\",{\"1\":{\"519\":1}}],[\"以及在本文中的创意思考能力\",{\"1\":{\"524\":1}}],[\"以及在设计和制造过程中的情绪预测等具体议题\",{\"1\":{\"519\":1}}],[\"以及在辞职后作为个人设计师创业的项目经历\",{\"1\":{\"91\":1}}],[\"以及如何将这些知识应用于用户体验设计\",{\"1\":{\"532\":1}}],[\"以及如何在现实世界中测试和验证这些技术的假设\",{\"1\":{\"525\":1}}],[\"以及如何复制或改进这些关键元素\",{\"1\":{\"525\":1}}],[\"以及如何利用技术如大数据分析来提升决策效率和质量保证\",{\"1\":{\"519\":1}}],[\"以及如何利用这些技术带来的数据以实现更快速的响应\",{\"1\":{\"519\":1}}],[\"以及如何应对网络安全威胁的问题\",{\"1\":{\"519\":1}}],[\"以及如何控制在客户预算内进行设计\",{\"1\":{\"92\":1}}],[\"以及对实时事件数据的利用\",{\"1\":{\"519\":1}}],[\"以及利用数据科学方法进行知识合成和集成任务\",{\"1\":{\"519\":1}}],[\"以及利用数据科学方法来管理和风险评估\",{\"1\":{\"519\":1}}],[\"以及通过数据分析优化产品和服务\",{\"1\":{\"519\":1}}],[\"以及网络安全等问题\",{\"1\":{\"519\":1}}],[\"以及怎么配合和指导团队完成项目\",{\"1\":{\"92\":1}}],[\"以及任何生活中的日常材料都可以应用到空间置景中\",{\"1\":{\"92\":1}}],[\"以及学校里的人\",{\"1\":{\"83\":1}}],[\"以及科技产品对儿童的影响\",{\"1\":{\"80\":1}}],[\"以及基于我对cbbe模型的研究和分析\",{\"1\":{\"37\":1}}],[\"以及这种虚实结合的方式是否可以成为变现方式\",{\"1\":{\"28\":1}}],[\"以及小红书官方第二届社区熟人节的数字艺术主理人板块邀请\",{\"1\":{\"23\":1}}],[\"以及团队能力不足\",{\"1\":{\"23\":1}}],[\"以及接下来账号我该如何运营\",{\"1\":{\"21\":1}}],[\"以及部分设计项目策划和概念\",{\"1\":{\"21\":1}}],[\"就会发现这很困难\",{\"1\":{\"538\":1}}],[\"就像mbti人格一样\",{\"1\":{\"510\":1}}],[\"就像我参加的真浪数字艺术团队数字艺术馆合作\",{\"1\":{\"30\":1}}],[\"就比如伦敦大学金史密斯学院\",{\"1\":{\"509\":1}}],[\"就如有些同学未来希望从事编制\",{\"1\":{\"508\":1}}],[\"就如当时垃圾游乐场上的孩子一样\",{\"1\":{\"43\":1}}],[\"就可能会产生很多灵感爆炸\",{\"1\":{\"507\":1}}],[\"就和我经常提到的\",{\"1\":{\"31\":1}}],[\"就是你需要转换视角\",{\"1\":{\"535\":1}}],[\"就是留学读研\",{\"1\":{\"30\":1}}],[\"就是如何通过自媒体变现\",{\"1\":{\"20\":1}}],[\"就这样形成\",{\"1\":{\"23\":1}}],[\"被行业鞭策了几个月\",{\"1\":{\"534\":1}}],[\"被称为\",{\"1\":{\"48\":1}}],[\"被挂满各种齿轮\",{\"1\":{\"46\":1}}],[\"被送往这里\",{\"1\":{\"23\":1}}],[\"被中国时尚杂志\",{\"1\":{\"9\":1}}],[\"暗物质和光能量在矛盾体中不断对抗\",{\"1\":{\"23\":1}}],[\"细胞开始变异\",{\"1\":{\"23\":1}}],[\"时代变革太快\",{\"1\":{\"533\":1}}],[\"时间长了\",{\"1\":{\"521\":1}}],[\"时尚\",{\"1\":{\"521\":1}}],[\"时尚杂志商业策划等\",{\"1\":{\"21\":1}}],[\"时空出现裂缝\",{\"1\":{\"23\":1}}],[\"资源环境角度\",{\"1\":{\"534\":1}}],[\"资源消耗过度\",{\"1\":{\"23\":1}}],[\"资本加持\",{\"1\":{\"78\":1}}],[\"资讯\",{\"1\":{\"42\":1}}],[\"资格与技能\",{\"0\":{\"8\":1}}],[\"2️⃣\",{\"1\":{\"531\":2}}],[\"2️⃣数据不是取代设计师\",{\"1\":{\"527\":1}}],[\"2x\",{\"1\":{\"469\":1}}],[\"2b\",{\"1\":{\"377\":2}}],[\"2d\",{\"0\":{\"366\":1},\"1\":{\"407\":1}}],[\"2k\",{\"1\":{\"337\":1,\"412\":1}}],[\"2nd\",{\"1\":{\"325\":1,\"351\":1}}],[\"27\",{\"0\":{\"299\":1,\"484\":1},\"1\":{\"360\":1}}],[\"25$\",{\"1\":{\"427\":1}}],[\"25\",{\"1\":{\"293\":1,\"462\":1}}],[\"29\",{\"0\":{\"276\":1,\"464\":1},\"1\":{\"456\":1}}],[\"23\",{\"1\":{\"254\":1,\"302\":1,\"416\":1}}],[\"236\",{\"1\":{\"156\":1}}],[\"28\",{\"0\":{\"285\":1,\"473\":1},\"1\":{\"222\":1,\"339\":1,\"412\":1}}],[\"26fall申请的同学一点方向指引\",{\"1\":{\"506\":1}}],[\"264\",{\"1\":{\"351\":1}}],[\"26\",{\"0\":{\"493\":1},\"1\":{\"210\":1,\"344\":1}}],[\"2199\",{\"1\":{\"530\":1}}],[\"2199年\",{\"1\":{\"46\":1}}],[\"2199年间\",{\"1\":{\"23\":1}}],[\"21\",{\"1\":{\"175\":1,\"207\":1,\"214\":1,\"222\":1,\"362\":1,\"363\":1,\"365\":1,\"419\":1,\"420\":1}}],[\"24fall进入尾声了\",{\"1\":{\"506\":1}}],[\"249\",{\"1\":{\"369\":1}}],[\"2403\",{\"1\":{\"281\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"2404\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"282\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1}}],[\"248\",{\"1\":{\"149\":1}}],[\"24\",{\"1\":{\"106\":1,\"386\":1,\"409\":1,\"439\":1}}],[\"22mm\",{\"1\":{\"295\":1}}],[\"22\",{\"1\":{\"74\":1,\"190\":1,\"222\":1,\"232\":1,\"360\":2,\"393\":1}}],[\"2\",{\"0\":{\"372\":1,\"508\":1,\"515\":1},\"1\":{\"74\":1,\"101\":1,\"104\":1,\"133\":1,\"148\":1,\"151\":1,\"156\":1,\"179\":1,\"187\":1,\"188\":1,\"197\":1,\"199\":1,\"210\":1,\"231\":1,\"235\":1,\"246\":1,\"248\":1,\"261\":1,\"269\":1,\"271\":1,\"284\":1,\"298\":2,\"304\":1,\"308\":1,\"313\":1,\"319\":1,\"325\":1,\"328\":1,\"331\":1,\"334\":1,\"340\":1,\"348\":1,\"353\":2,\"355\":1,\"362\":1,\"365\":2,\"366\":2,\"369\":2,\"372\":1,\"378\":1,\"391\":2,\"409\":1,\"411\":1,\"418\":1,\"427\":1,\"438\":1,\"447\":2,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"461\":1,\"462\":2,\"470\":1,\"478\":1,\"481\":1,\"486\":4,\"487\":1,\"492\":1,\"495\":1,\"497\":1,\"521\":1}}],[\"20k\",{\"1\":{\"412\":2,\"434\":1}}],[\"20630\",{\"1\":{\"307\":1}}],[\"20306v1\",{\"1\":{\"465\":1}}],[\"2030\",{\"1\":{\"216\":1,\"239\":1}}],[\"20046v1\",{\"1\":{\"470\":1}}],[\"20097v1\",{\"1\":{\"283\":1}}],[\"200\",{\"1\":{\"159\":1,\"313\":1,\"365\":1,\"377\":1}}],[\"2001\",{\"1\":{\"81\":1}}],[\"200多位小红书作者\",{\"1\":{\"26\":1}}],[\"20\",{\"1\":{\"49\":1,\"197\":1,\"210\":1,\"214\":2,\"252\":1,\"265\":1,\"323\":1,\"333\":1,\"361\":1,\"393\":1,\"400\":1,\"426\":1,\"453\":1}}],[\"20252v1\",{\"1\":{\"468\":1}}],[\"2025年7月香港理工大学\",{\"1\":{\"2\":1}}],[\"20279v1\",{\"1\":{\"467\":1}}],[\"20288v1\",{\"1\":{\"466\":1}}],[\"20246v1\",{\"1\":{\"281\":1}}],[\"2024\",{\"0\":{\"95\":1,\"109\":1,\"134\":1,\"145\":1,\"147\":1,\"162\":1,\"168\":1,\"177\":1,\"193\":1,\"208\":1,\"225\":1,\"247\":1,\"262\":1,\"267\":1,\"276\":1,\"285\":1,\"299\":1,\"311\":1,\"320\":1,\"332\":1,\"346\":1,\"351\":1,\"352\":1,\"357\":1,\"367\":1,\"372\":1,\"373\":1,\"383\":1,\"388\":1,\"397\":1,\"404\":1,\"406\":1,\"421\":1,\"439\":1,\"440\":1,\"451\":1,\"464\":1,\"473\":1,\"484\":1,\"493\":1,\"518\":1},\"1\":{\"147\":1,\"325\":1,\"352\":1,\"372\":1,\"388\":1,\"404\":1,\"428\":1,\"502\":1}}],[\"2024年9月\",{\"1\":{\"2\":1}}],[\"2021年o\",{\"1\":{\"80\":1}}],[\"2021年nft热潮席卷全球\",{\"1\":{\"33\":1}}],[\"2021年\",{\"1\":{\"20\":1,\"43\":1}}],[\"2021\",{\"1\":{\"9\":2,\"36\":1,\"215\":1,\"271\":1,\"457\":1}}],[\"2023c\",{\"1\":{\"404\":1}}],[\"2023年参加的重要展览和活动\",{\"1\":{\"31\":1}}],[\"2023总结回顾\",{\"0\":{\"30\":1}}],[\"2023广州设计周合作大艺博联名参展\",{\"0\":{\"28\":1}}],[\"2023\",{\"0\":{\"29\":1},\"1\":{\"7\":2,\"9\":4,\"42\":1,\"126\":1,\"351\":1}}],[\"2020年\",{\"1\":{\"34\":1}}],[\"2020\",{\"1\":{\"6\":3,\"307\":2,\"428\":1,\"492\":1,\"536\":1}}],[\"2022就像是元宇宙\",{\"1\":{\"30\":1}}],[\"2022年yq期间\",{\"1\":{\"536\":1}}],[\"2022年7月刊电子封面展示\",{\"1\":{\"27\":1}}],[\"2022年正式诞生了\",{\"1\":{\"22\":1}}],[\"2022年\",{\"1\":{\"14\":1,\"15\":1}}],[\"2022年11月\",{\"1\":{\"4\":1}}],[\"2022\",{\"0\":{\"30\":1},\"1\":{\"4\":1,\"6\":1,\"9\":7,\"492\":1}}],[\"20137v1\",{\"1\":{\"469\":1}}],[\"2013\",{\"1\":{\"81\":1}}],[\"2014年一月\",{\"1\":{\"78\":1}}],[\"2014年前后\",{\"1\":{\"78\":1}}],[\"2015年起\",{\"1\":{\"78\":1}}],[\"2015年6月\",{\"1\":{\"78\":1}}],[\"2015年9月\",{\"1\":{\"2\":1}}],[\"2015\",{\"1\":{\"36\":1,\"81\":1}}],[\"2012年由广州华艺大艺文化艺术发展有限公司创办于年广州\",{\"1\":{\"28\":1}}],[\"2018\",{\"1\":{\"6\":1,\"7\":1}}],[\"2019未来展\",{\"1\":{\"7\":1}}],[\"2019年yq前ux用户体验行业开始越发成熟并成为主流\",{\"1\":{\"536\":1}}],[\"2019年左右\",{\"1\":{\"78\":1}}],[\"2019年\",{\"1\":{\"7\":2,\"530\":1}}],[\"2019年6月广州美术学院\",{\"1\":{\"2\":1}}],[\"2019\",{\"1\":{\"6\":1,\"7\":3,\"126\":1,\"234\":1}}],[\"延续这样的故事背景\",{\"1\":{\"23\":1}}],[\"错过船的孩子似乎便无法再进入这个重生的世界里\",{\"1\":{\"23\":1,\"46\":1}}],[\"最近看到一篇论文叫做design\",{\"1\":{\"510\":1}}],[\"最大化去告诉招生官\",{\"1\":{\"508\":1}}],[\"最大的原因可能是中间产生了对结果没有把握的质疑心理\",{\"1\":{\"43\":1}}],[\"最真实的你\",{\"1\":{\"508\":1}}],[\"最初的stem教育只关注项目本身\",{\"1\":{\"78\":1}}],[\"最后也很幸运成为了优秀作品被学院留存在教学楼\",{\"1\":{\"534\":1}}],[\"最后形成你的个体空间\",{\"1\":{\"509\":1}}],[\"最后\",{\"1\":{\"74\":1,\"519\":2}}],[\"最后它们也被带走离开被城市的处理厂消化\",{\"1\":{\"72\":1}}],[\"最后是孩子们的创造天地\",{\"1\":{\"58\":1}}],[\"最后才形成一个有三维立体化的创作场景\",{\"1\":{\"43\":1}}],[\"最后的结尾留白给人想象\",{\"1\":{\"23\":1}}],[\"最开始引发这个毕设主题的来源是那段垃圾游乐场的历史故事\",{\"1\":{\"43\":1}}],[\"最具价值的部分\",{\"1\":{\"525\":1}}],[\"最具艺术价值的优秀青年艺术家\",{\"1\":{\"28\":1}}],[\"最具市场前景\",{\"1\":{\"28\":1}}],[\"最终利用ar技术将游戏与教育相结合\",{\"1\":{\"76\":1}}],[\"最终将用户心智绑定到平台\",{\"1\":{\"39\":1}}],[\"最终得到了观众的喜爱\",{\"1\":{\"24\":1}}],[\"最终的答案或许需要更多的阅读者与参与者去解读与实现\",{\"1\":{\"17\":1}}],[\"0对设计工程提出了多维度的挑战与机遇\",{\"1\":{\"519\":1}}],[\"0通过结合技术与人性化的设计\",{\"1\":{\"519\":1}}],[\"0中对创新\",{\"1\":{\"519\":1}}],[\"0中面临的问题\",{\"1\":{\"519\":1}}],[\"0中的概念\",{\"1\":{\"519\":1}}],[\"0中的数字化生产和服务化有关\",{\"1\":{\"519\":1}}],[\"0的发展\",{\"1\":{\"519\":1}}],[\"0的到来\",{\"1\":{\"519\":2}}],[\"0时代的设计工程需要综合多方面的能力\",{\"1\":{\"519\":1}}],[\"0时代的设计工程学\",{\"1\":{\"510\":1}}],[\"0时代的工程设计\",{\"1\":{\"519\":1}}],[\"0时代的挑战呼唤设计思维的全面应用\",{\"1\":{\"519\":1}}],[\"0时代\",{\"1\":{\"519\":3}}],[\"0x\",{\"1\":{\"487\":1}}],[\"00216v1\",{\"1\":{\"463\":1}}],[\"00242v1\",{\"1\":{\"462\":1}}],[\"00246v1\",{\"1\":{\"275\":1}}],[\"00267v1\",{\"1\":{\"461\":1}}],[\"00600v2\",{\"1\":{\"446\":1}}],[\"00640v2\",{\"1\":{\"445\":1}}],[\"00675v2\",{\"1\":{\"444\":1}}],[\"00699v1\",{\"1\":{\"443\":1}}],[\"00634v1\",{\"1\":{\"263\":1}}],[\"00701v1\",{\"1\":{\"442\":1}}],[\"00725v1\",{\"1\":{\"441\":1}}],[\"00899v1\",{\"1\":{\"439\":1}}],[\"00925v1\",{\"1\":{\"438\":1}}],[\"00971v1\",{\"1\":{\"437\":1}}],[\"00938v1\",{\"1\":{\"261\":1}}],[\"00\",{\"1\":{\"335\":1,\"492\":1}}],[\"00131v1\",{\"1\":{\"280\":1}}],[\"00161v1\",{\"1\":{\"279\":1}}],[\"00171v1\",{\"1\":{\"278\":1}}],[\"00192v1\",{\"1\":{\"277\":1}}],[\"003\",{\"1\":{\"492\":1}}],[\"00303v1\",{\"1\":{\"460\":1}}],[\"00300v1\",{\"1\":{\"274\":1}}],[\"00344v1\",{\"1\":{\"458\":1}}],[\"00333v1\",{\"1\":{\"273\":1}}],[\"00392v1\",{\"1\":{\"272\":1}}],[\"00456v1\",{\"1\":{\"456\":1}}],[\"00457v1\",{\"1\":{\"455\":1}}],[\"00459v1\",{\"1\":{\"454\":1}}],[\"00486v1\",{\"1\":{\"453\":1}}],[\"00487v1\",{\"1\":{\"268\":1,\"452\":1}}],[\"00405v1\",{\"1\":{\"271\":1,\"457\":1}}],[\"00431v1\",{\"1\":{\"270\":1}}],[\"00442v1\",{\"1\":{\"269\":1}}],[\"00532v1\",{\"1\":{\"450\":1}}],[\"00557v1\",{\"1\":{\"449\":1}}],[\"00526v1\",{\"1\":{\"265\":1}}],[\"00573v1\",{\"1\":{\"264\":1,\"448\":1}}],[\"000个决策\",{\"1\":{\"532\":1}}],[\"000\",{\"1\":{\"172\":1,\"344\":1,\"365\":1,\"370\":1,\"416\":1}}],[\"01129v1\",{\"1\":{\"432\":1}}],[\"01147v1\",{\"1\":{\"431\":1}}],[\"01151v1\",{\"1\":{\"430\":1}}],[\"01940v1\",{\"1\":{\"416\":1}}],[\"01997v1\",{\"1\":{\"233\":1}}],[\"01334v1\",{\"1\":{\"459\":1}}],[\"01339v1\",{\"1\":{\"266\":1}}],[\"01343v1\",{\"1\":{\"447\":1}}],[\"01353v1\",{\"1\":{\"436\":1}}],[\"01327v1\",{\"1\":{\"282\":1}}],[\"01365v1\",{\"1\":{\"427\":1}}],[\"01361v1\",{\"1\":{\"258\":1,\"433\":1}}],[\"01364v1\",{\"1\":{\"255\":1}}],[\"01041v2\",{\"1\":{\"435\":1}}],[\"01096v1\",{\"1\":{\"434\":1}}],[\"01050v1\",{\"1\":{\"260\":1}}],[\"01063v1\",{\"1\":{\"259\":1}}],[\"01230v1\",{\"1\":{\"429\":1}}],[\"01268v1\",{\"1\":{\"428\":1}}],[\"01240v1\",{\"1\":{\"257\":1}}],[\"01250v1\",{\"1\":{\"256\":1}}],[\"01283v1\",{\"1\":{\"254\":1}}],[\"01430v1\",{\"1\":{\"425\":1}}],[\"01453v1\",{\"1\":{\"424\":1}}],[\"01403v1\",{\"1\":{\"253\":1}}],[\"01425v1\",{\"1\":{\"252\":1,\"426\":1}}],[\"01461v1\",{\"1\":{\"251\":1,\"423\":1}}],[\"01485v1\",{\"1\":{\"250\":1}}],[\"01488v1\",{\"1\":{\"249\":1}}],[\"01535v1\",{\"1\":{\"422\":1}}],[\"01527v1\",{\"1\":{\"248\":1}}],[\"01576v1\",{\"1\":{\"246\":1}}],[\"01\",{\"0\":{\"247\":1,\"421\":1}}],[\"01616v1\",{\"1\":{\"420\":1}}],[\"01616v2\",{\"1\":{\"419\":1}}],[\"01615v1\",{\"1\":{\"244\":1}}],[\"01602v1\",{\"1\":{\"245\":1}}],[\"01622v1\",{\"1\":{\"243\":1}}],[\"01644v1\",{\"1\":{\"242\":1}}],[\"01651v1\",{\"1\":{\"241\":1}}],[\"01702v1\",{\"1\":{\"240\":1}}],[\"01713v1\",{\"1\":{\"239\":1}}],[\"01833v1\",{\"1\":{\"418\":1}}],[\"01855v1\",{\"1\":{\"417\":1}}],[\"01816v1\",{\"1\":{\"238\":1}}],[\"01845v1\",{\"1\":{\"237\":1}}],[\"01848v1\",{\"1\":{\"236\":1}}],[\"01862v1\",{\"1\":{\"235\":1}}],[\"01872v1\",{\"1\":{\"234\":1}}],[\"02532v1\",{\"1\":{\"403\":1}}],[\"02565v1\",{\"1\":{\"220\":1}}],[\"02616v1\",{\"1\":{\"402\":1}}],[\"02015v1\",{\"1\":{\"414\":1}}],[\"02056v1\",{\"1\":{\"413\":1}}],[\"02060v1\",{\"1\":{\"412\":1}}],[\"02078v1\",{\"1\":{\"411\":1}}],[\"02009v1\",{\"1\":{\"232\":1}}],[\"02081v1\",{\"1\":{\"231\":1}}],[\"02183v1\",{\"1\":{\"415\":1}}],[\"02138v1\",{\"1\":{\"410\":1}}],[\"02151v1\",{\"1\":{\"409\":1}}],[\"02109v1\",{\"1\":{\"230\":1}}],[\"02147v1\",{\"1\":{\"229\":1}}],[\"02261v1\",{\"1\":{\"408\":1}}],[\"02294v1\",{\"1\":{\"407\":1}}],[\"02213v1\",{\"1\":{\"228\":1}}],[\"02277v1\",{\"1\":{\"227\":1}}],[\"02317v1\",{\"1\":{\"226\":1}}],[\"02\",{\"0\":{\"225\":1,\"406\":1}}],[\"02422v1\",{\"1\":{\"405\":1}}],[\"02474v1\",{\"1\":{\"404\":1}}],[\"02475v1\",{\"1\":{\"222\":1}}],[\"02411v1\",{\"1\":{\"224\":1}}],[\"02439v1\",{\"1\":{\"223\":1}}],[\"02761v1\",{\"1\":{\"400\":1}}],[\"02706v1\",{\"1\":{\"219\":1,\"401\":1}}],[\"02718v1\",{\"1\":{\"218\":1}}],[\"02743v1\",{\"1\":{\"217\":1}}],[\"02798v1\",{\"1\":{\"216\":1}}],[\"02838v1\",{\"1\":{\"399\":1}}],[\"02806v1\",{\"1\":{\"215\":1}}],[\"02880v1\",{\"1\":{\"214\":1}}],[\"02990v1\",{\"1\":{\"213\":1}}],[\"03413v1\",{\"1\":{\"393\":1}}],[\"03498v1\",{\"1\":{\"201\":1}}],[\"03565v1\",{\"1\":{\"392\":1}}],[\"03602v1\",{\"1\":{\"391\":1}}],[\"03623v1\",{\"1\":{\"390\":1}}],[\"03626v1\",{\"1\":{\"389\":1}}],[\"03694v1\",{\"1\":{\"221\":1}}],[\"03612v1\",{\"1\":{\"200\":1}}],[\"03044v1\",{\"1\":{\"398\":1}}],[\"03023v1\",{\"1\":{\"211\":1}}],[\"03085v1\",{\"1\":{\"210\":1}}],[\"03\",{\"0\":{\"208\":1,\"262\":1,\"267\":1,\"276\":1,\"285\":1,\"299\":1,\"397\":1,\"440\":1,\"451\":1,\"464\":1,\"473\":1,\"484\":1,\"493\":1}}],[\"03122v1\",{\"1\":{\"396\":1}}],[\"03134v1\",{\"1\":{\"395\":1}}],[\"03130v1\",{\"1\":{\"207\":1}}],[\"03192v1\",{\"1\":{\"394\":1}}],[\"03108v1\",{\"1\":{\"209\":1}}],[\"03165v1\",{\"1\":{\"206\":1}}],[\"03206v1\",{\"1\":{\"205\":1}}],[\"03239v1\",{\"1\":{\"204\":1}}],[\"03337v1\",{\"1\":{\"203\":1}}],[\"03356v1\",{\"1\":{\"202\":1}}],[\"03732v1\",{\"1\":{\"388\":1}}],[\"03746v1\",{\"1\":{\"386\":1}}],[\"03745v1\",{\"1\":{\"198\":1,\"387\":1}}],[\"03713v1\",{\"1\":{\"199\":1}}],[\"03787v1\",{\"1\":{\"197\":1}}],[\"03868v1\",{\"1\":{\"382\":1}}],[\"03891v1\",{\"1\":{\"381\":1}}],[\"03899v1\",{\"1\":{\"191\":1}}],[\"03800v1\",{\"1\":{\"196\":1}}],[\"03814v1\",{\"1\":{\"195\":1}}],[\"03831v1\",{\"1\":{\"194\":1}}],[\"03845v1\",{\"1\":{\"192\":1}}],[\"03914v1\",{\"1\":{\"190\":1}}],[\"03965v1\",{\"1\":{\"189\":1}}],[\"0\",{\"0\":{\"519\":1},\"1\":{\"161\":2,\"181\":1,\"194\":3,\"219\":1,\"223\":2,\"253\":1,\"335\":2,\"351\":1,\"352\":1,\"354\":2,\"369\":5,\"372\":3,\"401\":1,\"405\":3,\"456\":1,\"458\":2,\"460\":4,\"467\":1,\"486\":3,\"502\":1,\"510\":1,\"519\":5}}],[\"05144v1\",{\"1\":{\"356\":1}}],[\"05183v1\",{\"1\":{\"355\":1}}],[\"05103v1\",{\"1\":{\"163\":1}}],[\"05502v1\",{\"1\":{\"351\":1}}],[\"05520v2\",{\"1\":{\"350\":1}}],[\"05572v1\",{\"1\":{\"152\":1}}],[\"05674v1\",{\"1\":{\"349\":1}}],[\"05955v1\",{\"1\":{\"345\":1}}],[\"05993v1\",{\"1\":{\"344\":1}}],[\"05920v1\",{\"1\":{\"144\":1}}],[\"05\",{\"0\":{\"177\":1,\"373\":1},\"1\":{\"460\":1,\"486\":2}}],[\"05044v1\",{\"1\":{\"358\":1}}],[\"05007v1\",{\"1\":{\"165\":1}}],[\"05050v1\",{\"1\":{\"164\":1}}],[\"05221v1\",{\"1\":{\"353\":1}}],[\"05228v1\",{\"1\":{\"160\":1}}],[\"05213v1\",{\"1\":{\"161\":1,\"354\":1}}],[\"05238v1\",{\"1\":{\"159\":1}}],[\"05270v1\",{\"1\":{\"158\":1}}],[\"05317v1\",{\"1\":{\"157\":1}}],[\"05483v1\",{\"1\":{\"352\":1}}],[\"05417v1\",{\"1\":{\"156\":1}}],[\"05429v1\",{\"1\":{\"155\":1}}],[\"05442v1\",{\"1\":{\"154\":1}}],[\"05462v1\",{\"1\":{\"153\":1}}],[\"05719v1\",{\"1\":{\"151\":1,\"348\":1}}],[\"05825v1\",{\"1\":{\"347\":1}}],[\"05832v1\",{\"1\":{\"150\":1}}],[\"05842v1\",{\"1\":{\"149\":1}}],[\"05874v1\",{\"1\":{\"148\":1}}],[\"05889v1\",{\"1\":{\"147\":1}}],[\"05897v1\",{\"1\":{\"146\":1}}],[\"08\",{\"0\":{\"145\":1,\"346\":1}}],[\"09\",{\"0\":{\"134\":1,\"332\":1}}],[\"06371v1\",{\"1\":{\"339\":1}}],[\"06503v1\",{\"1\":{\"335\":1}}],[\"06644v1\",{\"1\":{\"333\":1}}],[\"06664v1\",{\"1\":{\"133\":1,\"331\":1}}],[\"06809v1\",{\"1\":{\"329\":1}}],[\"06833v1\",{\"1\":{\"328\":1}}],[\"06838v1\",{\"1\":{\"327\":1}}],[\"06820v1\",{\"1\":{\"129\":1}}],[\"06827v1\",{\"1\":{\"128\":1}}],[\"06\",{\"0\":{\"168\":1,\"367\":1}}],[\"06035v1\",{\"1\":{\"343\":1}}],[\"06039v1\",{\"1\":{\"141\":1}}],[\"06041v1\",{\"1\":{\"342\":1}}],[\"06082v1\",{\"1\":{\"341\":1}}],[\"06089v1\",{\"1\":{\"140\":1}}],[\"06021v1\",{\"1\":{\"143\":1}}],[\"06027v1\",{\"1\":{\"142\":1}}],[\"06283v1\",{\"1\":{\"340\":1}}],[\"06216v1\",{\"1\":{\"139\":1}}],[\"06227v1\",{\"1\":{\"138\":1}}],[\"06243v1\",{\"1\":{\"137\":1}}],[\"06411v1\",{\"1\":{\"338\":1}}],[\"06480v1\",{\"1\":{\"337\":1}}],[\"06488v1\",{\"1\":{\"336\":1}}],[\"06404v1\",{\"1\":{\"136\":1}}],[\"06432v1\",{\"1\":{\"135\":1}}],[\"06711v1\",{\"1\":{\"132\":1,\"330\":1}}],[\"06733v1\",{\"1\":{\"131\":1}}],[\"06762v1\",{\"1\":{\"130\":1}}],[\"06921v1\",{\"1\":{\"326\":1}}],[\"06948v2\",{\"1\":{\"325\":1}}],[\"06906v1\",{\"1\":{\"127\":1}}],[\"06919v1\",{\"1\":{\"126\":1}}],[\"06955v1\",{\"1\":{\"125\":1}}],[\"07235v1\",{\"1\":{\"363\":1}}],[\"07242v1\",{\"1\":{\"334\":1}}],[\"07546v1\",{\"1\":{\"317\":1}}],[\"07584v1\",{\"1\":{\"316\":1}}],[\"07517v1\",{\"1\":{\"103\":1}}],[\"07613v1\",{\"1\":{\"315\":1}}],[\"07677v1\",{\"1\":{\"314\":1}}],[\"07\",{\"0\":{\"162\":1,\"357\":1}}],[\"07005v1\",{\"1\":{\"124\":1}}],[\"07078v1\",{\"1\":{\"123\":1}}],[\"07108v2\",{\"1\":{\"324\":1}}],[\"07114v1\",{\"1\":{\"122\":1}}],[\"07148v1\",{\"1\":{\"121\":1}}],[\"07159v1\",{\"1\":{\"120\":1}}],[\"07161v1\",{\"1\":{\"119\":1}}],[\"07376v1\",{\"1\":{\"323\":1}}],[\"07347v1\",{\"1\":{\"118\":1}}],[\"07348v1\",{\"1\":{\"117\":1}}],[\"07351v1\",{\"1\":{\"116\":1}}],[\"07360v1\",{\"1\":{\"115\":1}}],[\"07362v1\",{\"1\":{\"114\":1}}],[\"07364v1\",{\"1\":{\"113\":1}}],[\"07382v1\",{\"1\":{\"322\":1}}],[\"07381v1\",{\"1\":{\"112\":1}}],[\"07386v1\",{\"1\":{\"111\":1}}],[\"07387v1\",{\"1\":{\"110\":1,\"321\":1}}],[\"07449v1\",{\"1\":{\"319\":1}}],[\"07456v1\",{\"1\":{\"318\":1}}],[\"07403v1\",{\"1\":{\"108\":1}}],[\"07409v1\",{\"1\":{\"107\":1}}],[\"07427v1\",{\"1\":{\"106\":1}}],[\"07479v1\",{\"1\":{\"105\":1}}],[\"07498v1\",{\"1\":{\"104\":1}}],[\"07725v1\",{\"1\":{\"102\":1}}],[\"07754v1\",{\"1\":{\"101\":1}}],[\"07865v1\",{\"1\":{\"100\":1}}],[\"07883v1\",{\"1\":{\"99\":1}}],[\"07921v1\",{\"1\":{\"313\":1}}],[\"07926v1\",{\"1\":{\"97\":1,\"312\":1}}],[\"07901v1\",{\"1\":{\"98\":1}}],[\"07934v1\",{\"1\":{\"96\":1}}],[\"04302v1\",{\"1\":{\"384\":1}}],[\"04346v1\",{\"1\":{\"376\":1}}],[\"04376v1\",{\"1\":{\"375\":1}}],[\"04392v1\",{\"1\":{\"374\":1}}],[\"04809v1\",{\"1\":{\"365\":1}}],[\"04834v1\",{\"1\":{\"364\":1}}],[\"04869v1\",{\"1\":{\"362\":1}}],[\"04067v1\",{\"1\":{\"379\":1}}],[\"04066v1\",{\"1\":{\"184\":1,\"380\":1}}],[\"04001v1\",{\"1\":{\"188\":1}}],[\"04006v1\",{\"1\":{\"187\":1}}],[\"04011v1\",{\"1\":{\"186\":1}}],[\"04036v1\",{\"1\":{\"185\":1}}],[\"04070v1\",{\"1\":{\"183\":1}}],[\"04167v1\",{\"1\":{\"377\":1}}],[\"04102v1\",{\"1\":{\"378\":1}}],[\"04101v1\",{\"1\":{\"212\":1}}],[\"04100v1\",{\"1\":{\"182\":1}}],[\"04123v1\",{\"1\":{\"181\":1}}],[\"04298v1\",{\"1\":{\"385\":1}}],[\"04204v1\",{\"1\":{\"180\":1}}],[\"04249v1\",{\"1\":{\"179\":1}}],[\"04444v1\",{\"1\":{\"178\":1}}],[\"04485v1\",{\"1\":{\"176\":1}}],[\"04510v1\",{\"1\":{\"372\":1}}],[\"04516v1\",{\"1\":{\"175\":1}}],[\"04570v1\",{\"1\":{\"174\":1,\"371\":1}}],[\"04579v1\",{\"1\":{\"173\":1}}],[\"04631v1\",{\"1\":{\"369\":1}}],[\"04638v1\",{\"1\":{\"171\":1}}],[\"04689v1\",{\"1\":{\"368\":1}}],[\"04603v1\",{\"1\":{\"172\":1,\"370\":1}}],[\"04695v1\",{\"1\":{\"170\":1}}],[\"04902v1\",{\"1\":{\"361\":1}}],[\"04906v1\",{\"1\":{\"167\":1}}],[\"04966v1\",{\"1\":{\"360\":1}}],[\"04965v2\",{\"1\":{\"166\":1}}],[\"04997v1\",{\"1\":{\"359\":1}}],[\"04\",{\"0\":{\"95\":1,\"109\":1,\"134\":1,\"145\":1,\"162\":1,\"168\":1,\"177\":1,\"193\":2,\"208\":1,\"225\":1,\"247\":1,\"311\":1,\"320\":1,\"332\":1,\"346\":1,\"357\":1,\"367\":1,\"373\":1,\"383\":2,\"397\":1,\"406\":1,\"421\":1}}],[\"04793v1\",{\"1\":{\"366\":1}}],[\"04728v1\",{\"1\":{\"169\":1}}],[\"047号的船长问\",{\"1\":{\"46\":1}}],[\"047号\",{\"1\":{\"46\":1}}],[\"047\",{\"1\":{\"23\":3,\"43\":1,\"46\":5}}],[\"0加密艺术展\",{\"1\":{\"4\":1,\"29\":1}}],[\"国内硬件编程类机构崛起\",{\"1\":{\"78\":1}}],[\"国内nft市场初试ip\",{\"0\":{\"23\":1}}],[\"国务院关于大力推进大众创业7创新若干政策措施的意见\",{\"1\":{\"78\":1}}],[\"国际大学生艺术博览会\",{\"1\":{\"28\":1}}],[\"国际儿童艺术节展厅档案\",{\"1\":{\"9\":1}}],[\"国际儿童艺术节\",{\"1\":{\"9\":1}}],[\"禁地\",{\"1\":{\"22\":1}}],[\"名字源于\",{\"1\":{\"22\":1}}],[\"物理系统\",{\"1\":{\"519\":1}}],[\"物联网或者自动化系统的革新\",{\"1\":{\"510\":1}}],[\"物体的质量\",{\"1\":{\"81\":1}}],[\"物质能量转换出现混乱\",{\"1\":{\"23\":1}}],[\"物\",{\"1\":{\"22\":1}}],[\"和之前所提的\",{\"1\":{\"537\":1}}],[\"和用户体验\",{\"1\":{\"535\":1}}],[\"和我之前提到的知识诅咒类似\",{\"1\":{\"533\":1}}],[\"和谷歌地图对社会的影响凸显了质疑和创新的变革力量\",{\"1\":{\"533\":1}}],[\"和谷歌地图进入我们日常生活也说明了艺术思维如何挑战和改变社会规范\",{\"1\":{\"533\":1}}],[\"和虚拟现实\",{\"1\":{\"531\":1}}],[\"和大家分享一些过去的故事\",{\"1\":{\"530\":1}}],[\"和以人为中心的设计\",{\"1\":{\"522\":1}}],[\"和团队协同\",{\"1\":{\"91\":1}}],[\"和stem有机地融合在一起\",{\"1\":{\"78\":1}}],[\"和他们一起玩得开心\",{\"1\":{\"46\":1}}],[\"和那帮孩子去的地方是一样的\",{\"1\":{\"23\":1}}],[\"和\",{\"1\":{\"22\":1,\"511\":1}}],[\"人脉资源等来帮我更好面对未来的挑战\",{\"1\":{\"523\":1}}],[\"人物信息内容含量与这个虚拟形象所要表现的\",{\"1\":{\"521\":1}}],[\"人物用户可以是从研究数据中抽取出来可以作为样例的虚拟形象\",{\"1\":{\"521\":1}}],[\"人际交往能力和批判性思维能力\",{\"1\":{\"518\":1}}],[\"人类如何能在日常生活中发现并欣赏那些被遗忘的美\",{\"1\":{\"511\":1}}],[\"人类日益增长的危险欲望\",{\"1\":{\"500\":1}}],[\"人工智能的一个子领域\",{\"1\":{\"539\":1}}],[\"人工智能电子邮件生成器通过分析用户需求\",{\"1\":{\"539\":1}}],[\"人工智能电子邮件生成器如何帮助提高电子邮件的打开率和参与度\",{\"1\":{\"539\":1}}],[\"人工智能电子邮件生成器\",{\"1\":{\"539\":1}}],[\"人工智能应用\",{\"1\":{\"539\":1}}],[\"人工智能\",{\"1\":{\"78\":2,\"535\":1,\"539\":1}}],[\"人们可以继续听音乐\",{\"1\":{\"515\":1}}],[\"人们在满城垃圾的世界里奔波\",{\"1\":{\"46\":1}}],[\"人们的灵感能够不断迸发\",{\"1\":{\"22\":1}}],[\"人\",{\"1\":{\"22\":1,\"522\":1}}],[\"rrectness\",{\"1\":{\"483\":1}}],[\"rhui\",{\"1\":{\"436\":1}}],[\"r2d2\",{\"1\":{\"409\":2}}],[\"rc\",{\"1\":{\"340\":2}}],[\"ryosuke\",{\"1\":{\"459\":1}}],[\"ryo\",{\"1\":{\"391\":1}}],[\"ryoo\",{\"1\":{\"319\":1}}],[\"ryan\",{\"1\":{\"104\":1,\"398\":1}}],[\"rmse\",{\"1\":{\"309\":1}}],[\"rtd\",{\"1\":{\"236\":2}}],[\"rpa\",{\"1\":{\"222\":3}}],[\"rsr\",{\"1\":{\"176\":2}}],[\"rlkf\",{\"1\":{\"489\":3}}],[\"rlhf\",{\"1\":{\"453\":1}}],[\"rl\",{\"0\":{\"300\":1,\"489\":1},\"1\":{\"121\":1,\"300\":1}}],[\"rushang\",{\"1\":{\"490\":1}}],[\"russinovich\",{\"1\":{\"418\":1}}],[\"russian\",{\"1\":{\"416\":1}}],[\"russell\",{\"1\":{\"376\":1}}],[\"russell是格洛斯特大学的一名讲师\",{\"1\":{\"50\":1}}],[\"russo\",{\"1\":{\"166\":1}}],[\"rumen\",{\"1\":{\"468\":1}}],[\"ruvan\",{\"1\":{\"384\":1}}],[\"ruochen\",{\"1\":{\"443\":1}}],[\"ruobing\",{\"1\":{\"411\":1}}],[\"ruotong\",{\"1\":{\"329\":1}}],[\"ruolin\",{\"1\":{\"297\":1}}],[\"rutger\",{\"1\":{\"297\":1}}],[\"rule\",{\"1\":{\"266\":1}}],[\"rules\",{\"1\":{\"205\":1,\"245\":1}}],[\"rukshani\",{\"1\":{\"204\":1}}],[\"ruyi\",{\"1\":{\"173\":1}}],[\"rubegni\",{\"1\":{\"171\":1}}],[\"ruben\",{\"1\":{\"126\":2}}],[\"rupar\",{\"1\":{\"167\":1}}],[\"ruifeng\",{\"1\":{\"437\":1}}],[\"rui\",{\"1\":{\"391\":1}}],[\"ruibin\",{\"1\":{\"377\":1}}],[\"ruicong\",{\"1\":{\"359\":1}}],[\"ruisi\",{\"1\":{\"359\":1}}],[\"ruiming\",{\"1\":{\"318\":1}}],[\"ruiwei\",{\"1\":{\"228\":1}}],[\"ruihong\",{\"1\":{\"156\":1}}],[\"ruiyi\",{\"1\":{\"135\":1,\"286\":1,\"474\":1}}],[\"ruiz\",{\"1\":{\"126\":1,\"205\":1}}],[\"ruijia\",{\"1\":{\"110\":1,\"321\":1}}],[\"ruhan\",{\"1\":{\"113\":1,\"115\":1}}],[\"rugs\",{\"1\":{\"105\":1}}],[\"running\",{\"1\":{\"441\":1,\"475\":1}}],[\"runyu\",{\"1\":{\"414\":1}}],[\"runtime\",{\"0\":{\"326\":1},\"1\":{\"170\":2,\"326\":1,\"434\":1}}],[\"run\",{\"1\":{\"104\":1,\"284\":1,\"441\":1}}],[\"rooted\",{\"1\":{\"478\":1}}],[\"root\",{\"1\":{\"425\":1,\"445\":2,\"462\":1}}],[\"rooms\",{\"1\":{\"165\":1}}],[\"room\",{\"0\":{\"105\":1}}],[\"ronen\",{\"1\":{\"418\":1}}],[\"rong\",{\"1\":{\"116\":1,\"118\":1}}],[\"rokhlenko\",{\"1\":{\"405\":1}}],[\"ropo\",{\"1\":{\"378\":4}}],[\"rotates\",{\"1\":{\"456\":1}}],[\"rotated\",{\"0\":{\"456\":1}}],[\"rotations\",{\"1\":{\"456\":1}}],[\"rotation\",{\"1\":{\"224\":1}}],[\"roth\",{\"1\":{\"368\":1}}],[\"romaissa\",{\"1\":{\"460\":1}}],[\"roman\",{\"1\":{\"351\":1,\"352\":1}}],[\"romero\",{\"1\":{\"315\":1}}],[\"rohban\",{\"1\":{\"333\":1}}],[\"rohan\",{\"1\":{\"266\":1}}],[\"roy\",{\"1\":{\"326\":1,\"434\":1,\"441\":1}}],[\"rover\",{\"1\":{\"300\":1}}],[\"rodrigo\",{\"1\":{\"315\":1}}],[\"rodriguez\",{\"1\":{\"126\":2}}],[\"rodríguez\",{\"1\":{\"294\":1}}],[\"rosa\",{\"1\":{\"492\":1}}],[\"ros\",{\"1\":{\"201\":1}}],[\"rosenberg\",{\"1\":{\"227\":1}}],[\"rose\",{\"1\":{\"175\":1}}],[\"rouge\",{\"1\":{\"324\":1,\"335\":1,\"488\":1}}],[\"roughly\",{\"1\":{\"181\":1,\"302\":1,\"335\":1}}],[\"rouhsedaghat\",{\"1\":{\"323\":1}}],[\"roumeli\",{\"1\":{\"207\":1}}],[\"rousi\",{\"1\":{\"202\":1}}],[\"round\",{\"1\":{\"139\":1}}],[\"route\",{\"1\":{\"270\":1,\"362\":1}}],[\"routes\",{\"0\":{\"270\":1},\"1\":{\"270\":3}}],[\"routines\",{\"1\":{\"214\":1}}],[\"routine\",{\"1\":{\"100\":1,\"475\":1}}],[\"routledge\",{\"1\":{\"74\":1}}],[\"roadmap\",{\"0\":{\"252\":1,\"426\":1},\"1\":{\"169\":1,\"252\":1,\"426\":1}}],[\"road\",{\"0\":{\"138\":1,\"364\":1},\"1\":{\"138\":1,\"297\":2,\"298\":1,\"407\":1}}],[\"robinson\",{\"1\":{\"236\":1}}],[\"roberts\",{\"1\":{\"389\":1}}],[\"roberta\",{\"1\":{\"352\":1}}],[\"roberto\",{\"1\":{\"338\":1}}],[\"robert\",{\"1\":{\"156\":1,\"217\":1}}],[\"robust\",{\"0\":{\"378\":1,\"395\":1},\"1\":{\"178\":1,\"194\":1,\"236\":1,\"240\":1,\"260\":1,\"328\":1,\"329\":1,\"334\":1,\"349\":1,\"359\":1,\"378\":1,\"395\":1,\"405\":1,\"409\":1,\"417\":1,\"422\":1,\"428\":1,\"432\":1,\"478\":1,\"480\":1}}],[\"robustness\",{\"0\":{\"422\":1},\"1\":{\"138\":1,\"172\":1,\"190\":1,\"240\":1,\"246\":1,\"344\":1,\"353\":1,\"364\":1,\"370\":1,\"372\":1,\"410\":1,\"422\":5}}],[\"robustly\",{\"1\":{\"137\":1,\"207\":1,\"395\":1}}],[\"robotic\",{\"0\":{\"164\":1,\"222\":1,\"407\":1},\"1\":{\"164\":1,\"181\":1,\"201\":2,\"222\":1,\"269\":1,\"300\":3,\"486\":1}}],[\"robotics\",{\"0\":{\"115\":1,\"300\":1},\"1\":{\"115\":1,\"140\":1,\"164\":1,\"181\":1,\"269\":1,\"362\":1,\"381\":1,\"430\":1}}],[\"robot\",{\"0\":{\"140\":1,\"173\":2,\"201\":1,\"240\":1,\"261\":1,\"269\":1,\"300\":1,\"303\":1,\"307\":1,\"486\":1},\"1\":{\"107\":7,\"137\":1,\"140\":4,\"173\":4,\"184\":1,\"201\":2,\"238\":7,\"240\":3,\"261\":1,\"263\":1,\"269\":9,\"300\":2,\"303\":3,\"307\":7,\"380\":1,\"407\":1,\"486\":2}}],[\"robots\",{\"0\":{\"107\":1,\"164\":1,\"179\":1,\"181\":1,\"184\":1,\"380\":1},\"1\":{\"103\":1,\"107\":1,\"140\":5,\"164\":5,\"173\":1,\"179\":1,\"181\":1,\"184\":5,\"201\":1,\"224\":1,\"261\":1,\"269\":1,\"283\":1,\"300\":1,\"303\":8,\"380\":5,\"381\":3}}],[\"roccapriore\",{\"1\":{\"112\":1}}],[\"rogers\",{\"1\":{\"111\":1}}],[\"roles\",{\"1\":{\"173\":1,\"175\":1,\"205\":1,\"254\":2,\"271\":1,\"275\":1,\"290\":1,\"403\":1,\"457\":1,\"477\":1,\"498\":1}}],[\"role\",{\"0\":{\"416\":1},\"1\":{\"106\":1,\"112\":1,\"116\":1,\"129\":1,\"197\":1,\"204\":2,\"205\":1,\"209\":1,\"217\":1,\"238\":1,\"245\":2,\"248\":1,\"253\":1,\"254\":1,\"265\":1,\"270\":1,\"326\":1,\"349\":1,\"360\":1,\"364\":1,\"388\":1,\"430\":1,\"468\":1,\"489\":1}}],[\"rastogi\",{\"1\":{\"434\":1}}],[\"rassar\",{\"0\":{\"105\":1},\"1\":{\"105\":3}}],[\"rare\",{\"1\":{\"408\":1,\"467\":1}}],[\"rarely\",{\"1\":{\"191\":1,\"224\":1,\"286\":1,\"474\":1}}],[\"raluca\",{\"1\":{\"326\":1}}],[\"rahmani\",{\"1\":{\"438\":1}}],[\"rahul\",{\"1\":{\"325\":1,\"434\":1}}],[\"rahim\",{\"1\":{\"298\":1}}],[\"rag\",{\"0\":{\"341\":1,\"384\":1},\"1\":{\"323\":1,\"329\":3,\"341\":1,\"372\":1,\"384\":4,\"404\":1,\"453\":1}}],[\"rakibul\",{\"1\":{\"298\":1}}],[\"rakib\",{\"1\":{\"298\":1}}],[\"raúl\",{\"1\":{\"294\":1}}],[\"radio\",{\"0\":{\"498\":1},\"1\":{\"282\":2}}],[\"radoslav\",{\"1\":{\"240\":1}}],[\"razvan\",{\"1\":{\"255\":1}}],[\"razi\",{\"1\":{\"136\":1}}],[\"raquel\",{\"1\":{\"236\":1}}],[\"rao\",{\"1\":{\"196\":1,\"220\":1}}],[\"ramviyas\",{\"1\":{\"486\":1}}],[\"ramitha\",{\"1\":{\"384\":1}}],[\"ramirez\",{\"1\":{\"315\":1}}],[\"ramana\",{\"1\":{\"442\":1}}],[\"rama\",{\"1\":{\"185\":1}}],[\"ramon\",{\"1\":{\"96\":1,\"419\":1,\"420\":1}}],[\"rainer\",{\"1\":{\"238\":1}}],[\"rair\",{\"1\":{\"176\":2}}],[\"raised\",{\"1\":{\"394\":1,\"460\":1}}],[\"raise\",{\"1\":{\"213\":1,\"469\":1}}],[\"raises\",{\"1\":{\"159\":1,\"171\":1}}],[\"raising\",{\"1\":{\"160\":1}}],[\"raw\",{\"1\":{\"172\":1,\"370\":1,\"475\":2}}],[\"rafael\",{\"1\":{\"147\":1}}],[\"raphaël\",{\"1\":{\"365\":1}}],[\"raphael\",{\"1\":{\"137\":1}}],[\"rap\",{\"1\":{\"353\":1}}],[\"rapidly\",{\"1\":{\"221\":1,\"256\":1,\"272\":1,\"349\":1,\"396\":1,\"398\":1,\"492\":1}}],[\"rapid\",{\"1\":{\"104\":1,\"119\":1,\"188\":2,\"243\":1,\"298\":1,\"313\":1,\"316\":1,\"329\":1,\"359\":1,\"422\":1,\"463\":1,\"478\":1,\"498\":1}}],[\"ravaut\",{\"1\":{\"443\":1}}],[\"raveendran\",{\"1\":{\"137\":1}}],[\"ravi\",{\"1\":{\"133\":1,\"331\":1}}],[\"rating\",{\"1\":{\"486\":2}}],[\"ratings\",{\"1\":{\"152\":1,\"413\":1}}],[\"ratios\",{\"0\":{\"250\":1},\"1\":{\"250\":1,\"453\":1}}],[\"rationales\",{\"1\":{\"470\":1}}],[\"rationale\",{\"1\":{\"258\":1,\"260\":1,\"433\":1}}],[\"rational\",{\"1\":{\"226\":1}}],[\"ratio\",{\"1\":{\"151\":2,\"250\":2,\"348\":2,\"495\":1}}],[\"rater\",{\"1\":{\"335\":1}}],[\"rates\",{\"1\":{\"215\":1,\"284\":1,\"318\":1,\"327\":2,\"333\":1,\"389\":1,\"418\":1,\"460\":2,\"486\":1,\"495\":1}}],[\"rated\",{\"1\":{\"198\":1,\"387\":1,\"431\":3}}],[\"rate\",{\"0\":{\"188\":1},\"1\":{\"120\":1,\"140\":1,\"152\":1,\"190\":1,\"196\":2,\"222\":1,\"283\":2,\"293\":1,\"298\":3,\"313\":1,\"318\":1,\"350\":1,\"362\":1,\"409\":2,\"479\":1}}],[\"rather\",{\"1\":{\"103\":1,\"197\":1,\"229\":1,\"249\":1,\"251\":1,\"263\":1,\"300\":1,\"303\":1,\"306\":1,\"423\":1,\"470\":1}}],[\"ranran\",{\"1\":{\"391\":1}}],[\"ranasinghe\",{\"1\":{\"319\":1}}],[\"rana2149\",{\"1\":{\"137\":1}}],[\"ran\",{\"1\":{\"226\":1,\"402\":1}}],[\"rankgpt\",{\"1\":{\"394\":1}}],[\"ranker\",{\"1\":{\"394\":1}}],[\"rankers\",{\"0\":{\"394\":1}}],[\"ranked\",{\"1\":{\"325\":1}}],[\"rank\",{\"0\":{\"394\":1},\"1\":{\"198\":1,\"387\":1,\"436\":1}}],[\"rankings\",{\"1\":{\"155\":1}}],[\"ranking\",{\"0\":{\"155\":1,\"272\":1,\"482\":1},\"1\":{\"155\":6,\"351\":1,\"352\":1,\"378\":4,\"386\":1,\"394\":5,\"417\":1,\"441\":2,\"482\":1,\"485\":1}}],[\"ranjay\",{\"1\":{\"140\":1}}],[\"ranjan\",{\"1\":{\"113\":1}}],[\"randomly\",{\"1\":{\"369\":1,\"432\":1}}],[\"randomized\",{\"1\":{\"187\":2}}],[\"random\",{\"1\":{\"139\":1,\"409\":2}}],[\"ranging\",{\"1\":{\"133\":1,\"183\":1,\"331\":1,\"350\":1,\"356\":1,\"361\":1,\"378\":1,\"416\":1,\"434\":1,\"466\":1}}],[\"ranges\",{\"1\":{\"191\":1,\"249\":1,\"337\":1}}],[\"ranged\",{\"1\":{\"135\":1}}],[\"range\",{\"1\":{\"100\":1,\"114\":1,\"135\":1,\"137\":1,\"146\":1,\"151\":1,\"167\":1,\"191\":1,\"217\":2,\"220\":1,\"239\":1,\"287\":1,\"302\":1,\"317\":2,\"329\":1,\"333\":1,\"344\":1,\"348\":1,\"366\":1,\"369\":1,\"390\":1,\"410\":1,\"412\":1,\"432\":1,\"437\":1,\"482\":1}}],[\"racing\",{\"1\":{\"414\":1}}],[\"racial\",{\"1\":{\"106\":1}}],[\"race\",{\"0\":{\"106\":1},\"1\":{\"106\":6,\"179\":1}}],[\"riddles\",{\"1\":{\"404\":1}}],[\"riccardo\",{\"1\":{\"368\":1}}],[\"richer\",{\"1\":{\"384\":1}}],[\"richard\",{\"1\":{\"178\":1,\"522\":1}}],[\"rich\",{\"0\":{\"122\":1},\"1\":{\"81\":1,\"122\":1,\"153\":1,\"155\":1,\"224\":1,\"260\":1,\"314\":1,\"333\":1,\"345\":1,\"355\":1,\"412\":1,\"450\":1,\"485\":1}}],[\"rie\",{\"1\":{\"293\":1}}],[\"riedl\",{\"1\":{\"233\":1}}],[\"riedlinger\",{\"1\":{\"182\":1}}],[\"riku\",{\"1\":{\"239\":1}}],[\"right\",{\"1\":{\"319\":1}}],[\"rights\",{\"1\":{\"205\":1,\"334\":1}}],[\"rigau\",{\"1\":{\"315\":1}}],[\"rigid\",{\"1\":{\"240\":1}}],[\"rigoni\",{\"1\":{\"189\":1}}],[\"rigor\",{\"0\":{\"161\":1,\"354\":1},\"1\":{\"236\":1}}],[\"rigorous\",{\"1\":{\"154\":1,\"161\":1,\"191\":1,\"333\":1,\"354\":1,\"356\":1}}],[\"rizzo\",{\"1\":{\"171\":1}}],[\"rita\",{\"1\":{\"171\":1}}],[\"rising\",{\"1\":{\"369\":1}}],[\"risi\",{\"1\":{\"296\":1}}],[\"risen\",{\"1\":{\"418\":1}}],[\"rise\",{\"1\":{\"102\":1,\"154\":1,\"298\":1,\"437\":1,\"443\":1,\"453\":1,\"479\":1}}],[\"risky\",{\"1\":{\"226\":1,\"437\":1}}],[\"risks\",{\"1\":{\"178\":1,\"198\":1,\"211\":1,\"243\":1,\"258\":1,\"261\":2,\"298\":1,\"326\":1,\"344\":1,\"387\":1,\"396\":1,\"433\":1,\"463\":1,\"492\":2}}],[\"risk\",{\"0\":{\"97\":1,\"278\":1,\"312\":1},\"1\":{\"97\":3,\"124\":1,\"133\":1,\"187\":2,\"227\":1,\"252\":1,\"278\":1,\"284\":1,\"298\":1,\"302\":2,\"312\":3,\"326\":1,\"331\":1,\"344\":3,\"378\":1,\"426\":1,\"435\":2,\"446\":1,\"453\":2}}],[\"rivtower\",{\"1\":{\"9\":1}}],[\"reevaluation\",{\"1\":{\"394\":1}}],[\"reuben\",{\"1\":{\"376\":1}}],[\"reut\",{\"1\":{\"340\":1}}],[\"reusable\",{\"1\":{\"361\":1}}],[\"reuse\",{\"1\":{\"306\":2,\"384\":1,\"395\":2}}],[\"reused\",{\"1\":{\"306\":1}}],[\"reusing\",{\"0\":{\"306\":1},\"1\":{\"131\":1,\"306\":1}}],[\"reimplement\",{\"1\":{\"316\":1}}],[\"reinforces\",{\"1\":{\"246\":1}}],[\"reinforcement\",{\"1\":{\"116\":1,\"121\":1,\"150\":1,\"200\":1,\"300\":1,\"362\":1,\"489\":1}}],[\"reinforcing\",{\"1\":{\"167\":1}}],[\"reinstatement\",{\"1\":{\"165\":3}}],[\"reheat\",{\"1\":{\"251\":2,\"423\":2}}],[\"rehabilitative\",{\"1\":{\"220\":1}}],[\"rehabilitation\",{\"1\":{\"103\":1}}],[\"rejuvenated\",{\"1\":{\"243\":1}}],[\"rejecting\",{\"1\":{\"489\":1}}],[\"rejection\",{\"0\":{\"489\":1},\"1\":{\"403\":1,\"489\":2}}],[\"reject\",{\"0\":{\"107\":1},\"1\":{\"107\":1,\"467\":1,\"494\":1}}],[\"renders\",{\"1\":{\"399\":1}}],[\"rendering\",{\"1\":{\"313\":1,\"359\":1,\"408\":1}}],[\"renze\",{\"1\":{\"391\":1}}],[\"renjing\",{\"1\":{\"362\":1}}],[\"renjie\",{\"1\":{\"316\":1}}],[\"ren\",{\"1\":{\"210\":1,\"257\":1}}],[\"reben\",{\"1\":{\"533\":1}}],[\"rebekah\",{\"1\":{\"202\":1}}],[\"rebate\",{\"1\":{\"192\":1}}],[\"rewording\",{\"1\":{\"482\":1}}],[\"rewritten\",{\"1\":{\"481\":1}}],[\"rewrite\",{\"1\":{\"402\":1}}],[\"rewriting\",{\"1\":{\"124\":2,\"434\":2}}],[\"reward\",{\"0\":{\"278\":1},\"1\":{\"353\":2,\"411\":2,\"489\":1}}],[\"rewarding\",{\"1\":{\"200\":1}}],[\"rethink\",{\"1\":{\"470\":1}}],[\"rethinking\",{\"0\":{\"238\":1,\"317\":1},\"1\":{\"470\":1}}],[\"retention\",{\"1\":{\"260\":1,\"359\":1,\"456\":1}}],[\"return\",{\"1\":{\"246\":1}}],[\"retains\",{\"1\":{\"456\":1}}],[\"retaining\",{\"1\":{\"255\":1,\"327\":1}}],[\"retail\",{\"1\":{\"183\":1}}],[\"retargeting\",{\"1\":{\"191\":7}}],[\"retargeted\",{\"0\":{\"191\":1},\"1\":{\"191\":2}}],[\"retracing\",{\"1\":{\"260\":1}}],[\"retrieving\",{\"1\":{\"317\":1,\"399\":1,\"425\":1}}],[\"retrievalsystems\",{\"0\":{\"420\":1}}],[\"retrieval\",{\"0\":{\"347\":2,\"365\":1,\"384\":1,\"419\":1,\"481\":1,\"492\":1},\"1\":{\"165\":2,\"252\":1,\"317\":1,\"323\":1,\"329\":3,\"333\":1,\"347\":4,\"372\":1,\"382\":1,\"384\":3,\"386\":2,\"394\":1,\"396\":1,\"404\":1,\"419\":5,\"420\":5,\"426\":1,\"453\":1,\"481\":2,\"492\":1}}],[\"retrieves\",{\"1\":{\"382\":1}}],[\"retriever\",{\"1\":{\"347\":1}}],[\"retrieved\",{\"1\":{\"264\":1,\"365\":1,\"425\":1,\"448\":1}}],[\"retrieve\",{\"0\":{\"481\":1},\"1\":{\"111\":1,\"481\":1,\"488\":1}}],[\"retrospective\",{\"0\":{\"227\":1},\"1\":{\"106\":1,\"121\":1,\"227\":2}}],[\"reynolds\",{\"1\":{\"166\":1}}],[\"reza\",{\"1\":{\"159\":1}}],[\"re\",{\"0\":{\"155\":1},\"1\":{\"153\":1,\"155\":1,\"224\":1,\"248\":1,\"339\":2}}],[\"rex\",{\"1\":{\"135\":1}}],[\"reddit\",{\"0\":{\"431\":1},\"1\":{\"431\":1}}],[\"reddy\",{\"1\":{\"391\":1}}],[\"redundant\",{\"1\":{\"402\":1}}],[\"redundancy\",{\"1\":{\"305\":1,\"462\":1,\"479\":1}}],[\"reduce\",{\"1\":{\"150\":1,\"165\":1,\"241\":1,\"282\":1,\"288\":1,\"389\":1,\"396\":1,\"408\":1,\"416\":1,\"435\":1,\"463\":1,\"471\":1}}],[\"reduced\",{\"0\":{\"281\":1},\"1\":{\"138\":1,\"165\":1,\"220\":2,\"281\":1,\"408\":1,\"460\":1,\"483\":1}}],[\"reduces\",{\"1\":{\"110\":1,\"158\":1,\"224\":1,\"238\":2,\"242\":1,\"319\":1,\"321\":1,\"374\":1,\"425\":1,\"461\":1,\"479\":1}}],[\"reducing\",{\"0\":{\"150\":1,\"425\":1},\"1\":{\"131\":1,\"234\":1,\"260\":1,\"361\":1,\"368\":1,\"374\":1,\"396\":1,\"407\":1,\"435\":1,\"479\":1,\"496\":1}}],[\"reduction\",{\"0\":{\"165\":1},\"1\":{\"111\":2,\"239\":1,\"281\":3,\"301\":2,\"427\":1,\"436\":1,\"462\":4,\"477\":1}}],[\"reductions\",{\"0\":{\"111\":1},\"1\":{\"366\":1,\"422\":2}}],[\"redefining\",{\"1\":{\"263\":1}}],[\"red\",{\"0\":{\"133\":1,\"331\":1},\"1\":{\"133\":2,\"331\":2}}],[\"revert\",{\"1\":{\"326\":1}}],[\"revealing\",{\"0\":{\"435\":1},\"1\":{\"133\":1,\"213\":1,\"237\":1,\"256\":1,\"293\":1,\"331\":1,\"345\":1,\"424\":1}}],[\"reveals\",{\"1\":{\"133\":1,\"212\":1,\"323\":1,\"331\":1,\"353\":1,\"411\":1,\"412\":1,\"428\":1}}],[\"reveal\",{\"1\":{\"117\":1,\"165\":1,\"183\":1,\"251\":1,\"260\":1,\"301\":1,\"327\":1,\"328\":1,\"365\":1,\"404\":1,\"417\":1,\"423\":1,\"428\":1,\"441\":1,\"461\":1,\"467\":1,\"481\":1}}],[\"revealed\",{\"1\":{\"98\":1,\"106\":1,\"120\":1,\"135\":1,\"139\":1,\"142\":1,\"189\":1,\"195\":1,\"203\":1,\"237\":1,\"263\":1,\"274\":1,\"304\":1,\"350\":1,\"412\":1,\"460\":1}}],[\"revolutionize\",{\"1\":{\"356\":1,\"358\":1}}],[\"revolutionized\",{\"1\":{\"138\":1,\"242\":1}}],[\"revolutions\",{\"1\":{\"253\":1}}],[\"revolution\",{\"1\":{\"253\":2}}],[\"reviewers\",{\"1\":{\"335\":1}}],[\"reviewed\",{\"1\":{\"155\":1,\"307\":1}}],[\"reviews\",{\"1\":{\"174\":1,\"371\":1}}],[\"review\",{\"1\":{\"136\":1,\"169\":1,\"174\":1,\"292\":1,\"371\":1,\"429\":1,\"492\":1}}],[\"revisite\",{\"1\":{\"463\":1}}],[\"revisiting\",{\"0\":{\"197\":1}}],[\"revising\",{\"1\":{\"292\":1}}],[\"revision\",{\"0\":{\"324\":1},\"1\":{\"133\":1,\"259\":1,\"324\":5,\"331\":1}}],[\"revised\",{\"0\":{\"129\":1}}],[\"requiring\",{\"1\":{\"126\":1,\"139\":1,\"153\":1,\"222\":1,\"238\":1,\"273\":1,\"283\":1,\"333\":1,\"444\":1,\"454\":1,\"460\":1,\"478\":1}}],[\"requirement\",{\"1\":{\"189\":1,\"402\":1}}],[\"requirements\",{\"0\":{\"339\":1},\"1\":{\"149\":1,\"166\":1,\"195\":1,\"219\":1,\"224\":1,\"249\":1,\"253\":2,\"339\":6,\"358\":1,\"396\":2,\"401\":1,\"408\":1,\"422\":2,\"427\":1,\"465\":1,\"481\":1,\"490\":1}}],[\"require\",{\"0\":{\"446\":1},\"1\":{\"137\":1,\"238\":1,\"275\":1,\"283\":1,\"284\":2,\"292\":1,\"315\":1,\"375\":1,\"384\":1,\"385\":1,\"389\":1,\"419\":1,\"420\":1,\"425\":1,\"427\":1,\"441\":1,\"442\":1,\"453\":1,\"468\":1,\"469\":1,\"475\":1,\"490\":1,\"491\":1}}],[\"required\",{\"1\":{\"125\":1,\"128\":2,\"149\":1,\"396\":1,\"408\":1,\"447\":1,\"469\":1}}],[\"requires\",{\"1\":{\"99\":1,\"114\":1,\"140\":1,\"154\":1,\"156\":1,\"199\":1,\"216\":1,\"244\":1,\"253\":1,\"335\":1,\"349\":1,\"360\":1,\"399\":1,\"412\":1,\"434\":1}}],[\"requests\",{\"1\":{\"228\":1,\"414\":1}}],[\"request\",{\"1\":{\"124\":1,\"232\":2}}],[\"refactoring\",{\"1\":{\"434\":1}}],[\"refuse\",{\"0\":{\"489\":1}}],[\"refusal\",{\"0\":{\"403\":1},\"1\":{\"489\":1}}],[\"refutes\",{\"1\":{\"241\":1}}],[\"reformulations\",{\"1\":{\"386\":1}}],[\"reformulation\",{\"0\":{\"386\":1},\"1\":{\"386\":2}}],[\"refer\",{\"1\":{\"487\":1}}],[\"referencing\",{\"1\":{\"418\":1}}],[\"references\",{\"1\":{\"470\":2}}],[\"reference\",{\"1\":{\"324\":1,\"349\":2,\"497\":1}}],[\"refers\",{\"1\":{\"251\":1,\"423\":1}}],[\"referred\",{\"1\":{\"198\":1,\"284\":1,\"387\":1}}],[\"referring\",{\"1\":{\"151\":2,\"348\":2,\"395\":1}}],[\"reflection\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"227\":2,\"268\":2,\"314\":2,\"452\":2}}],[\"reflections\",{\"1\":{\"161\":1,\"248\":1,\"296\":1,\"307\":1,\"354\":1}}],[\"reflective\",{\"1\":{\"175\":1,\"234\":1}}],[\"reflecting\",{\"1\":{\"163\":1}}],[\"reflect\",{\"1\":{\"156\":1,\"160\":1,\"218\":1,\"227\":1,\"248\":2,\"286\":1,\"379\":1,\"474\":1}}],[\"reflected\",{\"1\":{\"148\":1,\"156\":1,\"189\":1}}],[\"reflects\",{\"1\":{\"124\":1,\"297\":1}}],[\"refining\",{\"0\":{\"385\":1},\"1\":{\"130\":1,\"356\":1,\"442\":1}}],[\"refined\",{\"0\":{\"459\":1},\"1\":{\"307\":1,\"358\":1,\"479\":1}}],[\"refinement\",{\"1\":{\"235\":1,\"259\":1,\"359\":1}}],[\"refine\",{\"1\":{\"104\":1,\"144\":1,\"338\":1,\"355\":1,\"360\":1}}],[\"regressor\",{\"1\":{\"325\":1}}],[\"regret\",{\"1\":{\"196\":1,\"344\":1}}],[\"registering\",{\"1\":{\"393\":1}}],[\"regitse\",{\"1\":{\"296\":1}}],[\"region\",{\"1\":{\"151\":1,\"254\":1,\"348\":1}}],[\"regions\",{\"1\":{\"112\":1}}],[\"regueiro\",{\"1\":{\"282\":1}}],[\"regularize\",{\"1\":{\"438\":1}}],[\"regularly\",{\"1\":{\"252\":1,\"426\":1}}],[\"regular\",{\"1\":{\"244\":1,\"252\":1,\"426\":1}}],[\"regulators\",{\"1\":{\"498\":1}}],[\"regulatory\",{\"1\":{\"212\":1,\"498\":1}}],[\"regulating\",{\"1\":{\"317\":1}}],[\"regulations\",{\"1\":{\"178\":1,\"254\":2}}],[\"regulation\",{\"0\":{\"498\":1},\"1\":{\"120\":1,\"144\":1,\"206\":1,\"498\":1}}],[\"regulate\",{\"1\":{\"205\":1}}],[\"regenerate\",{\"1\":{\"224\":1}}],[\"regardless\",{\"1\":{\"254\":1,\"436\":1}}],[\"regard\",{\"1\":{\"176\":1,\"439\":1}}],[\"regarded\",{\"1\":{\"161\":1,\"354\":1}}],[\"regarding\",{\"0\":{\"198\":1,\"387\":1},\"1\":{\"139\":1,\"156\":1,\"160\":1,\"189\":1,\"198\":1,\"206\":1,\"211\":1,\"303\":1,\"366\":1,\"387\":1}}],[\"repository\",{\"1\":{\"495\":2}}],[\"repositories\",{\"1\":{\"495\":1}}],[\"reported\",{\"0\":{\"273\":1},\"1\":{\"273\":2}}],[\"reports\",{\"1\":{\"204\":1,\"286\":1,\"372\":1,\"474\":1,\"497\":1}}],[\"report\",{\"0\":{\"202\":1,\"302\":1},\"1\":{\"135\":2,\"189\":1,\"202\":1,\"209\":1,\"214\":1,\"248\":1,\"302\":2,\"308\":1,\"358\":1,\"477\":1}}],[\"repairs\",{\"1\":{\"350\":1}}],[\"repair\",{\"0\":{\"350\":1},\"1\":{\"350\":3}}],[\"rept\",{\"1\":{\"227\":3}}],[\"reproducibility\",{\"1\":{\"165\":1,\"369\":1,\"428\":1}}],[\"representing\",{\"1\":{\"301\":1,\"485\":1}}],[\"represent\",{\"1\":{\"143\":1,\"181\":1,\"204\":1,\"231\":1,\"301\":2}}],[\"representativeness\",{\"0\":{\"251\":1,\"423\":1},\"1\":{\"251\":7,\"423\":7}}],[\"representative\",{\"1\":{\"128\":1,\"135\":1,\"302\":1,\"365\":1,\"366\":1}}],[\"representations\",{\"0\":{\"390\":1,\"449\":1},\"1\":{\"126\":1,\"137\":2,\"188\":1,\"190\":3,\"199\":1,\"205\":1,\"213\":2,\"226\":1,\"280\":1,\"289\":1,\"319\":1,\"384\":1,\"390\":2,\"438\":1,\"449\":1,\"479\":1,\"485\":1,\"488\":1}}],[\"representation\",{\"0\":{\"432\":1},\"1\":{\"106\":1,\"110\":1,\"137\":1,\"174\":1,\"191\":1,\"199\":1,\"213\":1,\"255\":2,\"282\":1,\"321\":1,\"371\":1,\"375\":1,\"390\":2,\"394\":1,\"430\":1,\"432\":2,\"438\":1,\"442\":2,\"450\":1,\"454\":1}}],[\"represents\",{\"1\":{\"120\":1,\"147\":1,\"268\":1,\"356\":1,\"379\":1,\"390\":2,\"452\":1}}],[\"represented\",{\"1\":{\"117\":1,\"281\":1,\"408\":1,\"454\":1}}],[\"repeating\",{\"0\":{\"395\":1}}],[\"repeatedly\",{\"1\":{\"233\":1,\"257\":1}}],[\"repeated\",{\"1\":{\"165\":5,\"441\":1,\"479\":1}}],[\"repetitive\",{\"1\":{\"114\":1,\"486\":1}}],[\"republicans\",{\"1\":{\"155\":1}}],[\"replies\",{\"1\":{\"278\":1,\"403\":1,\"418\":1}}],[\"replicate\",{\"1\":{\"116\":2,\"149\":1,\"188\":1}}],[\"reply\",{\"1\":{\"232\":1,\"403\":1}}],[\"replacing\",{\"1\":{\"229\":1}}],[\"replacement\",{\"1\":{\"196\":1,\"460\":2}}],[\"replace\",{\"1\":{\"138\":1,\"196\":1,\"300\":1}}],[\"rephrased\",{\"1\":{\"127\":1}}],[\"relu\",{\"1\":{\"427\":1}}],[\"relieve\",{\"1\":{\"282\":1}}],[\"relied\",{\"1\":{\"215\":1}}],[\"relies\",{\"1\":{\"112\":1,\"197\":1,\"351\":1,\"352\":1,\"390\":1,\"444\":1}}],[\"reliant\",{\"1\":{\"384\":1}}],[\"reliance\",{\"0\":{\"176\":1},\"1\":{\"171\":2,\"176\":6,\"286\":1,\"474\":1}}],[\"reliable\",{\"1\":{\"324\":1,\"337\":1,\"338\":1,\"368\":1,\"437\":1,\"443\":1,\"482\":1,\"489\":1}}],[\"reliably\",{\"1\":{\"196\":1,\"322\":1,\"338\":1,\"385\":1}}],[\"reliability\",{\"0\":{\"489\":1},\"1\":{\"203\":1,\"223\":1,\"245\":1,\"246\":1,\"329\":1,\"335\":1,\"378\":1,\"391\":1,\"461\":1,\"489\":3}}],[\"releasing\",{\"1\":{\"443\":1}}],[\"released\",{\"1\":{\"480\":1}}],[\"release\",{\"1\":{\"159\":1,\"199\":1,\"211\":1,\"271\":1,\"323\":1,\"326\":1,\"344\":1,\"369\":1,\"381\":1,\"431\":1,\"457\":1}}],[\"relevance\",{\"0\":{\"309\":1,\"402\":1},\"1\":{\"264\":1,\"305\":1,\"309\":1,\"356\":1,\"363\":1,\"386\":1,\"398\":1,\"402\":7,\"448\":1}}],[\"relevant\",{\"1\":{\"111\":1,\"132\":1,\"174\":1,\"269\":1,\"309\":1,\"318\":1,\"328\":2,\"330\":1,\"335\":1,\"371\":1,\"382\":1,\"384\":2,\"386\":2,\"396\":1,\"430\":1,\"458\":1,\"481\":1}}],[\"relatable\",{\"1\":{\"266\":1}}],[\"relation\",{\"1\":{\"455\":1}}],[\"relational\",{\"1\":{\"301\":1}}],[\"relationships\",{\"1\":{\"227\":1,\"244\":1,\"252\":1,\"255\":1,\"301\":1,\"304\":1,\"376\":1,\"399\":1,\"424\":2,\"426\":1}}],[\"relationship\",{\"0\":{\"273\":1},\"1\":{\"199\":1,\"202\":1,\"204\":1,\"209\":1,\"221\":1,\"250\":1,\"253\":1,\"273\":1,\"292\":1,\"304\":1}}],[\"relating\",{\"1\":{\"249\":1}}],[\"relatively\",{\"1\":{\"322\":1,\"369\":1,\"412\":1,\"471\":1,\"486\":1}}],[\"relative\",{\"1\":{\"123\":1,\"176\":2,\"197\":1,\"213\":1,\"386\":2,\"399\":1}}],[\"related\",{\"0\":{\"328\":1},\"1\":{\"119\":1,\"123\":1,\"144\":1,\"155\":1,\"174\":1,\"178\":1,\"211\":1,\"222\":1,\"228\":1,\"235\":1,\"257\":1,\"258\":1,\"272\":1,\"278\":1,\"286\":1,\"308\":1,\"325\":1,\"328\":1,\"345\":2,\"350\":1,\"371\":1,\"394\":1,\"429\":1,\"433\":1,\"444\":1,\"474\":1}}],[\"relying\",{\"1\":{\"163\":1,\"254\":1}}],[\"rely\",{\"1\":{\"96\":1,\"113\":1,\"123\":1,\"133\":1,\"135\":3,\"180\":1,\"240\":1,\"279\":1,\"288\":1,\"331\":1,\"353\":1,\"390\":1,\"417\":1,\"434\":1,\"482\":1,\"488\":1}}],[\"recsys\",{\"0\":{\"485\":1}}],[\"rectify\",{\"1\":{\"350\":1}}],[\"rectangular\",{\"1\":{\"112\":1}}],[\"recipes\",{\"1\":{\"334\":1}}],[\"reciprocal\",{\"1\":{\"303\":1}}],[\"recursive\",{\"1\":{\"314\":1}}],[\"recurring\",{\"1\":{\"149\":1}}],[\"recruit\",{\"1\":{\"307\":1}}],[\"recycling\",{\"1\":{\"306\":1}}],[\"recall\",{\"0\":{\"264\":1,\"448\":1},\"1\":{\"165\":1,\"176\":1,\"264\":6,\"390\":1,\"391\":1,\"419\":1,\"420\":1,\"448\":6,\"488\":1}}],[\"receptive\",{\"1\":{\"453\":1}}],[\"receives\",{\"1\":{\"232\":1,\"407\":1}}],[\"received\",{\"1\":{\"160\":2,\"200\":1,\"249\":1}}],[\"receive\",{\"1\":{\"132\":1,\"200\":1,\"233\":1,\"322\":1,\"330\":1}}],[\"receiving\",{\"1\":{\"131\":1,\"303\":1,\"356\":1}}],[\"recently\",{\"1\":{\"132\":1,\"243\":1,\"315\":1,\"318\":1,\"325\":1,\"330\":1,\"337\":1,\"347\":1,\"375\":1,\"386\":1,\"417\":1,\"428\":1,\"435\":1,\"450\":1}}],[\"recent\",{\"1\":{\"107\":1,\"112\":1,\"126\":1,\"151\":1,\"154\":1,\"163\":1,\"192\":1,\"197\":1,\"199\":1,\"205\":1,\"211\":1,\"228\":1,\"241\":1,\"258\":1,\"261\":1,\"292\":1,\"313\":1,\"317\":1,\"322\":1,\"325\":1,\"328\":1,\"342\":1,\"348\":1,\"350\":1,\"360\":1,\"362\":1,\"363\":1,\"382\":1,\"409\":1,\"410\":1,\"415\":1,\"418\":1,\"425\":2,\"432\":1,\"433\":1,\"442\":1,\"443\":2,\"469\":2,\"470\":1,\"472\":1,\"479\":1,\"491\":1}}],[\"reconstructs\",{\"1\":{\"246\":1}}],[\"reconstructions\",{\"1\":{\"256\":1}}],[\"reconstruction\",{\"0\":{\"256\":1},\"1\":{\"224\":1,\"256\":4,\"438\":1}}],[\"reconstruct\",{\"1\":{\"224\":1}}],[\"records\",{\"1\":{\"205\":1,\"356\":1}}],[\"recordings\",{\"1\":{\"232\":1}}],[\"recording\",{\"1\":{\"196\":1,\"335\":1}}],[\"recorded\",{\"1\":{\"182\":1,\"256\":1,\"290\":1,\"351\":1,\"355\":1}}],[\"recourse\",{\"0\":{\"158\":1},\"1\":{\"158\":3}}],[\"recommender\",{\"1\":{\"485\":1}}],[\"recommended\",{\"1\":{\"135\":2,\"234\":1,\"265\":1}}],[\"recommend\",{\"1\":{\"195\":1,\"197\":1,\"203\":1,\"308\":1}}],[\"recommending\",{\"1\":{\"167\":1,\"245\":1}}],[\"recommendation\",{\"0\":{\"167\":1,\"417\":1},\"1\":{\"135\":1,\"167\":2,\"196\":1,\"234\":1,\"263\":1,\"417\":6,\"485\":10}}],[\"recommendations\",{\"1\":{\"99\":1,\"121\":1,\"128\":1,\"167\":1,\"178\":1,\"199\":1,\"234\":1,\"263\":3,\"417\":2,\"485\":2}}],[\"recognise\",{\"1\":{\"204\":1}}],[\"recognising\",{\"1\":{\"123\":1}}],[\"recognizer\",{\"1\":{\"450\":2}}],[\"recognizers\",{\"0\":{\"450\":1},\"1\":{\"450\":2}}],[\"recognized\",{\"1\":{\"290\":1,\"459\":1}}],[\"recognize\",{\"1\":{\"118\":1,\"158\":1,\"244\":1,\"257\":1,\"264\":1,\"412\":1,\"448\":1,\"475\":1}}],[\"recognizing\",{\"0\":{\"305\":1},\"1\":{\"96\":1,\"130\":1,\"257\":1,\"266\":1,\"279\":1,\"376\":1,\"437\":2}}],[\"recognition\",{\"0\":{\"96\":1,\"137\":1,\"323\":1},\"1\":{\"96\":6,\"103\":1,\"116\":1,\"118\":1,\"137\":4,\"138\":1,\"151\":1,\"204\":1,\"243\":1,\"253\":1,\"305\":1,\"323\":1,\"348\":1,\"355\":1,\"376\":1,\"437\":1,\"450\":1,\"455\":1,\"459\":1,\"486\":1}}],[\"remez\",{\"1\":{\"441\":1}}],[\"remember\",{\"0\":{\"122\":1},\"1\":{\"131\":1}}],[\"remind\",{\"1\":{\"219\":1,\"251\":1,\"401\":1,\"423\":1}}],[\"removes\",{\"1\":{\"456\":1}}],[\"removing\",{\"1\":{\"155\":1,\"434\":1,\"449\":1}}],[\"remotely\",{\"1\":{\"173\":1}}],[\"remote\",{\"0\":{\"106\":1,\"173\":1,\"246\":1},\"1\":{\"101\":1,\"106\":2,\"108\":1,\"173\":4,\"435\":3}}],[\"remarkably\",{\"1\":{\"349\":1}}],[\"remarkable\",{\"1\":{\"101\":1,\"212\":1,\"251\":1,\"258\":1,\"314\":1,\"318\":1,\"323\":1,\"342\":1,\"377\":1,\"392\":1,\"414\":1,\"423\":1,\"427\":1,\"433\":1,\"467\":1}}],[\"remaining\",{\"1\":{\"234\":1,\"304\":1}}],[\"remains\",{\"1\":{\"142\":1,\"160\":1,\"345\":1,\"353\":1,\"391\":1,\"394\":1,\"415\":1,\"424\":1,\"443\":1,\"479\":1}}],[\"remain\",{\"1\":{\"130\":1,\"167\":1,\"234\":1,\"263\":1,\"350\":1,\"390\":1,\"434\":1}}],[\"remco\",{\"1\":{\"111\":1}}],[\"rearrangement\",{\"1\":{\"469\":1}}],[\"rearrange\",{\"1\":{\"469\":1}}],[\"reactions\",{\"1\":{\"278\":1}}],[\"reaching\",{\"1\":{\"261\":1,\"360\":1,\"365\":1}}],[\"reach\",{\"1\":{\"180\":1,\"360\":1}}],[\"reached\",{\"1\":{\"101\":1,\"443\":1}}],[\"reassess\",{\"1\":{\"160\":1}}],[\"reasoner\",{\"0\":{\"483\":1}}],[\"reasoners\",{\"0\":{\"353\":1},\"1\":{\"353\":1}}],[\"reasonable\",{\"1\":{\"181\":1,\"339\":1,\"417\":1,\"422\":1,\"430\":1,\"447\":2}}],[\"reasons\",{\"1\":{\"171\":1,\"284\":1,\"470\":2}}],[\"reason\",{\"1\":{\"107\":1,\"169\":1,\"218\":1,\"381\":2,\"412\":1,\"495\":1}}],[\"reasoning\",{\"0\":{\"123\":1,\"319\":1,\"353\":1,\"381\":1,\"384\":1,\"395\":1,\"411\":1,\"429\":1,\"454\":1,\"470\":1,\"471\":1,\"475\":1,\"494\":1},\"1\":{\"96\":1,\"138\":2,\"151\":2,\"242\":1,\"249\":1,\"251\":3,\"271\":1,\"275\":1,\"283\":2,\"288\":1,\"314\":2,\"318\":1,\"319\":1,\"333\":1,\"338\":1,\"348\":2,\"353\":10,\"364\":1,\"372\":3,\"376\":1,\"381\":1,\"384\":1,\"390\":2,\"391\":1,\"395\":2,\"399\":1,\"411\":7,\"412\":1,\"417\":1,\"423\":3,\"429\":5,\"434\":1,\"447\":1,\"454\":1,\"457\":1,\"458\":1,\"462\":2,\"470\":3,\"471\":2,\"475\":8,\"480\":2,\"483\":8,\"494\":2}}],[\"readily\",{\"1\":{\"389\":1,\"419\":1,\"420\":1,\"482\":1}}],[\"readings\",{\"1\":{\"298\":1}}],[\"reading\",{\"0\":{\"127\":2,\"152\":1,\"291\":1,\"340\":1,\"476\":1},\"1\":{\"127\":4,\"152\":3,\"291\":3,\"340\":1,\"476\":3}}],[\"reads\",{\"1\":{\"282\":1,\"462\":1}}],[\"readers\",{\"1\":{\"155\":1,\"197\":1,\"219\":2,\"401\":2}}],[\"readability\",{\"1\":{\"152\":1,\"280\":1,\"327\":1,\"356\":1,\"438\":1}}],[\"read\",{\"1\":{\"131\":1,\"219\":2,\"401\":2,\"454\":1}}],[\"ready\",{\"1\":{\"81\":1,\"236\":1,\"529\":1}}],[\"reallocation\",{\"0\":{\"441\":1}}],[\"realhumaneval\",{\"0\":{\"215\":1},\"1\":{\"215\":3}}],[\"realize\",{\"1\":{\"413\":1}}],[\"realizing\",{\"1\":{\"181\":1,\"233\":1,\"326\":1}}],[\"realities\",{\"1\":{\"195\":1}}],[\"reality\",{\"0\":{\"105\":1,\"117\":1,\"119\":1,\"120\":1,\"140\":1,\"147\":1,\"191\":1,\"195\":1,\"201\":1,\"274\":1},\"1\":{\"105\":1,\"117\":2,\"119\":1,\"120\":1,\"127\":1,\"140\":1,\"147\":1,\"157\":1,\"165\":1,\"191\":1,\"195\":1,\"201\":1,\"204\":1,\"227\":1,\"239\":2,\"304\":1,\"305\":1,\"430\":1}}],[\"realism\",{\"1\":{\"165\":1,\"203\":1}}],[\"realistic\",{\"0\":{\"238\":1},\"1\":{\"101\":2,\"179\":1,\"180\":1,\"213\":1,\"238\":1,\"273\":1,\"305\":1,\"391\":1,\"412\":1,\"444\":1}}],[\"real\",{\"0\":{\"170\":1,\"251\":1,\"298\":1,\"423\":1,\"480\":1,\"491\":1},\"1\":{\"101\":1,\"105\":1,\"112\":2,\"116\":1,\"120\":2,\"127\":1,\"132\":1,\"140\":1,\"141\":1,\"143\":2,\"170\":2,\"182\":1,\"183\":1,\"186\":1,\"188\":3,\"191\":1,\"207\":1,\"212\":1,\"238\":6,\"240\":1,\"269\":1,\"275\":1,\"282\":1,\"283\":2,\"286\":1,\"290\":1,\"301\":2,\"309\":1,\"326\":2,\"329\":1,\"330\":1,\"345\":1,\"359\":1,\"360\":1,\"379\":2,\"382\":1,\"405\":1,\"412\":1,\"417\":1,\"434\":1,\"447\":1,\"467\":1,\"468\":1,\"471\":1,\"474\":1,\"479\":1,\"480\":1,\"486\":1,\"491\":1}}],[\"realmistake\",{\"1\":{\"391\":4}}],[\"realm\",{\"1\":{\"96\":1,\"157\":1,\"174\":1,\"362\":1,\"371\":1,\"412\":1,\"460\":1}}],[\"restarts\",{\"1\":{\"409\":1}}],[\"restrict\",{\"1\":{\"167\":1,\"206\":1,\"409\":1}}],[\"restricted\",{\"1\":{\"133\":1,\"331\":1,\"409\":1,\"427\":1}}],[\"residual\",{\"1\":{\"456\":1}}],[\"resistant\",{\"1\":{\"436\":1}}],[\"resistance\",{\"1\":{\"374\":1}}],[\"resist\",{\"1\":{\"418\":1}}],[\"resilient\",{\"1\":{\"334\":1}}],[\"resilience\",{\"1\":{\"329\":1}}],[\"resampled\",{\"1\":{\"352\":1}}],[\"rescue\",{\"1\":{\"269\":1,\"288\":1}}],[\"resemblance\",{\"1\":{\"349\":1}}],[\"resembles\",{\"1\":{\"251\":1,\"423\":1}}],[\"researches\",{\"1\":{\"318\":1}}],[\"researcher\",{\"1\":{\"316\":1}}],[\"researchers\",{\"1\":{\"101\":1,\"105\":1,\"133\":1,\"146\":1,\"159\":1,\"175\":1,\"205\":1,\"206\":1,\"269\":1,\"286\":2,\"292\":2,\"296\":1,\"297\":1,\"316\":1,\"331\":1,\"375\":1,\"395\":1,\"398\":2,\"474\":2}}],[\"researched\",{\"1\":{\"160\":1}}],[\"research\",{\"0\":{\"136\":2,\"161\":1,\"169\":1,\"236\":1,\"286\":1,\"307\":1,\"354\":1,\"474\":1,\"492\":1},\"1\":{\"96\":1,\"97\":6,\"106\":1,\"107\":1,\"126\":1,\"136\":3,\"139\":1,\"142\":1,\"143\":1,\"148\":1,\"152\":1,\"154\":1,\"156\":4,\"157\":1,\"159\":2,\"161\":1,\"166\":2,\"167\":1,\"169\":3,\"174\":2,\"180\":1,\"182\":1,\"185\":1,\"186\":1,\"189\":1,\"194\":1,\"198\":1,\"199\":1,\"200\":3,\"204\":1,\"206\":1,\"212\":1,\"214\":2,\"216\":1,\"217\":3,\"226\":1,\"232\":1,\"233\":2,\"236\":2,\"243\":1,\"246\":1,\"251\":1,\"254\":1,\"257\":1,\"269\":2,\"272\":1,\"273\":1,\"280\":3,\"286\":5,\"292\":1,\"293\":1,\"296\":1,\"300\":1,\"304\":1,\"305\":1,\"307\":8,\"308\":1,\"309\":1,\"312\":6,\"315\":2,\"326\":1,\"328\":1,\"334\":1,\"338\":3,\"344\":1,\"345\":1,\"350\":1,\"353\":1,\"354\":1,\"356\":1,\"359\":1,\"362\":2,\"364\":5,\"365\":1,\"371\":2,\"372\":1,\"374\":1,\"377\":1,\"387\":1,\"391\":2,\"392\":1,\"398\":2,\"400\":2,\"417\":1,\"423\":1,\"424\":1,\"428\":2,\"429\":1,\"431\":1,\"437\":1,\"439\":2,\"442\":1,\"450\":1,\"458\":2,\"459\":1,\"461\":1,\"463\":1,\"466\":1,\"467\":1,\"470\":1,\"472\":1,\"474\":5,\"477\":1,\"485\":1,\"488\":1,\"489\":1,\"491\":1,\"492\":2}}],[\"resnet\",{\"0\":{\"137\":1}}],[\"respiration\",{\"1\":{\"194\":1}}],[\"respiratory\",{\"1\":{\"120\":1,\"194\":2}}],[\"respond\",{\"0\":{\"317\":1},\"1\":{\"317\":1,\"334\":1}}],[\"respondents\",{\"1\":{\"221\":1,\"468\":1}}],[\"respondent\",{\"1\":{\"175\":1,\"221\":1}}],[\"responsive\",{\"1\":{\"269\":1}}],[\"responsiveness\",{\"0\":{\"269\":1}}],[\"responsibility\",{\"1\":{\"286\":1,\"474\":1}}],[\"responsibilities\",{\"1\":{\"205\":1}}],[\"responsible\",{\"0\":{\"492\":1},\"1\":{\"136\":1,\"181\":1,\"291\":1,\"308\":1,\"335\":1,\"403\":1,\"418\":1,\"476\":1}}],[\"response\",{\"0\":{\"298\":1,\"392\":1,\"481\":1},\"1\":{\"187\":2,\"203\":1,\"212\":1,\"260\":1,\"264\":1,\"279\":1,\"333\":2,\"341\":1,\"378\":3,\"392\":2,\"448\":1,\"467\":4,\"468\":1,\"481\":1,\"486\":2}}],[\"responses\",{\"0\":{\"120\":1,\"385\":1,\"391\":1,\"403\":1},\"1\":{\"114\":1,\"120\":2,\"130\":1,\"228\":1,\"234\":1,\"256\":1,\"266\":1,\"280\":1,\"282\":1,\"283\":1,\"305\":1,\"334\":6,\"378\":3,\"385\":1,\"391\":4,\"430\":2,\"432\":5,\"447\":1,\"449\":1,\"458\":1,\"466\":1,\"467\":4,\"468\":2,\"486\":2,\"489\":1}}],[\"respecting\",{\"1\":{\"167\":1,\"206\":1,\"300\":1}}],[\"respective\",{\"1\":{\"153\":1,\"217\":1,\"272\":1}}],[\"respectively\",{\"1\":{\"103\":2,\"176\":1,\"194\":1,\"223\":1,\"325\":1,\"335\":1,\"361\":1,\"363\":1,\"369\":1,\"388\":1,\"393\":1,\"486\":1,\"487\":1}}],[\"respect\",{\"0\":{\"155\":1},\"1\":{\"126\":1,\"155\":1,\"199\":1,\"221\":1,\"250\":1,\"339\":1}}],[\"resulted\",{\"1\":{\"176\":1,\"226\":1,\"410\":1}}],[\"resultant\",{\"1\":{\"142\":1}}],[\"result\",{\"1\":{\"126\":1,\"129\":1,\"167\":1,\"220\":1,\"232\":1,\"279\":1,\"302\":1,\"324\":1,\"334\":1,\"381\":1,\"403\":1,\"425\":1,\"430\":1,\"432\":1,\"443\":1,\"456\":1}}],[\"results\",{\"1\":{\"101\":2,\"104\":1,\"107\":1,\"111\":1,\"116\":1,\"121\":1,\"123\":1,\"126\":1,\"127\":1,\"130\":1,\"135\":1,\"137\":1,\"141\":1,\"146\":4,\"149\":2,\"152\":2,\"154\":1,\"158\":1,\"160\":1,\"161\":1,\"166\":1,\"176\":1,\"183\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"195\":1,\"197\":1,\"198\":1,\"202\":1,\"204\":1,\"209\":2,\"222\":1,\"223\":1,\"226\":1,\"232\":1,\"235\":1,\"237\":1,\"245\":1,\"246\":1,\"254\":2,\"255\":2,\"256\":1,\"257\":1,\"260\":1,\"263\":1,\"265\":2,\"269\":1,\"273\":1,\"282\":1,\"284\":1,\"289\":2,\"292\":1,\"295\":1,\"298\":1,\"302\":1,\"303\":1,\"304\":1,\"308\":1,\"324\":1,\"329\":1,\"337\":1,\"339\":1,\"340\":1,\"347\":2,\"352\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"360\":2,\"362\":2,\"365\":1,\"369\":1,\"384\":1,\"385\":1,\"387\":1,\"393\":1,\"405\":3,\"411\":1,\"414\":1,\"416\":1,\"418\":1,\"422\":1,\"430\":1,\"432\":1,\"437\":1,\"438\":1,\"441\":1,\"442\":1,\"449\":1,\"455\":1,\"471\":1,\"475\":2,\"479\":1,\"483\":1,\"485\":1,\"488\":1,\"489\":1,\"494\":1}}],[\"resulting\",{\"1\":{\"96\":1,\"108\":1,\"150\":1,\"189\":1,\"235\":1,\"241\":1,\"277\":1,\"298\":2,\"301\":2,\"309\":1,\"316\":1,\"318\":1,\"319\":2,\"333\":1,\"385\":1,\"388\":1,\"432\":2,\"442\":1,\"445\":1,\"462\":1,\"468\":1,\"469\":1}}],[\"resonant\",{\"1\":{\"305\":1}}],[\"resonance\",{\"1\":{\"256\":1}}],[\"resonated\",{\"1\":{\"204\":1}}],[\"resonate\",{\"1\":{\"98\":1,\"266\":1}}],[\"resolved\",{\"1\":{\"495\":1}}],[\"resolve\",{\"1\":{\"224\":1,\"495\":3}}],[\"resolving\",{\"0\":{\"170\":1,\"453\":1},\"1\":{\"170\":1,\"360\":1,\"495\":1}}],[\"resolution\",{\"0\":{\"495\":1},\"1\":{\"101\":1,\"151\":1,\"180\":1,\"227\":1,\"345\":1,\"348\":1,\"495\":2}}],[\"resources\",{\"1\":{\"132\":1,\"133\":1,\"178\":1,\"210\":1,\"227\":1,\"231\":1,\"235\":1,\"323\":1,\"330\":1,\"331\":1,\"408\":1,\"414\":2,\"417\":1,\"478\":1}}],[\"resource\",{\"0\":{\"365\":1,\"405\":1,\"408\":1},\"1\":{\"74\":1,\"212\":1,\"334\":3,\"345\":1,\"365\":4,\"398\":1,\"405\":1,\"408\":3,\"414\":1,\"519\":1}}],[\"reshaped\",{\"1\":{\"142\":1}}],[\"reshape\",{\"1\":{\"97\":1,\"312\":1}}],[\"r\",{\"1\":{\"22\":2,\"96\":1,\"159\":1,\"176\":1,\"179\":1,\"201\":1,\"281\":1,\"369\":2,\"431\":1,\"497\":1}}],[\"趁着数字藏品\",{\"1\":{\"22\":1}}],[\"一些行业正在衰落\",{\"1\":{\"538\":1}}],[\"一些行业正在崛起\",{\"1\":{\"538\":1}}],[\"一些研究者也是当年有幸乘上那艘\",{\"1\":{\"23\":1}}],[\"一\",{\"1\":{\"534\":2}}],[\"一定在明确自己定位的时候去选择相应的软件\",{\"1\":{\"509\":1}}],[\"一种计算机科学领域\",{\"1\":{\"539\":1}}],[\"一种以人为本的解决问题方法\",{\"1\":{\"524\":1}}],[\"一种工具\",{\"1\":{\"524\":1}}],[\"一种是kinect或者leapmotion这种现成的深度摄像头\",{\"1\":{\"509\":1}}],[\"一种是aduino这类传感器模块\",{\"1\":{\"509\":1}}],[\"一种属于孩子们才能读懂的创造\",{\"1\":{\"74\":1,\"530\":1}}],[\"一是我构想了\",{\"1\":{\"530\":1}}],[\"一是建模软件\",{\"1\":{\"509\":1}}],[\"一是技术点\",{\"1\":{\"509\":1}}],[\"一般3\",{\"1\":{\"92\":1}}],[\"一旦孩子们能够超越这种有限的思维\",{\"1\":{\"79\":1}}],[\"一生留给后人60多本专著\",{\"1\":{\"79\":1}}],[\"一天的疯狂又落下帷幕\",{\"1\":{\"73\":1}}],[\"一朵绽放江面的花在绽放\",{\"1\":{\"73\":1}}],[\"一场疯狂的\",{\"1\":{\"70\":1}}],[\"一群不同职业身份的人\",{\"1\":{\"56\":1}}],[\"一词\",{\"1\":{\"48\":1,\"74\":1}}],[\"一词在英语中具有更多的负面价值\",{\"1\":{\"48\":1}}],[\"一词在丹麦语中具有积极的内涵\",{\"1\":{\"48\":1}}],[\"一班又一班的孩子看着各个新奇的岛走上他们不同的道路\",{\"1\":{\"46\":1}}],[\"一夜暴富\",{\"1\":{\"42\":1}}],[\"一大波热点的推动在上半年带来了许多话题\",{\"1\":{\"30\":1}}],[\"一个设计师正在尝试将ai技术融入产品设计\",{\"1\":{\"535\":1}}],[\"一个传统设计师将数据科学\",{\"1\":{\"535\":1}}],[\"一个国际性的学生评估项目\",{\"1\":{\"524\":1}}],[\"一个概念模型的产生不是要去告诉用户怎么使用\",{\"1\":{\"521\":1}}],[\"一个美好的设想更多是为了提醒和告诉人们孩子们还有许多巨大的潜力或许尚未发现\",{\"1\":{\"74\":1}}],[\"一个齿轮运转的机械缆车系统贯穿着两边的土地\",{\"1\":{\"58\":1}}],[\"一个光怪陆离的世界\",{\"1\":{\"53\":1}}],[\"一个可变式的围墙包围着这片乐土\",{\"1\":{\"53\":1}}],[\"一个可以叫做\",{\"1\":{\"43\":1}}],[\"一个可以展示你成长\",{\"1\":{\"31\":1}}],[\"一个儿童可以创造和塑造\",{\"1\":{\"48\":1}}],[\"一个孩子马上接上了话\",{\"1\":{\"46\":1}}],[\"一个老家伙突然发话了\",{\"1\":{\"46\":1}}],[\"一个石洞门突然打开\",{\"1\":{\"46\":1}}],[\"一个普通的社会场所里是否在细节上也为孩子去考究考量\",{\"1\":{\"43\":1}}],[\"一个视频\",{\"1\":{\"31\":1}}],[\"一个梦境般的地方\",{\"1\":{\"23\":1}}],[\"一个就是符合最开始想法的设计项目\",{\"1\":{\"21\":1}}],[\"一直扰乱了世界的平衡\",{\"1\":{\"23\":1}}],[\"一开始只是将我的一些过往作品分享在平台上\",{\"1\":{\"20\":1}}],[\"由前facebook设计副总裁julie\",{\"1\":{\"527\":1}}],[\"由上到下\",{\"1\":{\"521\":1}}],[\"由于市场的大量饱和\",{\"1\":{\"538\":1}}],[\"由于全球经济环境的变化和自动化技术的发展\",{\"1\":{\"537\":1}}],[\"由于\",{\"1\":{\"81\":1}}],[\"由于它们一次只能聚焦或居中对象的一个属性\",{\"1\":{\"79\":1}}],[\"由于所谓的安全问题\",{\"1\":{\"49\":1}}],[\"由于我当时账号定位还未成熟\",{\"1\":{\"21\":1}}],[\"由花的主体去衍生梦的世界\",{\"1\":{\"24\":1}}],[\"由广州市天河区住房和建设水务局颁发\",{\"1\":{\"7\":1}}],[\"大批新人进入这个行业\",{\"1\":{\"536\":1}}],[\"大学也就这么一次机会能够真正地表达你的思维\",{\"1\":{\"534\":1}}],[\"大脑的决策系统\",{\"1\":{\"532\":1}}],[\"大脑中有两个系统在起着作用\",{\"1\":{\"532\":1}}],[\"大数据和人工智能等\",{\"1\":{\"531\":1}}],[\"大数据和人工智能的应用\",{\"1\":{\"519\":1}}],[\"大胆的创新可能是必要的\",{\"1\":{\"525\":1}}],[\"大人们无法进入到这个地方\",{\"1\":{\"59\":1}}],[\"大人无法抵达\",{\"1\":{\"53\":1}}],[\"大约20世纪60年代\",{\"1\":{\"48\":1}}],[\"大\",{\"1\":{\"42\":1}}],[\"大湾区规划周年\",{\"1\":{\"42\":1}}],[\"大艺博当时在推出online的app\",{\"1\":{\"28\":1}}],[\"大艺博是国内最大规模的集中展示美术专业当届毕业生及青年艺术家艺术创作与艺术探索的平台\",{\"1\":{\"28\":1}}],[\"大艺博\",{\"1\":{\"28\":1}}],[\"大艺博联合数字艺术家\",{\"1\":{\"9\":1}}],[\"大家可以聊聊\",{\"1\":{\"520\":1}}],[\"大家自然而然知道这个东西的价值\",{\"1\":{\"42\":1}}],[\"大家有共识的一点就是新的互联网时代确实在向我们走来\",{\"1\":{\"30\":1}}],[\"大家好像又遗忘了元宇宙这个概念\",{\"1\":{\"30\":1}}],[\"大家觉得元宇宙很酷很有新鲜感\",{\"1\":{\"30\":1}}],[\"大家对于艺术ip这个概念重视起来的体现\",{\"1\":{\"27\":1}}],[\"大家非常熟悉的一个概念\",{\"1\":{\"21\":1}}],[\"次年\",{\"1\":{\"21\":1}}],[\"继续发布一些数字作品\",{\"1\":{\"21\":1}}],[\"不可能每家公司都需要\",{\"1\":{\"538\":1}}],[\"不要说\",{\"1\":{\"535\":1}}],[\"不要为了交互而做交互\",{\"1\":{\"509\":1}}],[\"不要为了做艺术装置而做艺术装置\",{\"1\":{\"509\":1}}],[\"不同工具如rytr\",{\"1\":{\"539\":1}}],[\"不同点\",{\"1\":{\"531\":1}}],[\"不同的平台在它们之间过度连接\",{\"1\":{\"58\":1}}],[\"不同的穿戴式设备和生成内容的方式\",{\"1\":{\"31\":1}}],[\"不舒适体验的商业化也许可以创造更多经济价值\",{\"1\":{\"520\":1}}],[\"不舒适体验产品也可用于社会问题的宣传教育\",{\"1\":{\"520\":1}}],[\"不舒适体验产品有望成为心理健康的创新服务形式\",{\"1\":{\"520\":1}}],[\"不妨试着思考如何将这些来自于身边的记忆通过设计\",{\"1\":{\"511\":1}}],[\"不乏有同学的作品质量在自媒体平台或者线下展览这些场合被观众赏识\",{\"1\":{\"509\":1}}],[\"不只是从传统设计的思维出发了\",{\"1\":{\"508\":1}}],[\"不只是停留在电脑绘图的概念上\",{\"1\":{\"92\":1}}],[\"不是软件技术方面的\",{\"1\":{\"507\":1}}],[\"不是各种无用的流量笔记\",{\"1\":{\"20\":1}}],[\"不适体验\",{\"0\":{\"520\":1},\"1\":{\"502\":1}}],[\"不局限于线性的思维方式\",{\"1\":{\"93\":1}}],[\"不断优化设计方案\",{\"1\":{\"525\":1}}],[\"不断更新和扩展自己的技能组合\",{\"1\":{\"518\":1}}],[\"不断提升沟通能力就是最好的方式\",{\"1\":{\"92\":1}}],[\"不断地成为机械化的程序\",{\"1\":{\"56\":1}}],[\"不再是类比高考的提前填鸭式教育\",{\"1\":{\"77\":1}}],[\"不到\",{\"1\":{\"49\":1}}],[\"不敢下去\",{\"1\":{\"46\":1}}],[\"不\",{\"1\":{\"46\":1}}],[\"不管未来是不是叫web3形态的互联网\",{\"1\":{\"42\":1}}],[\"不管是真正的学习探索还是热度的炒作\",{\"1\":{\"30\":1}}],[\"不仅是一种趋势\",{\"1\":{\"533\":1}}],[\"不仅是一段酷炫的动画来向观众展示\",{\"1\":{\"31\":1}}],[\"不仅限于个人层面的创新活动\",{\"1\":{\"524\":1}}],[\"不仅仅局限于视觉设计\",{\"1\":{\"521\":1}}],[\"不仅仅可以用于简单的路线导航\",{\"1\":{\"512\":1}}],[\"不仅仅是技术层面的\",{\"1\":{\"31\":1}}],[\"不过路还很长\",{\"1\":{\"31\":1}}],[\"不过我还是发现了许多弊端\",{\"1\":{\"31\":1}}],[\"不过最后我还是为雪橇熊的一个系列做了一个小小的整理\",{\"1\":{\"23\":1}}],[\"不过基本以纯分享形式为主\",{\"1\":{\"20\":1}}],[\"不用眼睛就能判断方向\",{\"1\":{\"23\":1}}],[\"也会有可能性消失和丧失\",{\"1\":{\"538\":1}}],[\"也越来越有利可图\",{\"1\":{\"538\":1}}],[\"也有异曲同工的意义\",{\"1\":{\"537\":1}}],[\"也有一些客户聊到项目合作的事情\",{\"1\":{\"21\":1}}],[\"也想分享一点心得\",{\"1\":{\"537\":1}}],[\"也想结合我的申请经历给大家一点启发\",{\"1\":{\"510\":1}}],[\"也更容易共同构建更加人性化\",{\"1\":{\"535\":1}}],[\"也希望通过一个独一无二的作品来打开我的转行道路\",{\"1\":{\"534\":1}}],[\"也希望给大家带来一些新的启发\",{\"1\":{\"530\":1}}],[\"也让我惯于在某一条线上持续一段时间\",{\"1\":{\"533\":1}}],[\"也通过自己创业打开了圈子\",{\"1\":{\"523\":1}}],[\"也被称为第四工业革命\",{\"1\":{\"519\":1}}],[\"也分享一些案例给大家作为灵感参考\",{\"1\":{\"512\":1}}],[\"也许答案应该由更多的观众去解读\",{\"1\":{\"530\":1}}],[\"也许更希望通过用户的互动来产生记录而不是单纯的浏览产生痕迹\",{\"1\":{\"521\":1}}],[\"也许通过这种方式也会让你未来有更多可能性\",{\"1\":{\"509\":1}}],[\"也许最终答案应该由更多的阅读者去解读\",{\"1\":{\"74\":1}}],[\"也算是打造了一个个人体系\",{\"1\":{\"507\":1}}],[\"也看到steam教育越来越被重视\",{\"1\":{\"77\":1}}],[\"也创造了\",{\"1\":{\"48\":1}}],[\"也没办法\",{\"1\":{\"46\":1}}],[\"也不知道过了过久\",{\"1\":{\"46\":1}}],[\"也就是大家口中的金匠\",{\"1\":{\"509\":1}}],[\"也就是现在申请上的ime前身\",{\"1\":{\"508\":1}}],[\"也就是2022年年初\",{\"1\":{\"21\":1}}],[\"也就形成了\",{\"1\":{\"43\":1}}],[\"也是因为毕业设计的思维被挖掘\",{\"1\":{\"534\":1}}],[\"也是企业在ai时代蓬勃发展的必要条件\",{\"1\":{\"533\":1}}],[\"也是企业在ai时代保持领先地位的必要条件\",{\"1\":{\"533\":1}}],[\"也是一种不舒适的体验\",{\"1\":{\"520\":1}}],[\"也是具备很强的跨学科环境\",{\"1\":{\"508\":1}}],[\"也是水上巴士的航线之一\",{\"1\":{\"51\":1,\"52\":1}}],[\"也是能留住作品里最真诚的一部分的原因\",{\"1\":{\"43\":1}}],[\"也是第一次粉丝快速增长期\",{\"1\":{\"20\":1}}],[\"也影响了后期感兴趣的人进场的心态\",{\"1\":{\"42\":1}}],[\"也可能出现许多前期无法预判的事情\",{\"1\":{\"92\":1}}],[\"也可能是纸张残存记忆的巨幅拼图碎片\",{\"1\":{\"69\":1}}],[\"也可能让web3领域有了更多元化的场景\",{\"1\":{\"31\":1}}],[\"也可以被融合\",{\"1\":{\"27\":1}}],[\"也很荣幸和各位艺术家及作品一起在展览见面\",{\"1\":{\"29\":1}}],[\"也找到我进行合作\",{\"1\":{\"28\":1}}],[\"也在创造\",{\"1\":{\"24\":1}}],[\"也体现了我对跨学科的理解和从设计师到复合型岗位发展的转变\",{\"1\":{\"12\":1}}],[\"类比国外的pinterest和instagram\",{\"1\":{\"20\":1}}],[\"的确\",{\"1\":{\"537\":1}}],[\"的时候\",{\"1\":{\"535\":1}}],[\"的时间和屏幕时间\",{\"1\":{\"81\":1}}],[\"的例子以及\",{\"1\":{\"533\":1}}],[\"的视角在构思一些\",{\"1\":{\"530\":1}}],[\"的叙事建筑理论\",{\"1\":{\"530\":1}}],[\"的商业工程\",{\"1\":{\"525\":1}}],[\"的学生对自己的创造力有信心\",{\"1\":{\"524\":1}}],[\"的学生对自身创造力有信心\",{\"1\":{\"524\":1}}],[\"的概念被提出\",{\"1\":{\"519\":1}}],[\"的\",{\"1\":{\"512\":1,\"516\":1}}],[\"的marsbot是提供音频\",{\"1\":{\"512\":1}}],[\"的soundscape和\",{\"1\":{\"512\":1}}],[\"的新书分享了她二十年来探索废弃城市的经历\",{\"1\":{\"511\":1}}],[\"的身份在设计艺术领域有一段快速上升期\",{\"1\":{\"507\":1}}],[\"的身临其境性为已经在努力限制孩子屏幕时间的家长们增加了新的挑战\",{\"1\":{\"81\":1}}],[\"的延续项目\",{\"1\":{\"76\":1}}],[\"的成功在于它就\",{\"1\":{\"49\":1}}],[\"的船\",{\"1\":{\"46\":1}}],[\"的船载着这堆垃圾带孩子逃离世界而来到许多奇幻的岛屿\",{\"1\":{\"23\":1,\"46\":1}}],[\"的地方\",{\"1\":{\"43\":1}}],[\"的故事文本\",{\"1\":{\"46\":1}}],[\"的故事来自于内心最深处的某些触动而编写成的\",{\"1\":{\"43\":1}}],[\"的故事拼贴文本结合历史背景加以穿插\",{\"1\":{\"23\":1}}],[\"的共识占据\",{\"1\":{\"42\":1}}],[\"的模式\",{\"1\":{\"39\":1}}],[\"的数字藏品联合发售活动邀请\",{\"1\":{\"24\":1}}],[\"的首个创作\",{\"1\":{\"23\":1}}],[\"的发展\",{\"0\":{\"18\":1}}],[\"的平台\",{\"1\":{\"9\":1}}],[\"而未来\",{\"1\":{\"537\":1}}],[\"而往后对于设计行业可能面临越来越不平等的市场竞争\",{\"1\":{\"536\":1}}],[\"而非设计师独有\",{\"1\":{\"535\":1}}],[\"而非隔离\",{\"1\":{\"535\":1}}],[\"而艺术思维则强调创造性的探索和质疑\",{\"1\":{\"533\":1}}],[\"而消费设计则侧重于创造欲望和需求\",{\"1\":{\"531\":1}}],[\"而消费设计则通过产品和服务本身的体验来影响消费者\",{\"1\":{\"531\":1}}],[\"而消费设计涉及产品设计\",{\"1\":{\"531\":1}}],[\"而消费设计侧重于产品或服务的物理和用户体验设计\",{\"1\":{\"531\":1}}],[\"而通常这类题材更适合去做一个开放性的反思话题\",{\"1\":{\"530\":1}}],[\"而你寻找了某些\",{\"1\":{\"530\":1}}],[\"而长期竞争则可能引发行业重组\",{\"1\":{\"525\":1}}],[\"而忽视了培养学生的自信心和勇气\",{\"1\":{\"524\":1}}],[\"而忽略了外部环境和文化因素的影响\",{\"1\":{\"522\":1}}],[\"而忽略了更广泛的社会文化背景\",{\"1\":{\"522\":1}}],[\"而忽略了对人本身和背景的关注\",{\"1\":{\"78\":1}}],[\"而机会在于提升研究技能\",{\"1\":{\"518\":1}}],[\"而同时利用ai技术也可以精准定位客户\",{\"1\":{\"518\":1}}],[\"而同样需要一个地方去创造去圆梦\",{\"1\":{\"57\":1}}],[\"而其机会则在于通过数据分析洞察市场趋势\",{\"1\":{\"518\":1}}],[\"而其机会则在于培养领导者的创新能力和适应变革的能力\",{\"1\":{\"518\":1}}],[\"而其机会则在于构建更加高效\",{\"1\":{\"518\":1}}],[\"而不仅仅是\",{\"1\":{\"538\":1}}],[\"而不仅仅是实验\",{\"1\":{\"516\":1}}],[\"而不必用逻辑来证明它们\",{\"1\":{\"523\":1}}],[\"而不必时刻盯着屏幕\",{\"1\":{\"515\":1}}],[\"而不是过程\",{\"1\":{\"538\":1}}],[\"而不是深化现有道路\",{\"1\":{\"533\":1}}],[\"而不是一个艺术家在自由发挥\",{\"1\":{\"530\":1}}],[\"而不是一堆交互模块堆叠而成的视觉艺术品\",{\"1\":{\"509\":1}}],[\"而不是让数据完全主导\",{\"1\":{\"527\":1}}],[\"而不是从人的角度出发\",{\"1\":{\"522\":1}}],[\"而不是上层出现前就完成了\",{\"1\":{\"521\":1}}],[\"而不是突兀地插入\",{\"1\":{\"516\":1}}],[\"而不是仅仅破坏它们\",{\"1\":{\"511\":1}}],[\"而不是在平板上玩游戏\",{\"1\":{\"87\":1}}],[\"而不是完全让他们沉浸在内容中\",{\"1\":{\"82\":1}}],[\"而在其他情况下\",{\"1\":{\"525\":1}}],[\"而在郊区\",{\"1\":{\"512\":1}}],[\"而在2021年\",{\"1\":{\"34\":1}}],[\"而做出来的作品集项目也更加真实和打动人\",{\"1\":{\"511\":1}}],[\"而且如今只有\",{\"1\":{\"538\":1}}],[\"而且如我所做的调研分析\",{\"1\":{\"42\":1}}],[\"而且那几年确实也有一定红利期\",{\"1\":{\"534\":1}}],[\"而且对于这类行业影响甚大\",{\"1\":{\"534\":1}}],[\"而且我更希望去接触一些商科体系的内容来作为我的补充\",{\"1\":{\"508\":1}}],[\"而废弃材料的利用又经由孩子之手得到创造\",{\"1\":{\"74\":1,\"530\":1}}],[\"而欧洲出现的街头抗议也是表明孩子们在冒险游乐场\",{\"1\":{\"74\":1}}],[\"而纸同样成为他们在这片塑料城的一种搭建材料\",{\"1\":{\"69\":1}}],[\"而另一部分被制成气球\",{\"1\":{\"68\":1}}],[\"而船开始运送废料到各个储存罐中\",{\"1\":{\"65\":1}}],[\"而船长也示意着拨下按钮\",{\"1\":{\"46\":1}}],[\"而晚上\",{\"1\":{\"59\":1}}],[\"而一片可变的\",{\"1\":{\"59\":1}}],[\"而一艘\",{\"1\":{\"23\":1,\"46\":1}}],[\"而又可以成为空间的载体\",{\"1\":{\"57\":1}}],[\"而\",{\"1\":{\"48\":1}}],[\"而身后令人发恶的城市也消失于视野之中\",{\"1\":{\"46\":1}}],[\"而我也通过一个美好的假想去回应这个结果\",{\"1\":{\"74\":1,\"530\":1}}],[\"而我也将持续关注与推动这一领域的创新与发展\",{\"1\":{\"17\":1}}],[\"而我尝试去构想一片属于孩子们自己的乌托邦天地\",{\"1\":{\"45\":1}}],[\"而最终篇我要重新回到当今城市去构造一个乌托邦的时候\",{\"1\":{\"43\":1}}],[\"而最终上传的作品更多就是一张图片\",{\"1\":{\"31\":1}}],[\"而是需要结合实际商业价值\",{\"1\":{\"537\":1}}],[\"而是更应该去互补\",{\"1\":{\"533\":1}}],[\"而是关于我们提出的问题\",{\"1\":{\"533\":1}}],[\"而是为了促进颠覆性创新\",{\"1\":{\"533\":1}}],[\"而是作为设计过程的有力补充\",{\"1\":{\"527\":1}}],[\"而是一个必备的能力\",{\"1\":{\"537\":1}}],[\"而是一种轻量级的\",{\"1\":{\"527\":1}}],[\"而是一段思考过程\",{\"1\":{\"31\":1}}],[\"而是让他们产生直觉形成的交互行为能够与\",{\"1\":{\"521\":1}}],[\"而是扩展到营销\",{\"1\":{\"519\":1}}],[\"而是帮助你站在更多维度去思考媒体技术的不同发展\",{\"1\":{\"508\":1}}],[\"而是要为儿童设计和实施一个互动系统\",{\"1\":{\"81\":1}}],[\"而是向更全面的能力发展\",{\"1\":{\"77\":1}}],[\"而是会在世界各地出现很多这样的地方\",{\"1\":{\"43\":1}}],[\"而是存在相互交错又不完全重叠的关系\",{\"1\":{\"43\":1}}],[\"而是可以尽情发挥各种潜力和特色的创作者们\",{\"1\":{\"31\":1}}],[\"而成为吸引大量艺术机构\",{\"1\":{\"28\":1}}],[\"而终点的结果无论如何都不能放弃\",{\"1\":{\"24\":1}}],[\"而这一次\",{\"1\":{\"73\":1}}],[\"而这一个运输塔也不只是运输着废料\",{\"1\":{\"72\":1}}],[\"而这一趟旅途中\",{\"1\":{\"23\":1}}],[\"而这段故事也是重新捕捉我这些幻想而形成的\",{\"1\":{\"43\":1}}],[\"而这次的各大团队尝试了一个商业模式的探索\",{\"1\":{\"30\":1}}],[\"而这些潜力往往在自由探索与冒险中得以释放\",{\"1\":{\"17\":1}}],[\"而设计工作室内容转移到新的小红书账号上\",{\"1\":{\"21\":1}}],[\"激发更多关于孩子创造力教育的讨论与实践\",{\"1\":{\"17\":1}}],[\"激发孩子们的创造力与探索精神\",{\"1\":{\"17\":1}}],[\"这对于需要大量撰写电子邮件的个人和团队特别有用\",{\"1\":{\"539\":1}}],[\"这意味着会有新的领域和行业需要探索\",{\"1\":{\"538\":1}}],[\"这意味着设计师必须保持灵活\",{\"1\":{\"525\":1}}],[\"这意味着设计师需要超越对个体用户的关注\",{\"1\":{\"522\":1}}],[\"这得益于新的人工智能技术\",{\"1\":{\"538\":1}}],[\"这就是知识诅咒在悄悄起作用\",{\"1\":{\"535\":1}}],[\"这就是老人所说盛会\",{\"1\":{\"46\":1}}],[\"这可能包括技术\",{\"1\":{\"525\":1}}],[\"这可能是因为教育系统过于注重学术成就\",{\"1\":{\"524\":1}}],[\"这可能会导致眼睛疲劳和头痛\",{\"1\":{\"81\":1}}],[\"这顶帽子代表谨慎和评估风险\",{\"1\":{\"523\":1}}],[\"这顶帽子代表着感觉和本能\",{\"1\":{\"523\":1}}],[\"这顶帽子时\",{\"1\":{\"523\":1}}],[\"这导致设计倾向于从技术角度解决问题\",{\"1\":{\"522\":1}}],[\"这类经历在当时我会多更多\",{\"1\":{\"534\":1}}],[\"这类产品能够作为教育资源\",{\"1\":{\"520\":1}}],[\"这类群体的精神关注十分不足\",{\"1\":{\"499\":1}}],[\"这启示我们\",{\"1\":{\"519\":1}}],[\"这反映了工业4\",{\"1\":{\"519\":1}}],[\"这强调了设计的数字化沟通和连贯性的重要性\",{\"1\":{\"519\":1}}],[\"这篇文章探讨了在行业4\",{\"1\":{\"519\":1}}],[\"这涉及到数据驱动的设计优化\",{\"1\":{\"519\":1}}],[\"这涉及如何在技术进步的同时处理人类互动和合作的问题\",{\"1\":{\"519\":1}}],[\"这促使制造商从单纯的利润导向转变为学习型组织\",{\"1\":{\"519\":1}}],[\"这要求设计工程必须聚焦于培养创新生态系统\",{\"1\":{\"519\":1}}],[\"这将决定该行业的下一个十年及以后的发展\",{\"1\":{\"538\":1}}],[\"这将有助于克服教育成就与个人创新自信之间的差距\",{\"1\":{\"524\":1}}],[\"这将有助于推动制造业朝着更智能\",{\"1\":{\"519\":1}}],[\"这将使ar教育游戏更加具有体验感\",{\"1\":{\"90\":1}}],[\"这有助于提升生产效率并适应变化\",{\"1\":{\"519\":1}}],[\"这通常涉及一个头戴式设备\",{\"1\":{\"514\":1}}],[\"这也可能会成为创作游戏时的独特素材\",{\"1\":{\"511\":1}}],[\"这也是他们能够创意领域引人瞩目的原因\",{\"1\":{\"509\":1}}],[\"这也是造梦城的最顶端\",{\"1\":{\"66\":1}}],[\"这也是我们在准备筹划的事情\",{\"1\":{\"509\":1}}],[\"这也是我们叫百果岛的原因\",{\"1\":{\"46\":1}}],[\"这也是我拥有跨学科能力的一个基础\",{\"1\":{\"93\":1}}],[\"这也是我当时所行的一些新的收获\",{\"1\":{\"26\":1}}],[\"这也是当时nft市场下\",{\"1\":{\"27\":1}}],[\"这种现象阻碍了有效沟通\",{\"1\":{\"535\":1}}],[\"这种创新方法体现在\",{\"1\":{\"533\":1}}],[\"这种趋势的背后\",{\"1\":{\"531\":1}}],[\"这种方法有助于创建能够有效引导用户决策的设计\",{\"1\":{\"532\":1}}],[\"这种方法有助于在设计决策中找到平衡\",{\"1\":{\"527\":1}}],[\"这种方法特别适用于ux\",{\"1\":{\"527\":1}}],[\"这种方法使他们能够通过实验和迭代来创造指数级的机会\",{\"1\":{\"525\":1}}],[\"这种方法使得产品能够适应消费者生态系统\",{\"1\":{\"519\":1}}],[\"这种能力对于创新\",{\"1\":{\"525\":1}}],[\"这种转变不仅能够更好地服务于用户\",{\"1\":{\"522\":1}}],[\"这种转变不仅要求制造业更新其生产模式\",{\"1\":{\"519\":1}}],[\"这种模式强调合作与资源的流动\",{\"1\":{\"519\":1}}],[\"这种交互方式比传统的屏幕触摸更为自然\",{\"1\":{\"515\":1}}],[\"这种技术通过耳机和语音交互来传递信息\",{\"1\":{\"512\":1}}],[\"这种对废弃空间的热爱是如何转化为在电子游戏中创造独特世界观的\",{\"1\":{\"511\":1}}],[\"这种观点启示我们\",{\"1\":{\"511\":1}}],[\"这种才是真实的\",{\"1\":{\"509\":1}}],[\"这才是一个具有探索心的人应该被看到的东西\",{\"1\":{\"509\":1}}],[\"这句话听起来很老套\",{\"1\":{\"507\":1}}],[\"这款教育游戏通过一个概念地图探索儿童与玩耍指导者之间多样的角色和互动\",{\"1\":{\"84\":1}}],[\"这在具体的操作阶段变得明显\",{\"1\":{\"79\":1}}],[\"这被称为中心化\",{\"1\":{\"79\":1}}],[\"这几个字母只是一个启示\",{\"1\":{\"510\":1}}],[\"这几个老家伙笑了几下\",{\"1\":{\"46\":1}}],[\"这几年对我最有价值的也是设计方面以外的东西\",{\"1\":{\"93\":1}}],[\"这几年经过更多学习交流\",{\"1\":{\"77\":1}}],[\"这几年\",{\"1\":{\"77\":1}}],[\"这几年我通过这个ip获得了许多交流机会\",{\"1\":{\"77\":1}}],[\"这面\",{\"1\":{\"73\":1}}],[\"这上方一片巨大的塑料城都是孩子们的建造空间\",{\"1\":{\"69\":1}}],[\"这片孩子们的造梦天地绽放在江面上\",{\"1\":{\"59\":1}}],[\"这片水域是否可以成为孩子们自由的乌托邦世界\",{\"1\":{\"52\":1}}],[\"这片水域更像是一个被孤立\",{\"1\":{\"52\":1}}],[\"这个概念\",{\"1\":{\"537\":1}}],[\"这个系统负责处理复杂的决策\",{\"1\":{\"532\":1}}],[\"这个系统负责处理日常生活中的简单决策\",{\"1\":{\"532\":1}}],[\"这个思考法可以是由一人也可以是团队来完成\",{\"1\":{\"523\":1}}],[\"这个关键词指的是在industry\",{\"1\":{\"519\":1}}],[\"这个问题像是回到了以前高考对于学校和专业排位选择的思考\",{\"1\":{\"523\":1}}],[\"这个问题答案就出来了\",{\"1\":{\"509\":1}}],[\"这个问题一直被头显制造商在产品上张贴警告所传播\",{\"1\":{\"81\":1}}],[\"这个词来指代垃圾游戏的哲学和理论\",{\"1\":{\"48\":1}}],[\"这个区域充满了创意\",{\"1\":{\"40\":1}}],[\"这不是你们该来的地方\",{\"1\":{\"46\":1}}],[\"这不仅需要跨行业\",{\"1\":{\"36\":1}}],[\"这真的真的像是一场梦\",{\"1\":{\"46\":1}}],[\"这群孩子还没听完就已经兴奋地跑到许多地方去\",{\"1\":{\"46\":1}}],[\"这时另一个洞门被打开\",{\"1\":{\"46\":1}}],[\"这时候的再设计\",{\"1\":{\"530\":1}}],[\"这时候其实已经进入数字藏品\",{\"1\":{\"28\":1}}],[\"这时候我集合了艺术创作\",{\"1\":{\"23\":1}}],[\"这时候我便开始分成两个部分去做内容\",{\"1\":{\"21\":1}}],[\"这时候我也意识到一个新的问题\",{\"1\":{\"20\":1}}],[\"这样才容易打动教授\",{\"1\":{\"510\":1}}],[\"这样才能有效率\",{\"1\":{\"509\":1}}],[\"这样的艺术家\",{\"1\":{\"533\":1}}],[\"这样的世界里会产生怎么样的反应\",{\"1\":{\"74\":1,\"530\":1}}],[\"这样的一个精神乌托邦\",{\"1\":{\"43\":1}}],[\"这样子是不是我们的家是不是就能重新回来了\",{\"1\":{\"46\":1}}],[\"这船经常带回满满的蔬果和各种新鲜稀奇的东西回来给这些被垃圾夺走家园的孩子\",{\"1\":{\"46\":1}}],[\"这艘船每天会在港口运走这一批垃圾\",{\"1\":{\"46\":1}}],[\"这里还藏着一个秘密之地\",{\"1\":{\"72\":1}}],[\"这里没有平台没有停止的地方\",{\"1\":{\"66\":1}}],[\"这里没有横穿的天桥\",{\"1\":{\"52\":1}}],[\"这里有一个通天塔\",{\"1\":{\"66\":1}}],[\"这里有遍地的摩天大楼和及高度的现代规划\",{\"1\":{\"51\":1}}],[\"这里遍布着奇奇怪怪的房子\",{\"1\":{\"46\":1}}],[\"这里也会整理你们那边世界的各种信息作为记忆呢\",{\"1\":{\"46\":1}}],[\"这里也提供了一片丰富的科技商业土壤\",{\"1\":{\"42\":1}}],[\"这里呀\",{\"1\":{\"46\":1}}],[\"这里是百果岛的一个地下生产屋\",{\"1\":{\"46\":1}}],[\"这里是哪里呀\",{\"1\":{\"46\":1}}],[\"这里的树巨大无比\",{\"1\":{\"46\":1}}],[\"这里的孩子似乎合作得十分娴熟\",{\"1\":{\"46\":1}}],[\"这里的创造不仅仅是一种废料的再造\",{\"1\":{\"45\":1}}],[\"这一概念源于20世纪初的消费者工程\",{\"1\":{\"531\":1}}],[\"这一概念您是如何理解的呢\",{\"1\":{\"43\":1}}],[\"这一部分后面我会再复盘给后面申请的同学一些参考\",{\"1\":{\"508\":1}}],[\"这一点在留学申请中也应用到\",{\"1\":{\"92\":1}}],[\"这一个水下的世界是一种对原始垃圾游乐场的回归\",{\"1\":{\"72\":1}}],[\"这一片就如名字般的景象\",{\"1\":{\"46\":1}}],[\"这一次的数据基本开始打开了小红书的流量池\",{\"1\":{\"20\":1}}],[\"这件作品从对场景的构思到最后的呈现经历了怎样的过程\",{\"1\":{\"43\":1}}],[\"这段历史故事也引起我很大的兴趣与共鸣\",{\"1\":{\"43\":1}}],[\"这段期间\",{\"1\":{\"29\":1}}],[\"这段经历不仅让我突破了后端设计的角色局限\",{\"1\":{\"15\":1}}],[\"这仍然是一个陌生的词\",{\"1\":{\"29\":1}}],[\"这两年ai\",{\"1\":{\"26\":1}}],[\"这么多年来一直被保密\",{\"1\":{\"23\":1}}],[\"这些ai生成器是否还能用于创建其他类型的内容\",{\"1\":{\"539\":1}}],[\"这些工具基于机器学习和自然语言处理\",{\"1\":{\"539\":1}}],[\"这些工具能够个性化邮件\",{\"1\":{\"539\":1}}],[\"这些问题将引领我们走向更有趣的现实\",{\"1\":{\"533\":1}}],[\"这些问题主要体现在如何在数字化时代中实现高效决策制定\",{\"1\":{\"519\":1}}],[\"这些决策不仅包括日常琐事\",{\"1\":{\"532\":1}}],[\"这些需要得到某类群体的认同\",{\"1\":{\"521\":1}}],[\"这些词描绘的并没有具体化的数据指标\",{\"1\":{\"521\":1}}],[\"这些技术能够帮助设计师预见并应对可能出现的问题\",{\"1\":{\"519\":1}}],[\"这些设备配备了麦克风和扬声器\",{\"1\":{\"514\":1}}],[\"这些应用程序以及类似的实验取得了重要进展\",{\"1\":{\"512\":1}}],[\"这些地方可以成为回忆和理解过去时代的线索\",{\"1\":{\"511\":1}}],[\"这些一定是申请准备时候最应该的提前筹划的\",{\"1\":{\"508\":1}}],[\"这些是我在毕业的前两年做的一些项目案例\",{\"1\":{\"91\":1}}],[\"这些创新世界将塑造我们的未来\",{\"1\":{\"80\":1}}],[\"这些能力是在随后的具体操作阶段发展起来的\",{\"1\":{\"79\":1}}],[\"这些废弃蔬果的独特颜色和香味被机器所提取\",{\"1\":{\"71\":1}}],[\"这些曾经贫困的地区现在已经变质\",{\"1\":{\"50\":1}}],[\"这些东西在我们祖先发明的这些机器中混合再处理最终会变成岛中的各种肥料\",{\"1\":{\"46\":1}}],[\"这些东西还会通过齿轮再往上运输\",{\"1\":{\"46\":1}}],[\"这些垃圾不知道随着船只去哪了\",{\"1\":{\"46\":1}}],[\"这些都是构成一个\",{\"1\":{\"43\":1}}],[\"这些nft可能代表了特殊的权益\",{\"1\":{\"40\":1}}],[\"这些活动可能包括挑战\",{\"1\":{\"40\":1}}],[\"这些领域都是非常陌生的\",{\"1\":{\"31\":1}}],[\"这些\",{\"1\":{\"23\":2,\"48\":1}}],[\"这堆垃圾在这些岛屿里得到新生\",{\"1\":{\"23\":1,\"46\":1}}],[\"这是利用人工智能技术自动生成电子邮件内容的工具\",{\"1\":{\"539\":1}}],[\"这是关于突破创造力的界限\",{\"1\":{\"533\":1}}],[\"这是关于开辟新道路\",{\"1\":{\"533\":1}}],[\"这是数字营销和消费设计相结合的例子\",{\"1\":{\"531\":1}}],[\"这是属于一片属于孩子们的自由世界\",{\"1\":{\"59\":1}}],[\"这是一片水下的岩石世界\",{\"1\":{\"72\":1}}],[\"这是一朵存在又似乎不属于这个城市的精神之花\",{\"1\":{\"59\":1}}],[\"这是一份孩子们的力量结晶\",{\"1\":{\"53\":1}}],[\"这是一个有意识\",{\"1\":{\"532\":1}}],[\"这是一个无意识\",{\"1\":{\"532\":1}}],[\"这是一个无比新奇的世界\",{\"1\":{\"46\":1}}],[\"这是一个关键概念\",{\"1\":{\"519\":1}}],[\"这是一个现实\",{\"1\":{\"50\":1}}],[\"这是一个充满创造机会的区域\",{\"1\":{\"40\":1}}],[\"这是一个链接权益nft\",{\"1\":{\"40\":1}}],[\"这是一个由用户拥有的地块组成的区域\",{\"1\":{\"40\":1}}],[\"这是一个看似理想的模式\",{\"1\":{\"31\":1}}],[\"这是一个开放性的命题\",{\"1\":{\"17\":1,\"74\":1,\"530\":1}}],[\"这是竟是灯火通明\",{\"1\":{\"46\":1}}],[\"这是百果岛\",{\"1\":{\"46\":1}}],[\"这是哪里呀\",{\"1\":{\"46\":1}}],[\"这是ip方\",{\"1\":{\"40\":1}}],[\"这是我一直坚持的\",{\"1\":{\"31\":1}}],[\"这是我的基本商业模式\",{\"1\":{\"31\":1}}],[\"这是我在平台中的一个录制视频\",{\"1\":{\"29\":1}}],[\"这是我本科的个人毕业设计项目和延续\",{\"1\":{\"17\":1}}],[\"这是\",{\"1\":{\"23\":1}}],[\"这是第二次粉丝快速增长期\",{\"1\":{\"21\":1}}],[\"mcdaniel\",{\"1\":{\"461\":1}}],[\"mcgowan\",{\"1\":{\"257\":1}}],[\"mcgrath\",{\"1\":{\"244\":1}}],[\"mcguire\",{\"1\":{\"156\":1}}],[\"mpc\",{\"1\":{\"407\":1}}],[\"mgts\",{\"1\":{\"352\":1}}],[\"mt\",{\"1\":{\"344\":1,\"365\":3}}],[\"mt5\",{\"0\":{\"315\":1},\"1\":{\"315\":2}}],[\"mbpp\",{\"1\":{\"342\":1}}],[\"méndez\",{\"1\":{\"282\":1}}],[\"mérouane\",{\"1\":{\"239\":1}}],[\"mdes学位的ibd\",{\"1\":{\"510\":1}}],[\"mddiffusion\",{\"1\":{\"235\":1}}],[\"md\",{\"1\":{\"229\":1,\"270\":1,\"298\":2,\"460\":1}}],[\"möller\",{\"1\":{\"189\":1,\"203\":1}}],[\"msc学位的ime\",{\"1\":{\"510\":1}}],[\"mse\",{\"1\":{\"458\":1}}],[\"msa\",{\"1\":{\"434\":2}}],[\"msrvtt\",{\"1\":{\"393\":1}}],[\"msvd\",{\"1\":{\"393\":1}}],[\"msmarco\",{\"1\":{\"386\":1}}],[\"ms\",{\"1\":{\"166\":7}}],[\"mmlu\",{\"1\":{\"454\":1}}],[\"mm\",{\"1\":{\"132\":5,\"330\":5}}],[\"myriad\",{\"1\":{\"359\":1}}],[\"myra\",{\"1\":{\"241\":1}}],[\"my\",{\"0\":{\"122\":1,\"170\":1,\"264\":1,\"448\":1},\"1\":{\"307\":1}}],[\"mrr\",{\"1\":{\"386\":1}}],[\"mrmr\",{\"1\":{\"305\":1}}],[\"mrhp\",{\"1\":{\"117\":2}}],[\"mr\",{\"1\":{\"117\":1}}],[\"m\",{\"0\":{\"286\":1,\"474\":1},\"1\":{\"112\":1,\"136\":1,\"147\":1,\"176\":1,\"220\":1,\"281\":1,\"298\":2,\"392\":1,\"442\":1}}],[\"mlfs\",{\"1\":{\"436\":1}}],[\"mllm\",{\"1\":{\"151\":1,\"348\":1,\"349\":1}}],[\"mllms\",{\"1\":{\"151\":3,\"345\":5,\"348\":3}}],[\"ml\",{\"1\":{\"101\":3,\"104\":1,\"148\":4,\"183\":1,\"204\":1,\"210\":2,\"231\":1,\"539\":1}}],[\"muxserve\",{\"0\":{\"414\":1},\"1\":{\"414\":4}}],[\"muhammed\",{\"1\":{\"408\":1}}],[\"muhammad\",{\"1\":{\"237\":1,\"284\":1,\"453\":1}}],[\"mungall\",{\"1\":{\"398\":1}}],[\"munzner\",{\"1\":{\"249\":1,\"250\":1}}],[\"mutators\",{\"1\":{\"422\":1}}],[\"mutation\",{\"1\":{\"289\":1}}],[\"mutual\",{\"1\":{\"156\":1,\"173\":1,\"255\":2}}],[\"muzammel\",{\"1\":{\"221\":1}}],[\"mukund\",{\"1\":{\"196\":1}}],[\"muresan\",{\"1\":{\"169\":1}}],[\"murong\",{\"1\":{\"132\":1,\"330\":1}}],[\"much\",{\"0\":{\"443\":1},\"1\":{\"123\":1,\"200\":1,\"221\":1,\"255\":1,\"257\":1,\"272\":1,\"292\":1,\"302\":1,\"326\":1,\"340\":1,\"374\":1,\"389\":1,\"391\":1,\"402\":1,\"462\":1,\"467\":1}}],[\"mulates\",{\"1\":{\"414\":1}}],[\"mullins\",{\"1\":{\"104\":1}}],[\"multitude\",{\"1\":{\"498\":1}}],[\"multitask\",{\"0\":{\"413\":1},\"1\":{\"413\":1}}],[\"multiarith\",{\"1\":{\"494\":1}}],[\"multicalibration\",{\"0\":{\"368\":1},\"1\":{\"368\":3}}],[\"multicultural\",{\"0\":{\"133\":1,\"331\":1},\"1\":{\"133\":4,\"331\":4}}],[\"multidisciplinary\",{\"1\":{\"377\":1}}],[\"multidimensional\",{\"1\":{\"207\":1}}],[\"multidomain\",{\"1\":{\"352\":1}}],[\"multigenerator\",{\"1\":{\"352\":1}}],[\"multimatch\",{\"1\":{\"273\":1}}],[\"multimodal\",{\"0\":{\"138\":1,\"151\":1,\"201\":1,\"345\":1,\"348\":1,\"349\":1,\"393\":1},\"1\":{\"151\":1,\"201\":2,\"204\":1,\"226\":1,\"261\":1,\"274\":1,\"345\":3,\"348\":1,\"349\":1,\"351\":1,\"361\":1,\"393\":1,\"486\":2}}],[\"multistage\",{\"1\":{\"436\":1}}],[\"multisensory\",{\"0\":{\"191\":1},\"1\":{\"191\":5}}],[\"multiscale\",{\"1\":{\"156\":3}}],[\"multi\",{\"0\":{\"132\":1,\"146\":1,\"189\":1,\"204\":1,\"269\":1,\"293\":1,\"325\":1,\"330\":1,\"334\":1,\"362\":1,\"364\":1,\"403\":1,\"415\":1,\"418\":1,\"471\":1,\"477\":1,\"495\":1},\"1\":{\"130\":1,\"139\":1,\"176\":1,\"197\":4,\"204\":1,\"224\":1,\"233\":1,\"239\":2,\"242\":2,\"245\":1,\"249\":1,\"252\":1,\"254\":1,\"257\":1,\"269\":5,\"293\":1,\"334\":1,\"338\":1,\"362\":2,\"364\":2,\"375\":1,\"390\":1,\"402\":1,\"403\":2,\"411\":1,\"415\":2,\"418\":1,\"419\":2,\"420\":2,\"426\":1,\"429\":2,\"455\":1,\"471\":1,\"477\":1,\"483\":2,\"495\":1}}],[\"multilingual\",{\"0\":{\"315\":1},\"1\":{\"127\":1,\"315\":3,\"328\":2,\"334\":1,\"352\":1,\"413\":1,\"419\":1,\"420\":1,\"438\":1}}],[\"multiplications\",{\"1\":{\"456\":1}}],[\"multiplication\",{\"1\":{\"415\":1}}],[\"multiplayer\",{\"1\":{\"120\":1}}],[\"multiplexing\",{\"0\":{\"414\":1},\"1\":{\"414\":3}}],[\"multiplex\",{\"1\":{\"253\":1,\"414\":2}}],[\"multiple\",{\"0\":{\"166\":1,\"228\":1,\"240\":1,\"249\":1,\"414\":1},\"1\":{\"99\":1,\"111\":1,\"132\":1,\"146\":2,\"166\":1,\"204\":1,\"213\":2,\"220\":2,\"224\":1,\"240\":1,\"242\":1,\"249\":1,\"270\":1,\"283\":1,\"305\":1,\"330\":1,\"334\":1,\"344\":2,\"372\":1,\"386\":1,\"400\":1,\"405\":1,\"409\":1,\"412\":1,\"414\":2,\"418\":1,\"449\":1,\"459\":1,\"467\":1,\"481\":2}}],[\"multifaceted\",{\"1\":{\"100\":1,\"290\":1}}],[\"musical\",{\"0\":{\"269\":1}}],[\"musk\",{\"1\":{\"179\":1}}],[\"must\",{\"1\":{\"100\":1,\"107\":1,\"135\":1,\"136\":1,\"240\":1,\"261\":1,\"302\":1,\"308\":1,\"322\":1,\"481\":1,\"498\":1}}],[\"museums\",{\"1\":{\"249\":1,\"296\":2}}],[\"museum\",{\"0\":{\"249\":1},\"1\":{\"74\":1,\"249\":5,\"296\":3}}],[\"maeda\",{\"1\":{\"533\":1}}],[\"mauch\",{\"1\":{\"444\":1}}],[\"maurizio\",{\"1\":{\"203\":1}}],[\"mauricio\",{\"1\":{\"186\":1}}],[\"maurice\",{\"1\":{\"152\":1}}],[\"mao\",{\"1\":{\"429\":1}}],[\"maosong\",{\"1\":{\"316\":1,\"411\":1}}],[\"maike\",{\"1\":{\"400\":1}}],[\"mainly\",{\"1\":{\"284\":1,\"318\":1,\"337\":1,\"437\":1,\"496\":1}}],[\"maintenance\",{\"1\":{\"179\":1,\"214\":1,\"284\":1,\"398\":1,\"495\":1}}],[\"maintainers\",{\"1\":{\"445\":1}}],[\"maintains\",{\"1\":{\"427\":1,\"462\":1}}],[\"maintaining\",{\"1\":{\"188\":1,\"244\":1,\"329\":1,\"491\":1,\"496\":1}}],[\"maintain\",{\"1\":{\"167\":1,\"214\":1,\"252\":1,\"284\":1,\"327\":1,\"426\":1,\"491\":1}}],[\"main\",{\"1\":{\"126\":1,\"149\":1,\"171\":1,\"179\":1,\"232\":1,\"235\":1,\"269\":1,\"289\":1,\"292\":1,\"355\":1,\"389\":1,\"469\":2}}],[\"mambai\",{\"0\":{\"365\":1},\"1\":{\"365\":3}}],[\"mamtaj\",{\"1\":{\"206\":1}}],[\"maxwell\",{\"1\":{\"254\":1}}],[\"maximum\",{\"1\":{\"304\":2,\"305\":1,\"365\":1}}],[\"maximise\",{\"1\":{\"350\":2}}],[\"maximilian\",{\"1\":{\"203\":1,\"456\":1}}],[\"maximize\",{\"1\":{\"146\":1,\"197\":1,\"291\":1,\"409\":1,\"414\":1,\"476\":1}}],[\"maxim\",{\"1\":{\"112\":1}}],[\"ma\",{\"1\":{\"212\":1,\"324\":1,\"353\":1,\"377\":1,\"425\":1,\"447\":2,\"480\":1,\"489\":1}}],[\"malmasi\",{\"1\":{\"405\":1}}],[\"maleki\",{\"1\":{\"358\":1}}],[\"malicious\",{\"1\":{\"334\":1,\"374\":1,\"403\":2}}],[\"malik\",{\"1\":{\"237\":1}}],[\"malone\",{\"1\":{\"271\":1,\"457\":1}}],[\"mallory\",{\"1\":{\"207\":1}}],[\"malawi\",{\"0\":{\"178\":1},\"1\":{\"178\":2}}],[\"maja\",{\"1\":{\"261\":1}}],[\"major\",{\"1\":{\"198\":2,\"210\":1,\"259\":1,\"261\":2,\"387\":2,\"434\":1,\"443\":1}}],[\"majority\",{\"0\":{\"176\":1},\"1\":{\"106\":1,\"174\":1,\"176\":3,\"231\":1,\"371\":1,\"391\":1,\"494\":1}}],[\"majidi\",{\"1\":{\"184\":1,\"380\":1}}],[\"magis\",{\"0\":{\"495\":1},\"1\":{\"495\":4}}],[\"magaki\",{\"1\":{\"176\":1}}],[\"magnini\",{\"1\":{\"315\":1}}],[\"magnitudes\",{\"1\":{\"249\":1}}],[\"magnitude\",{\"1\":{\"188\":1,\"496\":1}}],[\"magnify\",{\"1\":{\"151\":1,\"348\":1}}],[\"magnetoencephalography\",{\"1\":{\"256\":1}}],[\"magnetic\",{\"1\":{\"207\":1,\"256\":1}}],[\"magnetite\",{\"1\":{\"207\":1}}],[\"magnets\",{\"1\":{\"115\":2}}],[\"magnet\",{\"0\":{\"115\":1}}],[\"mahmudi\",{\"1\":{\"365\":1}}],[\"mahdieh\",{\"1\":{\"333\":1}}],[\"mahesh\",{\"1\":{\"265\":1}}],[\"maha\",{\"1\":{\"284\":1}}],[\"mahasweta\",{\"1\":{\"205\":1}}],[\"mahajan\",{\"1\":{\"154\":1}}],[\"mahjabin\",{\"1\":{\"198\":1,\"387\":1}}],[\"mahoney\",{\"1\":{\"166\":1}}],[\"mapo\",{\"0\":{\"328\":1}}],[\"mapped\",{\"1\":{\"174\":1,\"371\":1}}],[\"mapping\",{\"0\":{\"428\":1},\"1\":{\"174\":2,\"287\":1,\"371\":2,\"430\":1}}],[\"map\",{\"0\":{\"174\":1,\"371\":1},\"1\":{\"159\":1,\"260\":3,\"270\":1,\"377\":1,\"386\":1,\"407\":1,\"430\":1}}],[\"maps的智能手机地图上显示了一个严重的交通拥堵点\",{\"1\":{\"533\":1}}],[\"maps\",{\"1\":{\"102\":3,\"159\":2,\"207\":1,\"213\":1,\"273\":1,\"301\":1,\"430\":1,\"442\":1}}],[\"madhusudan\",{\"1\":{\"431\":1}}],[\"madalitso\",{\"1\":{\"178\":1}}],[\"madugalla\",{\"1\":{\"154\":1}}],[\"made\",{\"1\":{\"107\":1,\"108\":1,\"123\":1,\"176\":1,\"196\":1,\"238\":1,\"253\":1,\"265\":1,\"275\":1,\"338\":1,\"391\":2,\"393\":1,\"412\":1,\"422\":1,\"463\":1,\"469\":1,\"470\":1}}],[\"masud\",{\"1\":{\"431\":1}}],[\"master\",{\"0\":{\"458\":1}}],[\"mastermind\",{\"0\":{\"429\":1}}],[\"mastodon\",{\"1\":{\"230\":1}}],[\"mask\",{\"1\":{\"460\":1}}],[\"masks\",{\"1\":{\"407\":1}}],[\"masking\",{\"1\":{\"139\":1}}],[\"masoud\",{\"1\":{\"323\":1}}],[\"mason\",{\"1\":{\"106\":1,\"146\":1}}],[\"massie\",{\"1\":{\"384\":1}}],[\"massimi\",{\"1\":{\"279\":1}}],[\"massimo\",{\"1\":{\"158\":1}}],[\"massive\",{\"1\":{\"272\":1,\"377\":1,\"412\":1,\"497\":1}}],[\"massachi\",{\"1\":{\"268\":1,\"452\":1}}],[\"masses\",{\"0\":{\"245\":1}}],[\"mass\",{\"1\":{\"179\":1,\"282\":1}}],[\"matrix\",{\"1\":{\"456\":1}}],[\"matthijs\",{\"1\":{\"496\":1}}],[\"matthias\",{\"1\":{\"296\":1}}],[\"mattia\",{\"1\":{\"492\":1}}],[\"matt\",{\"1\":{\"269\":2,\"294\":1}}],[\"matters\",{\"0\":{\"432\":1}}],[\"matter\",{\"0\":{\"275\":1},\"1\":{\"112\":1,\"207\":1,\"304\":1}}],[\"matarić\",{\"1\":{\"261\":1}}],[\"materialize\",{\"1\":{\"399\":1}}],[\"material\",{\"1\":{\"207\":1}}],[\"materials\",{\"0\":{\"207\":1},\"1\":{\"207\":5,\"249\":1,\"250\":1,\"259\":1,\"365\":1}}],[\"matsumura\",{\"1\":{\"207\":1}}],[\"matching\",{\"0\":{\"472\":1},\"1\":{\"402\":1,\"419\":2,\"420\":2,\"455\":1,\"472\":1,\"488\":1}}],[\"matched\",{\"1\":{\"230\":1}}],[\"matches\",{\"1\":{\"165\":1}}],[\"match\",{\"1\":{\"196\":1,\"399\":1,\"419\":2,\"420\":2,\"453\":1,\"468\":3}}],[\"maturity\",{\"1\":{\"179\":2}}],[\"mathbf\",{\"1\":{\"462\":2}}],[\"mathstackexchange\",{\"1\":{\"458\":1}}],[\"mathieu\",{\"1\":{\"443\":1}}],[\"mathematical\",{\"0\":{\"153\":1},\"1\":{\"132\":2,\"187\":1,\"264\":1,\"330\":2,\"422\":3,\"448\":1,\"458\":3,\"489\":1,\"494\":2}}],[\"mathematics\",{\"0\":{\"132\":1,\"330\":1},\"1\":{\"153\":1,\"333\":1,\"411\":1,\"428\":1,\"458\":3,\"494\":1}}],[\"mathvc\",{\"0\":{\"132\":1,\"330\":1},\"1\":{\"132\":2,\"330\":2}}],[\"math\",{\"0\":{\"458\":2},\"1\":{\"130\":1,\"132\":1,\"330\":1,\"458\":2,\"494\":1}}],[\"manufacturing\",{\"1\":{\"519\":1}}],[\"manually\",{\"1\":{\"356\":1,\"477\":1}}],[\"manual\",{\"1\":{\"154\":2,\"182\":1,\"242\":1,\"339\":1,\"356\":1,\"365\":2,\"398\":1,\"434\":1,\"458\":1,\"459\":1,\"488\":1}}],[\"mantissas\",{\"1\":{\"469\":1}}],[\"mantawy\",{\"1\":{\"127\":1}}],[\"manli\",{\"1\":{\"430\":1}}],[\"man\",{\"1\":{\"429\":1}}],[\"manning\",{\"1\":{\"428\":1}}],[\"manner\",{\"1\":{\"133\":1,\"161\":1,\"324\":1,\"331\":1,\"354\":1,\"382\":1,\"418\":1,\"459\":1,\"498\":1}}],[\"manderson\",{\"1\":{\"407\":1}}],[\"mandel\",{\"1\":{\"272\":1}}],[\"mandal\",{\"1\":{\"372\":1}}],[\"mansi\",{\"1\":{\"258\":1,\"433\":1}}],[\"managed\",{\"1\":{\"415\":1}}],[\"manager\",{\"1\":{\"414\":1,\"495\":1}}],[\"managers\",{\"1\":{\"233\":1,\"254\":1}}],[\"manage\",{\"1\":{\"293\":1}}],[\"management\",{\"0\":{\"244\":1,\"293\":1,\"366\":1},\"1\":{\"183\":1,\"214\":1,\"244\":1,\"361\":1,\"477\":1,\"498\":1}}],[\"managing\",{\"1\":{\"192\":1,\"222\":1,\"359\":1}}],[\"maneuvers\",{\"0\":{\"186\":1},\"1\":{\"186\":3}}],[\"mangili\",{\"1\":{\"171\":1,\"292\":1}}],[\"mani\",{\"1\":{\"475\":1}}],[\"maniple\",{\"1\":{\"350\":3}}],[\"manipulating\",{\"1\":{\"159\":1,\"273\":1,\"375\":1}}],[\"manipulation\",{\"0\":{\"141\":1,\"240\":1,\"480\":1},\"1\":{\"141\":3,\"165\":1,\"269\":1,\"334\":1,\"337\":1,\"375\":2,\"480\":1}}],[\"manipulations\",{\"1\":{\"139\":1,\"141\":3}}],[\"manipulates\",{\"1\":{\"334\":1}}],[\"manipulated\",{\"1\":{\"263\":1}}],[\"manipulate\",{\"1\":{\"141\":1,\"197\":1,\"334\":2}}],[\"manish\",{\"1\":{\"215\":1}}],[\"manifest\",{\"1\":{\"269\":1,\"286\":1,\"474\":1}}],[\"manifestations\",{\"1\":{\"237\":1}}],[\"manifests\",{\"1\":{\"100\":1}}],[\"manifold\",{\"1\":{\"188\":1}}],[\"many\",{\"1\":{\"131\":1,\"137\":1,\"171\":1,\"179\":1,\"182\":1,\"189\":1,\"212\":2,\"250\":1,\"275\":1,\"282\":2,\"284\":1,\"301\":1,\"315\":1,\"326\":1,\"333\":1,\"334\":1,\"350\":2,\"374\":1,\"375\":1,\"382\":1,\"386\":1,\"391\":1,\"402\":1,\"409\":1,\"419\":1,\"420\":1,\"422\":1,\"427\":2,\"428\":1,\"437\":1,\"449\":1,\"475\":1,\"482\":1,\"494\":1}}],[\"may\",{\"1\":{\"111\":1,\"121\":2,\"124\":1,\"135\":1,\"158\":1,\"161\":1,\"166\":1,\"170\":1,\"183\":3,\"197\":1,\"199\":1,\"230\":1,\"243\":1,\"251\":2,\"265\":1,\"278\":1,\"279\":1,\"283\":1,\"286\":1,\"292\":1,\"296\":1,\"304\":1,\"336\":1,\"354\":1,\"363\":1,\"375\":1,\"408\":1,\"411\":1,\"412\":1,\"423\":2,\"424\":2,\"434\":1,\"443\":2,\"446\":1,\"453\":1,\"460\":1,\"474\":1}}],[\"maksym\",{\"1\":{\"409\":1}}],[\"makes\",{\"0\":{\"483\":1},\"1\":{\"153\":1,\"192\":1,\"266\":1,\"365\":1,\"446\":1}}],[\"make\",{\"0\":{\"214\":1},\"1\":{\"107\":2,\"108\":1,\"124\":1,\"139\":1,\"152\":1,\"159\":1,\"172\":1,\"180\":1,\"202\":1,\"254\":1,\"282\":2,\"335\":1,\"338\":1,\"370\":1,\"399\":1,\"405\":1,\"412\":1,\"434\":1,\"441\":1,\"458\":1,\"463\":1,\"469\":1,\"470\":1,\"475\":1,\"494\":1,\"496\":1}}],[\"makeblock等\",{\"1\":{\"78\":1}}],[\"making\",{\"0\":{\"160\":1},\"1\":{\"104\":1,\"106\":1,\"115\":1,\"124\":1,\"146\":1,\"156\":1,\"160\":7,\"171\":3,\"176\":4,\"188\":1,\"206\":1,\"217\":4,\"224\":1,\"226\":3,\"248\":1,\"251\":2,\"261\":1,\"266\":1,\"318\":1,\"334\":1,\"337\":1,\"356\":1,\"393\":1,\"423\":2,\"429\":1,\"437\":1,\"456\":1,\"465\":1,\"466\":2,\"470\":1,\"471\":1,\"479\":1,\"496\":1}}],[\"marzia\",{\"1\":{\"333\":1}}],[\"margins\",{\"1\":{\"378\":2,\"411\":1}}],[\"margin\",{\"1\":{\"369\":1,\"389\":1,\"442\":1}}],[\"marginally\",{\"1\":{\"368\":1}}],[\"marginal\",{\"1\":{\"317\":1}}],[\"margetts\",{\"1\":{\"302\":1}}],[\"maryanne\",{\"1\":{\"336\":1}}],[\"mary\",{\"1\":{\"268\":1,\"452\":1}}],[\"mara\",{\"1\":{\"249\":1,\"250\":1,\"422\":1}}],[\"marcin\",{\"1\":{\"398\":1}}],[\"marc\",{\"1\":{\"379\":1,\"400\":1}}],[\"marcano\",{\"1\":{\"186\":1}}],[\"marco\",{\"1\":{\"120\":1,\"171\":1,\"390\":1,\"467\":1}}],[\"martens\",{\"1\":{\"297\":1}}],[\"martelaro\",{\"1\":{\"164\":1}}],[\"marti\",{\"1\":{\"280\":1}}],[\"martin\",{\"1\":{\"181\":1,\"326\":1,\"368\":1,\"384\":1,\"456\":1}}],[\"maría\",{\"1\":{\"147\":1}}],[\"marie\",{\"1\":{\"379\":1}}],[\"marieke\",{\"1\":{\"297\":1}}],[\"marius\",{\"1\":{\"297\":1}}],[\"marinov\",{\"1\":{\"238\":1}}],[\"mariza\",{\"1\":{\"117\":1}}],[\"maria\",{\"1\":{\"98\":1,\"123\":1,\"133\":1,\"171\":1,\"221\":1,\"315\":1,\"331\":1}}],[\"marks\",{\"1\":{\"356\":1,\"364\":1}}],[\"markus\",{\"1\":{\"297\":1}}],[\"markov\",{\"1\":{\"288\":1}}],[\"markers\",{\"0\":{\"461\":1},\"1\":{\"246\":1,\"461\":3}}],[\"marker\",{\"1\":{\"246\":1}}],[\"markets\",{\"0\":{\"477\":1}}],[\"market\",{\"1\":{\"179\":2,\"233\":2,\"282\":1,\"477\":3}}],[\"markedly\",{\"1\":{\"359\":1,\"460\":1}}],[\"marked\",{\"1\":{\"152\":1,\"442\":1}}],[\"mark\",{\"1\":{\"102\":1,\"108\":1,\"217\":1,\"398\":1,\"418\":1}}],[\"marvin\",{\"1\":{\"102\":1}}],[\"macario\",{\"1\":{\"157\":1}}],[\"machines\",{\"0\":{\"159\":1},\"1\":{\"125\":1,\"179\":1,\"244\":1}}],[\"machine\",{\"0\":{\"102\":1,\"148\":1,\"150\":1,\"160\":1,\"210\":1,\"294\":1,\"305\":1,\"365\":1,\"439\":1},\"1\":{\"100\":1,\"101\":1,\"102\":1,\"110\":2,\"130\":1,\"148\":1,\"158\":1,\"159\":1,\"160\":2,\"183\":2,\"204\":1,\"210\":1,\"231\":1,\"235\":1,\"237\":3,\"253\":4,\"266\":1,\"275\":3,\"283\":1,\"294\":2,\"297\":1,\"309\":2,\"321\":2,\"352\":2,\"365\":1,\"416\":1,\"419\":1,\"420\":1,\"435\":1,\"439\":2,\"460\":1,\"475\":2,\"539\":1}}],[\"maclellan\",{\"1\":{\"99\":1}}],[\"mim\",{\"1\":{\"460\":1}}],[\"mimic\",{\"1\":{\"222\":1,\"309\":1,\"324\":1,\"361\":1,\"379\":1}}],[\"miller\",{\"1\":{\"398\":1}}],[\"million\",{\"1\":{\"395\":1,\"479\":1}}],[\"millions\",{\"1\":{\"317\":1,\"376\":1,\"443\":1}}],[\"milp\",{\"1\":{\"392\":1}}],[\"mihir\",{\"1\":{\"266\":1}}],[\"miyashita\",{\"1\":{\"264\":1,\"448\":1}}],[\"miguel\",{\"1\":{\"277\":1}}],[\"mig2mdqkqxm\",{\"1\":{\"258\":1,\"433\":1}}],[\"might\",{\"1\":{\"98\":1,\"158\":1,\"176\":1,\"203\":1,\"215\":1,\"231\":2,\"243\":1,\"256\":1,\"278\":1,\"301\":1,\"428\":1,\"437\":1}}],[\"mixing\",{\"1\":{\"459\":1}}],[\"mixtral\",{\"1\":{\"340\":1,\"365\":1,\"369\":3}}],[\"mixture\",{\"0\":{\"334\":1,\"427\":1},\"1\":{\"334\":1,\"427\":1}}],[\"mix\",{\"0\":{\"402\":1},\"1\":{\"250\":1,\"365\":1}}],[\"mixed\",{\"0\":{\"98\":1,\"117\":1,\"439\":1},\"1\":{\"98\":6,\"106\":4,\"117\":2,\"127\":1,\"135\":1,\"227\":1,\"252\":1,\"254\":1,\"278\":1,\"290\":1,\"302\":1,\"426\":1,\"439\":4,\"468\":1}}],[\"mitchell\",{\"1\":{\"180\":1}}],[\"mitoses\",{\"1\":{\"176\":2}}],[\"mitigation\",{\"1\":{\"437\":2,\"443\":1,\"467\":1,\"496\":2}}],[\"mitigations\",{\"1\":{\"241\":1}}],[\"mitigating\",{\"1\":{\"136\":1,\"167\":1,\"237\":1}}],[\"mitigated\",{\"1\":{\"341\":1}}],[\"mitigates\",{\"1\":{\"293\":1}}],[\"mitigate\",{\"1\":{\"105\":1,\"110\":1,\"128\":1,\"140\":1,\"150\":1,\"298\":1,\"302\":1,\"304\":1,\"321\":1,\"326\":1,\"328\":1,\"329\":2,\"424\":1,\"437\":1,\"459\":1,\"483\":1,\"488\":1}}],[\"mizuho\",{\"1\":{\"165\":1}}],[\"middle\",{\"1\":{\"178\":2}}],[\"mid\",{\"0\":{\"149\":1},\"1\":{\"149\":4,\"236\":2}}],[\"mifdal\",{\"1\":{\"132\":1,\"330\":1}}],[\"misclassifications\",{\"0\":{\"496\":1},\"1\":{\"496\":2}}],[\"misalign\",{\"1\":{\"437\":1}}],[\"misaligned\",{\"1\":{\"240\":2,\"334\":3}}],[\"mistake\",{\"1\":{\"470\":1}}],[\"mistakes\",{\"0\":{\"470\":1},\"1\":{\"325\":1,\"362\":1,\"470\":6}}],[\"mistral\",{\"1\":{\"356\":3,\"374\":1,\"411\":1,\"466\":2,\"497\":2}}],[\"misunderstandings\",{\"1\":{\"396\":1}}],[\"misu\",{\"1\":{\"303\":1}}],[\"misuse\",{\"1\":{\"213\":1,\"334\":1,\"439\":1}}],[\"misogyny\",{\"1\":{\"302\":2}}],[\"miss\",{\"1\":{\"491\":1}}],[\"mission\",{\"1\":{\"274\":1,\"358\":1}}],[\"missing\",{\"0\":{\"135\":1},\"1\":{\"108\":1,\"219\":1,\"234\":1,\"235\":1,\"240\":2,\"401\":1}}],[\"missed\",{\"1\":{\"250\":1,\"313\":1}}],[\"misleading\",{\"1\":{\"228\":1,\"439\":1}}],[\"mishra\",{\"1\":{\"213\":1}}],[\"mismatch\",{\"1\":{\"190\":1,\"231\":1}}],[\"misinformation\",{\"1\":{\"161\":1,\"241\":1,\"336\":1,\"354\":1}}],[\"misinterpretation\",{\"1\":{\"124\":1}}],[\"microcontroller\",{\"1\":{\"298\":2}}],[\"micro\",{\"1\":{\"284\":1,\"434\":1}}],[\"microsoft\",{\"1\":{\"122\":1,\"512\":2}}],[\"microscopy\",{\"1\":{\"112\":1}}],[\"michaeli\",{\"1\":{\"290\":1}}],[\"michael\",{\"1\":{\"114\":2,\"180\":1,\"182\":1,\"188\":1,\"248\":1,\"279\":1,\"304\":1,\"319\":1,\"408\":1,\"441\":1}}],[\"minerva\",{\"1\":{\"494\":1}}],[\"minervini\",{\"1\":{\"466\":1}}],[\"minerals\",{\"1\":{\"207\":1}}],[\"minkyoung\",{\"1\":{\"356\":1}}],[\"minh\",{\"1\":{\"297\":1}}],[\"minutes\",{\"1\":{\"265\":1,\"376\":1}}],[\"mina\",{\"1\":{\"261\":1}}],[\"minamizawa\",{\"1\":{\"108\":1,\"142\":1}}],[\"minjun\",{\"1\":{\"218\":1}}],[\"minor\",{\"1\":{\"198\":2,\"246\":1,\"387\":2,\"488\":1}}],[\"min\",{\"1\":{\"173\":1}}],[\"mindarm\",{\"0\":{\"284\":1},\"1\":{\"284\":5}}],[\"mindsets\",{\"1\":{\"274\":2}}],[\"mindset\",{\"0\":{\"274\":1},\"1\":{\"274\":4}}],[\"mindscape\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"268\":4,\"452\":4}}],[\"mind\",{\"1\":{\"161\":1,\"284\":2,\"295\":1,\"354\":1}}],[\"mindlin2024beyond\",{\"1\":{\"159\":1}}],[\"mingrui\",{\"1\":{\"347\":1}}],[\"minglei\",{\"1\":{\"235\":1}}],[\"ming\",{\"1\":{\"223\":1,\"355\":2}}],[\"mingzhe\",{\"1\":{\"160\":1,\"164\":1}}],[\"mingwei\",{\"1\":{\"111\":1}}],[\"minigpt\",{\"1\":{\"393\":1}}],[\"minigpt4\",{\"0\":{\"393\":1},\"1\":{\"393\":3}}],[\"miniature\",{\"0\":{\"295\":1},\"1\":{\"295\":1}}],[\"mining\",{\"1\":{\"143\":2,\"343\":2,\"419\":1,\"420\":1}}],[\"minimum\",{\"1\":{\"305\":1}}],[\"minimise\",{\"1\":{\"199\":1}}],[\"minimizing\",{\"1\":{\"196\":1,\"334\":1,\"408\":1,\"489\":1}}],[\"minimize\",{\"0\":{\"496\":1},\"1\":{\"185\":1,\"435\":1}}],[\"minimizes\",{\"1\":{\"139\":1}}],[\"minimal\",{\"1\":{\"260\":1,\"326\":1,\"434\":1,\"478\":1,\"487\":1}}],[\"minimally\",{\"1\":{\"166\":1}}],[\"minima\",{\"1\":{\"112\":1}}],[\"minsuk\",{\"1\":{\"104\":1,\"258\":1,\"433\":1}}],[\"miranda\",{\"1\":{\"302\":1}}],[\"mirrors\",{\"1\":{\"290\":1}}],[\"mirror\",{\"1\":{\"198\":1,\"202\":1,\"266\":1,\"387\":1}}],[\"mirroring\",{\"1\":{\"100\":1}}],[\"mirbaha\",{\"1\":{\"176\":1}}],[\"miriam\",{\"1\":{\"152\":1,\"327\":1}}],[\"miro\",{\"1\":{\"8\":1}}],[\"mei\",{\"1\":{\"463\":1}}],[\"mechtaev\",{\"1\":{\"350\":1}}],[\"mechanoreceptor\",{\"1\":{\"220\":1}}],[\"mechanistic\",{\"1\":{\"390\":1}}],[\"mechanism\",{\"1\":{\"187\":2,\"219\":1,\"259\":1,\"314\":1,\"317\":1,\"334\":1,\"359\":1,\"396\":1,\"401\":1,\"403\":2,\"432\":1,\"456\":1,\"494\":1}}],[\"mechanisms\",{\"1\":{\"101\":1,\"128\":1,\"142\":1,\"205\":1,\"279\":2,\"359\":1,\"390\":1,\"417\":1,\"489\":1}}],[\"mechanized\",{\"0\":{\"284\":1},\"1\":{\"284\":1}}],[\"mechanical\",{\"1\":{\"266\":1,\"355\":1}}],[\"mechanics\",{\"0\":{\"98\":1},\"1\":{\"98\":2,\"271\":1,\"457\":1}}],[\"meltem\",{\"1\":{\"461\":1}}],[\"melika\",{\"1\":{\"323\":1}}],[\"melanie\",{\"1\":{\"120\":1,\"244\":1}}],[\"me\",{\"0\":{\"264\":1,\"448\":1}}],[\"meger\",{\"1\":{\"407\":1}}],[\"meg\",{\"1\":{\"256\":1}}],[\"meets\",{\"0\":{\"444\":1}}],[\"meet\",{\"1\":{\"228\":1,\"396\":1}}],[\"meetings\",{\"1\":{\"279\":2,\"307\":1}}],[\"meeting\",{\"0\":{\"279\":1},\"1\":{\"174\":1,\"249\":1,\"279\":3,\"371\":1}}],[\"meshes\",{\"1\":{\"246\":1}}],[\"message\",{\"1\":{\"230\":2,\"278\":1,\"363\":1}}],[\"messages\",{\"1\":{\"124\":2,\"278\":1,\"363\":1}}],[\"mesa\",{\"1\":{\"194\":1}}],[\"mehta\",{\"1\":{\"325\":1}}],[\"mehrotra\",{\"1\":{\"303\":1}}],[\"mehbub\",{\"1\":{\"298\":1}}],[\"mehner\",{\"1\":{\"187\":1}}],[\"mehar\",{\"1\":{\"133\":1,\"331\":1}}],[\"meyer\",{\"1\":{\"176\":1,\"296\":1}}],[\"meaning\",{\"0\":{\"432\":1},\"1\":{\"281\":1,\"327\":1,\"432\":1,\"460\":1}}],[\"meaningful\",{\"1\":{\"143\":1,\"163\":1,\"226\":1,\"243\":1,\"301\":1,\"391\":1,\"485\":1}}],[\"meanwhile\",{\"1\":{\"212\":1}}],[\"means\",{\"1\":{\"159\":1,\"221\":1,\"288\":1,\"340\":1,\"418\":1,\"437\":1}}],[\"mean\",{\"1\":{\"156\":2,\"197\":2,\"232\":1,\"492\":2}}],[\"meant\",{\"1\":{\"133\":1,\"331\":1}}],[\"measuring\",{\"1\":{\"400\":1}}],[\"measurement\",{\"1\":{\"194\":1}}],[\"measurements\",{\"1\":{\"189\":1}}],[\"measured\",{\"1\":{\"135\":1,\"255\":1,\"304\":1}}],[\"measure\",{\"0\":{\"278\":1,\"338\":1},\"1\":{\"126\":2,\"133\":1,\"156\":1,\"215\":3,\"255\":1,\"278\":1,\"289\":1,\"331\":1,\"335\":2,\"345\":1,\"395\":1,\"428\":2,\"482\":1,\"483\":1,\"489\":1,\"490\":1}}],[\"measures\",{\"1\":{\"120\":1,\"155\":1,\"204\":1,\"224\":1,\"245\":1,\"273\":2,\"278\":1,\"368\":1,\"400\":1,\"435\":1}}],[\"measurable\",{\"1\":{\"121\":1}}],[\"member\",{\"1\":{\"205\":1}}],[\"members\",{\"1\":{\"108\":2,\"230\":2,\"254\":1,\"302\":1}}],[\"memorize\",{\"1\":{\"410\":1}}],[\"memories\",{\"1\":{\"264\":3,\"448\":3}}],[\"memory\",{\"0\":{\"264\":1,\"392\":1,\"434\":1,\"448\":1},\"1\":{\"165\":1,\"175\":1,\"224\":1,\"246\":1,\"264\":5,\"366\":1,\"392\":1,\"414\":1,\"427\":1,\"434\":1,\"448\":5,\"453\":2,\"462\":7,\"465\":1,\"469\":4,\"471\":1,\"479\":1}}],[\"memorability\",{\"1\":{\"131\":2}}],[\"memorable\",{\"0\":{\"131\":1},\"1\":{\"245\":1}}],[\"meditron\",{\"1\":{\"466\":2}}],[\"medico\",{\"1\":{\"253\":1}}],[\"medicine\",{\"1\":{\"238\":1,\"253\":1}}],[\"medical\",{\"0\":{\"189\":1,\"315\":2,\"466\":1},\"1\":{\"176\":4,\"189\":2,\"196\":1,\"220\":1,\"253\":2,\"298\":1,\"315\":7,\"323\":4,\"335\":1,\"356\":5,\"392\":1,\"466\":1}}],[\"medium\",{\"1\":{\"122\":1,\"381\":1}}],[\"mediating\",{\"1\":{\"315\":1}}],[\"mediated\",{\"1\":{\"108\":1,\"142\":1,\"261\":1}}],[\"media\",{\"1\":{\"101\":1,\"152\":1,\"155\":1,\"205\":1,\"239\":3,\"282\":1,\"336\":1,\"431\":1,\"460\":1}}],[\"merx\",{\"1\":{\"365\":1}}],[\"mervyn\",{\"1\":{\"297\":1}}],[\"merely\",{\"1\":{\"142\":1,\"424\":1,\"425\":1}}],[\"merging\",{\"0\":{\"478\":1},\"1\":{\"119\":2,\"478\":2}}],[\"merges\",{\"1\":{\"180\":1,\"442\":1}}],[\"merge\",{\"1\":{\"98\":1}}],[\"mert\",{\"1\":{\"116\":1,\"118\":1}}],[\"men\",{\"0\":{\"302\":1},\"1\":{\"302\":4}}],[\"mendo\",{\"1\":{\"232\":1}}],[\"mentions\",{\"1\":{\"241\":1,\"278\":1}}],[\"mention\",{\"0\":{\"241\":1},\"1\":{\"241\":4}}],[\"mental\",{\"0\":{\"211\":1},\"1\":{\"211\":4,\"237\":1,\"261\":2}}],[\"mentor\",{\"1\":{\"180\":1}}],[\"menassa\",{\"1\":{\"201\":1}}],[\"mengdi\",{\"1\":{\"453\":1}}],[\"meng\",{\"1\":{\"223\":1,\"439\":1}}],[\"mengzhuo\",{\"1\":{\"219\":1,\"401\":1}}],[\"mengzhen\",{\"1\":{\"108\":1}}],[\"mengyan\",{\"1\":{\"167\":1}}],[\"meneguzzi\",{\"1\":{\"96\":1}}],[\"meticulous\",{\"1\":{\"246\":1}}],[\"meticulously\",{\"1\":{\"151\":1,\"174\":1,\"323\":1,\"333\":1,\"348\":1,\"371\":1,\"412\":1,\"424\":1,\"470\":1}}],[\"metaie\",{\"0\":{\"455\":1},\"1\":{\"455\":4}}],[\"metadata\",{\"1\":{\"333\":1,\"358\":1}}],[\"metacheckgpt\",{\"0\":{\"325\":1}}],[\"metaverse\",{\"0\":{\"157\":1},\"1\":{\"157\":2}}],[\"metaxa\",{\"1\":{\"148\":1}}],[\"meta\",{\"0\":{\"129\":1,\"325\":1,\"455\":1},\"1\":{\"129\":2,\"132\":1,\"325\":1,\"330\":1,\"455\":7,\"516\":1}}],[\"metric\",{\"0\":{\"324\":1},\"1\":{\"126\":3,\"245\":1,\"275\":1,\"324\":4,\"335\":1,\"369\":2}}],[\"metrics~\",{\"1\":{\"152\":1}}],[\"metrics\",{\"1\":{\"101\":1,\"121\":1,\"123\":1,\"176\":1,\"210\":2,\"215\":1,\"238\":2,\"245\":1,\"256\":1,\"278\":1,\"282\":1,\"289\":1,\"293\":2,\"309\":5,\"316\":2,\"324\":2,\"335\":1,\"338\":3,\"353\":1,\"362\":1,\"412\":1,\"429\":1,\"432\":2,\"489\":2,\"491\":1}}],[\"methodology\",{\"1\":{\"246\":1,\"249\":1,\"328\":1,\"358\":1,\"359\":1,\"362\":1,\"365\":1,\"377\":1,\"445\":2,\"469\":1,\"470\":1,\"478\":2}}],[\"methodologies\",{\"1\":{\"186\":1,\"210\":1,\"314\":1,\"333\":1,\"358\":1,\"359\":1,\"377\":1,\"398\":2,\"429\":1,\"459\":1}}],[\"methodological\",{\"1\":{\"146\":1,\"492\":1}}],[\"methodical\",{\"1\":{\"174\":1,\"371\":1}}],[\"method\",{\"0\":{\"137\":1,\"146\":1,\"224\":1,\"341\":1},\"1\":{\"106\":1,\"113\":2,\"116\":2,\"118\":3,\"139\":1,\"143\":1,\"146\":1,\"150\":1,\"159\":1,\"174\":1,\"190\":1,\"199\":1,\"203\":1,\"224\":1,\"234\":2,\"240\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"269\":1,\"271\":1,\"281\":1,\"305\":1,\"308\":1,\"323\":1,\"335\":1,\"340\":1,\"341\":2,\"349\":1,\"353\":1,\"355\":1,\"360\":2,\"371\":1,\"378\":2,\"389\":2,\"403\":1,\"407\":1,\"416\":1,\"427\":1,\"432\":1,\"434\":1,\"436\":1,\"442\":2,\"444\":1,\"457\":1,\"459\":1,\"460\":1,\"467\":2,\"471\":2,\"478\":1,\"480\":1,\"483\":1,\"485\":1,\"487\":1,\"488\":2,\"491\":1,\"494\":2,\"495\":1}}],[\"methods\",{\"0\":{\"203\":1,\"238\":1,\"460\":1,\"466\":1},\"1\":{\"97\":2,\"101\":1,\"102\":2,\"103\":1,\"104\":2,\"106\":1,\"123\":2,\"131\":1,\"133\":1,\"135\":1,\"137\":2,\"139\":1,\"141\":1,\"144\":1,\"146\":2,\"165\":2,\"176\":1,\"182\":1,\"185\":1,\"188\":2,\"190\":1,\"191\":2,\"194\":1,\"196\":1,\"199\":2,\"203\":1,\"204\":2,\"205\":1,\"207\":1,\"217\":1,\"223\":1,\"227\":2,\"234\":2,\"237\":2,\"246\":1,\"252\":1,\"253\":1,\"256\":2,\"278\":1,\"312\":2,\"318\":1,\"324\":1,\"325\":1,\"331\":1,\"334\":2,\"335\":1,\"343\":1,\"349\":1,\"350\":1,\"355\":1,\"360\":1,\"368\":1,\"378\":3,\"382\":2,\"384\":1,\"392\":1,\"393\":1,\"398\":2,\"399\":1,\"403\":1,\"404\":2,\"413\":1,\"418\":1,\"419\":1,\"420\":1,\"426\":1,\"427\":1,\"435\":1,\"442\":1,\"443\":1,\"449\":1,\"453\":1,\"459\":1,\"460\":4,\"463\":4,\"467\":2,\"470\":2,\"471\":3,\"472\":1,\"475\":1,\"477\":1,\"481\":2,\"488\":2,\"491\":2}}],[\"mo\",{\"1\":{\"479\":1}}],[\"moe\",{\"1\":{\"427\":2}}],[\"moma\",{\"0\":{\"349\":1},\"1\":{\"349\":3}}],[\"moments\",{\"0\":{\"214\":1},\"1\":{\"376\":1}}],[\"mosaicml\",{\"1\":{\"374\":1}}],[\"mosca\",{\"1\":{\"327\":1}}],[\"mostly\",{\"1\":{\"139\":1,\"191\":1,\"219\":1,\"235\":1,\"315\":1,\"395\":1,\"401\":1,\"495\":1}}],[\"most\",{\"1\":{\"127\":1,\"128\":1,\"132\":1,\"149\":3,\"151\":1,\"165\":1,\"180\":1,\"189\":1,\"197\":1,\"204\":1,\"210\":1,\"216\":1,\"220\":1,\"226\":1,\"228\":1,\"230\":1,\"232\":1,\"234\":1,\"236\":1,\"253\":1,\"254\":1,\"257\":2,\"271\":1,\"301\":1,\"305\":1,\"317\":1,\"330\":1,\"348\":1,\"362\":1,\"366\":1,\"369\":2,\"402\":1,\"409\":1,\"412\":1,\"442\":1,\"445\":1,\"446\":1,\"456\":1,\"457\":1,\"458\":1,\"469\":1,\"475\":1,\"479\":1}}],[\"mozhdeh\",{\"1\":{\"323\":1}}],[\"mozannar\",{\"1\":{\"215\":1}}],[\"mozafari\",{\"1\":{\"169\":1}}],[\"molinet\",{\"1\":{\"315\":1}}],[\"moldovan\",{\"1\":{\"255\":1}}],[\"moltani\",{\"1\":{\"120\":1}}],[\"mohtashami\",{\"1\":{\"456\":1}}],[\"mohaghegh\",{\"1\":{\"323\":1}}],[\"mohammed\",{\"1\":{\"215\":1,\"434\":1}}],[\"mohammadamin\",{\"1\":{\"304\":1}}],[\"mohammadi\",{\"1\":{\"204\":1,\"492\":1}}],[\"mohammad\",{\"1\":{\"159\":1,\"176\":1,\"333\":2,\"408\":1,\"481\":1}}],[\"mohamed\",{\"1\":{\"127\":1,\"393\":1}}],[\"mohila\",{\"1\":{\"176\":1}}],[\"moocs\",{\"1\":{\"519\":1}}],[\"mood\",{\"1\":{\"282\":1}}],[\"moodflow\",{\"0\":{\"499\":1},\"1\":{\"16\":1}}],[\"moon\",{\"1\":{\"238\":1}}],[\"moore\",{\"1\":{\"175\":1}}],[\"mobility\",{\"0\":{\"164\":1},\"1\":{\"164\":1,\"173\":1,\"300\":1,\"303\":1,\"417\":1}}],[\"mobile\",{\"0\":{\"151\":1,\"219\":1,\"348\":1,\"401\":1},\"1\":{\"105\":2,\"151\":1,\"206\":1,\"214\":1,\"219\":1,\"257\":3,\"348\":1,\"401\":1}}],[\"mourad\",{\"1\":{\"460\":1}}],[\"moudenc\",{\"1\":{\"232\":1}}],[\"mouse\",{\"1\":{\"149\":1}}],[\"mounted\",{\"1\":{\"139\":1,\"272\":1,\"306\":2}}],[\"moving\",{\"1\":{\"140\":1}}],[\"move\",{\"0\":{\"417\":1},\"1\":{\"228\":1,\"269\":1,\"278\":1,\"284\":1}}],[\"movements\",{\"1\":{\"127\":1,\"191\":1,\"235\":1,\"246\":2,\"269\":1,\"273\":1}}],[\"movement\",{\"1\":{\"108\":1,\"152\":1,\"182\":5,\"417\":2,\"469\":1}}],[\"moves\",{\"1\":{\"108\":1,\"210\":1}}],[\"motives\",{\"1\":{\"274\":1}}],[\"motivating\",{\"1\":{\"215\":1,\"220\":1}}],[\"motivation\",{\"1\":{\"204\":1}}],[\"motivations\",{\"1\":{\"114\":1}}],[\"motivated\",{\"1\":{\"150\":1,\"279\":1,\"450\":1,\"495\":1}}],[\"motion\",{\"0\":{\"235\":1},\"1\":{\"140\":1,\"235\":6,\"246\":1,\"284\":1,\"407\":1}}],[\"motorbike\",{\"0\":{\"298\":1},\"1\":{\"298\":4}}],[\"motor\",{\"1\":{\"126\":3,\"184\":1,\"380\":1}}],[\"morphology\",{\"1\":{\"497\":1}}],[\"morstatter\",{\"1\":{\"461\":1}}],[\"mortensen\",{\"1\":{\"497\":1}}],[\"morteza\",{\"1\":{\"358\":1,\"461\":1}}],[\"mortality\",{\"1\":{\"121\":1}}],[\"moritz\",{\"1\":{\"210\":1,\"458\":1}}],[\"morality\",{\"1\":{\"461\":1}}],[\"moral\",{\"1\":{\"403\":1}}],[\"morales\",{\"1\":{\"126\":1,\"148\":1}}],[\"moran\",{\"1\":{\"146\":1,\"257\":1}}],[\"moreover\",{\"1\":{\"187\":1,\"191\":1,\"204\":1,\"264\":1,\"313\":1,\"324\":1,\"329\":1,\"337\":1,\"403\":1,\"415\":1,\"428\":1,\"445\":1,\"447\":1,\"448\":1,\"450\":1,\"455\":1,\"496\":1}}],[\"more\",{\"1\":{\"98\":2,\"100\":2,\"102\":1,\"107\":3,\"108\":1,\"113\":1,\"115\":1,\"123\":1,\"131\":4,\"133\":1,\"135\":1,\"137\":1,\"143\":1,\"148\":1,\"151\":1,\"152\":1,\"156\":1,\"158\":1,\"164\":1,\"175\":1,\"180\":1,\"181\":1,\"187\":1,\"189\":1,\"192\":1,\"197\":2,\"198\":1,\"203\":2,\"206\":1,\"209\":2,\"221\":1,\"231\":2,\"235\":1,\"237\":1,\"238\":2,\"240\":2,\"242\":1,\"249\":1,\"254\":1,\"256\":2,\"261\":2,\"265\":1,\"266\":3,\"271\":1,\"275\":1,\"288\":1,\"292\":1,\"301\":1,\"302\":2,\"303\":1,\"305\":1,\"313\":2,\"322\":1,\"324\":1,\"331\":1,\"334\":2,\"337\":1,\"340\":2,\"342\":3,\"344\":1,\"345\":1,\"348\":1,\"359\":1,\"374\":1,\"377\":1,\"381\":1,\"382\":1,\"387\":1,\"399\":1,\"403\":1,\"411\":1,\"412\":2,\"414\":1,\"416\":1,\"428\":3,\"430\":1,\"437\":1,\"441\":1,\"442\":2,\"457\":1,\"463\":1,\"465\":2,\"466\":1,\"480\":1,\"487\":1,\"494\":1,\"496\":1}}],[\"monotonic\",{\"1\":{\"350\":1,\"355\":1}}],[\"monolingual\",{\"1\":{\"328\":1,\"352\":1}}],[\"monajatipoor\",{\"1\":{\"323\":1}}],[\"monai\",{\"1\":{\"238\":1}}],[\"mondada\",{\"1\":{\"292\":1}}],[\"monetary\",{\"1\":{\"233\":1,\"408\":1}}],[\"money\",{\"1\":{\"107\":1,\"158\":1}}],[\"monroy\",{\"1\":{\"147\":1}}],[\"months\",{\"1\":{\"126\":1,\"469\":1}}],[\"montambault\",{\"1\":{\"111\":1}}],[\"monitor\",{\"1\":{\"112\":1,\"120\":1,\"132\":1,\"175\":1,\"330\":1}}],[\"monitoring\",{\"0\":{\"101\":1,\"295\":1,\"298\":1},\"1\":{\"101\":2,\"112\":1,\"137\":1,\"170\":1,\"194\":2,\"278\":1,\"295\":2,\"298\":4,\"477\":2}}],[\"monster\",{\"1\":{\"40\":1}}],[\"monsterisland\",{\"1\":{\"22\":2,\"23\":5,\"39\":1}}],[\"modi\",{\"1\":{\"372\":1}}],[\"modified\",{\"1\":{\"422\":2,\"428\":2}}],[\"modification\",{\"1\":{\"191\":1,\"428\":2}}],[\"modify\",{\"1\":{\"160\":1,\"191\":1,\"415\":1,\"422\":1}}],[\"modifying\",{\"1\":{\"140\":1,\"142\":1,\"167\":1,\"222\":1,\"269\":1,\"463\":1}}],[\"modal\",{\"0\":{\"362\":1,\"419\":1,\"420\":1},\"1\":{\"257\":1,\"340\":2,\"375\":1,\"419\":2,\"420\":2}}],[\"modality\",{\"0\":{\"163\":1},\"1\":{\"163\":2,\"185\":2,\"190\":2,\"220\":1,\"240\":1,\"355\":4,\"362\":2}}],[\"modalities\",{\"0\":{\"240\":1},\"1\":{\"99\":1,\"123\":1,\"240\":2}}],[\"modulate\",{\"1\":{\"274\":1}}],[\"modulation\",{\"0\":{\"274\":1}}],[\"modularity\",{\"1\":{\"316\":1}}],[\"modularized\",{\"1\":{\"316\":1}}],[\"modular\",{\"0\":{\"338\":1},\"1\":{\"186\":1,\"300\":1,\"338\":1,\"353\":1,\"398\":1}}],[\"modules\",{\"1\":{\"218\":3,\"298\":1,\"314\":1,\"438\":1}}],[\"module\",{\"0\":{\"343\":1},\"1\":{\"103\":1,\"186\":1,\"234\":2,\"235\":1,\"284\":2,\"287\":1,\"349\":1,\"355\":1,\"438\":2}}],[\"mode\",{\"1\":{\"226\":2,\"269\":2,\"271\":5,\"457\":5}}],[\"moderate\",{\"1\":{\"304\":1}}],[\"moderates\",{\"1\":{\"233\":2}}],[\"moderators\",{\"1\":{\"200\":1}}],[\"moderation\",{\"0\":{\"344\":1},\"1\":{\"200\":1,\"230\":1,\"344\":1}}],[\"modern\",{\"0\":{\"216\":1},\"1\":{\"98\":1,\"133\":3,\"196\":1,\"216\":2,\"253\":1,\"331\":3,\"381\":1,\"465\":2}}],[\"modes\",{\"0\":{\"271\":1,\"457\":1},\"1\":{\"133\":1,\"226\":3,\"269\":2,\"271\":1,\"331\":1,\"457\":1}}],[\"modelling\",{\"1\":{\"282\":1}}],[\"modeling\",{\"0\":{\"259\":2},\"1\":{\"131\":1,\"132\":2,\"138\":3,\"201\":1,\"223\":1,\"237\":1,\"241\":1,\"246\":1,\"259\":9,\"330\":2,\"402\":2,\"410\":1,\"411\":1,\"431\":1,\"454\":1,\"468\":1,\"472\":1,\"475\":1,\"479\":1}}],[\"models~\",{\"1\":{\"445\":1}}],[\"models\",{\"0\":{\"97\":1,\"101\":1,\"102\":1,\"121\":1,\"136\":1,\"175\":1,\"180\":1,\"183\":1,\"201\":1,\"210\":1,\"215\":1,\"245\":1,\"251\":1,\"261\":1,\"275\":1,\"289\":1,\"308\":1,\"312\":1,\"325\":1,\"347\":2,\"353\":1,\"369\":1,\"381\":1,\"394\":1,\"423\":1,\"429\":1,\"444\":1,\"446\":1,\"458\":1,\"497\":1,\"498\":1},\"1\":{\"99\":1,\"101\":3,\"102\":1,\"110\":1,\"114\":1,\"123\":1,\"124\":1,\"125\":1,\"127\":1,\"130\":1,\"132\":1,\"133\":1,\"136\":1,\"138\":1,\"148\":1,\"151\":1,\"154\":1,\"158\":1,\"169\":1,\"172\":2,\"175\":1,\"180\":1,\"183\":3,\"184\":1,\"187\":1,\"198\":1,\"199\":2,\"204\":1,\"207\":1,\"210\":5,\"213\":4,\"215\":3,\"216\":1,\"224\":2,\"228\":1,\"229\":3,\"234\":1,\"237\":1,\"238\":2,\"241\":1,\"242\":1,\"244\":2,\"245\":1,\"246\":1,\"251\":1,\"252\":1,\"258\":3,\"259\":1,\"260\":1,\"261\":1,\"268\":1,\"283\":1,\"286\":3,\"289\":1,\"291\":1,\"300\":1,\"308\":1,\"309\":1,\"313\":2,\"314\":1,\"315\":3,\"316\":4,\"317\":2,\"318\":1,\"319\":2,\"321\":1,\"322\":2,\"323\":1,\"324\":1,\"325\":2,\"326\":1,\"327\":2,\"328\":1,\"329\":5,\"330\":1,\"331\":1,\"333\":1,\"334\":3,\"335\":1,\"336\":5,\"337\":3,\"338\":1,\"339\":4,\"340\":7,\"341\":1,\"342\":1,\"343\":1,\"344\":6,\"345\":1,\"347\":2,\"348\":1,\"349\":1,\"350\":2,\"352\":1,\"353\":1,\"355\":1,\"356\":1,\"359\":3,\"360\":1,\"362\":5,\"363\":1,\"364\":1,\"365\":1,\"368\":1,\"369\":2,\"370\":2,\"372\":2,\"374\":2,\"376\":2,\"377\":1,\"378\":2,\"379\":2,\"380\":1,\"381\":1,\"382\":2,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":2,\"390\":2,\"391\":1,\"392\":1,\"393\":1,\"394\":4,\"395\":7,\"396\":1,\"398\":1,\"400\":1,\"403\":4,\"405\":1,\"407\":1,\"409\":4,\"410\":1,\"411\":4,\"412\":2,\"414\":1,\"416\":1,\"417\":2,\"418\":3,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":3,\"425\":2,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"431\":1,\"432\":2,\"433\":3,\"434\":1,\"435\":1,\"436\":4,\"437\":1,\"438\":1,\"439\":1,\"441\":5,\"442\":1,\"443\":2,\"444\":2,\"446\":1,\"447\":2,\"449\":3,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":2,\"458\":1,\"459\":3,\"460\":3,\"461\":1,\"462\":1,\"463\":3,\"465\":4,\"466\":3,\"467\":3,\"468\":1,\"470\":2,\"471\":5,\"472\":1,\"474\":3,\"475\":2,\"476\":1,\"478\":1,\"479\":2,\"481\":3,\"482\":1,\"483\":1,\"485\":3,\"486\":1,\"487\":2,\"489\":1,\"490\":1,\"491\":1,\"492\":2,\"494\":1,\"495\":1,\"496\":3,\"497\":7,\"498\":1}}],[\"model\",{\"0\":{\"114\":1,\"116\":1,\"138\":1,\"167\":1,\"223\":2,\"235\":1,\"242\":1,\"313\":1,\"324\":1,\"335\":1,\"339\":1,\"377\":1,\"402\":1,\"408\":1,\"455\":1,\"468\":1},\"1\":{\"96\":1,\"102\":1,\"104\":1,\"112\":1,\"118\":1,\"121\":1,\"137\":1,\"148\":3,\"150\":1,\"151\":2,\"159\":2,\"167\":2,\"170\":1,\"171\":2,\"174\":1,\"175\":2,\"183\":1,\"187\":2,\"190\":1,\"192\":1,\"194\":1,\"196\":2,\"199\":1,\"201\":1,\"204\":2,\"210\":3,\"213\":1,\"215\":1,\"216\":1,\"219\":1,\"221\":1,\"223\":3,\"231\":2,\"234\":1,\"235\":1,\"237\":1,\"238\":1,\"240\":2,\"251\":3,\"255\":1,\"258\":2,\"259\":1,\"264\":2,\"266\":2,\"269\":1,\"275\":2,\"284\":1,\"294\":1,\"305\":1,\"309\":1,\"313\":2,\"315\":1,\"316\":1,\"317\":1,\"319\":1,\"322\":3,\"324\":1,\"325\":4,\"327\":1,\"328\":2,\"329\":2,\"334\":4,\"336\":1,\"337\":3,\"339\":3,\"347\":3,\"348\":2,\"349\":5,\"350\":2,\"353\":2,\"355\":2,\"356\":1,\"358\":1,\"359\":1,\"362\":2,\"366\":1,\"371\":1,\"377\":2,\"379\":1,\"384\":1,\"385\":1,\"388\":2,\"390\":2,\"393\":5,\"396\":1,\"399\":2,\"400\":1,\"401\":1,\"402\":5,\"403\":6,\"404\":1,\"407\":3,\"411\":1,\"415\":1,\"416\":1,\"418\":3,\"419\":1,\"420\":1,\"423\":3,\"427\":1,\"430\":1,\"431\":2,\"433\":2,\"435\":6,\"439\":1,\"441\":3,\"442\":3,\"443\":1,\"448\":2,\"449\":2,\"450\":3,\"453\":1,\"454\":2,\"455\":7,\"456\":2,\"465\":1,\"466\":1,\"467\":3,\"468\":4,\"469\":2,\"470\":1,\"477\":1,\"479\":1,\"480\":2,\"481\":1,\"485\":4,\"487\":4,\"488\":2,\"489\":4,\"492\":2,\"494\":1,\"496\":1}}],[\"探索而不是深化\",{\"1\":{\"533\":1}}],[\"探索也是一种创造\",{\"1\":{\"530\":1}}],[\"探索的正是儿童的创造力培养问题\",{\"1\":{\"78\":1}}],[\"探索科技艺术如何赋能儿童教育\",{\"1\":{\"77\":1}}],[\"探索未知事物某种角度也可以说一种创造\",{\"1\":{\"74\":1}}],[\"探索\",{\"1\":{\"74\":1,\"530\":1}}],[\"探索多元化的展现方式\",{\"1\":{\"16\":1}}],[\"探索数字艺术的无限可能\",{\"1\":{\"14\":1}}],[\"还可以用于创建博客文章\",{\"1\":{\"539\":1}}],[\"还是确实有更深刻的事情正在发生\",{\"1\":{\"538\":1}}],[\"还是先问问自己什么是当下最想得到的\",{\"1\":{\"508\":1}}],[\"还重新定义了人们如何看待和与家的概念互动\",{\"1\":{\"533\":1}}],[\"还包括更重要的决定\",{\"1\":{\"532\":1}}],[\"还包括产品\",{\"1\":{\"522\":1}}],[\"还能通过实验和迭代来测试业务模型\",{\"1\":{\"525\":1}}],[\"还能够促进社会的整体福祉\",{\"1\":{\"522\":1}}],[\"还为解决社会问题提供了创新思路\",{\"1\":{\"520\":1}}],[\"还需员工具备跨学科能力和创新技能\",{\"1\":{\"519\":1}}],[\"还需要进行更多的研究来确认潜在的风险\",{\"1\":{\"82\":1}}],[\"还扩展到市场营销和客户服务\",{\"1\":{\"519\":1}}],[\"还有你缺少的信息\",{\"1\":{\"523\":1}}],[\"还有学校带来的资源和人脉\",{\"1\":{\"507\":1}}],[\"还有那种平等相处能让他们忘却出身背景的氛围\",{\"1\":{\"74\":1}}],[\"还有其他还多小孩呢\",{\"1\":{\"46\":1}}],[\"还有水底下的大苹果\",{\"1\":{\"46\":1}}],[\"还有\",{\"1\":{\"46\":1,\"516\":1}}],[\"还有呀\",{\"1\":{\"46\":1}}],[\"还有不同的废料集中营\",{\"1\":{\"43\":1}}],[\"还有艺术家恩利的杂志数字艺术合作\",{\"1\":{\"23\":1}}],[\"还涉及到标准授权\",{\"1\":{\"36\":1}}],[\"还负责团队管理与运营\",{\"1\":{\"15\":1}}],[\"还全方位提升了我的专业能力\",{\"1\":{\"15\":1}}],[\"kv\",{\"0\":{\"366\":1},\"1\":{\"366\":6,\"456\":2,\"462\":4,\"469\":2}}],[\"kseniia\",{\"1\":{\"351\":1,\"352\":1}}],[\"kyung\",{\"1\":{\"274\":1}}],[\"kyle\",{\"1\":{\"269\":1,\"384\":1}}],[\"kgc\",{\"1\":{\"382\":3}}],[\"kg\",{\"1\":{\"252\":5,\"314\":1,\"382\":1,\"426\":5}}],[\"kgs\",{\"1\":{\"252\":3,\"314\":4,\"426\":3}}],[\"khodadadi\",{\"1\":{\"408\":1}}],[\"kholodna\",{\"1\":{\"408\":1}}],[\"khoyane\",{\"1\":{\"232\":1}}],[\"khashabi\",{\"1\":{\"385\":1}}],[\"khayyam\",{\"0\":{\"333\":1},\"1\":{\"333\":3}}],[\"khadiza\",{\"1\":{\"298\":1}}],[\"khan\",{\"1\":{\"257\":1}}],[\"khanlou\",{\"1\":{\"176\":1}}],[\"khameh\",{\"1\":{\"176\":1}}],[\"kwon\",{\"1\":{\"213\":1}}],[\"klakow\",{\"1\":{\"395\":1}}],[\"klasén\",{\"1\":{\"181\":1}}],[\"kleiven\",{\"1\":{\"269\":1}}],[\"kleinman\",{\"1\":{\"98\":1}}],[\"kleesiek\",{\"1\":{\"238\":1,\"379\":1}}],[\"knee\",{\"1\":{\"246\":1}}],[\"knuth\",{\"1\":{\"203\":1}}],[\"knobs\",{\"1\":{\"465\":2}}],[\"knoll\",{\"1\":{\"189\":1}}],[\"know\",{\"1\":{\"454\":1}}],[\"known\",{\"1\":{\"128\":1,\"165\":1,\"183\":1,\"194\":1,\"199\":1,\"214\":1,\"221\":1,\"238\":1,\"239\":1,\"251\":1,\"257\":1,\"282\":1,\"286\":1,\"289\":1,\"326\":1,\"333\":1,\"410\":1,\"418\":1,\"423\":1,\"453\":1,\"463\":1,\"468\":1,\"474\":1,\"489\":1}}],[\"knowledge\",{\"0\":{\"133\":1,\"192\":1,\"211\":1,\"252\":1,\"314\":1,\"328\":1,\"331\":1,\"340\":1,\"382\":1,\"390\":1,\"426\":1,\"463\":1,\"489\":1},\"1\":{\"99\":1,\"107\":1,\"112\":1,\"125\":1,\"131\":1,\"132\":1,\"133\":3,\"138\":1,\"153\":1,\"171\":1,\"189\":2,\"190\":2,\"192\":3,\"205\":1,\"206\":1,\"211\":3,\"222\":1,\"223\":1,\"236\":1,\"240\":1,\"251\":2,\"252\":2,\"254\":1,\"283\":2,\"314\":4,\"315\":1,\"317\":1,\"318\":4,\"323\":1,\"328\":3,\"329\":2,\"330\":1,\"331\":3,\"340\":8,\"378\":1,\"382\":1,\"384\":3,\"386\":1,\"390\":6,\"391\":1,\"392\":1,\"409\":1,\"423\":2,\"425\":2,\"426\":2,\"428\":1,\"430\":1,\"437\":1,\"445\":1,\"449\":2,\"450\":2,\"453\":1,\"463\":5,\"475\":2,\"477\":1,\"481\":1,\"486\":2,\"489\":6}}],[\"knight\",{\"1\":{\"125\":1}}],[\"kraske\",{\"1\":{\"288\":1}}],[\"kryptos\",{\"1\":{\"192\":1}}],[\"kristofer\",{\"1\":{\"398\":1}}],[\"kristina\",{\"1\":{\"241\":1}}],[\"kristjan\",{\"1\":{\"216\":1}}],[\"krishna\",{\"1\":{\"140\":1}}],[\"krishnasamy\",{\"1\":{\"137\":1}}],[\"kriglstein\",{\"1\":{\"129\":1}}],[\"krithik\",{\"1\":{\"113\":1}}],[\"krokos\",{\"1\":{\"119\":1}}],[\"kuang\",{\"1\":{\"355\":1}}],[\"kullback\",{\"1\":{\"273\":1}}],[\"kuc\",{\"1\":{\"268\":1,\"452\":1}}],[\"kundu\",{\"1\":{\"436\":1}}],[\"kunpeng\",{\"1\":{\"349\":1}}],[\"kun\",{\"1\":{\"185\":1,\"296\":1,\"432\":1}}],[\"kuno\",{\"1\":{\"152\":1,\"182\":1}}],[\"kumar\",{\"1\":{\"172\":1,\"190\":1,\"265\":1,\"303\":1,\"370\":1,\"374\":2}}],[\"kuzuoka\",{\"1\":{\"165\":1}}],[\"kuzminykh\",{\"1\":{\"163\":1}}],[\"kurzhals\",{\"1\":{\"152\":1,\"182\":1}}],[\"kuotian\",{\"1\":{\"207\":1}}],[\"kuo\",{\"1\":{\"116\":1,\"118\":1}}],[\"kuttal\",{\"1\":{\"106\":1}}],[\"kompella\",{\"1\":{\"442\":1}}],[\"koraş\",{\"1\":{\"379\":1}}],[\"koala\",{\"0\":{\"376\":1},\"1\":{\"376\":1}}],[\"ko\",{\"1\":{\"356\":1}}],[\"kou\",{\"1\":{\"293\":1}}],[\"kouta\",{\"1\":{\"108\":1,\"142\":1}}],[\"kodali\",{\"1\":{\"220\":1}}],[\"kojić\",{\"1\":{\"203\":1}}],[\"kostis\",{\"1\":{\"217\":1}}],[\"kostas\",{\"1\":{\"192\":1}}],[\"koshy\",{\"1\":{\"200\":1}}],[\"koesten\",{\"1\":{\"189\":1}}],[\"kokorin\",{\"1\":{\"166\":1}}],[\"kochmar\",{\"1\":{\"351\":1,\"352\":1}}],[\"koch\",{\"1\":{\"152\":1,\"182\":1}}],[\"konrad\",{\"1\":{\"399\":1}}],[\"kontar\",{\"1\":{\"150\":1}}],[\"konda\",{\"1\":{\"148\":1}}],[\"koo\",{\"1\":{\"114\":1}}],[\"ke\",{\"1\":{\"471\":1}}],[\"keiichi\",{\"1\":{\"468\":1}}],[\"kexun\",{\"1\":{\"462\":1}}],[\"keqing\",{\"1\":{\"449\":1}}],[\"kelly\",{\"1\":{\"304\":1}}],[\"kenny\",{\"1\":{\"271\":1,\"457\":1}}],[\"kebe\",{\"1\":{\"232\":1}}],[\"kesavaraj\",{\"1\":{\"190\":1}}],[\"kerne\",{\"1\":{\"156\":1}}],[\"kernel\",{\"1\":{\"112\":1,\"462\":1}}],[\"keepers\",{\"0\":{\"461\":1}}],[\"keep\",{\"1\":{\"443\":1,\"482\":1,\"498\":1}}],[\"keeping\",{\"1\":{\"119\":1,\"161\":1,\"354\":1}}],[\"kee\",{\"1\":{\"356\":1}}],[\"keefe\",{\"1\":{\"325\":1}}],[\"keen\",{\"1\":{\"151\":1,\"348\":1}}],[\"kevin\",{\"1\":{\"112\":1,\"220\":1,\"257\":1,\"522\":1}}],[\"keyuan\",{\"1\":{\"453\":1}}],[\"keys\",{\"1\":{\"192\":1}}],[\"keyser\",{\"1\":{\"156\":1}}],[\"keywords\",{\"1\":{\"190\":1,\"278\":1,\"282\":1,\"386\":1}}],[\"keyword\",{\"0\":{\"190\":1},\"1\":{\"190\":1,\"278\":1}}],[\"key\",{\"0\":{\"376\":1,\"430\":1},\"1\":{\"105\":1,\"125\":1,\"127\":1,\"192\":1,\"212\":1,\"213\":1,\"224\":1,\"227\":1,\"229\":1,\"237\":2,\"241\":1,\"246\":1,\"257\":1,\"271\":1,\"298\":1,\"304\":1,\"305\":1,\"338\":1,\"353\":1,\"360\":1,\"366\":1,\"376\":2,\"378\":1,\"384\":1,\"396\":2,\"414\":1,\"415\":1,\"417\":1,\"430\":2,\"438\":1,\"457\":1,\"492\":1}}],[\"kabir\",{\"1\":{\"460\":1}}],[\"kakodkar\",{\"1\":{\"407\":1}}],[\"kaustubh\",{\"1\":{\"386\":1}}],[\"kaur\",{\"1\":{\"106\":1}}],[\"kazakov\",{\"1\":{\"351\":1,\"352\":1}}],[\"kangli\",{\"1\":{\"480\":1}}],[\"kang\",{\"1\":{\"324\":1,\"392\":1}}],[\"kanchana\",{\"1\":{\"319\":1}}],[\"kanij\",{\"1\":{\"154\":1}}],[\"kayser\",{\"1\":{\"298\":1}}],[\"kapania\",{\"1\":{\"286\":1,\"474\":1}}],[\"kappa$\",{\"1\":{\"194\":1}}],[\"kappa\",{\"1\":{\"194\":1,\"335\":1}}],[\"kamoi\",{\"1\":{\"391\":1}}],[\"kamiya\",{\"1\":{\"341\":1}}],[\"kamuni\",{\"1\":{\"265\":1}}],[\"kamat\",{\"1\":{\"201\":1}}],[\"kamamia\",{\"1\":{\"178\":1}}],[\"kamaraj\",{\"1\":{\"150\":1}}],[\"kaleb\",{\"1\":{\"379\":1}}],[\"kaleen\",{\"1\":{\"261\":1}}],[\"kalinin\",{\"1\":{\"112\":1}}],[\"karia\",{\"1\":{\"490\":1}}],[\"karidi\",{\"1\":{\"328\":1}}],[\"karlinsky\",{\"1\":{\"454\":1}}],[\"karla\",{\"1\":{\"240\":1}}],[\"karachiwalla\",{\"1\":{\"184\":1,\"380\":1}}],[\"karam\",{\"1\":{\"176\":1}}],[\"karunanayake\",{\"1\":{\"125\":1}}],[\"kafai\",{\"1\":{\"148\":1}}],[\"kaiqi\",{\"1\":{\"462\":1}}],[\"kairouz\",{\"1\":{\"435\":1}}],[\"kai\",{\"1\":{\"147\":1,\"323\":1,\"337\":1,\"392\":1,\"489\":1}}],[\"kaiming\",{\"1\":{\"105\":1}}],[\"kao\",{\"1\":{\"142\":1}}],[\"kasneci\",{\"1\":{\"127\":1,\"139\":1,\"273\":1,\"290\":1}}],[\"kahng\",{\"1\":{\"104\":1,\"258\":1,\"433\":1}}],[\"katrina\",{\"1\":{\"365\":1}}],[\"katherine\",{\"1\":{\"236\":1}}],[\"katja\",{\"1\":{\"233\":1}}],[\"katya\",{\"1\":{\"158\":1}}],[\"kate\",{\"1\":{\"102\":1,\"376\":1}}],[\"kato\",{\"1\":{\"81\":1}}],[\"k\",{\"1\":{\"15\":3,\"462\":1,\"486\":1}}],[\"kilian\",{\"1\":{\"494\":1}}],[\"kishore\",{\"1\":{\"424\":1}}],[\"kian\",{\"1\":{\"261\":1}}],[\"kiosk\",{\"1\":{\"249\":1}}],[\"kitchen\",{\"1\":{\"381\":1}}],[\"kit\",{\"1\":{\"205\":1,\"398\":1}}],[\"kitto\",{\"1\":{\"125\":1}}],[\"kidney\",{\"0\":{\"196\":1},\"1\":{\"196\":5}}],[\"kids艺术家采访丨他们眼中的城市\",{\"1\":{\"43\":1,\"74\":1}}],[\"kids国际儿童艺术节创始人\",{\"1\":{\"80\":1}}],[\"kids国际儿童艺术节\",{\"1\":{\"17\":1,\"43\":1,\"80\":2}}],[\"kids\",{\"1\":{\"9\":2,\"248\":1}}],[\"kirolos\",{\"1\":{\"393\":1}}],[\"kiril\",{\"1\":{\"338\":1}}],[\"kirill\",{\"1\":{\"166\":1}}],[\"kiruthu\",{\"1\":{\"178\":1}}],[\"kirsty\",{\"1\":{\"125\":1}}],[\"kirsten\",{\"1\":{\"119\":1,\"308\":1}}],[\"kim\",{\"1\":{\"159\":1,\"191\":1,\"238\":1,\"269\":1,\"274\":2,\"356\":5}}],[\"kinetic\",{\"1\":{\"246\":1}}],[\"kinesthetic\",{\"1\":{\"140\":1}}],[\"kinematic\",{\"1\":{\"103\":2,\"246\":1}}],[\"kinga\",{\"1\":{\"108\":1}}],[\"kinds\",{\"0\":{\"455\":1},\"1\":{\"495\":1}}],[\"kind\",{\"1\":{\"6\":2,\"302\":1,\"455\":1}}],[\"我所有的朋友都放弃了设计\",{\"1\":{\"538\":1}}],[\"我从一开始就得有个框架了\",{\"1\":{\"534\":1}}],[\"我先确定了我的目标人群是大众\",{\"1\":{\"534\":1}}],[\"我把毕业设计当成一门\",{\"1\":{\"534\":1}}],[\"我个人并不特别倾向某种思维\",{\"1\":{\"533\":1}}],[\"我最近学习一些数字营销相关的内容\",{\"1\":{\"531\":1}}],[\"我总结了三点\",{\"1\":{\"530\":1}}],[\"我总结了四个关键的品牌组成部分\",{\"1\":{\"37\":1}}],[\"我用建筑叙事的思维完成了毕业设计\",{\"1\":{\"530\":1}}],[\"我已经把香港理工锁定好了\",{\"1\":{\"523\":1}}],[\"我关注的便不只有专业的层面了\",{\"1\":{\"523\":1}}],[\"我现在要思考的问题是\",{\"1\":{\"523\":1}}],[\"我尝试去思考其在未来的商业价值\",{\"1\":{\"520\":1}}],[\"我之前做了一个项目the\",{\"1\":{\"520\":1}}],[\"我知道rca和ual是大家非常热门的讨论话题\",{\"1\":{\"509\":1}}],[\"我觉得得先问问自己\",{\"1\":{\"509\":1}}],[\"我觉得去找就读的学长学姐是最直接的\",{\"1\":{\"509\":1}}],[\"我觉得不能以以往常规的思考方式再让我徘徊\",{\"1\":{\"43\":1}}],[\"我身边有认识从这里毕业的同学\",{\"1\":{\"509\":1}}],[\"我本身也非常支持同学们可以去深化\",{\"1\":{\"509\":1}}],[\"我本科是美院景观设计\",{\"1\":{\"507\":1}}],[\"我更看重设计之外的学习\",{\"1\":{\"534\":1}}],[\"我更希望透过毕业展的传播\",{\"1\":{\"534\":1}}],[\"我更在意有更多机会去对接真实市场\",{\"1\":{\"508\":1}}],[\"我更喜欢看这些有趣的形状\",{\"1\":{\"87\":1}}],[\"我未来的职业发展也是希望和多媒体行业有关的\",{\"1\":{\"508\":1}}],[\"我目标很明确\",{\"1\":{\"508\":1}}],[\"我组织了一个工作坊\",{\"1\":{\"500\":1}}],[\"我以此出发做了一个社会创新设计项目\",{\"1\":{\"499\":1}}],[\"我发现这几年交互设计体系中\",{\"1\":{\"510\":1}}],[\"我发现他们经常因为生存压力和狭窄的生活社交圈子产生情绪问题且无法合理宣泄\",{\"1\":{\"499\":1}}],[\"我发现收藏家对新产品的购买能力不足\",{\"1\":{\"23\":1}}],[\"我通过自学得到了很多技能\",{\"1\":{\"523\":1}}],[\"我通过线上的app设计一个符号体系来创造情绪社交空间\",{\"1\":{\"499\":1}}],[\"我通过研究社会流动人口发现\",{\"1\":{\"499\":1}}],[\"我通过虚幻的构造和真实的过去去提出一个设想\",{\"1\":{\"74\":1,\"530\":1}}],[\"我可以学到更多软技能\",{\"1\":{\"91\":1}}],[\"我设计了一个主要场景的框架\",{\"1\":{\"90\":1}}],[\"我希望能够将ar眼镜与app相匹配\",{\"1\":{\"90\":1}}],[\"我想告诉大家一些小启发\",{\"1\":{\"510\":1}}],[\"我想说的一点是\",{\"1\":{\"509\":1}}],[\"我想要下载这个应用\",{\"1\":{\"87\":1}}],[\"我想留在这里\",{\"1\":{\"46\":1}}],[\"我往下搜集了一些儿童创造力教育相关的资料\",{\"1\":{\"80\":1}}],[\"我当时的毕业项目\",{\"1\":{\"78\":1}}],[\"我收到一些教育机构的交流邀请\",{\"1\":{\"77\":1}}],[\"我与当地艺术机构开展工作坊\",{\"1\":{\"76\":1}}],[\"我对儿童创造力教育进行了总结和思考\",{\"1\":{\"76\":1}}],[\"我意识到\",{\"1\":{\"50\":1}}],[\"我清理完垃圾就要走了\",{\"1\":{\"46\":1}}],[\"我今天才来到这个城市\",{\"1\":{\"46\":1}}],[\"我还要去倒垃圾\",{\"1\":{\"46\":1}}],[\"我还推出了各种促销活动和激励措施\",{\"1\":{\"23\":1}}],[\"我就带你们回港口吧\",{\"1\":{\"46\":1}}],[\"我需要关注留学读书学习以外的内容\",{\"1\":{\"523\":1}}],[\"我需要更多专业引导\",{\"1\":{\"523\":1}}],[\"我需要一个更加具象的图面来展示我的一个乌托邦世界逻辑\",{\"1\":{\"43\":1}}],[\"我需要做一些取舍\",{\"1\":{\"21\":1}}],[\"我在这段期间得到更多实践落地的经验\",{\"1\":{\"92\":1}}],[\"我在创作过程中有好几个月处于停滞状态\",{\"1\":{\"43\":1}}],[\"我在2022\",{\"1\":{\"31\":1}}],[\"我是不能去抛弃前面所讲的这个故事的\",{\"1\":{\"43\":1}}],[\"我\",{\"1\":{\"43\":3}}],[\"我有幸参加了在重庆举行的o\",{\"1\":{\"80\":1}}],[\"我有幸作为特别嘉宾参加了在重庆举办的o\",{\"1\":{\"43\":1}}],[\"我有许多童真的幻想\",{\"1\":{\"43\":1}}],[\"我有时候在想\",{\"1\":{\"42\":1}}],[\"我提出了一个服务策略框架\",{\"1\":{\"83\":1}}],[\"我提出了一个基于web3底层逻辑的商业系统\",{\"1\":{\"33\":1}}],[\"我提出一个设想\",{\"1\":{\"17\":1}}],[\"我认为大家不要在这个阶段追求绝对的完美\",{\"1\":{\"509\":1}}],[\"我认为我过去的工作经历抛开职位名字的刻板印象\",{\"1\":{\"508\":1}}],[\"我认为未来这些传统概念都会被更新\",{\"1\":{\"508\":1}}],[\"我认为也不见得\",{\"1\":{\"42\":1}}],[\"我认为\",{\"1\":{\"33\":1}}],[\"我加强了数字艺术家的个人形象\",{\"1\":{\"31\":1}}],[\"我加快了会员获取标准\",{\"1\":{\"23\":1}}],[\"我其实已经发现nft当下的惨淡状态国家的规范暂不明确\",{\"1\":{\"29\":1}}],[\"我们杀死了它\",{\"1\":{\"538\":1}}],[\"我们团队本身包括和其他创业者的交流\",{\"1\":{\"537\":1}}],[\"我们应该分析并了解用户的情境\",{\"1\":{\"532\":1}}],[\"我们应该多思考如何通过设计更具创造性和战略性的工作内容\",{\"1\":{\"518\":1}}],[\"我们自身也受到双系统理论的影响\",{\"1\":{\"532\":1}}],[\"我们的专业知识反而成了沟通的障碍\",{\"1\":{\"535\":1}}],[\"我们的决策是如何形成的\",{\"1\":{\"532\":1}}],[\"我们的生活仿佛陷入了一个无休止的消费狂热\",{\"1\":{\"531\":1}}],[\"我们都在进行着大量的决策\",{\"1\":{\"532\":1}}],[\"我们都是在新时代中不断探索这个交界点\",{\"1\":{\"31\":1}}],[\"我们不断被诱导购买我们不需要甚至不想要的东西\",{\"1\":{\"531\":1}}],[\"我们要清楚\",{\"1\":{\"530\":1}}],[\"我们要上船\",{\"1\":{\"46\":1}}],[\"我们可以设计出更有效\",{\"1\":{\"532\":1}}],[\"我们可以理解工业4\",{\"1\":{\"519\":1}}],[\"我们可以把握机会\",{\"1\":{\"518\":1}}],[\"我们可以保持清醒并与周围的世界互动\",{\"1\":{\"516\":1}}],[\"我们看到了引人注目的产品\",{\"1\":{\"516\":1}}],[\"我们看到了nft的不成熟以及监管不完善下的潜在风险\",{\"1\":{\"23\":1}}],[\"我们下期再见\",{\"1\":{\"511\":1}}],[\"我们没有办法去精通每一款软件硬件\",{\"1\":{\"509\":1}}],[\"我们设置了不同的传感器\",{\"1\":{\"500\":1}}],[\"我们构建了一个反乌托邦的叙事世界\",{\"1\":{\"500\":1}}],[\"我们也完成了一个产品蓝图\",{\"1\":{\"520\":1}}],[\"我们也都会先反问回去\",{\"1\":{\"509\":1}}],[\"我们也认识了\",{\"1\":{\"80\":1}}],[\"我们也很难真正走入孩子的心中\",{\"1\":{\"74\":1}}],[\"我们知道这对他们有好处\",{\"1\":{\"50\":1}}],[\"我们害怕\",{\"1\":{\"46\":1}}],[\"我们呀\",{\"1\":{\"46\":1}}],[\"我们喜欢用不同材料来尝试做有意思的屋子\",{\"1\":{\"46\":1}}],[\"我们岛中的孩子每天都很开心\",{\"1\":{\"46\":1}}],[\"我们城市的垃圾就是来到这里了吗\",{\"1\":{\"46\":1}}],[\"我们需要深思熟虑的设计\",{\"1\":{\"516\":1}}],[\"我们需要开放标准来与耳机集成并理解上下文\",{\"1\":{\"516\":1}}],[\"我们需要解决一些关键的交互挑战\",{\"1\":{\"516\":1}}],[\"我们需要你们\",{\"1\":{\"46\":1}}],[\"我们需要给予儿童什么\",{\"1\":{\"43\":1}}],[\"我们需要在正确的监管下去发挥其最大价值\",{\"1\":{\"33\":1}}],[\"我们是如何做到的\",{\"1\":{\"42\":1}}],[\"我们确实敢说这是2023年国内开年最大型的元宇宙数字艺术展\",{\"1\":{\"29\":1}}],[\"我也得让我的成果能最大化在展览现场表现\",{\"1\":{\"534\":1}}],[\"我也得到新的启发\",{\"1\":{\"77\":1}}],[\"我也非常荣幸最后收到了dream\",{\"1\":{\"523\":1}}],[\"我也希望去构建更多产品的可能性来赋能儿童教育领域\",{\"1\":{\"90\":1}}],[\"我也认识到中国儿童教育的发展\",{\"1\":{\"77\":1}}],[\"我也后悔了\",{\"1\":{\"46\":1}}],[\"我也看到了每个人内心的梦\",{\"1\":{\"43\":1}}],[\"我也看到了他们内心的无限创造力\",{\"1\":{\"43\":1}}],[\"我也看到了一些好的导向结果\",{\"1\":{\"43\":1}}],[\"我也运用了拼贴的手法去描绘这些抽象虚构的画面\",{\"1\":{\"43\":1}}],[\"我也时刻关注国家的政策\",{\"1\":{\"42\":1}}],[\"我也尝试在当下将工作室转换成一个平台\",{\"1\":{\"31\":1}}],[\"我也和团队一起探讨目前的行业发展现状以及技术难题\",{\"1\":{\"30\":1}}],[\"我也听到了一些特别有意思的声音\",{\"1\":{\"28\":1}}],[\"我也参与了广州设计周活动的一些发布机制讨论\",{\"1\":{\"28\":1}}],[\"我也没有完成预计的计划\",{\"1\":{\"23\":1}}],[\"我得到了芒果tv发布的\",{\"1\":{\"24\":1}}],[\"我虽然收获了国内nft不完善体系的\",{\"1\":{\"23\":1}}],[\"我召开了与内部收藏家的一个dao会议\",{\"1\":{\"23\":1}}],[\"我将同样以一个拼贴画和故事文本去推动一个新的创作\",{\"1\":{\"23\":1}}],[\"我创作的两个主要系列是\",{\"1\":{\"22\":1}}],[\"我创立了o\",{\"1\":{\"15\":1}}],[\"我创立了数字艺术实验室\",{\"1\":{\"14\":1}}],[\"我正式设立了这个工作室\",{\"1\":{\"22\":1}}],[\"我决定趁着这波浪潮\",{\"1\":{\"21\":1}}],[\"我决定将我的作品心得也进行分享\",{\"1\":{\"20\":1}}],[\"我辞职了\",{\"1\":{\"21\":1}}],[\"我的角色也从一个设计师的角色开始向其他岗位能力发展\",{\"1\":{\"91\":1}}],[\"我的角色逐步向多岗位能力发展\",{\"1\":{\"15\":1}}],[\"我的作品虽然并没有直接导向一个目的或者结果\",{\"1\":{\"43\":1}}],[\"我的ip雪橇熊也参与到了这个梦幻的数字展馆中\",{\"1\":{\"29\":1}}],[\"我的毕业创作phantasisland\",{\"1\":{\"23\":1}}],[\"我的粉丝\",{\"1\":{\"20\":1}}],[\"我开始正式接触小红书的内容创作\",{\"1\":{\"20\":1}}],[\"我期待通过这个项目\",{\"1\":{\"17\":1}}],[\"我相信行业一定会慢慢向阳发展\",{\"1\":{\"42\":1}}],[\"我相信\",{\"1\":{\"17\":1}}],[\"我探讨如何为孩子们创造一个充满自由与创造的空间\",{\"1\":{\"17\":1}}],[\"我不断提升对技术的理解\",{\"1\":{\"16\":1}}],[\"我不仅亲自参与项目的策划与设计执行\",{\"1\":{\"15\":1}}],[\"我坚信未来各行业都需要复合型人才\",{\"1\":{\"16\":1}}],[\"我积累了丰富的舞美\",{\"1\":{\"15\":1}}],[\"在市场营销中\",{\"1\":{\"539\":1}}],[\"在市场营销领域\",{\"1\":{\"533\":1}}],[\"在快速发展的未来\",{\"1\":{\"535\":1}}],[\"在快速变化的市场中\",{\"1\":{\"525\":1}}],[\"在毕业展被许多媒体传播和观众喜爱\",{\"1\":{\"534\":1}}],[\"在毕业前的两年里\",{\"1\":{\"15\":1}}],[\"在产品设计领域\",{\"1\":{\"533\":1}}],[\"在各行各业中\",{\"1\":{\"533\":1}}],[\"在不断发展的商业和技术格局中\",{\"1\":{\"533\":1}}],[\"在设计过程中\",{\"1\":{\"532\":1}}],[\"在设计思维中\",{\"1\":{\"519\":1}}],[\"在当今这个时代\",{\"1\":{\"531\":1}}],[\"在当前充满不确定性的时期\",{\"1\":{\"527\":1}}],[\"在当下的留学选择中是最适合我的\",{\"1\":{\"523\":1}}],[\"在当下空间计算概念提出后\",{\"1\":{\"90\":1}}],[\"在某些情况下\",{\"1\":{\"525\":1}}],[\"在创意系统画布中\",{\"1\":{\"524\":1}}],[\"在申请阶段\",{\"1\":{\"523\":1}}],[\"在面对技术进步时\",{\"1\":{\"519\":1}}],[\"在工业设计和工程中\",{\"1\":{\"519\":1}}],[\"在ai时代\",{\"1\":{\"518\":2}}],[\"在audio\",{\"1\":{\"517\":1}}],[\"在人工智能快速发展的背景下\",{\"1\":{\"518\":1}}],[\"在人机交互和系统设计领域\",{\"1\":{\"16\":1}}],[\"在健身应用中\",{\"1\":{\"517\":1}}],[\"在户外运动或驾驶时\",{\"1\":{\"515\":1}}],[\"在密集的城市环境中\",{\"1\":{\"512\":1}}],[\"在游戏设计中\",{\"1\":{\"511\":1}}],[\"在数字化工作场所中的重要性\",{\"1\":{\"519\":1}}],[\"在数字化创新时代下也更鼓励学生跳出传统学科界限\",{\"1\":{\"510\":1}}],[\"在数字艺术领域的技能我都是靠自学的\",{\"1\":{\"507\":1}}],[\"在我们进入ai时代时\",{\"1\":{\"533\":1}}],[\"在我的从业经历中\",{\"1\":{\"509\":1}}],[\"在我毕业展期间\",{\"1\":{\"80\":1}}],[\"在意落户问题\",{\"1\":{\"508\":1}}],[\"在一定的时间过后他们便会产生厌烦心理\",{\"1\":{\"89\":1}}],[\"在一片昏暗之中船突然停下\",{\"1\":{\"46\":1}}],[\"在教育场所来指导儿童进行教育游戏\",{\"1\":{\"83\":1}}],[\"在呈现三维图像时\",{\"1\":{\"81\":1}}],[\"在虚拟现实\",{\"1\":{\"81\":1}}],[\"在2021年\",{\"1\":{\"80\":1}}],[\"在2022年\",{\"1\":{\"21\":1}}],[\"在线教育开始快还发展\",{\"1\":{\"78\":1}}],[\"在几年前的毕业展期间\",{\"1\":{\"77\":1}}],[\"在飞行器的协助下\",{\"1\":{\"69\":1}}],[\"在水下传输系统和运输塔的传送下\",{\"1\":{\"65\":1}}],[\"在水天线中渐渐消失了\",{\"1\":{\"46\":1}}],[\"在白天\",{\"1\":{\"59\":1}}],[\"在繁荣的地段\",{\"1\":{\"52\":1}}],[\"在繁华的都市中心\",{\"1\":{\"50\":1}}],[\"在所有的事情中\",{\"1\":{\"50\":1}}],[\"在索伦森生命的后期\",{\"1\":{\"50\":1}}],[\"在这一过程中\",{\"1\":{\"538\":1}}],[\"在这片水下岩石层中\",{\"1\":{\"72\":1}}],[\"在这样宁静的夜晚\",{\"1\":{\"46\":1}}],[\"在这样的虚构和背景之下\",{\"1\":{\"45\":1}}],[\"在这个杂糅的世界中\",{\"1\":{\"74\":1,\"530\":1}}],[\"在这个城市最中心繁华的地方\",{\"1\":{\"62\":1}}],[\"在这个混乱市场中\",{\"1\":{\"23\":1}}],[\"在这个时间节点\",{\"1\":{\"23\":1}}],[\"在他们创作的作品中\",{\"1\":{\"43\":1}}],[\"在现代化城市中\",{\"1\":{\"43\":1}}],[\"在现场\",{\"1\":{\"28\":1}}],[\"在您看来\",{\"1\":{\"43\":1}}],[\"在您的作品\",{\"1\":{\"43\":2}}],[\"在\",{\"1\":{\"42\":1}}],[\"在早期就被一些可以\",{\"1\":{\"42\":1}}],[\"在未来城市\",{\"1\":{\"40\":1}}],[\"在锻造城市\",{\"1\":{\"40\":1}}],[\"在群岛区域\",{\"1\":{\"40\":1}}],[\"在怪兽广场\",{\"1\":{\"40\":1}}],[\"在诞生的初期就创新了ip授权模式\",{\"1\":{\"36\":1}}],[\"在covid19期间\",{\"1\":{\"26\":1}}],[\"在芒果tv旗下平台小芒平台进行社区发布和成员投票\",{\"1\":{\"24\":1}}],[\"在雪橇熊发售一段时间后\",{\"1\":{\"24\":1}}],[\"在第二阶段的发布计划中\",{\"1\":{\"23\":1}}],[\"在第一个赋能计划中\",{\"1\":{\"23\":1}}],[\"在调研情况后\",{\"1\":{\"23\":1}}],[\"在真实与虚幻的世界之间交错衍生\",{\"1\":{\"23\":1}}],[\"在策划案中各种概念图就尤为重要\",{\"1\":{\"20\":1}}],[\"在伦敦\",{\"1\":{\"9\":1}}],[\"www\",{\"1\":{\"539\":1}}],[\"wwqa\",{\"1\":{\"245\":1}}],[\"wynter\",{\"1\":{\"429\":1,\"436\":1}}],[\"wyatt\",{\"1\":{\"105\":1}}],[\"wbrdm\",{\"1\":{\"250\":1}}],[\"wunmin\",{\"1\":{\"290\":1}}],[\"wunsch\",{\"1\":{\"139\":2,\"281\":1}}],[\"wu\",{\"1\":{\"167\":1,\"170\":1,\"219\":1,\"235\":2,\"270\":1,\"275\":1,\"300\":1,\"316\":1,\"317\":1,\"347\":1,\"355\":1,\"378\":2,\"394\":2,\"401\":1,\"403\":1,\"428\":1,\"429\":1,\"471\":1,\"483\":1,\"494\":1}}],[\"w\",{\"1\":{\"137\":1,\"271\":1,\"304\":1,\"457\":1}}],[\"waze\",{\"1\":{\"533\":2}}],[\"watermark\",{\"1\":{\"410\":1}}],[\"watermarked\",{\"1\":{\"410\":3}}],[\"watermarking\",{\"1\":{\"410\":9}}],[\"watermarks\",{\"0\":{\"410\":1}}],[\"watching\",{\"1\":{\"116\":1}}],[\"wagner\",{\"1\":{\"400\":1}}],[\"waghmare\",{\"1\":{\"207\":1}}],[\"wai\",{\"1\":{\"345\":1,\"430\":1}}],[\"waiting\",{\"0\":{\"214\":1},\"1\":{\"214\":7}}],[\"wajcman\",{\"1\":{\"302\":1}}],[\"wanlong\",{\"1\":{\"328\":1}}],[\"want\",{\"1\":{\"252\":1,\"399\":1,\"426\":1}}],[\"wang\",{\"1\":{\"103\":1,\"108\":1,\"116\":1,\"118\":1,\"135\":1,\"140\":1,\"167\":1,\"170\":1,\"197\":1,\"210\":1,\"212\":1,\"218\":1,\"219\":2,\"223\":1,\"242\":1,\"243\":1,\"251\":1,\"257\":1,\"258\":1,\"259\":1,\"283\":1,\"286\":1,\"317\":1,\"318\":2,\"322\":1,\"329\":1,\"337\":1,\"353\":2,\"359\":1,\"360\":1,\"366\":1,\"376\":1,\"378\":1,\"394\":1,\"399\":1,\"401\":2,\"411\":2,\"423\":1,\"429\":1,\"430\":1,\"433\":1,\"437\":1,\"449\":1,\"453\":1,\"455\":2,\"462\":1,\"463\":1,\"470\":2,\"471\":1,\"474\":1,\"478\":2}}],[\"wade\",{\"1\":{\"232\":1}}],[\"warranting\",{\"1\":{\"304\":1}}],[\"warrant\",{\"1\":{\"291\":1,\"476\":1}}],[\"warsinke\",{\"1\":{\"203\":1}}],[\"warning\",{\"0\":{\"198\":1,\"387\":1},\"1\":{\"198\":3,\"387\":3}}],[\"waveforms\",{\"1\":{\"194\":1}}],[\"wallets\",{\"1\":{\"192\":1}}],[\"walther\",{\"1\":{\"153\":1}}],[\"waste\",{\"1\":{\"306\":1}}],[\"wassim\",{\"1\":{\"239\":1}}],[\"wassenaar\",{\"1\":{\"188\":1}}],[\"was\",{\"1\":{\"107\":1,\"108\":2,\"135\":1,\"140\":1,\"166\":2,\"171\":1,\"176\":1,\"181\":1,\"182\":1,\"189\":2,\"195\":1,\"209\":1,\"223\":2,\"227\":1,\"237\":1,\"263\":1,\"265\":1,\"269\":2,\"271\":1,\"273\":1,\"274\":1,\"282\":1,\"286\":1,\"304\":2,\"409\":1,\"457\":1,\"469\":1,\"474\":1,\"492\":1}}],[\"wayfarer\",{\"1\":{\"516\":1}}],[\"ways\",{\"0\":{\"296\":1},\"1\":{\"172\":1,\"175\":1,\"241\":1,\"248\":2,\"265\":1,\"286\":2,\"296\":1,\"308\":1,\"334\":1,\"370\":1,\"474\":2}}],[\"waypoints\",{\"1\":{\"140\":2}}],[\"way\",{\"1\":{\"97\":1,\"98\":1,\"100\":1,\"146\":1,\"192\":1,\"231\":2,\"236\":1,\"237\":1,\"240\":1,\"258\":1,\"282\":1,\"305\":1,\"309\":1,\"312\":1,\"377\":1,\"409\":1,\"410\":1,\"433\":1,\"437\":1,\"456\":1,\"459\":1,\"465\":1,\"469\":1,\"480\":1}}],[\"wrist\",{\"1\":{\"220\":1}}],[\"writes\",{\"1\":{\"462\":1}}],[\"write\",{\"0\":{\"418\":1},\"1\":{\"209\":1,\"375\":1}}],[\"written\",{\"0\":{\"125\":1},\"1\":{\"280\":1,\"287\":1,\"434\":1,\"439\":3}}],[\"writings\",{\"1\":{\"428\":1}}],[\"writing\",{\"0\":{\"209\":1},\"1\":{\"98\":1,\"124\":4,\"125\":2,\"209\":1,\"229\":1,\"286\":1,\"324\":4,\"428\":2,\"461\":4,\"474\":1}}],[\"wrote\",{\"1\":{\"209\":3}}],[\"wrong\",{\"1\":{\"102\":2,\"496\":1}}],[\"why\",{\"1\":{\"293\":1,\"495\":1}}],[\"whose\",{\"1\":{\"428\":1,\"494\":1}}],[\"whole\",{\"0\":{\"238\":1},\"1\":{\"217\":1,\"238\":1,\"434\":1}}],[\"who\",{\"1\":{\"156\":1,\"159\":1,\"160\":3,\"164\":1,\"181\":1,\"184\":1,\"196\":1,\"214\":1,\"233\":1,\"245\":1,\"252\":1,\"271\":1,\"282\":1,\"380\":1,\"399\":1,\"403\":1,\"426\":1,\"457\":1}}],[\"whom\",{\"1\":{\"132\":1,\"330\":1}}],[\"whisper\",{\"1\":{\"407\":1}}],[\"whilst\",{\"1\":{\"183\":1,\"432\":1}}],[\"while\",{\"1\":{\"107\":1,\"118\":1,\"121\":1,\"133\":1,\"136\":1,\"137\":1,\"139\":2,\"143\":1,\"144\":1,\"153\":1,\"158\":2,\"161\":1,\"163\":1,\"167\":2,\"172\":1,\"176\":2,\"181\":1,\"183\":1,\"184\":1,\"188\":1,\"189\":1,\"192\":1,\"197\":1,\"206\":2,\"212\":1,\"214\":2,\"216\":1,\"231\":1,\"234\":1,\"235\":1,\"238\":3,\"239\":1,\"244\":1,\"249\":1,\"251\":1,\"253\":1,\"255\":1,\"256\":1,\"258\":1,\"269\":1,\"273\":1,\"275\":1,\"278\":1,\"279\":1,\"286\":1,\"288\":1,\"291\":1,\"292\":1,\"293\":1,\"300\":1,\"301\":1,\"302\":2,\"315\":2,\"322\":1,\"327\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"338\":1,\"340\":2,\"354\":1,\"359\":1,\"360\":1,\"370\":1,\"380\":1,\"389\":2,\"395\":1,\"400\":1,\"405\":1,\"410\":1,\"415\":2,\"423\":1,\"424\":1,\"427\":1,\"429\":1,\"432\":1,\"433\":1,\"435\":2,\"436\":1,\"447\":1,\"449\":1,\"450\":1,\"453\":2,\"458\":1,\"460\":1,\"466\":1,\"470\":1,\"474\":1,\"476\":1,\"489\":2,\"491\":1,\"496\":2,\"497\":1,\"498\":1}}],[\"whitley\",{\"1\":{\"119\":1}}],[\"white\",{\"1\":{\"106\":1,\"237\":1}}],[\"which\",{\"0\":{\"185\":1},\"1\":{\"96\":2,\"97\":1,\"98\":2,\"100\":1,\"108\":1,\"112\":1,\"113\":1,\"114\":2,\"118\":1,\"124\":1,\"126\":1,\"135\":2,\"137\":1,\"141\":1,\"143\":1,\"146\":1,\"148\":1,\"150\":1,\"156\":1,\"158\":1,\"165\":1,\"171\":1,\"172\":1,\"174\":1,\"175\":1,\"179\":1,\"181\":1,\"183\":1,\"184\":1,\"194\":3,\"196\":1,\"199\":2,\"203\":1,\"209\":1,\"213\":1,\"215\":2,\"219\":1,\"220\":1,\"223\":1,\"231\":1,\"234\":1,\"245\":1,\"249\":1,\"250\":1,\"256\":1,\"269\":1,\"271\":1,\"272\":2,\"273\":1,\"275\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":2,\"283\":2,\"284\":1,\"286\":2,\"293\":1,\"294\":1,\"296\":1,\"301\":1,\"306\":1,\"312\":1,\"313\":1,\"315\":1,\"317\":1,\"322\":1,\"325\":1,\"329\":1,\"334\":3,\"337\":1,\"339\":1,\"340\":1,\"350\":1,\"360\":1,\"363\":1,\"369\":1,\"370\":1,\"371\":1,\"378\":1,\"379\":1,\"380\":1,\"384\":1,\"386\":2,\"390\":1,\"393\":1,\"394\":1,\"401\":1,\"402\":3,\"409\":1,\"411\":1,\"412\":1,\"417\":2,\"419\":1,\"420\":1,\"425\":2,\"427\":1,\"430\":1,\"432\":1,\"435\":2,\"437\":1,\"443\":2,\"447\":1,\"449\":1,\"450\":2,\"453\":2,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"470\":2,\"474\":2,\"475\":1,\"480\":1,\"490\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":2}}],[\"whether\",{\"1\":{\"107\":3,\"159\":1,\"161\":1,\"191\":1,\"215\":1,\"228\":1,\"233\":1,\"240\":1,\"278\":1,\"308\":2,\"327\":1,\"340\":1,\"354\":1,\"470\":1,\"480\":1,\"491\":1}}],[\"when\",{\"0\":{\"161\":1,\"354\":1,\"446\":1},\"1\":{\"102\":2,\"107\":5,\"108\":1,\"113\":1,\"121\":1,\"122\":2,\"125\":1,\"128\":4,\"132\":1,\"135\":2,\"142\":1,\"159\":1,\"164\":1,\"172\":1,\"179\":1,\"206\":1,\"209\":2,\"211\":1,\"215\":1,\"219\":1,\"223\":1,\"228\":1,\"230\":1,\"246\":1,\"250\":1,\"251\":2,\"252\":2,\"265\":1,\"266\":1,\"271\":1,\"277\":1,\"280\":3,\"281\":1,\"289\":1,\"301\":1,\"308\":1,\"318\":1,\"330\":1,\"340\":3,\"355\":3,\"361\":1,\"363\":1,\"370\":1,\"372\":1,\"382\":1,\"390\":1,\"401\":1,\"405\":1,\"413\":1,\"422\":1,\"423\":2,\"426\":2,\"435\":1,\"441\":1,\"453\":2,\"454\":1,\"457\":1,\"461\":2,\"466\":1,\"471\":1,\"472\":1,\"479\":1,\"480\":1,\"482\":2}}],[\"whereby\",{\"1\":{\"389\":1}}],[\"wherein\",{\"1\":{\"338\":1,\"424\":1}}],[\"whereas\",{\"1\":{\"165\":2,\"183\":1}}],[\"where\",{\"0\":{\"159\":1,\"179\":1,\"417\":1},\"1\":{\"100\":3,\"101\":1,\"102\":1,\"104\":1,\"110\":1,\"112\":1,\"116\":1,\"124\":1,\"125\":1,\"135\":1,\"137\":1,\"152\":1,\"161\":1,\"189\":1,\"200\":1,\"201\":1,\"207\":1,\"216\":1,\"250\":1,\"253\":2,\"272\":1,\"275\":1,\"292\":1,\"300\":1,\"307\":2,\"321\":1,\"324\":1,\"338\":1,\"354\":1,\"355\":1,\"369\":1,\"376\":1,\"381\":1,\"382\":1,\"384\":1,\"407\":1,\"417\":1,\"429\":1,\"435\":1,\"441\":1,\"444\":2,\"449\":1,\"455\":1,\"456\":1,\"466\":1,\"482\":1,\"497\":1}}],[\"what\",{\"0\":{\"163\":1},\"1\":{\"101\":1,\"122\":1,\"156\":1,\"158\":1,\"179\":1,\"202\":2,\"219\":1,\"241\":1,\"243\":1,\"271\":1,\"350\":1,\"401\":1,\"418\":2,\"428\":1,\"441\":1,\"457\":1}}],[\"woo\",{\"1\":{\"430\":1}}],[\"wolof\",{\"0\":{\"232\":1},\"1\":{\"232\":1}}],[\"women\",{\"0\":{\"181\":1,\"302\":1},\"1\":{\"181\":3,\"302\":7}}],[\"would\",{\"1\":{\"107\":1,\"159\":1,\"280\":1,\"322\":1,\"342\":1,\"350\":1,\"375\":1,\"381\":1,\"385\":1,\"389\":1}}],[\"worse\",{\"1\":{\"389\":1,\"391\":1,\"395\":1}}],[\"worsening\",{\"1\":{\"155\":1}}],[\"worst\",{\"1\":{\"369\":1}}],[\"worldwide\",{\"1\":{\"221\":1}}],[\"world\",{\"0\":{\"275\":1,\"491\":1},\"1\":{\"112\":1,\"117\":1,\"141\":1,\"143\":2,\"157\":1,\"181\":1,\"182\":1,\"183\":1,\"207\":2,\"212\":1,\"223\":1,\"237\":1,\"275\":4,\"283\":3,\"286\":1,\"290\":1,\"301\":2,\"318\":2,\"326\":2,\"329\":1,\"340\":2,\"353\":2,\"359\":1,\"379\":2,\"382\":1,\"412\":1,\"417\":1,\"434\":1,\"447\":1,\"467\":1,\"471\":1,\"474\":1,\"475\":1,\"479\":1,\"480\":1,\"492\":1}}],[\"wordnet\",{\"1\":{\"460\":1}}],[\"words\",{\"1\":{\"226\":1,\"241\":2,\"282\":1,\"317\":1,\"347\":1,\"369\":1,\"497\":1}}],[\"worddecipher\",{\"0\":{\"124\":1},\"1\":{\"124\":4}}],[\"word\",{\"1\":{\"104\":1,\"124\":1,\"152\":1,\"190\":1,\"209\":1,\"241\":1,\"282\":1,\"391\":1,\"438\":1,\"467\":1,\"497\":1}}],[\"workarounds\",{\"1\":{\"286\":1,\"474\":1}}],[\"worker\",{\"1\":{\"279\":1,\"300\":1}}],[\"workers\",{\"0\":{\"275\":1,\"279\":1,\"293\":1,\"300\":1},\"1\":{\"201\":3,\"246\":1,\"279\":5,\"293\":8,\"300\":1}}],[\"worked\",{\"1\":{\"149\":1}}],[\"workforce\",{\"1\":{\"180\":1,\"277\":1,\"300\":1}}],[\"workflows\",{\"0\":{\"112\":1,\"231\":1,\"498\":1},\"1\":{\"112\":1,\"114\":1,\"119\":1,\"212\":2,\"231\":2,\"252\":1,\"259\":1,\"300\":1,\"316\":1,\"356\":1,\"426\":1}}],[\"workflow\",{\"1\":{\"110\":2,\"119\":1,\"222\":1,\"231\":1,\"242\":2,\"243\":1,\"258\":1,\"300\":1,\"316\":1,\"321\":2,\"433\":1}}],[\"workload\",{\"0\":{\"246\":1,\"304\":1},\"1\":{\"173\":1,\"201\":1,\"304\":5}}],[\"workstation\",{\"1\":{\"287\":1}}],[\"works\",{\"1\":{\"235\":1,\"238\":1,\"256\":2,\"333\":1,\"382\":2,\"394\":1,\"450\":3,\"470\":1,\"479\":1}}],[\"workspace\",{\"0\":{\"124\":1},\"1\":{\"124\":3}}],[\"workshops\",{\"1\":{\"142\":1,\"164\":1,\"218\":1}}],[\"workshop\",{\"0\":{\"147\":1,\"217\":1,\"243\":1,\"297\":1},\"1\":{\"97\":1,\"133\":1,\"136\":1,\"147\":1,\"148\":2,\"206\":1,\"217\":4,\"243\":2,\"248\":1,\"297\":2,\"312\":1,\"331\":1}}],[\"workings\",{\"1\":{\"148\":1}}],[\"working\",{\"1\":{\"97\":1,\"110\":1,\"170\":2,\"214\":1,\"284\":1,\"292\":1,\"312\":1,\"321\":1}}],[\"work\",{\"0\":{\"179\":1,\"201\":1,\"492\":1},\"1\":{\"97\":1,\"100\":1,\"101\":1,\"104\":2,\"114\":1,\"116\":1,\"118\":1,\"119\":1,\"123\":1,\"130\":1,\"131\":1,\"132\":1,\"135\":1,\"136\":2,\"137\":2,\"148\":1,\"156\":4,\"157\":1,\"159\":1,\"163\":1,\"170\":1,\"171\":1,\"172\":1,\"175\":3,\"176\":1,\"179\":1,\"180\":2,\"182\":1,\"184\":1,\"186\":1,\"199\":1,\"201\":1,\"207\":1,\"214\":1,\"217\":1,\"220\":1,\"221\":1,\"226\":1,\"228\":1,\"229\":1,\"234\":1,\"235\":1,\"241\":1,\"243\":1,\"245\":1,\"249\":1,\"251\":1,\"252\":1,\"256\":1,\"257\":3,\"260\":1,\"261\":2,\"268\":1,\"269\":2,\"272\":2,\"278\":1,\"279\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":2,\"297\":2,\"302\":1,\"312\":1,\"313\":3,\"319\":1,\"328\":1,\"330\":1,\"335\":1,\"342\":1,\"349\":1,\"355\":1,\"360\":1,\"363\":1,\"366\":1,\"369\":1,\"370\":1,\"380\":1,\"382\":1,\"388\":2,\"390\":2,\"391\":1,\"395\":1,\"400\":1,\"410\":3,\"423\":1,\"426\":1,\"430\":1,\"431\":1,\"435\":1,\"437\":1,\"443\":1,\"444\":1,\"445\":1,\"450\":1,\"452\":1,\"463\":2,\"468\":1,\"475\":1,\"476\":1,\"488\":1,\"490\":1,\"491\":1,\"492\":3,\"496\":1,\"497\":1}}],[\"wowwedding\",{\"1\":{\"6\":1}}],[\"wikitext\",{\"1\":{\"456\":1}}],[\"wiratunga\",{\"1\":{\"384\":1}}],[\"wireless\",{\"1\":{\"498\":1}}],[\"wirelessly\",{\"1\":{\"298\":1}}],[\"wire\",{\"1\":{\"186\":1}}],[\"wi\",{\"1\":{\"298\":1}}],[\"wilms\",{\"1\":{\"400\":1}}],[\"wild\",{\"0\":{\"212\":1},\"1\":{\"236\":1}}],[\"willing\",{\"1\":{\"418\":1}}],[\"willingness\",{\"1\":{\"135\":3,\"155\":1}}],[\"williams\",{\"1\":{\"195\":1}}],[\"william\",{\"1\":{\"180\":1,\"268\":1,\"452\":1}}],[\"will\",{\"0\":{\"251\":1,\"423\":1},\"1\":{\"123\":1,\"161\":1,\"178\":1,\"179\":4,\"206\":1,\"217\":1,\"229\":1,\"238\":1,\"243\":1,\"265\":1,\"268\":1,\"271\":1,\"287\":1,\"323\":1,\"345\":1,\"354\":1,\"363\":1,\"412\":1,\"427\":1,\"437\":1,\"447\":1,\"452\":1,\"457\":1,\"470\":1}}],[\"window\",{\"1\":{\"343\":1,\"350\":1,\"359\":1,\"382\":1,\"412\":2,\"453\":2,\"475\":1}}],[\"windows\",{\"1\":{\"108\":1,\"389\":1}}],[\"winning\",{\"1\":{\"325\":1}}],[\"wing\",{\"1\":{\"257\":1}}],[\"win\",{\"1\":{\"192\":2}}],[\"witnessing\",{\"1\":{\"179\":1}}],[\"within\",{\"1\":{\"120\":1,\"127\":1,\"137\":1,\"143\":1,\"167\":1,\"171\":1,\"172\":1,\"174\":1,\"179\":1,\"201\":1,\"204\":1,\"206\":1,\"207\":1,\"212\":1,\"217\":1,\"226\":2,\"236\":1,\"242\":1,\"245\":1,\"250\":1,\"259\":1,\"261\":1,\"263\":1,\"266\":1,\"274\":1,\"289\":1,\"290\":1,\"295\":1,\"297\":1,\"300\":1,\"304\":1,\"308\":1,\"316\":1,\"318\":1,\"326\":1,\"350\":1,\"355\":1,\"361\":1,\"362\":1,\"368\":1,\"370\":1,\"371\":1,\"381\":1,\"398\":1,\"399\":1,\"410\":1,\"412\":1,\"414\":1,\"424\":1,\"425\":1,\"427\":1,\"430\":1,\"439\":1,\"458\":1,\"460\":1,\"475\":1,\"477\":1,\"480\":1,\"481\":1,\"485\":1,\"494\":1,\"495\":1,\"497\":1}}],[\"without\",{\"0\":{\"147\":1,\"259\":1,\"282\":1,\"435\":1,\"483\":1},\"1\":{\"114\":1,\"119\":1,\"123\":2,\"140\":1,\"147\":1,\"189\":1,\"191\":1,\"198\":1,\"203\":1,\"222\":1,\"224\":1,\"242\":1,\"260\":1,\"305\":1,\"306\":1,\"308\":2,\"317\":1,\"333\":1,\"340\":1,\"350\":1,\"382\":1,\"387\":1,\"402\":1,\"422\":1,\"444\":1,\"445\":1,\"456\":2,\"465\":1,\"469\":2,\"483\":1,\"487\":2,\"489\":1,\"490\":1,\"491\":1}}],[\"with\",{\"0\":{\"101\":1,\"104\":1,\"110\":1,\"111\":1,\"113\":1,\"121\":1,\"124\":1,\"125\":1,\"131\":1,\"136\":1,\"147\":1,\"148\":1,\"151\":1,\"152\":1,\"154\":1,\"164\":1,\"166\":1,\"167\":1,\"174\":1,\"180\":1,\"192\":1,\"195\":1,\"201\":1,\"209\":1,\"216\":1,\"227\":1,\"248\":1,\"250\":1,\"269\":1,\"270\":1,\"281\":1,\"287\":1,\"295\":1,\"321\":1,\"322\":1,\"337\":1,\"339\":1,\"340\":1,\"344\":1,\"348\":1,\"351\":1,\"353\":1,\"355\":1,\"362\":1,\"363\":1,\"371\":1,\"378\":1,\"385\":1,\"392\":1,\"393\":1,\"395\":1,\"405\":1,\"409\":1,\"411\":1,\"412\":1,\"429\":1,\"430\":1,\"447\":2,\"459\":1,\"462\":1,\"469\":1,\"477\":1,\"485\":1,\"487\":1,\"488\":1,\"494\":1,\"498\":1},\"1\":{\"96\":2,\"97\":3,\"98\":6,\"99\":3,\"100\":3,\"102\":2,\"103\":1,\"104\":2,\"105\":4,\"106\":2,\"107\":5,\"108\":1,\"110\":5,\"111\":3,\"112\":2,\"114\":1,\"116\":2,\"117\":4,\"119\":4,\"120\":6,\"121\":1,\"122\":3,\"123\":1,\"124\":2,\"125\":5,\"126\":3,\"129\":1,\"130\":1,\"131\":1,\"132\":3,\"133\":6,\"135\":4,\"136\":1,\"137\":4,\"138\":3,\"139\":2,\"140\":3,\"141\":1,\"143\":1,\"147\":1,\"148\":1,\"149\":3,\"151\":3,\"152\":1,\"153\":2,\"154\":2,\"155\":2,\"156\":4,\"157\":1,\"158\":2,\"159\":1,\"161\":2,\"164\":2,\"165\":1,\"166\":6,\"167\":3,\"169\":3,\"170\":1,\"172\":4,\"173\":3,\"174\":3,\"175\":2,\"176\":4,\"178\":4,\"179\":1,\"180\":1,\"181\":2,\"182\":2,\"183\":1,\"184\":3,\"185\":1,\"186\":3,\"187\":2,\"189\":4,\"190\":2,\"191\":1,\"192\":1,\"195\":2,\"196\":2,\"198\":3,\"199\":3,\"200\":1,\"201\":3,\"203\":3,\"204\":3,\"205\":1,\"206\":1,\"207\":4,\"209\":2,\"210\":3,\"211\":3,\"212\":1,\"213\":3,\"214\":1,\"215\":3,\"216\":2,\"217\":1,\"218\":4,\"219\":2,\"220\":1,\"221\":2,\"222\":1,\"223\":2,\"224\":4,\"226\":3,\"228\":2,\"229\":2,\"230\":1,\"231\":1,\"233\":2,\"234\":1,\"235\":1,\"236\":1,\"237\":4,\"240\":4,\"241\":1,\"242\":4,\"243\":1,\"244\":1,\"245\":1,\"246\":3,\"249\":1,\"250\":3,\"252\":4,\"253\":1,\"254\":4,\"257\":3,\"258\":2,\"259\":2,\"261\":4,\"263\":4,\"268\":1,\"269\":1,\"270\":4,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":2,\"277\":1,\"279\":2,\"280\":1,\"281\":1,\"282\":4,\"283\":2,\"284\":3,\"286\":5,\"289\":1,\"290\":3,\"292\":1,\"293\":1,\"295\":4,\"297\":2,\"298\":1,\"300\":1,\"301\":1,\"302\":6,\"303\":3,\"304\":2,\"305\":1,\"306\":2,\"307\":1,\"309\":2,\"312\":3,\"313\":2,\"315\":3,\"317\":1,\"321\":5,\"322\":2,\"324\":1,\"325\":1,\"326\":6,\"327\":1,\"328\":1,\"329\":3,\"330\":3,\"331\":6,\"333\":1,\"334\":2,\"335\":3,\"337\":1,\"338\":2,\"339\":2,\"340\":6,\"343\":1,\"344\":5,\"345\":1,\"347\":1,\"348\":3,\"349\":1,\"350\":1,\"352\":1,\"353\":1,\"354\":2,\"355\":3,\"356\":1,\"358\":1,\"359\":2,\"360\":1,\"361\":3,\"362\":2,\"364\":2,\"365\":2,\"366\":1,\"368\":1,\"369\":1,\"370\":4,\"371\":3,\"375\":4,\"377\":1,\"378\":3,\"379\":1,\"380\":3,\"381\":2,\"382\":1,\"384\":1,\"386\":2,\"387\":3,\"388\":2,\"389\":2,\"390\":1,\"391\":1,\"392\":2,\"395\":3,\"396\":2,\"398\":2,\"399\":5,\"400\":2,\"401\":2,\"402\":1,\"403\":2,\"404\":1,\"408\":2,\"409\":3,\"411\":3,\"412\":1,\"413\":1,\"418\":3,\"419\":1,\"420\":1,\"424\":2,\"426\":4,\"427\":4,\"428\":2,\"429\":1,\"430\":2,\"432\":5,\"433\":2,\"434\":1,\"435\":1,\"437\":3,\"439\":1,\"441\":1,\"442\":1,\"443\":3,\"445\":4,\"447\":5,\"449\":1,\"450\":1,\"452\":1,\"453\":3,\"454\":1,\"455\":2,\"457\":1,\"459\":1,\"460\":3,\"461\":1,\"462\":2,\"463\":2,\"465\":1,\"466\":2,\"467\":4,\"469\":1,\"470\":2,\"471\":3,\"474\":5,\"475\":3,\"477\":1,\"478\":1,\"479\":3,\"480\":1,\"481\":1,\"482\":2,\"483\":1,\"485\":2,\"486\":3,\"487\":2,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":4,\"494\":1,\"495\":1,\"496\":2,\"497\":3,\"498\":5}}],[\"widget\",{\"1\":{\"151\":1,\"252\":1,\"348\":1,\"426\":1}}],[\"wider\",{\"1\":{\"174\":1,\"220\":1,\"253\":1,\"303\":1,\"371\":1}}],[\"wide\",{\"1\":{\"135\":1,\"146\":1,\"157\":1,\"287\":1,\"317\":1,\"329\":1,\"333\":2,\"344\":1,\"356\":1,\"366\":1,\"389\":1,\"410\":1,\"431\":1,\"432\":1,\"437\":1,\"459\":1,\"468\":1}}],[\"widely\",{\"1\":{\"104\":1,\"106\":1,\"111\":1,\"128\":1,\"139\":1,\"187\":1,\"191\":1,\"284\":1,\"290\":1,\"347\":1,\"391\":1,\"395\":1,\"417\":1,\"438\":1}}],[\"widespread\",{\"1\":{\"99\":1,\"166\":1,\"171\":1,\"198\":1,\"240\":1,\"302\":1,\"329\":1,\"334\":1,\"344\":1,\"387\":1,\"469\":1}}],[\"wish\",{\"1\":{\"375\":1}}],[\"wisely\",{\"1\":{\"462\":1}}],[\"wise\",{\"0\":{\"333\":1,\"366\":1},\"1\":{\"159\":1,\"366\":1,\"469\":1}}],[\"wissam\",{\"1\":{\"150\":1}}],[\"wisniewski\",{\"1\":{\"97\":1,\"206\":1,\"211\":1,\"312\":1}}],[\"wijdane\",{\"1\":{\"132\":1,\"330\":1}}],[\"weckert\",{\"1\":{\"533\":1}}],[\"weakest\",{\"1\":{\"446\":1}}],[\"weakness\",{\"1\":{\"333\":1}}],[\"weak\",{\"0\":{\"318\":1},\"1\":{\"318\":2,\"319\":1,\"403\":1}}],[\"wearable\",{\"1\":{\"120\":1,\"207\":2}}],[\"wese\",{\"0\":{\"318\":1},\"1\":{\"318\":2}}],[\"wer\",{\"1\":{\"232\":1}}],[\"werewolf\",{\"0\":{\"245\":1},\"1\":{\"245\":3}}],[\"were\",{\"1\":{\"98\":1,\"107\":4,\"120\":2,\"148\":1,\"149\":2,\"155\":1,\"160\":3,\"176\":1,\"181\":2,\"185\":1,\"196\":1,\"203\":2,\"226\":1,\"249\":1,\"263\":2,\"265\":2,\"286\":1,\"308\":2,\"358\":1,\"388\":1,\"389\":1,\"474\":1,\"491\":1}}],[\"wenqiang\",{\"1\":{\"495\":1}}],[\"wenda\",{\"1\":{\"494\":1}}],[\"wendy\",{\"1\":{\"50\":1,\"272\":1}}],[\"wen\",{\"1\":{\"461\":1}}],[\"wenfang\",{\"1\":{\"442\":1}}],[\"wenshan\",{\"1\":{\"429\":1}}],[\"wenlong\",{\"1\":{\"428\":1}}],[\"wenpeng\",{\"1\":{\"391\":1}}],[\"wenhu\",{\"1\":{\"377\":1,\"412\":1}}],[\"wenya\",{\"1\":{\"317\":1}}],[\"wenyue\",{\"1\":{\"485\":1}}],[\"wenyu\",{\"1\":{\"257\":1,\"328\":1}}],[\"weng\",{\"1\":{\"242\":1}}],[\"wenzhong\",{\"1\":{\"176\":1}}],[\"weerasinghe\",{\"1\":{\"384\":1}}],[\"weers\",{\"1\":{\"151\":1,\"348\":1}}],[\"weeks\",{\"0\":{\"279\":1},\"1\":{\"214\":1,\"279\":4}}],[\"weekday\",{\"1\":{\"183\":2}}],[\"week\",{\"0\":{\"279\":1},\"1\":{\"148\":1}}],[\"weissweiler\",{\"1\":{\"497\":1}}],[\"weiskopf\",{\"1\":{\"152\":1,\"185\":1}}],[\"weinberger\",{\"1\":{\"494\":1}}],[\"weinan\",{\"1\":{\"222\":1}}],[\"weiming\",{\"1\":{\"488\":1}}],[\"weipeng\",{\"1\":{\"478\":1}}],[\"weixin\",{\"1\":{\"428\":1}}],[\"weigh\",{\"1\":{\"400\":1}}],[\"weights\",{\"1\":{\"378\":1,\"456\":1,\"469\":2}}],[\"weighting\",{\"1\":{\"359\":1,\"378\":1}}],[\"weighted\",{\"1\":{\"351\":1}}],[\"weight\",{\"1\":{\"246\":3,\"269\":3,\"478\":1}}],[\"weirds\",{\"0\":{\"497\":1}}],[\"weiran\",{\"1\":{\"449\":1}}],[\"weir\",{\"1\":{\"385\":1}}],[\"weiwen\",{\"1\":{\"318\":1}}],[\"weiqi\",{\"1\":{\"222\":1}}],[\"wei\",{\"1\":{\"215\":1,\"242\":1,\"271\":1,\"300\":1,\"323\":1,\"324\":1,\"429\":1,\"457\":1,\"495\":1}}],[\"weihao\",{\"1\":{\"449\":1}}],[\"weihang\",{\"1\":{\"138\":1}}],[\"weihua\",{\"1\":{\"167\":1}}],[\"weijen\",{\"1\":{\"142\":1}}],[\"weller\",{\"1\":{\"385\":1}}],[\"wellbeing\",{\"1\":{\"291\":1,\"476\":1}}],[\"well\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"97\":1,\"104\":1,\"120\":1,\"149\":1,\"155\":1,\"181\":1,\"184\":1,\"185\":1,\"196\":2,\"204\":1,\"211\":1,\"213\":1,\"217\":1,\"221\":1,\"235\":1,\"251\":1,\"268\":2,\"286\":1,\"307\":1,\"308\":1,\"312\":1,\"318\":1,\"326\":1,\"338\":1,\"377\":1,\"380\":1,\"389\":2,\"400\":1,\"411\":1,\"412\":1,\"413\":1,\"423\":1,\"434\":3,\"452\":2,\"453\":1,\"456\":1,\"462\":1,\"465\":1,\"468\":2,\"474\":1,\"475\":1,\"480\":3,\"482\":1}}],[\"welcome\",{\"1\":{\"74\":1}}],[\"we\",{\"0\":{\"114\":1,\"179\":1},\"1\":{\"96\":5,\"97\":3,\"98\":3,\"99\":4,\"100\":1,\"101\":4,\"102\":1,\"103\":1,\"104\":2,\"105\":4,\"106\":1,\"108\":3,\"110\":4,\"111\":2,\"112\":4,\"113\":3,\"114\":3,\"115\":1,\"116\":2,\"117\":3,\"118\":3,\"119\":5,\"120\":1,\"121\":3,\"122\":6,\"123\":3,\"124\":2,\"125\":1,\"126\":2,\"128\":5,\"130\":2,\"131\":3,\"132\":3,\"133\":2,\"135\":5,\"137\":2,\"138\":1,\"139\":3,\"140\":2,\"141\":4,\"142\":5,\"143\":2,\"144\":3,\"148\":2,\"149\":2,\"150\":3,\"151\":5,\"152\":3,\"154\":2,\"155\":2,\"156\":7,\"158\":2,\"159\":3,\"160\":1,\"161\":3,\"163\":2,\"164\":3,\"165\":3,\"166\":3,\"169\":2,\"170\":1,\"171\":3,\"172\":2,\"173\":3,\"174\":4,\"175\":5,\"176\":1,\"178\":2,\"179\":3,\"180\":2,\"181\":2,\"182\":3,\"183\":2,\"184\":2,\"185\":2,\"187\":3,\"188\":2,\"189\":3,\"191\":3,\"192\":1,\"194\":3,\"195\":1,\"197\":1,\"198\":2,\"199\":4,\"200\":5,\"202\":1,\"204\":3,\"205\":1,\"206\":1,\"207\":8,\"209\":2,\"210\":2,\"211\":3,\"212\":1,\"213\":2,\"214\":2,\"215\":7,\"216\":3,\"217\":2,\"218\":5,\"219\":2,\"220\":4,\"221\":1,\"222\":1,\"224\":3,\"226\":1,\"227\":3,\"228\":2,\"229\":3,\"230\":4,\"231\":3,\"232\":1,\"233\":5,\"234\":2,\"235\":4,\"236\":2,\"238\":5,\"239\":2,\"240\":6,\"241\":3,\"242\":3,\"243\":4,\"245\":5,\"248\":3,\"249\":4,\"250\":6,\"251\":2,\"252\":3,\"254\":3,\"255\":2,\"256\":2,\"257\":3,\"258\":3,\"259\":2,\"260\":1,\"261\":1,\"264\":3,\"266\":2,\"268\":3,\"270\":6,\"271\":3,\"272\":4,\"274\":3,\"275\":3,\"277\":2,\"278\":5,\"279\":3,\"280\":2,\"281\":1,\"282\":3,\"283\":1,\"284\":2,\"286\":2,\"288\":2,\"289\":6,\"290\":2,\"291\":1,\"292\":4,\"293\":4,\"294\":3,\"295\":3,\"296\":5,\"297\":2,\"298\":2,\"300\":1,\"301\":3,\"302\":4,\"303\":3,\"305\":3,\"306\":3,\"308\":3,\"309\":3,\"312\":3,\"313\":2,\"314\":3,\"315\":2,\"316\":1,\"317\":5,\"319\":2,\"321\":4,\"322\":4,\"323\":1,\"325\":3,\"326\":5,\"327\":2,\"328\":3,\"329\":3,\"330\":3,\"331\":2,\"333\":2,\"334\":1,\"335\":6,\"336\":3,\"337\":2,\"338\":6,\"339\":2,\"340\":2,\"342\":4,\"344\":8,\"345\":3,\"347\":1,\"348\":5,\"349\":3,\"350\":5,\"351\":2,\"352\":2,\"353\":3,\"354\":3,\"355\":3,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":4,\"364\":1,\"365\":2,\"366\":3,\"368\":3,\"369\":5,\"370\":2,\"371\":4,\"372\":1,\"374\":3,\"375\":2,\"376\":3,\"377\":2,\"378\":1,\"379\":2,\"380\":2,\"381\":4,\"382\":4,\"384\":3,\"385\":4,\"386\":4,\"387\":2,\"388\":2,\"389\":5,\"390\":3,\"391\":1,\"392\":2,\"394\":1,\"395\":6,\"396\":4,\"399\":4,\"400\":3,\"401\":2,\"402\":2,\"403\":3,\"404\":3,\"405\":2,\"407\":1,\"408\":2,\"409\":8,\"410\":4,\"411\":2,\"412\":4,\"413\":3,\"414\":1,\"415\":2,\"416\":2,\"417\":4,\"418\":3,\"419\":2,\"420\":2,\"422\":4,\"423\":2,\"425\":3,\"426\":3,\"427\":3,\"428\":2,\"429\":1,\"430\":4,\"431\":4,\"432\":1,\"433\":3,\"434\":6,\"435\":4,\"436\":2,\"437\":4,\"438\":3,\"439\":2,\"441\":1,\"442\":2,\"443\":1,\"444\":4,\"445\":6,\"447\":3,\"448\":3,\"449\":1,\"450\":5,\"452\":3,\"453\":1,\"454\":6,\"455\":5,\"456\":1,\"457\":3,\"458\":5,\"460\":1,\"461\":2,\"462\":3,\"463\":2,\"465\":4,\"466\":3,\"467\":4,\"468\":5,\"469\":3,\"470\":4,\"471\":4,\"472\":2,\"474\":2,\"475\":4,\"476\":1,\"478\":2,\"479\":2,\"480\":4,\"481\":4,\"482\":3,\"483\":1,\"485\":2,\"486\":2,\"487\":2,\"488\":4,\"489\":2,\"490\":2,\"491\":2,\"492\":2,\"494\":2,\"495\":4,\"496\":4,\"497\":3,\"498\":1}}],[\"webpage\",{\"1\":{\"275\":1}}],[\"webxr\",{\"0\":{\"157\":1},\"1\":{\"157\":1}}],[\"website\",{\"1\":{\"137\":1,\"184\":1,\"316\":1,\"380\":1}}],[\"websites\",{\"1\":{\"128\":1,\"345\":1}}],[\"web\",{\"0\":{\"146\":1,\"149\":1,\"287\":1,\"345\":1},\"1\":{\"128\":1,\"146\":1,\"149\":4,\"157\":3,\"182\":1,\"215\":1,\"287\":2,\"345\":6,\"361\":2,\"477\":1,\"480\":1,\"482\":1}}],[\"web2社交媒体其实没法让用户拥有所有权\",{\"1\":{\"31\":1}}],[\"web3所提出的去中心化也形成了资本和新个体之间的博弈\",{\"1\":{\"42\":1}}],[\"web3的元年\",{\"1\":{\"30\":1}}],[\"web3商业形态构想\",{\"0\":{\"32\":1},\"1\":{\"14\":1}}],[\"从商业思维\",{\"1\":{\"534\":1}}],[\"从另一个视角去看待事物\",{\"1\":{\"533\":1}}],[\"从早晨起床决定是否按闹钟\",{\"1\":{\"532\":1}}],[\"从外在如收入到内在的核心资产进行层层剖析\",{\"1\":{\"525\":1}}],[\"从结果\",{\"1\":{\"525\":1}}],[\"从问题解决到系统设计\",{\"1\":{\"522\":1}}],[\"从为什么\",{\"1\":{\"521\":1}}],[\"从自助洗衣店到餐馆\",{\"1\":{\"512\":1}}],[\"从点到线再到面\",{\"1\":{\"509\":1}}],[\"从个人到系统层面的创造力培养\",{\"0\":{\"524\":1},\"1\":{\"502\":1}}],[\"从建筑材料到服装面料\",{\"1\":{\"92\":1}}],[\"从销售\",{\"1\":{\"92\":1}}],[\"从而发现新的解决方案\",{\"1\":{\"533\":1}}],[\"从而提升打开率和转化率\",{\"1\":{\"539\":1}}],[\"从而提升用户体验\",{\"1\":{\"532\":1}}],[\"从而提高整体生产效率\",{\"1\":{\"520\":1}}],[\"从而提高解决问题和推理的能力\",{\"1\":{\"81\":1}}],[\"从而实现快速\",{\"1\":{\"532\":1}}],[\"从而推动系统层面的创新\",{\"1\":{\"524\":1}}],[\"从而促进系统层面的创新\",{\"1\":{\"524\":1}}],[\"从而设计出更符合用户的需求的产品\",{\"1\":{\"522\":1}}],[\"从而优化生产流程中的安全性和生产力\",{\"1\":{\"519\":1}}],[\"从而在ai时代保持其独特的价值\",{\"1\":{\"518\":1}}],[\"从而来到这片秘密天地\",{\"1\":{\"72\":1}}],[\"从广州的华德福学校\",{\"1\":{\"77\":1}}],[\"从垃圾游乐场到冒险游乐场的进化\",{\"1\":{\"49\":1}}],[\"从乐土的诞生到逃离而最终是一种创造\",{\"1\":{\"45\":1}}],[\"从兴起推广到渐渐消失\",{\"1\":{\"45\":1}}],[\"从2199年的虚构垃圾城市世界开始\",{\"1\":{\"45\":1}}],[\"从很多小朋友的作品中\",{\"1\":{\"43\":1}}],[\"从精神层面来讲\",{\"1\":{\"43\":1}}],[\"从大的层面角度\",{\"1\":{\"43\":1}}],[\"从空间设计到数字艺术\",{\"1\":{\"14\":1}}],[\"从构思\",{\"1\":{\"6\":1}}],[\"車间爆炸\",{\"0\":{\"18\":1},\"1\":{\"14\":2}}],[\"项目管理\",{\"1\":{\"518\":1}}],[\"项目介绍\",{\"0\":{\"76\":1}}],[\"项目类型包括空间设计\",{\"1\":{\"21\":1}}],[\"项目经历\",{\"0\":{\"11\":1}}],[\"项目全流程负责总价约\",{\"1\":{\"6\":1}}],[\"规划中\",{\"1\":{\"10\":1}}],[\"ozcan\",{\"1\":{\"461\":1}}],[\"ozdel\",{\"1\":{\"116\":1,\"118\":1,\"139\":1}}],[\"odk\",{\"1\":{\"398\":1}}],[\"oda\",{\"0\":{\"314\":1},\"1\":{\"314\":3}}],[\"odala\",{\"1\":{\"178\":1}}],[\"odaily\",{\"1\":{\"42\":1}}],[\"owing\",{\"1\":{\"316\":1,\"359\":1}}],[\"ownership\",{\"0\":{\"209\":1},\"1\":{\"209\":5}}],[\"own\",{\"1\":{\"148\":1,\"227\":1,\"254\":1,\"275\":1,\"286\":1,\"307\":1,\"308\":1,\"366\":1,\"474\":1}}],[\"oh\",{\"1\":{\"300\":1}}],[\"osman\",{\"1\":{\"379\":1}}],[\"oswald\",{\"1\":{\"251\":1,\"423\":1}}],[\"osf\",{\"1\":{\"249\":1,\"250\":1,\"259\":1}}],[\"osborne\",{\"1\":{\"236\":1}}],[\"okamura\",{\"1\":{\"220\":1,\"269\":1}}],[\"oecd\",{\"1\":{\"216\":1}}],[\"oauth\",{\"1\":{\"192\":1}}],[\"oov\",{\"1\":{\"190\":1}}],[\"omid\",{\"1\":{\"319\":1,\"333\":1}}],[\"omidokun\",{\"1\":{\"246\":1}}],[\"omissions\",{\"1\":{\"459\":1}}],[\"omission\",{\"1\":{\"235\":1}}],[\"omar\",{\"1\":{\"180\":1,\"431\":1}}],[\"omran\",{\"1\":{\"171\":1}}],[\"olaf\",{\"1\":{\"458\":1}}],[\"olympiads\",{\"1\":{\"447\":1}}],[\"oleg\",{\"1\":{\"405\":1}}],[\"ole\",{\"1\":{\"289\":1}}],[\"olechowski\",{\"1\":{\"278\":1}}],[\"olbricht\",{\"1\":{\"281\":1}}],[\"olszewski\",{\"1\":{\"263\":1}}],[\"olson\",{\"1\":{\"105\":1}}],[\"oliver\",{\"1\":{\"194\":1}}],[\"older\",{\"1\":{\"184\":1,\"261\":1,\"380\":1}}],[\"old\",{\"1\":{\"126\":1}}],[\"obukhov\",{\"1\":{\"399\":1}}],[\"obligation\",{\"1\":{\"303\":1}}],[\"obligations\",{\"1\":{\"303\":1}}],[\"obafemi\",{\"1\":{\"281\":1}}],[\"object\",{\"0\":{\"296\":1,\"430\":1},\"1\":{\"181\":1,\"240\":1,\"296\":3,\"319\":1,\"349\":1,\"399\":3,\"430\":6,\"486\":1}}],[\"objectively\",{\"1\":{\"391\":1}}],[\"objective\",{\"1\":{\"179\":2,\"333\":1,\"391\":1,\"411\":1,\"477\":1,\"487\":1}}],[\"objectives\",{\"1\":{\"142\":1,\"172\":1,\"216\":1,\"271\":1,\"319\":2,\"370\":1,\"437\":1,\"457\":1}}],[\"objects\",{\"0\":{\"319\":1},\"1\":{\"151\":1,\"207\":2,\"296\":1,\"301\":1,\"348\":1,\"360\":2,\"430\":2,\"444\":1}}],[\"obstacle\",{\"1\":{\"359\":1,\"389\":1,\"479\":1}}],[\"obstacles\",{\"1\":{\"140\":1,\"272\":1}}],[\"obsolete\",{\"1\":{\"306\":1}}],[\"obscure\",{\"1\":{\"301\":1}}],[\"observing\",{\"1\":{\"303\":2}}],[\"observed\",{\"1\":{\"121\":1,\"195\":1,\"198\":1,\"202\":1,\"304\":1,\"314\":1,\"387\":1,\"391\":1,\"428\":1,\"437\":1,\"467\":1}}],[\"observe\",{\"1\":{\"112\":1,\"128\":1,\"183\":1,\"335\":2,\"385\":1,\"450\":1,\"455\":1}}],[\"observer\",{\"1\":{\"96\":1,\"116\":1}}],[\"observable\",{\"1\":{\"288\":1,\"325\":1}}],[\"observability\",{\"1\":{\"96\":1}}],[\"observational\",{\"1\":{\"173\":1,\"249\":1,\"303\":1}}],[\"observation\",{\"0\":{\"314\":1},\"1\":{\"120\":1,\"281\":1,\"314\":5,\"427\":1}}],[\"observations\",{\"0\":{\"303\":1},\"1\":{\"96\":3,\"128\":1,\"240\":1,\"260\":1,\"303\":1,\"339\":1,\"366\":1}}],[\"obfuscate\",{\"1\":{\"139\":1}}],[\"obtains\",{\"1\":{\"318\":1,\"455\":1}}],[\"obtaining\",{\"1\":{\"308\":1,\"478\":1}}],[\"obtained\",{\"1\":{\"255\":1,\"309\":1,\"336\":1,\"377\":1,\"482\":1,\"496\":1}}],[\"obtain\",{\"1\":{\"122\":1,\"154\":1,\"224\":1,\"235\":1,\"338\":1,\"436\":1,\"470\":1}}],[\"opus\",{\"1\":{\"334\":1}}],[\"opinion\",{\"0\":{\"245\":1},\"1\":{\"245\":9}}],[\"opinions\",{\"0\":{\"196\":1,\"302\":1},\"1\":{\"196\":2,\"302\":1}}],[\"optic\",{\"1\":{\"304\":3}}],[\"options\",{\"1\":{\"233\":1,\"245\":1,\"280\":1,\"404\":1}}],[\"option\",{\"1\":{\"195\":1}}],[\"optimisation\",{\"1\":{\"392\":1}}],[\"optimize\",{\"1\":{\"210\":1,\"234\":1,\"366\":2,\"403\":1}}],[\"optimized\",{\"1\":{\"138\":1,\"269\":1,\"411\":1,\"460\":1,\"496\":1}}],[\"optimizations\",{\"1\":{\"210\":1}}],[\"optimization\",{\"0\":{\"378\":1,\"415\":1,\"478\":1},\"1\":{\"128\":1,\"136\":1,\"210\":1,\"224\":2,\"260\":1,\"301\":2,\"309\":1,\"313\":2,\"378\":1,\"415\":1,\"478\":1}}],[\"optimizing\",{\"0\":{\"210\":1},\"1\":{\"121\":1,\"359\":2,\"366\":2,\"378\":1,\"465\":1}}],[\"optimus\",{\"1\":{\"179\":1}}],[\"optimal\",{\"0\":{\"366\":1},\"1\":{\"143\":1,\"189\":1,\"191\":2,\"203\":1,\"235\":1,\"318\":1,\"319\":1,\"350\":1,\"399\":1,\"414\":1,\"438\":1,\"453\":1}}],[\"opportunistically\",{\"1\":{\"272\":1}}],[\"opportunity\",{\"1\":{\"184\":1,\"202\":1,\"206\":1,\"278\":1,\"380\":1,\"478\":1}}],[\"opportunities\",{\"1\":{\"132\":1,\"136\":1,\"181\":2,\"199\":1,\"211\":1,\"243\":1,\"250\":1,\"252\":1,\"272\":2,\"286\":1,\"330\":1,\"364\":1,\"384\":1,\"426\":1,\"429\":1,\"443\":1,\"445\":1,\"474\":1,\"492\":1}}],[\"opposed\",{\"1\":{\"189\":1,\"431\":1}}],[\"opacity\",{\"1\":{\"171\":1}}],[\"opaque\",{\"1\":{\"148\":1,\"389\":1}}],[\"op\",{\"1\":{\"155\":1}}],[\"operates\",{\"1\":{\"428\":1}}],[\"operate\",{\"1\":{\"415\":1,\"441\":1,\"485\":1}}],[\"operated\",{\"1\":{\"219\":1,\"401\":1}}],[\"operations\",{\"1\":{\"222\":1,\"359\":1,\"447\":2,\"454\":1,\"469\":1}}],[\"operationalised\",{\"1\":{\"204\":1}}],[\"operational\",{\"1\":{\"146\":1,\"447\":1}}],[\"operation\",{\"1\":{\"139\":1,\"164\":1,\"179\":1,\"205\":1}}],[\"operating\",{\"1\":{\"138\":1,\"164\":1,\"201\":1,\"405\":1}}],[\"operators\",{\"0\":{\"223\":1},\"1\":{\"223\":2}}],[\"operator\",{\"1\":{\"96\":2,\"289\":1}}],[\"openwebtext\",{\"1\":{\"455\":1}}],[\"openai\",{\"1\":{\"327\":1,\"363\":1,\"494\":1,\"533\":1}}],[\"openbmb\",{\"1\":{\"316\":1}}],[\"opening\",{\"1\":{\"236\":1,\"255\":1}}],[\"open\",{\"0\":{\"157\":1,\"190\":1,\"313\":1,\"315\":1,\"413\":1,\"432\":1},\"1\":{\"101\":1,\"104\":1,\"151\":2,\"157\":2,\"159\":1,\"169\":1,\"190\":2,\"206\":1,\"215\":1,\"228\":1,\"231\":2,\"258\":1,\"283\":1,\"284\":1,\"296\":1,\"305\":1,\"313\":1,\"315\":1,\"318\":2,\"322\":1,\"326\":2,\"337\":3,\"339\":1,\"345\":1,\"348\":2,\"349\":3,\"350\":1,\"360\":1,\"361\":1,\"365\":1,\"369\":1,\"377\":1,\"378\":1,\"382\":1,\"398\":1,\"411\":2,\"432\":4,\"433\":1,\"442\":1,\"443\":1,\"447\":1,\"471\":1,\"482\":1,\"485\":1,\"497\":2,\"519\":1}}],[\"ocr\",{\"1\":{\"345\":1}}],[\"occupancy\",{\"1\":{\"475\":2}}],[\"occurs\",{\"1\":{\"209\":1}}],[\"occurrence\",{\"1\":{\"165\":1}}],[\"occur\",{\"1\":{\"107\":1,\"251\":1,\"298\":1,\"355\":1,\"423\":1}}],[\"occluding\",{\"1\":{\"220\":1}}],[\"oculus\",{\"1\":{\"81\":1}}],[\"otherwise\",{\"1\":{\"301\":1}}],[\"other\",{\"0\":{\"227\":1,\"435\":1},\"1\":{\"112\":2,\"163\":1,\"175\":1,\"183\":2,\"184\":1,\"196\":1,\"197\":1,\"200\":2,\"205\":1,\"218\":1,\"221\":1,\"224\":1,\"238\":1,\"243\":1,\"245\":1,\"254\":1,\"275\":1,\"284\":1,\"297\":1,\"302\":1,\"318\":1,\"324\":1,\"326\":1,\"329\":1,\"361\":1,\"369\":2,\"377\":1,\"380\":1,\"390\":1,\"392\":1,\"400\":2,\"402\":1,\"403\":1,\"404\":1,\"435\":1,\"441\":1,\"444\":1,\"445\":1,\"450\":1,\"455\":1,\"462\":1,\"469\":1,\"471\":1}}],[\"others\",{\"1\":{\"99\":1,\"108\":2,\"126\":1,\"170\":1,\"203\":1,\"227\":2,\"245\":1,\"256\":1,\"325\":1,\"429\":1}}],[\"ottley\",{\"1\":{\"100\":1}}],[\"ouyang\",{\"1\":{\"475\":1}}],[\"oussalah\",{\"1\":{\"460\":1}}],[\"oumar\",{\"1\":{\"232\":1}}],[\"outliers\",{\"0\":{\"469\":1},\"1\":{\"456\":1,\"469\":2}}],[\"outlier\",{\"0\":{\"456\":1}}],[\"outlines\",{\"1\":{\"277\":1}}],[\"outline\",{\"1\":{\"252\":1,\"396\":1,\"426\":1,\"445\":1}}],[\"outlining\",{\"1\":{\"239\":1}}],[\"outfitted\",{\"1\":{\"272\":1}}],[\"outdoors\",{\"1\":{\"207\":1}}],[\"outdated\",{\"1\":{\"133\":1,\"331\":1}}],[\"outside\",{\"1\":{\"404\":2}}],[\"outset\",{\"1\":{\"164\":1}}],[\"outstanding\",{\"1\":{\"151\":1,\"174\":1,\"218\":1,\"348\":1,\"371\":1,\"483\":1}}],[\"outcome\",{\"1\":{\"131\":1,\"259\":1}}],[\"outcomes\",{\"1\":{\"99\":1,\"120\":1,\"121\":1,\"125\":1,\"135\":2,\"154\":1,\"155\":2,\"176\":1,\"196\":3,\"273\":1,\"274\":1,\"290\":1,\"291\":1,\"308\":3,\"329\":1,\"350\":1,\"356\":1,\"394\":1,\"476\":1}}],[\"outperform\",{\"1\":{\"329\":1,\"444\":1,\"455\":1}}],[\"outperforming\",{\"1\":{\"257\":1,\"411\":1}}],[\"outperformed\",{\"1\":{\"140\":1,\"190\":1}}],[\"outperforms\",{\"1\":{\"118\":1,\"123\":1,\"194\":1,\"235\":1,\"283\":1,\"315\":1,\"349\":1,\"360\":1,\"376\":1,\"378\":1,\"389\":1,\"393\":1,\"399\":1,\"419\":1,\"420\":1,\"430\":1,\"442\":1,\"449\":1,\"458\":1,\"467\":1,\"488\":2,\"495\":1}}],[\"outputs\",{\"1\":{\"114\":1,\"159\":1,\"252\":2,\"326\":1,\"333\":1,\"334\":1,\"336\":2,\"362\":1,\"368\":1,\"384\":1,\"385\":1,\"389\":1,\"410\":2,\"426\":2,\"437\":1,\"441\":2,\"462\":1,\"467\":1,\"471\":1,\"489\":1}}],[\"output\",{\"0\":{\"114\":2},\"1\":{\"102\":1,\"104\":1,\"114\":4,\"255\":1,\"260\":1,\"289\":1,\"326\":2,\"335\":1,\"381\":1,\"384\":1,\"410\":5,\"441\":1,\"456\":1,\"472\":1}}],[\"out\",{\"0\":{\"488\":1},\"1\":{\"117\":1,\"149\":1,\"180\":1,\"190\":1,\"202\":1,\"203\":1,\"241\":1,\"413\":1,\"434\":1,\"446\":1,\"447\":1,\"478\":1,\"488\":1,\"489\":1,\"496\":1}}],[\"our\",{\"1\":{\"96\":1,\"97\":1,\"99\":1,\"100\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":3,\"110\":1,\"113\":2,\"116\":3,\"117\":1,\"118\":4,\"123\":3,\"125\":1,\"128\":1,\"130\":1,\"132\":1,\"133\":1,\"136\":1,\"137\":3,\"139\":3,\"141\":3,\"142\":1,\"149\":2,\"155\":1,\"158\":1,\"159\":5,\"160\":1,\"161\":3,\"164\":1,\"165\":1,\"167\":1,\"169\":1,\"171\":1,\"172\":4,\"174\":1,\"175\":1,\"176\":1,\"178\":2,\"180\":1,\"181\":1,\"182\":1,\"184\":2,\"185\":1,\"187\":3,\"188\":1,\"189\":2,\"190\":2,\"195\":3,\"197\":1,\"199\":2,\"200\":1,\"204\":2,\"207\":1,\"212\":2,\"214\":2,\"216\":1,\"217\":1,\"218\":2,\"219\":1,\"226\":1,\"227\":1,\"229\":1,\"231\":1,\"234\":3,\"235\":2,\"237\":2,\"238\":4,\"239\":2,\"240\":1,\"241\":1,\"243\":1,\"248\":1,\"249\":2,\"252\":2,\"254\":3,\"255\":1,\"256\":1,\"257\":1,\"258\":2,\"259\":2,\"260\":1,\"264\":1,\"266\":2,\"268\":1,\"272\":1,\"274\":2,\"275\":1,\"277\":1,\"279\":1,\"282\":1,\"284\":4,\"286\":1,\"288\":1,\"290\":1,\"293\":1,\"295\":1,\"300\":1,\"301\":2,\"302\":1,\"303\":1,\"305\":2,\"306\":1,\"308\":1,\"312\":1,\"313\":1,\"315\":1,\"318\":1,\"319\":2,\"321\":1,\"322\":1,\"323\":3,\"324\":6,\"325\":2,\"326\":1,\"327\":2,\"328\":2,\"329\":2,\"330\":1,\"331\":1,\"334\":1,\"336\":1,\"337\":1,\"339\":1,\"344\":1,\"349\":2,\"350\":2,\"351\":3,\"352\":4,\"354\":3,\"355\":1,\"356\":2,\"359\":3,\"360\":1,\"361\":1,\"362\":2,\"365\":3,\"366\":1,\"368\":1,\"369\":1,\"370\":4,\"371\":1,\"374\":1,\"376\":3,\"378\":2,\"379\":2,\"380\":2,\"381\":2,\"384\":1,\"385\":1,\"389\":1,\"390\":3,\"391\":2,\"393\":1,\"394\":1,\"396\":1,\"399\":1,\"401\":1,\"403\":1,\"404\":2,\"405\":2,\"408\":1,\"411\":2,\"412\":3,\"413\":1,\"415\":1,\"416\":4,\"418\":2,\"419\":4,\"420\":4,\"422\":3,\"424\":1,\"425\":2,\"426\":2,\"427\":1,\"428\":4,\"431\":1,\"432\":2,\"433\":2,\"434\":1,\"435\":1,\"437\":2,\"439\":2,\"441\":2,\"442\":2,\"444\":1,\"445\":1,\"447\":2,\"448\":1,\"449\":1,\"450\":3,\"452\":1,\"453\":1,\"455\":1,\"456\":1,\"458\":2,\"460\":4,\"461\":4,\"463\":1,\"466\":2,\"467\":3,\"468\":1,\"470\":2,\"471\":1,\"474\":1,\"475\":1,\"478\":2,\"481\":2,\"482\":2,\"485\":2,\"486\":1,\"488\":1,\"490\":1,\"491\":2,\"492\":3,\"494\":3,\"495\":1,\"496\":1}}],[\"orji\",{\"1\":{\"384\":1}}],[\"ortiz\",{\"1\":{\"272\":1}}],[\"ortega\",{\"1\":{\"126\":1,\"195\":1}}],[\"orson\",{\"1\":{\"268\":1,\"452\":1}}],[\"oral\",{\"1\":{\"232\":1}}],[\"orange\",{\"1\":{\"232\":4}}],[\"orion\",{\"1\":{\"385\":1}}],[\"orientations\",{\"1\":{\"182\":1,\"234\":1,\"279\":2}}],[\"oriented\",{\"0\":{\"449\":1},\"1\":{\"163\":1,\"172\":1,\"370\":1,\"449\":5}}],[\"orii\",{\"1\":{\"178\":1}}],[\"original\",{\"1\":{\"111\":2,\"139\":1,\"151\":1,\"157\":1,\"167\":1,\"187\":1,\"224\":1,\"260\":1,\"281\":1,\"327\":2,\"333\":1,\"348\":1,\"384\":1,\"386\":1,\"427\":1,\"444\":1,\"463\":1,\"485\":1,\"487\":1,\"491\":1}}],[\"ordinary\",{\"1\":{\"219\":1,\"401\":1}}],[\"ord\",{\"1\":{\"146\":1}}],[\"ordering\",{\"1\":{\"185\":2}}],[\"order\",{\"1\":{\"111\":1,\"126\":1,\"139\":1,\"156\":1,\"157\":1,\"188\":1,\"198\":1,\"265\":2,\"291\":1,\"387\":1,\"417\":1,\"439\":1,\"476\":1,\"492\":1,\"496\":1}}],[\"or\",{\"0\":{\"147\":1,\"228\":1,\"233\":1,\"395\":1},\"1\":{\"98\":1,\"100\":1,\"103\":1,\"104\":1,\"105\":2,\"107\":5,\"112\":1,\"114\":1,\"123\":3,\"124\":2,\"126\":1,\"128\":3,\"131\":1,\"133\":1,\"137\":1,\"146\":2,\"147\":1,\"149\":1,\"158\":1,\"161\":1,\"166\":1,\"172\":2,\"175\":1,\"179\":1,\"184\":1,\"188\":2,\"189\":2,\"191\":1,\"209\":1,\"211\":1,\"213\":1,\"215\":2,\"220\":1,\"224\":3,\"226\":1,\"227\":1,\"228\":2,\"229\":1,\"233\":1,\"234\":1,\"236\":1,\"238\":1,\"240\":5,\"241\":1,\"251\":2,\"252\":2,\"257\":1,\"263\":2,\"269\":3,\"272\":1,\"275\":1,\"278\":1,\"283\":1,\"284\":1,\"286\":1,\"298\":2,\"301\":2,\"304\":1,\"318\":2,\"326\":2,\"331\":1,\"333\":1,\"340\":1,\"344\":1,\"345\":1,\"347\":2,\"350\":1,\"353\":1,\"354\":1,\"355\":1,\"359\":1,\"368\":1,\"370\":2,\"374\":1,\"380\":1,\"391\":1,\"395\":1,\"405\":2,\"409\":1,\"410\":2,\"414\":1,\"418\":2,\"423\":2,\"426\":2,\"427\":2,\"428\":1,\"431\":1,\"437\":1,\"439\":1,\"442\":1,\"443\":1,\"447\":5,\"453\":1,\"454\":1,\"468\":1,\"470\":1,\"472\":1,\"474\":1,\"475\":1,\"477\":1,\"480\":1,\"482\":1,\"483\":1,\"485\":1,\"488\":1,\"494\":1,\"496\":2,\"497\":1}}],[\"organiza\",{\"1\":{\"414\":1}}],[\"organizational\",{\"1\":{\"274\":1,\"279\":2}}],[\"organizations\",{\"1\":{\"217\":1}}],[\"organization\",{\"1\":{\"163\":1,\"233\":1,\"447\":1}}],[\"organized\",{\"0\":{\"415\":1},\"1\":{\"223\":1,\"335\":2,\"415\":2,\"480\":1}}],[\"organize\",{\"1\":{\"156\":1,\"242\":1}}],[\"organizers\",{\"1\":{\"97\":1,\"312\":1}}],[\"organisation\",{\"1\":{\"154\":1}}],[\"organisations\",{\"1\":{\"117\":1}}],[\"org\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":2,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"overfit\",{\"1\":{\"368\":1}}],[\"overfitting\",{\"1\":{\"223\":1,\"460\":1,\"482\":1}}],[\"overgeneration\",{\"1\":{\"325\":1}}],[\"overreliance\",{\"1\":{\"291\":1,\"476\":1}}],[\"overwhelmingly\",{\"1\":{\"252\":1,\"426\":1}}],[\"overestimate\",{\"1\":{\"238\":1}}],[\"overcoming\",{\"1\":{\"192\":1,\"257\":1,\"425\":1}}],[\"overcomes\",{\"1\":{\"113\":1}}],[\"overcome\",{\"1\":{\"111\":1,\"127\":1,\"212\":1,\"219\":1,\"250\":1,\"306\":1,\"389\":1,\"401\":1,\"418\":1,\"470\":1,\"495\":1}}],[\"overtaking\",{\"1\":{\"186\":1}}],[\"overturn\",{\"1\":{\"158\":1}}],[\"oversight\",{\"0\":{\"446\":1},\"1\":{\"161\":1,\"354\":1,\"446\":2}}],[\"overhead\",{\"1\":{\"139\":1,\"359\":1,\"434\":1,\"487\":1}}],[\"overall\",{\"1\":{\"132\":1,\"142\":1,\"155\":1,\"174\":1,\"186\":1,\"302\":1,\"304\":1,\"309\":1,\"317\":1,\"330\":1,\"338\":1,\"356\":1,\"371\":1,\"413\":1,\"415\":3}}],[\"overlook\",{\"1\":{\"449\":1,\"463\":1}}],[\"overlooking\",{\"1\":{\"300\":1,\"314\":1,\"442\":1}}],[\"overlooked\",{\"1\":{\"245\":1}}],[\"overlaid\",{\"1\":{\"281\":1}}],[\"overlap\",{\"1\":{\"230\":1}}],[\"overlays\",{\"1\":{\"127\":1}}],[\"overly\",{\"1\":{\"107\":2,\"206\":1,\"463\":1}}],[\"overview\",{\"1\":{\"124\":1,\"126\":1,\"179\":1,\"213\":1}}],[\"over\",{\"0\":{\"146\":1,\"389\":1,\"475\":1},\"1\":{\"98\":1,\"103\":1,\"104\":1,\"108\":1,\"126\":4,\"141\":1,\"146\":1,\"153\":1,\"159\":1,\"166\":1,\"171\":1,\"172\":1,\"181\":1,\"182\":1,\"200\":1,\"214\":1,\"219\":1,\"227\":1,\"238\":1,\"239\":1,\"243\":1,\"244\":1,\"260\":1,\"272\":1,\"283\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"308\":1,\"350\":2,\"355\":2,\"363\":1,\"370\":1,\"386\":2,\"389\":3,\"395\":1,\"401\":1,\"412\":1,\"428\":2,\"434\":1,\"447\":1,\"460\":3,\"461\":2,\"462\":1,\"475\":1,\"476\":1,\"487\":1,\"495\":1,\"497\":1}}],[\"once\",{\"1\":{\"441\":1}}],[\"ong\",{\"1\":{\"417\":1}}],[\"ongoing\",{\"1\":{\"97\":1,\"136\":1,\"172\":1,\"236\":1,\"312\":1,\"359\":1,\"370\":1,\"492\":1}}],[\"ontologies\",{\"1\":{\"398\":1}}],[\"ontology\",{\"0\":{\"398\":1},\"1\":{\"205\":1,\"398\":8}}],[\"onto\",{\"1\":{\"281\":1,\"306\":1}}],[\"onboard\",{\"0\":{\"192\":1}}],[\"onsite\",{\"1\":{\"173\":1}}],[\"only=04389a2101a04e71a2c208a93bf2f7f2\",{\"1\":{\"250\":1}}],[\"only=4df33aad207144aca149982412125541\",{\"1\":{\"249\":1}}],[\"only\",{\"0\":{\"381\":1},\"1\":{\"107\":1,\"114\":1,\"124\":1,\"131\":1,\"137\":1,\"140\":1,\"151\":1,\"153\":1,\"164\":1,\"183\":1,\"204\":1,\"219\":1,\"240\":1,\"253\":1,\"266\":1,\"286\":1,\"313\":1,\"318\":1,\"322\":1,\"344\":1,\"348\":1,\"349\":1,\"350\":1,\"356\":1,\"358\":1,\"376\":1,\"381\":1,\"393\":1,\"395\":5,\"398\":1,\"401\":1,\"405\":1,\"419\":2,\"420\":2,\"422\":1,\"434\":1,\"436\":1,\"444\":2,\"459\":1,\"460\":1,\"470\":1,\"474\":1,\"477\":1,\"482\":2,\"495\":1}}],[\"online\",{\"0\":{\"97\":1,\"188\":1,\"206\":1,\"278\":1,\"302\":2,\"312\":1,\"344\":1,\"400\":1},\"1\":{\"28\":1,\"97\":3,\"128\":2,\"155\":3,\"166\":1,\"200\":2,\"205\":1,\"206\":4,\"211\":1,\"230\":1,\"241\":1,\"258\":1,\"273\":2,\"282\":1,\"302\":10,\"303\":1,\"312\":3,\"342\":1,\"344\":1,\"361\":1,\"400\":2,\"402\":1,\"431\":1,\"433\":1,\"447\":1}}],[\"on\",{\"0\":{\"100\":1,\"106\":1,\"114\":1,\"116\":1,\"118\":1,\"119\":1,\"128\":2,\"138\":1,\"152\":1,\"170\":1,\"178\":1,\"185\":1,\"186\":1,\"191\":1,\"192\":1,\"203\":1,\"217\":1,\"221\":1,\"222\":1,\"230\":1,\"243\":1,\"263\":1,\"273\":1,\"279\":1,\"283\":1,\"287\":1,\"295\":1,\"308\":1,\"323\":1,\"327\":1,\"334\":1,\"336\":1,\"342\":1,\"343\":1,\"365\":1,\"369\":1,\"394\":1,\"407\":1,\"413\":1,\"424\":1,\"431\":1,\"458\":1,\"460\":1,\"461\":1,\"463\":1,\"472\":1},\"1\":{\"96\":3,\"98\":1,\"100\":1,\"101\":1,\"103\":3,\"104\":5,\"106\":4,\"107\":1,\"108\":1,\"110\":3,\"112\":4,\"113\":1,\"114\":1,\"116\":2,\"118\":2,\"119\":2,\"120\":1,\"121\":1,\"123\":3,\"127\":1,\"128\":3,\"131\":1,\"133\":3,\"135\":7,\"136\":2,\"137\":1,\"139\":2,\"142\":2,\"143\":1,\"144\":1,\"148\":1,\"150\":1,\"151\":4,\"154\":1,\"155\":2,\"156\":2,\"157\":1,\"158\":4,\"159\":4,\"160\":3,\"161\":2,\"163\":4,\"164\":3,\"167\":2,\"169\":1,\"171\":3,\"172\":3,\"174\":2,\"175\":1,\"176\":4,\"178\":1,\"179\":2,\"180\":1,\"181\":1,\"182\":2,\"183\":3,\"184\":1,\"185\":4,\"186\":2,\"187\":1,\"189\":2,\"191\":3,\"194\":2,\"195\":3,\"196\":2,\"197\":2,\"199\":1,\"200\":6,\"203\":2,\"206\":1,\"209\":2,\"210\":3,\"211\":3,\"214\":4,\"215\":2,\"217\":1,\"218\":1,\"219\":1,\"220\":2,\"221\":1,\"222\":3,\"224\":4,\"226\":1,\"227\":3,\"228\":2,\"229\":2,\"230\":5,\"232\":1,\"234\":2,\"235\":2,\"238\":2,\"240\":3,\"242\":2,\"243\":2,\"244\":1,\"245\":3,\"246\":1,\"248\":2,\"249\":1,\"251\":5,\"252\":2,\"253\":1,\"254\":2,\"255\":1,\"256\":1,\"257\":4,\"261\":2,\"263\":3,\"265\":2,\"266\":2,\"268\":2,\"269\":1,\"272\":1,\"273\":3,\"274\":1,\"275\":1,\"279\":1,\"281\":1,\"282\":1,\"283\":2,\"284\":1,\"286\":4,\"288\":1,\"289\":2,\"292\":4,\"295\":2,\"296\":1,\"298\":2,\"300\":2,\"301\":2,\"304\":1,\"305\":1,\"307\":3,\"308\":3,\"309\":1,\"313\":2,\"314\":2,\"315\":3,\"317\":1,\"318\":2,\"321\":3,\"322\":4,\"325\":2,\"326\":1,\"327\":1,\"328\":2,\"329\":2,\"331\":3,\"335\":2,\"337\":2,\"338\":1,\"340\":3,\"342\":1,\"343\":1,\"344\":1,\"345\":2,\"347\":1,\"348\":4,\"350\":2,\"351\":2,\"352\":2,\"353\":3,\"354\":2,\"355\":3,\"358\":3,\"359\":1,\"360\":1,\"361\":2,\"362\":2,\"365\":2,\"366\":2,\"369\":1,\"370\":3,\"371\":2,\"372\":3,\"374\":1,\"376\":6,\"377\":2,\"378\":2,\"379\":1,\"380\":1,\"382\":3,\"384\":1,\"385\":3,\"386\":2,\"388\":3,\"389\":1,\"390\":3,\"391\":4,\"392\":1,\"393\":2,\"394\":1,\"395\":1,\"398\":1,\"400\":3,\"401\":1,\"403\":1,\"404\":2,\"405\":1,\"409\":5,\"410\":3,\"411\":3,\"412\":2,\"413\":1,\"415\":2,\"417\":3,\"418\":1,\"419\":3,\"420\":3,\"422\":1,\"423\":5,\"424\":1,\"425\":1,\"426\":2,\"427\":2,\"428\":4,\"429\":2,\"430\":1,\"431\":2,\"432\":1,\"434\":3,\"435\":1,\"437\":5,\"438\":2,\"439\":2,\"441\":1,\"442\":2,\"443\":6,\"444\":1,\"445\":4,\"446\":1,\"449\":2,\"450\":1,\"452\":2,\"453\":1,\"455\":4,\"456\":1,\"458\":5,\"461\":1,\"462\":2,\"463\":2,\"465\":3,\"466\":1,\"467\":1,\"468\":3,\"469\":4,\"470\":1,\"471\":5,\"472\":2,\"474\":4,\"475\":8,\"479\":2,\"481\":3,\"482\":7,\"483\":2,\"485\":5,\"486\":4,\"487\":1,\"488\":2,\"489\":2,\"491\":2,\"492\":2,\"494\":2,\"496\":2,\"497\":3}}],[\"ones\",{\"1\":{\"184\":1,\"214\":1,\"229\":1,\"334\":1,\"380\":1,\"441\":3,\"444\":3}}],[\"oney\",{\"1\":{\"170\":1}}],[\"one\",{\"0\":{\"444\":1},\"1\":{\"81\":1,\"112\":1,\"124\":1,\"146\":1,\"169\":1,\"171\":1,\"173\":1,\"176\":1,\"181\":1,\"199\":1,\"204\":1,\"216\":1,\"220\":1,\"228\":1,\"230\":1,\"231\":1,\"255\":1,\"278\":3,\"289\":1,\"313\":1,\"350\":1,\"351\":1,\"361\":1,\"369\":2,\"395\":2,\"399\":1,\"402\":1,\"434\":2,\"441\":1,\"444\":1,\"449\":1,\"469\":1,\"471\":1,\"481\":1,\"483\":1,\"497\":1}}],[\"offload\",{\"1\":{\"486\":1}}],[\"offline\",{\"1\":{\"362\":1,\"402\":1,\"479\":1}}],[\"office\",{\"0\":{\"480\":1},\"1\":{\"480\":1}}],[\"official\",{\"1\":{\"137\":1}}],[\"off\",{\"1\":{\"207\":1,\"255\":1,\"407\":1,\"438\":2,\"444\":1,\"490\":1}}],[\"offs\",{\"1\":{\"189\":1,\"226\":1,\"465\":2,\"468\":1}}],[\"offering\",{\"1\":{\"107\":1,\"110\":1,\"127\":1,\"146\":1,\"157\":1,\"198\":1,\"233\":3,\"259\":1,\"260\":1,\"292\":1,\"297\":1,\"303\":1,\"321\":1,\"329\":1,\"364\":1,\"387\":1}}],[\"offer\",{\"1\":{\"107\":4,\"136\":1,\"144\":2,\"183\":1,\"185\":1,\"234\":1,\"293\":1,\"295\":1,\"296\":1,\"303\":1,\"338\":1,\"368\":1,\"389\":1,\"392\":1,\"410\":1,\"455\":1,\"465\":1,\"470\":1,\"523\":1}}],[\"offered\",{\"1\":{\"107\":1}}],[\"offers\",{\"1\":{\"100\":1,\"107\":3,\"110\":1,\"143\":1,\"183\":1,\"222\":1,\"241\":1,\"258\":1,\"259\":1,\"297\":1,\"321\":1,\"324\":1,\"339\":1,\"343\":1,\"429\":1,\"433\":1,\"454\":1,\"460\":1,\"498\":1}}],[\"often\",{\"1\":{\"99\":2,\"103\":1,\"107\":1,\"111\":1,\"121\":1,\"123\":1,\"124\":1,\"132\":1,\"146\":1,\"151\":1,\"161\":1,\"164\":1,\"167\":1,\"188\":1,\"197\":1,\"201\":1,\"206\":2,\"229\":1,\"230\":1,\"231\":1,\"233\":1,\"234\":1,\"238\":1,\"240\":1,\"241\":1,\"242\":1,\"251\":1,\"253\":1,\"266\":1,\"269\":1,\"272\":1,\"298\":1,\"300\":1,\"306\":1,\"307\":1,\"309\":1,\"314\":1,\"315\":1,\"316\":1,\"328\":1,\"330\":1,\"335\":1,\"338\":1,\"339\":1,\"340\":2,\"348\":1,\"354\":1,\"400\":1,\"416\":1,\"423\":1,\"427\":1,\"453\":1,\"458\":1,\"488\":1,\"489\":1,\"490\":2,\"491\":1,\"496\":1}}],[\"of\",{\"0\":{\"100\":2,\"102\":2,\"111\":1,\"116\":1,\"119\":1,\"121\":1,\"122\":1,\"128\":1,\"129\":1,\"131\":1,\"133\":1,\"143\":1,\"146\":1,\"148\":1,\"153\":1,\"161\":1,\"165\":1,\"169\":1,\"171\":1,\"174\":1,\"175\":1,\"176\":2,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"191\":2,\"195\":1,\"198\":1,\"203\":1,\"211\":1,\"213\":1,\"214\":1,\"218\":1,\"219\":1,\"220\":1,\"227\":2,\"228\":1,\"230\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"243\":1,\"244\":1,\"245\":2,\"246\":1,\"254\":1,\"259\":1,\"265\":1,\"272\":1,\"275\":1,\"278\":1,\"279\":1,\"282\":1,\"286\":1,\"288\":1,\"289\":2,\"290\":1,\"292\":1,\"293\":1,\"296\":1,\"302\":1,\"304\":1,\"308\":1,\"313\":1,\"331\":1,\"335\":1,\"336\":1,\"342\":1,\"344\":1,\"352\":1,\"353\":1,\"354\":1,\"366\":1,\"369\":1,\"371\":1,\"372\":1,\"381\":1,\"387\":1,\"390\":1,\"394\":1,\"398\":1,\"401\":1,\"407\":1,\"413\":1,\"416\":2,\"417\":1,\"424\":1,\"427\":1,\"428\":1,\"429\":1,\"434\":1,\"449\":1,\"453\":2,\"455\":1,\"461\":2,\"465\":1,\"468\":1,\"471\":1,\"472\":1,\"474\":1,\"491\":2,\"496\":1,\"497\":1,\"500\":1,\"519\":1},\"1\":{\"16\":1,\"74\":1,\"96\":9,\"97\":5,\"98\":5,\"99\":4,\"100\":5,\"101\":7,\"102\":6,\"103\":4,\"104\":2,\"105\":5,\"106\":2,\"107\":1,\"108\":4,\"110\":4,\"111\":2,\"112\":11,\"113\":2,\"114\":3,\"115\":2,\"116\":4,\"117\":4,\"118\":5,\"119\":2,\"120\":7,\"121\":3,\"122\":2,\"123\":6,\"124\":3,\"125\":4,\"126\":10,\"127\":3,\"128\":11,\"129\":3,\"130\":4,\"131\":2,\"132\":2,\"133\":11,\"135\":8,\"137\":8,\"138\":1,\"139\":5,\"140\":4,\"141\":7,\"142\":4,\"143\":4,\"144\":1,\"146\":11,\"147\":1,\"148\":5,\"149\":3,\"150\":3,\"151\":5,\"153\":8,\"154\":2,\"155\":9,\"156\":7,\"157\":3,\"158\":5,\"159\":4,\"160\":4,\"161\":8,\"163\":2,\"164\":1,\"165\":3,\"166\":9,\"167\":6,\"169\":4,\"170\":3,\"171\":6,\"172\":8,\"173\":8,\"174\":8,\"175\":2,\"176\":6,\"178\":6,\"179\":7,\"180\":1,\"181\":8,\"182\":2,\"183\":2,\"184\":4,\"185\":4,\"186\":6,\"187\":9,\"188\":6,\"189\":6,\"190\":7,\"191\":9,\"192\":3,\"194\":8,\"195\":1,\"196\":3,\"197\":1,\"198\":6,\"199\":5,\"200\":4,\"201\":2,\"202\":2,\"203\":1,\"204\":5,\"205\":3,\"206\":1,\"207\":6,\"209\":5,\"210\":2,\"211\":5,\"212\":3,\"213\":8,\"214\":4,\"215\":7,\"216\":8,\"217\":8,\"218\":6,\"219\":7,\"220\":7,\"221\":5,\"222\":2,\"223\":3,\"224\":2,\"226\":3,\"227\":6,\"228\":3,\"229\":3,\"230\":8,\"231\":3,\"232\":9,\"233\":5,\"234\":8,\"235\":5,\"236\":4,\"237\":6,\"238\":2,\"239\":5,\"240\":5,\"241\":6,\"242\":6,\"243\":5,\"244\":16,\"245\":10,\"246\":7,\"248\":6,\"249\":8,\"250\":4,\"251\":7,\"252\":7,\"253\":6,\"254\":5,\"255\":2,\"256\":3,\"257\":15,\"258\":4,\"259\":9,\"260\":5,\"261\":6,\"263\":4,\"264\":2,\"265\":3,\"266\":1,\"268\":4,\"269\":4,\"270\":4,\"271\":7,\"272\":10,\"273\":4,\"275\":1,\"277\":6,\"278\":5,\"279\":1,\"280\":3,\"281\":5,\"282\":13,\"283\":1,\"284\":1,\"286\":6,\"287\":4,\"288\":7,\"289\":6,\"290\":2,\"291\":4,\"292\":9,\"293\":3,\"294\":1,\"295\":4,\"296\":7,\"297\":6,\"298\":6,\"300\":1,\"301\":6,\"302\":17,\"303\":4,\"304\":4,\"305\":1,\"306\":5,\"307\":4,\"308\":10,\"309\":2,\"312\":5,\"313\":6,\"314\":6,\"315\":7,\"316\":3,\"317\":9,\"318\":3,\"319\":1,\"321\":4,\"322\":4,\"323\":5,\"324\":3,\"325\":3,\"326\":7,\"327\":2,\"328\":5,\"329\":8,\"330\":2,\"331\":11,\"333\":13,\"334\":5,\"335\":14,\"336\":5,\"337\":8,\"338\":2,\"339\":8,\"340\":5,\"341\":1,\"342\":5,\"343\":1,\"344\":7,\"345\":7,\"347\":4,\"348\":5,\"349\":1,\"350\":13,\"351\":3,\"352\":2,\"353\":6,\"354\":8,\"355\":3,\"356\":6,\"358\":8,\"359\":7,\"360\":8,\"361\":2,\"362\":11,\"363\":3,\"364\":3,\"365\":6,\"366\":11,\"368\":5,\"369\":14,\"370\":8,\"371\":8,\"372\":6,\"374\":3,\"375\":4,\"376\":3,\"377\":3,\"378\":8,\"379\":4,\"380\":4,\"381\":4,\"382\":1,\"384\":6,\"385\":4,\"386\":6,\"387\":6,\"388\":5,\"389\":6,\"390\":13,\"391\":5,\"392\":1,\"393\":6,\"394\":3,\"395\":5,\"396\":6,\"398\":9,\"399\":2,\"400\":2,\"401\":7,\"402\":6,\"403\":5,\"404\":3,\"405\":2,\"407\":1,\"408\":7,\"409\":3,\"410\":13,\"411\":6,\"412\":2,\"413\":3,\"414\":3,\"415\":7,\"416\":2,\"417\":6,\"418\":3,\"419\":1,\"420\":1,\"422\":9,\"423\":7,\"424\":10,\"425\":5,\"426\":7,\"427\":6,\"428\":6,\"429\":5,\"430\":3,\"431\":4,\"432\":6,\"433\":4,\"434\":9,\"435\":3,\"436\":8,\"437\":12,\"438\":4,\"439\":12,\"441\":9,\"442\":3,\"443\":8,\"444\":4,\"445\":6,\"446\":5,\"447\":4,\"448\":2,\"449\":4,\"450\":4,\"452\":4,\"453\":6,\"454\":5,\"455\":6,\"456\":5,\"457\":7,\"458\":7,\"459\":7,\"460\":8,\"461\":11,\"462\":5,\"463\":6,\"465\":5,\"466\":6,\"467\":7,\"468\":7,\"469\":18,\"470\":5,\"471\":7,\"472\":2,\"474\":6,\"475\":4,\"476\":4,\"477\":4,\"478\":1,\"479\":14,\"480\":4,\"481\":2,\"482\":9,\"483\":1,\"485\":4,\"486\":7,\"487\":2,\"488\":4,\"489\":8,\"490\":6,\"491\":4,\"492\":9,\"494\":3,\"495\":8,\"496\":11,\"497\":9,\"498\":6,\"502\":1,\"510\":1,\"519\":1,\"520\":1}}],[\"o\",{\"1\":{\"9\":2,\"15\":4,\"43\":1,\"74\":1,\"80\":1,\"232\":1,\"261\":1,\"325\":1}}],[\"参考理念\",{\"0\":{\"538\":1}}],[\"参考内容来自互联网\",{\"1\":{\"537\":1}}],[\"参考来源\",{\"1\":{\"79\":1}}],[\"参考文献\",{\"1\":{\"74\":1}}],[\"参与者有机会获得珍稀nft\",{\"1\":{\"40\":1}}],[\"参加2021年o\",{\"1\":{\"17\":1}}],[\"参加\",{\"1\":{\"9\":1}}],[\"参展艺术家及艺术访谈嘉宾\",{\"1\":{\"9\":1}}],[\"8亿个技能到他们的个人资料中\",{\"1\":{\"518\":1}}],[\"8th\",{\"1\":{\"486\":1}}],[\"81\",{\"1\":{\"463\":1}}],[\"8b\",{\"1\":{\"378\":1}}],[\"8x7b\",{\"1\":{\"365\":1,\"369\":3}}],[\"88\",{\"1\":{\"350\":1}}],[\"87\",{\"1\":{\"314\":1,\"345\":1,\"369\":1}}],[\"84\",{\"1\":{\"284\":1,\"305\":1}}],[\"85\",{\"1\":{\"196\":1,\"283\":1,\"284\":1,\"467\":1}}],[\"82\",{\"1\":{\"159\":1,\"393\":1}}],[\"8日在上海百禧公园举行\",{\"1\":{\"26\":1}}],[\"8\",{\"0\":{\"352\":1,\"439\":1},\"1\":{\"9\":1,\"122\":1,\"126\":1,\"186\":1,\"190\":1,\"194\":1,\"295\":1,\"305\":1,\"314\":1,\"352\":1,\"414\":1,\"439\":1,\"460\":1,\"479\":1}}],[\"800\",{\"1\":{\"377\":1}}],[\"800+\",{\"1\":{\"210\":1}}],[\"80\",{\"1\":{\"6\":1,\"181\":1,\"361\":1,\"475\":1}}],[\"乌托邦之跃\",{\"1\":{\"9\":1}}],[\"画廊举办的展览\",{\"1\":{\"9\":1}}],[\"是的\",{\"1\":{\"539\":1}}],[\"是的没错\",{\"1\":{\"46\":1}}],[\"是消费设计的力量\",{\"1\":{\"531\":1}}],[\"是两种设计方法\",{\"1\":{\"522\":1}}],[\"是点击量\",{\"1\":{\"521\":1}}],[\"是什么→做什么\",{\"1\":{\"521\":1}}],[\"是否未来有巨大的潜力和广阔的发展前景呢\",{\"1\":{\"520\":1}}],[\"是工业化4\",{\"1\":{\"519\":1}}],[\"是那样的真诚和温暖\",{\"1\":{\"71\":1}}],[\"是一种通过设计来影响消费者行为的力量\",{\"1\":{\"531\":1}}],[\"是一种更加广泛的设计方法\",{\"1\":{\"522\":1}}],[\"是一种以用户为中心的设计方法\",{\"1\":{\"522\":1}}],[\"是一种新兴的技术\",{\"1\":{\"512\":1}}],[\"是一种逃离式的选择\",{\"1\":{\"52\":1}}],[\"是一个非常好的主题概念\",{\"1\":{\"43\":1}}],[\"是一个提供ip孵化机制的平台\",{\"1\":{\"39\":1}}],[\"是呀很巧\",{\"1\":{\"46\":1}}],[\"是ip故事\",{\"1\":{\"23\":1}}],[\"是我在大学这种开放创作环境下所建立和思考总结的\",{\"1\":{\"31\":1}}],[\"是我在自媒体创业阶段构建的ip世界\",{\"1\":{\"14\":1}}],[\"是我个人的虚拟艺术实验室\",{\"1\":{\"22\":1}}],[\"是\",{\"1\":{\"9\":1}}],[\"ff\",{\"1\":{\"427\":4}}],[\"fpgas\",{\"1\":{\"363\":1}}],[\"fmlama\",{\"1\":{\"328\":1}}],[\"fmri\",{\"1\":{\"256\":3}}],[\"fynn\",{\"1\":{\"234\":1}}],[\"füller\",{\"1\":{\"233\":1}}],[\"f1\",{\"1\":{\"232\":1,\"323\":2,\"351\":1,\"372\":1,\"460\":2}}],[\"fnirs\",{\"0\":{\"223\":1},\"1\":{\"223\":3}}],[\"fwgfcctrbfi\",{\"1\":{\"219\":1,\"401\":1}}],[\"f\",{\"1\":{\"130\":1,\"194\":1}}],[\"fly\",{\"1\":{\"366\":1}}],[\"flux\",{\"1\":{\"353\":1}}],[\"fluency\",{\"1\":{\"124\":1,\"300\":1}}],[\"flash\",{\"0\":{\"462\":1}}],[\"flammarion\",{\"1\":{\"409\":1}}],[\"flahy\",{\"1\":{\"350\":1}}],[\"flawed\",{\"1\":{\"329\":2}}],[\"fleisch\",{\"1\":{\"384\":1}}],[\"flek\",{\"1\":{\"336\":1}}],[\"fleets\",{\"1\":{\"272\":1}}],[\"flexi\",{\"1\":{\"414\":1}}],[\"flexibly\",{\"1\":{\"414\":1}}],[\"flexible\",{\"0\":{\"316\":1,\"414\":1},\"1\":{\"257\":1,\"287\":1,\"288\":1,\"318\":1,\"349\":1,\"382\":1,\"414\":1,\"487\":1}}],[\"flexibility\",{\"1\":{\"99\":1,\"113\":2,\"294\":1,\"463\":1,\"497\":2}}],[\"flexion\",{\"1\":{\"246\":2}}],[\"floating\",{\"1\":{\"469\":1}}],[\"floyd\",{\"1\":{\"359\":1}}],[\"florence\",{\"1\":{\"302\":1}}],[\"floris\",{\"1\":{\"151\":1,\"348\":1}}],[\"florian\",{\"1\":{\"129\":1,\"187\":1,\"435\":1}}],[\"flock\",{\"1\":{\"269\":2}}],[\"flocking\",{\"0\":{\"269\":1},\"1\":{\"269\":2,\"427\":1}}],[\"floor\",{\"1\":{\"182\":2}}],[\"flowrite和hypotenuse\",{\"1\":{\"539\":1}}],[\"flow\",{\"1\":{\"178\":1,\"203\":1,\"271\":1,\"304\":3,\"457\":1}}],[\"friede\",{\"1\":{\"338\":1}}],[\"friendly\",{\"1\":{\"316\":1,\"363\":1}}],[\"frith\",{\"1\":{\"248\":1}}],[\"frustrations\",{\"1\":{\"122\":1}}],[\"french\",{\"1\":{\"315\":2}}],[\"freiberger\",{\"1\":{\"296\":1}}],[\"frey\",{\"1\":{\"205\":1}}],[\"frequency\",{\"0\":{\"189\":1,\"223\":1},\"1\":{\"223\":1,\"264\":1,\"298\":1,\"305\":2,\"448\":1}}],[\"frequencies\",{\"1\":{\"142\":1}}],[\"frequently\",{\"1\":{\"110\":1,\"306\":1,\"321\":1,\"428\":1,\"467\":1}}],[\"frederick\",{\"1\":{\"114\":1,\"200\":1,\"251\":1,\"423\":1}}],[\"fred\",{\"1\":{\"110\":1,\"210\":1,\"321\":1,\"461\":1}}],[\"freedom\",{\"1\":{\"158\":1}}],[\"freely\",{\"1\":{\"158\":1}}],[\"free\",{\"0\":{\"107\":1,\"279\":1,\"442\":1,\"456\":1,\"463\":1},\"1\":{\"194\":1,\"279\":3,\"306\":1,\"316\":1,\"333\":1,\"349\":2,\"395\":1,\"407\":1,\"427\":1,\"438\":1}}],[\"front\",{\"1\":{\"153\":1,\"231\":1,\"361\":1}}],[\"frontier\",{\"0\":{\"239\":1},\"1\":{\"133\":1,\"268\":1,\"331\":1,\"452\":1}}],[\"froehlich\",{\"1\":{\"105\":1}}],[\"from\",{\"0\":{\"107\":1,\"164\":1,\"169\":1,\"187\":1,\"190\":1,\"194\":1,\"196\":1,\"200\":1,\"222\":1,\"226\":1,\"227\":1,\"230\":1,\"241\":1,\"242\":1,\"256\":1,\"278\":1,\"294\":1,\"297\":1,\"322\":1,\"324\":1,\"339\":1,\"374\":1,\"435\":1,\"455\":1,\"470\":1,\"483\":1,\"489\":1},\"1\":{\"97\":1,\"101\":1,\"103\":1,\"105\":1,\"107\":5,\"111\":2,\"114\":1,\"116\":1,\"118\":2,\"122\":1,\"123\":1,\"124\":2,\"128\":1,\"130\":1,\"132\":1,\"133\":1,\"136\":1,\"137\":1,\"140\":1,\"141\":1,\"142\":1,\"144\":2,\"146\":2,\"151\":1,\"154\":1,\"156\":1,\"163\":1,\"164\":2,\"165\":1,\"172\":2,\"173\":1,\"174\":1,\"176\":1,\"178\":1,\"180\":1,\"182\":1,\"183\":1,\"184\":1,\"190\":4,\"192\":1,\"194\":1,\"195\":1,\"196\":2,\"198\":1,\"200\":3,\"204\":1,\"205\":2,\"207\":1,\"210\":1,\"212\":2,\"213\":1,\"217\":2,\"220\":1,\"222\":2,\"223\":1,\"224\":3,\"226\":1,\"227\":1,\"228\":2,\"230\":1,\"231\":1,\"233\":1,\"234\":1,\"240\":3,\"241\":2,\"242\":1,\"246\":1,\"248\":1,\"249\":2,\"250\":2,\"251\":1,\"252\":6,\"253\":1,\"254\":1,\"255\":1,\"256\":3,\"257\":3,\"261\":1,\"264\":1,\"265\":1,\"266\":1,\"269\":3,\"272\":1,\"275\":1,\"278\":3,\"279\":1,\"281\":2,\"282\":2,\"284\":1,\"286\":1,\"289\":1,\"291\":1,\"292\":1,\"298\":1,\"301\":1,\"305\":1,\"306\":2,\"308\":1,\"309\":2,\"312\":1,\"313\":1,\"318\":1,\"322\":2,\"324\":2,\"329\":1,\"330\":1,\"331\":1,\"333\":4,\"334\":1,\"336\":1,\"337\":1,\"339\":5,\"340\":3,\"342\":2,\"345\":1,\"348\":1,\"350\":2,\"351\":1,\"352\":2,\"353\":1,\"355\":1,\"356\":1,\"359\":1,\"361\":1,\"362\":2,\"363\":2,\"365\":2,\"366\":2,\"369\":2,\"370\":2,\"371\":1,\"376\":1,\"377\":2,\"378\":2,\"379\":1,\"380\":1,\"382\":1,\"386\":1,\"387\":1,\"390\":2,\"394\":1,\"396\":1,\"399\":1,\"400\":2,\"402\":1,\"403\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":2,\"416\":2,\"417\":1,\"423\":1,\"424\":1,\"425\":2,\"426\":6,\"431\":1,\"435\":1,\"437\":1,\"441\":3,\"442\":2,\"444\":3,\"445\":1,\"447\":1,\"448\":1,\"455\":5,\"456\":1,\"458\":1,\"462\":1,\"466\":2,\"467\":1,\"470\":5,\"474\":1,\"475\":2,\"476\":1,\"479\":3,\"482\":1,\"483\":4,\"485\":2,\"488\":1,\"489\":1,\"492\":2,\"496\":1}}],[\"fragmented\",{\"0\":{\"214\":1}}],[\"fraga\",{\"1\":{\"96\":1}}],[\"frank\",{\"1\":{\"419\":1,\"420\":1}}],[\"franklin\",{\"1\":{\"164\":1}}],[\"franssen\",{\"1\":{\"297\":1}}],[\"francesco\",{\"1\":{\"292\":1,\"409\":1}}],[\"francesca\",{\"1\":{\"171\":2,\"292\":1,\"302\":1}}],[\"france\",{\"1\":{\"232\":1}}],[\"francisco\",{\"1\":{\"195\":1,\"282\":2}}],[\"franziska\",{\"1\":{\"152\":1}}],[\"fraction\",{\"1\":{\"137\":1}}],[\"frame\",{\"0\":{\"157\":1,\"376\":1},\"1\":{\"157\":1,\"236\":1,\"376\":1}}],[\"frames\",{\"1\":{\"137\":1,\"263\":1,\"376\":1,\"393\":1}}],[\"frameworks\",{\"1\":{\"153\":1,\"157\":1,\"184\":1,\"333\":1,\"380\":1,\"498\":1}}],[\"framework\",{\"0\":{\"141\":1,\"171\":1,\"244\":1,\"265\":1,\"272\":1,\"292\":1,\"338\":1,\"382\":1,\"415\":1,\"477\":1,\"495\":1},\"1\":{\"96\":2,\"120\":1,\"126\":1,\"127\":1,\"130\":2,\"137\":2,\"150\":2,\"171\":1,\"173\":1,\"180\":2,\"184\":2,\"190\":1,\"204\":1,\"212\":1,\"224\":2,\"235\":2,\"242\":1,\"244\":1,\"245\":1,\"246\":6,\"259\":4,\"265\":1,\"266\":1,\"272\":2,\"277\":4,\"279\":1,\"288\":1,\"292\":2,\"300\":1,\"305\":1,\"314\":1,\"316\":2,\"319\":1,\"325\":1,\"329\":2,\"333\":2,\"338\":1,\"344\":1,\"347\":2,\"359\":2,\"362\":1,\"372\":1,\"380\":2,\"382\":1,\"385\":1,\"390\":3,\"396\":1,\"398\":1,\"403\":1,\"415\":3,\"428\":1,\"432\":1,\"434\":1,\"438\":1,\"450\":4,\"453\":1,\"455\":1,\"475\":2,\"477\":5,\"483\":3,\"485\":2,\"487\":1,\"488\":1,\"489\":1,\"495\":2,\"529\":1}}],[\"framing\",{\"0\":{\"135\":1,\"263\":1},\"1\":{\"135\":1,\"263\":2}}],[\"fajie\",{\"1\":{\"479\":1}}],[\"fazlolah\",{\"1\":{\"323\":1}}],[\"fatalities\",{\"1\":{\"298\":1}}],[\"fatigue\",{\"1\":{\"234\":1}}],[\"fastest\",{\"1\":{\"428\":1}}],[\"faster\",{\"0\":{\"229\":1},\"1\":{\"416\":1}}],[\"fast\",{\"0\":{\"234\":1,\"349\":1},\"1\":{\"398\":1,\"460\":1,\"469\":1}}],[\"fashion\",{\"1\":{\"9\":2,\"463\":1}}],[\"fakeness\",{\"1\":{\"213\":1}}],[\"fake\",{\"0\":{\"491\":1},\"1\":{\"213\":5,\"491\":6}}],[\"fakes\",{\"0\":{\"198\":1,\"387\":1}}],[\"falcon\",{\"1\":{\"497\":1}}],[\"false\",{\"0\":{\"369\":1},\"1\":{\"211\":1,\"252\":1,\"369\":1,\"424\":1,\"426\":1}}],[\"fallacies\",{\"0\":{\"161\":1,\"354\":1},\"1\":{\"161\":1,\"354\":1}}],[\"fall\",{\"1\":{\"151\":1,\"348\":1}}],[\"falls\",{\"1\":{\"119\":1,\"161\":1,\"354\":1,\"441\":1}}],[\"family\",{\"0\":{\"206\":1},\"1\":{\"206\":3,\"469\":1}}],[\"familiar\",{\"1\":{\"199\":1,\"249\":1,\"254\":2,\"282\":1}}],[\"familiarity\",{\"1\":{\"165\":1,\"178\":1,\"303\":2,\"304\":3}}],[\"families\",{\"1\":{\"133\":1,\"197\":1,\"206\":1,\"331\":1,\"494\":1}}],[\"favor\",{\"1\":{\"412\":1}}],[\"favored\",{\"1\":{\"189\":1}}],[\"favorable\",{\"1\":{\"155\":1}}],[\"farnoosh\",{\"1\":{\"407\":1}}],[\"faraji\",{\"1\":{\"407\":1}}],[\"faraz\",{\"1\":{\"407\":1}}],[\"faraci\",{\"1\":{\"171\":1}}],[\"far\",{\"0\":{\"345\":1},\"1\":{\"169\":1,\"181\":1,\"239\":1,\"272\":1,\"282\":1,\"419\":1,\"420\":1,\"471\":1}}],[\"fabien\",{\"1\":{\"444\":1}}],[\"fabiano\",{\"1\":{\"446\":1}}],[\"fabian\",{\"1\":{\"182\":1,\"436\":1}}],[\"fabiana\",{\"1\":{\"149\":1}}],[\"fabrice\",{\"1\":{\"179\":1}}],[\"fabrication\",{\"1\":{\"113\":2,\"115\":1}}],[\"fabricating\",{\"0\":{\"113\":1}}],[\"fan\",{\"1\":{\"425\":1,\"489\":1}}],[\"fanny\",{\"1\":{\"217\":1}}],[\"fannie\",{\"1\":{\"147\":1}}],[\"fangyu\",{\"1\":{\"467\":1}}],[\"fangkai\",{\"1\":{\"443\":1}}],[\"fang\",{\"1\":{\"135\":1,\"326\":1,\"394\":1,\"437\":1}}],[\"fairly\",{\"0\":{\"394\":1}}],[\"fairness\",{\"0\":{\"196\":1,\"394\":1},\"1\":{\"160\":3,\"196\":5,\"394\":3}}],[\"fair\",{\"0\":{\"160\":2},\"1\":{\"160\":3,\"196\":1,\"394\":2}}],[\"failing\",{\"1\":{\"251\":1,\"345\":1,\"423\":1}}],[\"failures\",{\"1\":{\"355\":1}}],[\"failure\",{\"0\":{\"322\":1},\"1\":{\"241\":1,\"338\":1}}],[\"failed\",{\"1\":{\"165\":1,\"322\":3}}],[\"fail\",{\"1\":{\"133\":1,\"184\":1,\"241\":1,\"309\":1,\"319\":1,\"331\":1,\"340\":1,\"380\":1,\"417\":1,\"491\":1,\"495\":1}}],[\"faithfully\",{\"1\":{\"395\":1}}],[\"faithfulness\",{\"1\":{\"131\":1,\"349\":1,\"372\":1,\"391\":2}}],[\"faithful\",{\"1\":{\"131\":1,\"372\":1,\"395\":2}}],[\"facchini\",{\"1\":{\"171\":1}}],[\"faculties\",{\"1\":{\"153\":1}}],[\"factually\",{\"1\":{\"467\":1}}],[\"factual\",{\"1\":{\"372\":1,\"390\":7,\"437\":1,\"453\":1,\"463\":5,\"467\":1,\"486\":1}}],[\"factuality\",{\"0\":{\"463\":1},\"1\":{\"335\":1,\"463\":3,\"467\":3}}],[\"facto\",{\"1\":{\"233\":1,\"326\":4}}],[\"factory\",{\"1\":{\"228\":1}}],[\"factor\",{\"1\":{\"131\":1,\"389\":1,\"416\":1,\"469\":1}}],[\"factors\",{\"1\":{\"128\":1,\"131\":5,\"191\":1,\"197\":1,\"214\":2,\"221\":1,\"237\":1,\"238\":1,\"244\":1,\"264\":1,\"270\":1,\"274\":1,\"304\":2,\"353\":1,\"417\":1,\"439\":1,\"448\":1,\"472\":1,\"495\":2}}],[\"facts\",{\"1\":{\"152\":1,\"251\":1,\"328\":1,\"340\":1,\"350\":8,\"423\":1,\"463\":1,\"467\":1}}],[\"fact\",{\"0\":{\"350\":1,\"431\":1},\"1\":{\"149\":1,\"350\":5,\"431\":3,\"494\":1}}],[\"faced\",{\"1\":{\"340\":1}}],[\"facets\",{\"1\":{\"333\":1}}],[\"faceted\",{\"1\":{\"204\":1,\"364\":1}}],[\"faces\",{\"1\":{\"213\":1,\"322\":1,\"334\":1,\"402\":1}}],[\"facebook\",{\"1\":{\"192\":1}}],[\"face\",{\"0\":{\"445\":1},\"1\":{\"124\":1,\"138\":1,\"164\":1,\"192\":1,\"212\":1,\"277\":1,\"283\":1,\"338\":1,\"382\":1,\"408\":1,\"415\":1,\"430\":1,\"437\":1,\"495\":1}}],[\"facing\",{\"1\":{\"322\":1,\"355\":1}}],[\"facilitator\",{\"1\":{\"271\":1,\"457\":1}}],[\"facilitating\",{\"1\":{\"246\":1,\"271\":1,\"315\":1,\"349\":1,\"408\":1,\"457\":1}}],[\"facilitates\",{\"1\":{\"130\":1,\"377\":1,\"407\":1,\"447\":1}}],[\"facilitated\",{\"1\":{\"119\":1,\"216\":1,\"297\":1}}],[\"facilitate\",{\"0\":{\"296\":1},\"1\":{\"97\":2,\"131\":1,\"141\":1,\"151\":1,\"157\":1,\"169\":1,\"173\":2,\"176\":1,\"199\":1,\"215\":1,\"242\":1,\"243\":1,\"252\":1,\"287\":1,\"296\":1,\"312\":2,\"333\":1,\"348\":1,\"398\":1,\"411\":1,\"426\":1,\"450\":1,\"479\":1,\"486\":1,\"488\":1}}],[\"facility\",{\"1\":{\"183\":1,\"184\":1,\"380\":1}}],[\"facial\",{\"1\":{\"108\":1,\"204\":1}}],[\"figure\",{\"0\":{\"488\":1},\"1\":{\"488\":1}}],[\"figures\",{\"1\":{\"309\":1}}],[\"figma\",{\"1\":{\"8\":1}}],[\"firmly\",{\"1\":{\"298\":1}}],[\"firstly\",{\"1\":{\"149\":1,\"260\":1,\"318\":1,\"479\":1}}],[\"firsthand\",{\"1\":{\"136\":1}}],[\"first\",{\"1\":{\"96\":1,\"111\":1,\"117\":1,\"123\":1,\"125\":1,\"126\":1,\"129\":1,\"132\":1,\"152\":1,\"153\":1,\"159\":1,\"165\":1,\"179\":1,\"181\":1,\"217\":1,\"224\":1,\"232\":2,\"235\":1,\"240\":1,\"242\":1,\"245\":1,\"253\":1,\"270\":1,\"278\":1,\"279\":1,\"289\":1,\"296\":1,\"313\":1,\"315\":1,\"322\":1,\"330\":1,\"335\":1,\"342\":1,\"363\":1,\"378\":1,\"385\":1,\"391\":1,\"402\":1,\"403\":1,\"409\":2,\"428\":2,\"430\":1,\"439\":1,\"444\":1,\"445\":1,\"454\":1,\"458\":1,\"463\":1,\"467\":1,\"475\":1,\"486\":1,\"488\":1,\"497\":1}}],[\"fi\",{\"1\":{\"298\":1}}],[\"fixed\",{\"1\":{\"240\":1,\"350\":1,\"444\":1}}],[\"fixing\",{\"1\":{\"172\":1,\"350\":3,\"370\":1}}],[\"fit\",{\"1\":{\"233\":1,\"469\":1}}],[\"fits\",{\"1\":{\"216\":1,\"350\":1}}],[\"fitting\",{\"1\":{\"210\":1}}],[\"fictitious\",{\"1\":{\"198\":1,\"340\":1,\"387\":1}}],[\"fictional\",{\"1\":{\"158\":1}}],[\"fidelity\",{\"0\":{\"395\":1},\"1\":{\"171\":1,\"189\":4,\"236\":2,\"238\":1,\"256\":1,\"349\":1,\"369\":1,\"395\":2,\"399\":1,\"416\":1}}],[\"filipowicz\",{\"1\":{\"468\":1}}],[\"filice\",{\"1\":{\"405\":1}}],[\"filtration\",{\"1\":{\"460\":2}}],[\"filtering\",{\"1\":{\"405\":1}}],[\"filter\",{\"1\":{\"167\":1}}],[\"file\",{\"1\":{\"447\":1}}],[\"files\",{\"1\":{\"184\":1,\"380\":1,\"447\":1}}],[\"fill\",{\"1\":{\"182\":1,\"219\":1,\"379\":1,\"401\":1}}],[\"filled\",{\"1\":{\"100\":1}}],[\"fifty\",{\"1\":{\"149\":1}}],[\"fisher\",{\"1\":{\"147\":1,\"269\":1}}],[\"fierrez\",{\"1\":{\"126\":1}}],[\"fields\",{\"1\":{\"174\":1,\"180\":1,\"217\":1,\"222\":1,\"227\":1,\"371\":1,\"427\":1,\"449\":1,\"491\":1}}],[\"field\",{\"0\":{\"430\":1},\"1\":{\"102\":1,\"103\":1,\"127\":1,\"138\":1,\"159\":1,\"182\":1,\"188\":1,\"243\":1,\"261\":1,\"292\":2,\"307\":1,\"358\":1,\"363\":1,\"396\":1,\"422\":1,\"430\":2,\"443\":1,\"459\":1}}],[\"fiannaca\",{\"1\":{\"114\":1}}],[\"five\",{\"0\":{\"497\":1},\"1\":{\"105\":1,\"142\":1,\"174\":1,\"181\":1,\"185\":1,\"197\":1,\"204\":2,\"250\":1,\"257\":1,\"277\":1,\"334\":1,\"371\":1,\"411\":1,\"441\":2,\"460\":2,\"471\":1,\"479\":1,\"497\":1}}],[\"finite\",{\"1\":{\"257\":1}}],[\"fina\",{\"1\":{\"388\":1}}],[\"financial\",{\"0\":{\"477\":1},\"1\":{\"284\":1,\"477\":4}}],[\"finance\",{\"1\":{\"226\":1,\"258\":1,\"433\":1}}],[\"finalized\",{\"1\":{\"236\":1}}],[\"final\",{\"1\":{\"123\":1,\"184\":1,\"380\":1,\"399\":1,\"462\":1}}],[\"finally\",{\"1\":{\"101\":1,\"103\":1,\"122\":1,\"149\":1,\"179\":1,\"216\":1,\"231\":1,\"270\":1,\"286\":1,\"294\":1,\"342\":1,\"355\":1,\"374\":1,\"389\":1,\"396\":1,\"413\":1,\"474\":1,\"479\":1}}],[\"fingertips\",{\"1\":{\"220\":1}}],[\"find\",{\"0\":{\"431\":1},\"1\":{\"151\":1,\"175\":2,\"200\":2,\"215\":2,\"230\":2,\"233\":1,\"238\":1,\"254\":1,\"278\":1,\"282\":1,\"302\":3,\"317\":2,\"335\":1,\"344\":1,\"348\":1,\"363\":1,\"365\":1,\"386\":1,\"389\":1,\"395\":1,\"412\":1,\"413\":1,\"431\":1,\"471\":1,\"496\":1,\"497\":1}}],[\"finding\",{\"1\":{\"102\":1,\"143\":1,\"164\":1,\"199\":1,\"322\":1,\"409\":1}}],[\"findings\",{\"1\":{\"98\":1,\"102\":1,\"105\":1,\"106\":1,\"110\":1,\"121\":1,\"125\":1,\"142\":1,\"143\":1,\"150\":1,\"155\":1,\"156\":2,\"160\":1,\"163\":2,\"164\":1,\"165\":1,\"169\":1,\"178\":2,\"185\":1,\"187\":1,\"191\":1,\"195\":1,\"197\":1,\"202\":2,\"203\":1,\"204\":2,\"214\":1,\"221\":1,\"227\":3,\"228\":1,\"234\":1,\"238\":1,\"248\":1,\"252\":1,\"256\":1,\"263\":1,\"272\":1,\"274\":1,\"286\":1,\"290\":1,\"291\":1,\"293\":1,\"303\":1,\"304\":1,\"305\":1,\"308\":1,\"321\":1,\"323\":1,\"327\":1,\"328\":1,\"336\":1,\"350\":1,\"353\":1,\"356\":1,\"359\":1,\"365\":1,\"385\":1,\"391\":1,\"400\":1,\"404\":1,\"417\":1,\"424\":1,\"425\":1,\"426\":1,\"428\":3,\"431\":1,\"437\":1,\"439\":1,\"441\":1,\"458\":1,\"461\":2,\"466\":1,\"467\":1,\"474\":1,\"476\":1,\"491\":1,\"495\":1}}],[\"finetuning\",{\"1\":{\"470\":1}}],[\"finetuned\",{\"1\":{\"381\":1,\"411\":1}}],[\"finer\",{\"1\":{\"430\":1}}],[\"fine\",{\"0\":{\"322\":1,\"374\":1,\"416\":1,\"425\":1,\"472\":1},\"1\":{\"126\":1,\"141\":1,\"240\":1,\"258\":1,\"318\":1,\"319\":2,\"345\":1,\"351\":1,\"368\":1,\"374\":4,\"376\":1,\"392\":2,\"404\":1,\"405\":1,\"411\":1,\"416\":2,\"425\":1,\"430\":2,\"433\":1,\"436\":1,\"442\":1,\"453\":1,\"458\":1,\"468\":1,\"470\":1,\"471\":3,\"472\":3,\"487\":1}}],[\"fogliato\",{\"1\":{\"368\":1}}],[\"footprint\",{\"1\":{\"469\":1}}],[\"footprints\",{\"1\":{\"462\":2}}],[\"footnote\",{\"1\":{\"316\":1}}],[\"foo\",{\"1\":{\"438\":1}}],[\"food\",{\"0\":{\"328\":1},\"1\":{\"256\":1,\"328\":4}}],[\"foods\",{\"1\":{\"142\":2}}],[\"focbinllm\",{\"1\":{\"488\":1}}],[\"foc\",{\"0\":{\"488\":1},\"1\":{\"488\":7}}],[\"focal\",{\"1\":{\"239\":1,\"489\":1}}],[\"focuses\",{\"1\":{\"143\":1,\"172\":1,\"186\":1,\"200\":1,\"228\":1,\"307\":1,\"362\":1,\"370\":1,\"391\":1,\"437\":1,\"481\":1}}],[\"focused\",{\"1\":{\"139\":1,\"142\":1,\"167\":1,\"249\":1,\"257\":1,\"292\":1,\"300\":1,\"439\":1,\"442\":1,\"472\":1,\"480\":1,\"496\":1}}],[\"focusing\",{\"1\":{\"120\":1,\"131\":1,\"176\":1,\"186\":1,\"220\":1,\"235\":1,\"251\":1,\"273\":1,\"274\":1,\"292\":1,\"309\":1,\"337\":1,\"351\":1,\"352\":1,\"394\":1,\"410\":1,\"412\":1,\"423\":1,\"458\":1}}],[\"focus\",{\"1\":{\"108\":1,\"116\":1,\"119\":1,\"136\":1,\"155\":1,\"163\":1,\"178\":1,\"182\":1,\"187\":1,\"224\":1,\"235\":1,\"279\":3,\"290\":1,\"305\":1,\"315\":1,\"318\":1,\"324\":1,\"345\":1,\"358\":1,\"400\":1,\"422\":1,\"429\":1,\"455\":1,\"463\":1,\"468\":1,\"469\":1,\"471\":1,\"475\":1}}],[\"fourweekm\",{\"1\":{\"525\":1}}],[\"foursquare\",{\"1\":{\"512\":1}}],[\"fourth\",{\"1\":{\"253\":1,\"388\":1}}],[\"four\",{\"1\":{\"141\":1,\"155\":1,\"164\":1,\"173\":1,\"190\":1,\"194\":1,\"228\":1,\"238\":3,\"251\":1,\"271\":2,\"315\":2,\"317\":1,\"318\":1,\"333\":1,\"378\":1,\"379\":1,\"386\":1,\"391\":1,\"413\":1,\"423\":1,\"457\":2,\"495\":1}}],[\"foundation\",{\"1\":{\"229\":1,\"358\":1,\"374\":1,\"428\":1,\"485\":3}}],[\"foundational\",{\"1\":{\"112\":1,\"349\":1,\"374\":2,\"485\":1}}],[\"found\",{\"1\":{\"96\":1,\"99\":1,\"102\":1,\"107\":1,\"108\":1,\"119\":1,\"155\":1,\"161\":1,\"203\":1,\"204\":1,\"214\":1,\"220\":1,\"249\":1,\"265\":1,\"282\":1,\"303\":1,\"350\":1,\"354\":1,\"366\":1,\"374\":1,\"381\":1,\"388\":1,\"458\":1,\"482\":1,\"486\":1,\"494\":1,\"540\":1}}],[\"fox\",{\"1\":{\"140\":1,\"164\":1}}],[\"fostering\",{\"1\":{\"359\":1,\"428\":1}}],[\"fosters\",{\"1\":{\"212\":1,\"408\":1}}],[\"foster\",{\"0\":{\"129\":1},\"1\":{\"206\":1,\"288\":1,\"303\":1,\"377\":1}}],[\"fold\",{\"1\":{\"126\":1,\"495\":1}}],[\"follows\",{\"1\":{\"269\":1}}],[\"following\",{\"1\":{\"135\":1,\"150\":2,\"151\":1,\"161\":1,\"179\":1,\"202\":1,\"211\":1,\"261\":1,\"322\":1,\"335\":1,\"348\":1,\"353\":1,\"354\":1,\"391\":1,\"447\":1,\"455\":1,\"482\":1}}],[\"follow\",{\"1\":{\"114\":2,\"122\":1,\"182\":1,\"279\":1,\"317\":1}}],[\"followed\",{\"1\":{\"106\":1,\"121\":1,\"235\":1,\"274\":1,\"358\":1,\"369\":1,\"382\":1,\"497\":1}}],[\"foteinopoulou\",{\"1\":{\"123\":1}}],[\"forbes\",{\"1\":{\"539\":1}}],[\"forbidden\",{\"1\":{\"22\":2}}],[\"fortunately\",{\"1\":{\"427\":1}}],[\"forthcoming\",{\"1\":{\"359\":1}}],[\"forums\",{\"1\":{\"254\":1}}],[\"forced\",{\"1\":{\"254\":1}}],[\"force\",{\"0\":{\"246\":1}}],[\"forefront\",{\"0\":{\"465\":1},\"1\":{\"465\":1}}],[\"forearm\",{\"1\":{\"220\":1}}],[\"foret\",{\"1\":{\"210\":1}}],[\"forecast用户反应\",{\"1\":{\"532\":1}}],[\"forecasting\",{\"1\":{\"183\":1}}],[\"forecasts\",{\"0\":{\"183\":1},\"1\":{\"183\":1}}],[\"formula\",{\"1\":{\"422\":2}}],[\"formulas\",{\"1\":{\"422\":3}}],[\"formulation\",{\"1\":{\"353\":1,\"438\":1}}],[\"formulate\",{\"1\":{\"175\":1,\"417\":1}}],[\"formulated\",{\"1\":{\"117\":1}}],[\"formidable\",{\"1\":{\"359\":1,\"424\":1}}],[\"forming\",{\"1\":{\"106\":1,\"218\":1,\"462\":1}}],[\"formwork\",{\"1\":{\"300\":1}}],[\"former\",{\"1\":{\"183\":1,\"322\":1,\"325\":1}}],[\"form\",{\"1\":{\"161\":1,\"235\":1,\"257\":2,\"268\":1,\"271\":1,\"294\":1,\"296\":1,\"354\":1,\"368\":1,\"422\":1,\"429\":1,\"452\":1,\"457\":1,\"497\":1}}],[\"formalism\",{\"1\":{\"353\":1}}],[\"formalized\",{\"1\":{\"455\":1,\"494\":2}}],[\"formalize\",{\"1\":{\"322\":1,\"422\":1}}],[\"formally\",{\"0\":{\"490\":1},\"1\":{\"205\":1,\"414\":1}}],[\"formal\",{\"0\":{\"153\":1,\"490\":1},\"1\":{\"122\":1,\"153\":4,\"169\":2,\"205\":2,\"490\":4,\"494\":3}}],[\"formatting\",{\"1\":{\"454\":1,\"471\":1}}],[\"formatted\",{\"1\":{\"151\":1,\"348\":1}}],[\"formation\",{\"1\":{\"182\":1,\"290\":1}}],[\"formations\",{\"0\":{\"182\":1},\"1\":{\"182\":1}}],[\"formative\",{\"0\":{\"236\":1},\"1\":{\"105\":1,\"129\":1,\"131\":1,\"236\":3,\"242\":1,\"272\":1}}],[\"format\",{\"0\":{\"317\":1},\"1\":{\"114\":1,\"198\":1,\"228\":2,\"259\":1,\"317\":2,\"387\":1,\"411\":1,\"424\":4,\"450\":1,\"454\":1}}],[\"formats\",{\"1\":{\"114\":1,\"146\":1,\"163\":1,\"353\":1,\"469\":3,\"472\":2,\"480\":1}}],[\"forms\",{\"1\":{\"103\":1,\"112\":1,\"184\":1,\"215\":1,\"237\":1,\"244\":1,\"256\":1,\"302\":2,\"362\":1,\"380\":1}}],[\"forwarded\",{\"1\":{\"435\":1}}],[\"forward\",{\"1\":{\"100\":1,\"120\":1,\"228\":1,\"456\":1}}],[\"for\",{\"0\":{\"98\":1,\"99\":1,\"101\":1,\"102\":1,\"112\":1,\"116\":1,\"117\":1,\"118\":1,\"120\":1,\"122\":1,\"123\":1,\"124\":1,\"126\":1,\"127\":1,\"129\":2,\"130\":1,\"132\":1,\"133\":1,\"137\":1,\"139\":1,\"144\":1,\"146\":1,\"147\":1,\"149\":2,\"153\":1,\"157\":1,\"161\":1,\"166\":1,\"167\":1,\"171\":1,\"173\":1,\"183\":1,\"184\":1,\"185\":1,\"188\":1,\"189\":1,\"205\":1,\"206\":1,\"210\":1,\"217\":1,\"220\":1,\"224\":1,\"229\":1,\"231\":1,\"234\":1,\"236\":1,\"239\":1,\"244\":1,\"246\":2,\"250\":1,\"252\":1,\"258\":1,\"265\":1,\"271\":1,\"272\":1,\"280\":2,\"282\":1,\"288\":1,\"292\":1,\"293\":1,\"294\":1,\"297\":1,\"300\":1,\"302\":1,\"309\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":2,\"318\":1,\"322\":1,\"324\":1,\"326\":1,\"328\":1,\"330\":1,\"331\":1,\"335\":1,\"341\":1,\"343\":1,\"349\":1,\"351\":1,\"354\":1,\"355\":1,\"356\":1,\"359\":1,\"360\":1,\"361\":1,\"364\":1,\"368\":1,\"372\":2,\"375\":1,\"378\":1,\"379\":1,\"380\":1,\"382\":1,\"384\":2,\"386\":1,\"388\":1,\"393\":1,\"408\":1,\"410\":1,\"414\":1,\"417\":1,\"422\":1,\"425\":1,\"426\":1,\"427\":1,\"432\":1,\"433\":1,\"436\":1,\"439\":1,\"444\":1,\"447\":1,\"449\":1,\"454\":1,\"455\":1,\"457\":1,\"460\":1,\"462\":1,\"463\":1,\"467\":1,\"470\":1,\"472\":1,\"475\":1,\"486\":1,\"495\":1},\"1\":{\"74\":1,\"96\":1,\"97\":6,\"98\":3,\"99\":3,\"100\":2,\"101\":4,\"103\":1,\"104\":5,\"105\":1,\"106\":3,\"107\":1,\"108\":3,\"110\":4,\"111\":3,\"112\":4,\"113\":2,\"114\":5,\"115\":1,\"116\":2,\"118\":1,\"119\":2,\"120\":5,\"121\":2,\"122\":3,\"124\":2,\"125\":2,\"126\":2,\"128\":3,\"129\":1,\"130\":1,\"131\":2,\"132\":4,\"133\":2,\"135\":1,\"136\":2,\"137\":3,\"138\":4,\"139\":4,\"140\":3,\"141\":2,\"143\":2,\"144\":9,\"146\":3,\"147\":1,\"148\":1,\"149\":3,\"150\":2,\"151\":6,\"152\":4,\"153\":5,\"154\":3,\"155\":1,\"156\":2,\"158\":1,\"159\":4,\"160\":3,\"161\":3,\"163\":3,\"164\":3,\"165\":2,\"166\":7,\"170\":3,\"171\":1,\"172\":2,\"173\":3,\"174\":2,\"175\":1,\"178\":1,\"180\":5,\"181\":1,\"182\":6,\"183\":4,\"184\":6,\"185\":1,\"186\":1,\"188\":3,\"189\":2,\"190\":2,\"191\":2,\"192\":3,\"194\":2,\"195\":2,\"197\":1,\"198\":1,\"199\":2,\"200\":2,\"201\":1,\"203\":2,\"204\":1,\"205\":1,\"206\":3,\"207\":1,\"209\":1,\"211\":5,\"212\":1,\"213\":1,\"214\":3,\"215\":2,\"217\":3,\"218\":1,\"219\":2,\"220\":1,\"222\":4,\"223\":3,\"224\":5,\"226\":3,\"227\":1,\"229\":2,\"230\":1,\"231\":5,\"232\":1,\"233\":5,\"234\":2,\"235\":1,\"236\":2,\"237\":6,\"238\":1,\"239\":1,\"240\":4,\"241\":4,\"242\":1,\"243\":2,\"244\":2,\"245\":5,\"246\":6,\"249\":2,\"250\":1,\"252\":1,\"253\":2,\"254\":2,\"255\":1,\"256\":2,\"258\":4,\"259\":2,\"260\":1,\"261\":2,\"263\":1,\"264\":3,\"265\":3,\"269\":6,\"270\":2,\"271\":1,\"272\":5,\"273\":3,\"274\":2,\"275\":1,\"278\":2,\"279\":2,\"280\":2,\"282\":1,\"284\":8,\"287\":3,\"290\":2,\"291\":1,\"292\":3,\"294\":4,\"295\":3,\"296\":2,\"297\":3,\"298\":2,\"300\":2,\"301\":1,\"302\":2,\"303\":2,\"304\":1,\"305\":4,\"306\":4,\"307\":1,\"309\":3,\"312\":6,\"313\":3,\"314\":1,\"315\":7,\"316\":5,\"317\":1,\"318\":2,\"319\":1,\"321\":4,\"323\":3,\"324\":3,\"325\":1,\"326\":3,\"329\":1,\"330\":4,\"331\":2,\"333\":3,\"334\":3,\"335\":2,\"336\":1,\"337\":1,\"339\":3,\"341\":2,\"342\":1,\"343\":1,\"344\":1,\"345\":3,\"348\":6,\"349\":1,\"350\":2,\"351\":2,\"352\":1,\"353\":7,\"354\":3,\"355\":4,\"356\":5,\"358\":2,\"359\":6,\"360\":2,\"361\":1,\"362\":2,\"363\":2,\"364\":1,\"365\":6,\"366\":1,\"368\":3,\"369\":4,\"370\":2,\"371\":2,\"372\":1,\"374\":2,\"375\":3,\"376\":2,\"377\":2,\"378\":2,\"379\":1,\"380\":6,\"381\":6,\"382\":1,\"384\":1,\"385\":1,\"387\":1,\"388\":5,\"389\":3,\"392\":2,\"393\":2,\"394\":1,\"395\":5,\"396\":4,\"398\":4,\"399\":2,\"400\":2,\"401\":2,\"402\":2,\"403\":1,\"404\":3,\"407\":5,\"408\":3,\"409\":4,\"410\":5,\"411\":5,\"412\":4,\"413\":5,\"414\":4,\"415\":1,\"416\":2,\"417\":4,\"422\":4,\"424\":4,\"425\":3,\"426\":1,\"427\":1,\"429\":1,\"430\":2,\"431\":1,\"432\":5,\"433\":4,\"434\":1,\"435\":2,\"436\":3,\"437\":2,\"438\":1,\"439\":1,\"441\":1,\"442\":2,\"444\":1,\"445\":3,\"447\":5,\"448\":3,\"453\":3,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":4,\"459\":2,\"460\":1,\"461\":1,\"462\":5,\"463\":1,\"465\":4,\"466\":2,\"467\":4,\"468\":4,\"469\":2,\"470\":2,\"471\":3,\"472\":2,\"475\":8,\"476\":1,\"477\":1,\"478\":2,\"479\":5,\"480\":3,\"481\":3,\"482\":4,\"483\":3,\"485\":2,\"486\":4,\"487\":1,\"488\":2,\"491\":2,\"492\":1,\"494\":1,\"495\":3,\"496\":3,\"497\":1,\"498\":1,\"524\":1}}],[\"forgetting\",{\"0\":{\"165\":1},\"1\":{\"165\":4}}],[\"forge\",{\"1\":{\"40\":1,\"359\":1}}],[\"february\",{\"1\":{\"428\":1}}],[\"fever\",{\"1\":{\"390\":2}}],[\"feng\",{\"1\":{\"242\":1,\"378\":1,\"403\":1,\"417\":1,\"455\":1}}],[\"fenglong\",{\"1\":{\"212\":1}}],[\"fey\",{\"1\":{\"236\":1}}],[\"federal\",{\"1\":{\"234\":1}}],[\"federated\",{\"1\":{\"230\":1}}],[\"fediverse\",{\"0\":{\"230\":1},\"1\":{\"230\":2}}],[\"feddersen\",{\"1\":{\"183\":1}}],[\"ferrari\",{\"1\":{\"339\":1}}],[\"ferrero\",{\"1\":{\"315\":1}}],[\"ferret\",{\"0\":{\"151\":1,\"348\":1},\"1\":{\"151\":4,\"348\":4}}],[\"ferguson\",{\"1\":{\"278\":1,\"279\":1}}],[\"fernandez\",{\"1\":{\"181\":1}}],[\"fernando\",{\"1\":{\"181\":1}}],[\"felt\",{\"1\":{\"209\":1}}],[\"feldacker\",{\"1\":{\"178\":1}}],[\"felix\",{\"1\":{\"101\":1}}],[\"felipe\",{\"1\":{\"96\":1,\"282\":1}}],[\"fetched\",{\"1\":{\"169\":1}}],[\"fetching\",{\"1\":{\"74\":1}}],[\"fees\",{\"1\":{\"192\":3}}],[\"feedforward\",{\"1\":{\"255\":1,\"427\":1}}],[\"feeding\",{\"1\":{\"184\":1,\"380\":1}}],[\"feed\",{\"1\":{\"173\":1,\"456\":1}}],[\"feedback~\",{\"1\":{\"152\":1}}],[\"feedback\",{\"0\":{\"191\":1,\"489\":1},\"1\":{\"99\":1,\"108\":1,\"120\":2,\"122\":1,\"146\":1,\"156\":1,\"169\":1,\"180\":1,\"188\":1,\"191\":5,\"196\":3,\"218\":1,\"219\":1,\"220\":3,\"222\":1,\"228\":1,\"229\":1,\"259\":1,\"274\":2,\"289\":1,\"307\":1,\"324\":1,\"360\":1,\"375\":1,\"386\":3,\"401\":1,\"453\":2,\"466\":1,\"483\":2,\"489\":2}}],[\"feelings\",{\"1\":{\"209\":1,\"265\":1,\"303\":1}}],[\"feeling\",{\"1\":{\"202\":1,\"204\":1,\"282\":1}}],[\"feel\",{\"1\":{\"158\":1,\"164\":1,\"266\":1,\"302\":1}}],[\"fei\",{\"1\":{\"135\":1,\"256\":1,\"470\":1}}],[\"fewer\",{\"1\":{\"389\":1}}],[\"few\",{\"0\":{\"388\":1},\"1\":{\"104\":1,\"245\":1,\"308\":1,\"317\":1,\"323\":1,\"355\":1,\"365\":2,\"382\":1,\"388\":2,\"405\":2,\"412\":1,\"413\":1,\"443\":1,\"455\":2,\"470\":1,\"472\":1,\"475\":1,\"481\":1}}],[\"fear\",{\"1\":{\"302\":2}}],[\"fearful\",{\"1\":{\"302\":1}}],[\"fears\",{\"0\":{\"302\":1},\"1\":{\"302\":2}}],[\"feasibility\",{\"1\":{\"120\":1,\"121\":1,\"166\":1,\"179\":1,\"410\":1,\"435\":1,\"445\":1,\"459\":1}}],[\"feasible\",{\"1\":{\"100\":1,\"223\":1,\"355\":1,\"399\":1,\"400\":1,\"469\":1}}],[\"feature\",{\"0\":{\"103\":1,\"122\":1,\"281\":1,\"355\":1},\"1\":{\"103\":2,\"112\":1,\"122\":1,\"159\":2,\"171\":1,\"188\":1,\"192\":1,\"200\":1,\"217\":1,\"235\":1,\"237\":1,\"281\":2,\"288\":1,\"305\":2,\"326\":1,\"349\":1,\"355\":6,\"415\":1}}],[\"features\",{\"1\":{\"99\":1,\"103\":3,\"114\":1,\"118\":1,\"123\":2,\"137\":1,\"146\":1,\"151\":1,\"159\":2,\"163\":2,\"210\":1,\"223\":2,\"235\":1,\"237\":2,\"244\":1,\"245\":1,\"260\":2,\"270\":1,\"292\":2,\"305\":2,\"333\":2,\"348\":1,\"349\":3,\"352\":1,\"393\":1}}],[\"featuring\",{\"1\":{\"98\":1,\"201\":1,\"350\":1}}],[\"fuccella\",{\"1\":{\"492\":1}}],[\"fusion\",{\"0\":{\"223\":1},\"1\":{\"240\":1,\"355\":1}}],[\"fused\",{\"1\":{\"123\":1,\"223\":1,\"462\":1}}],[\"fuses\",{\"1\":{\"123\":1}}],[\"fuse\",{\"1\":{\"117\":1,\"355\":1}}],[\"fu\",{\"1\":{\"173\":1,\"359\":1,\"377\":1,\"378\":1,\"449\":1,\"483\":1}}],[\"full\",{\"1\":{\"361\":1,\"377\":1}}],[\"fully\",{\"1\":{\"146\":1,\"194\":1,\"204\":1,\"284\":1,\"353\":1,\"412\":1,\"413\":1,\"461\":1,\"491\":1}}],[\"fulfill\",{\"1\":{\"118\":1,\"169\":1}}],[\"furu\",{\"1\":{\"429\":1}}],[\"furlanello\",{\"1\":{\"120\":1}}],[\"further\",{\"1\":{\"112\":1,\"119\":1,\"120\":1,\"122\":1,\"130\":2,\"144\":1,\"151\":1,\"152\":1,\"172\":1,\"175\":1,\"181\":1,\"195\":1,\"199\":1,\"207\":2,\"219\":3,\"233\":1,\"235\":1,\"245\":1,\"246\":1,\"251\":1,\"255\":1,\"256\":1,\"259\":1,\"271\":1,\"273\":1,\"275\":1,\"283\":1,\"287\":1,\"290\":1,\"293\":1,\"297\":1,\"304\":1,\"338\":1,\"342\":1,\"344\":1,\"345\":1,\"348\":1,\"349\":1,\"356\":1,\"359\":1,\"364\":1,\"370\":1,\"377\":2,\"382\":1,\"386\":1,\"388\":1,\"389\":1,\"401\":3,\"412\":1,\"413\":1,\"417\":1,\"419\":1,\"420\":1,\"423\":1,\"438\":1,\"445\":1,\"450\":1,\"454\":1,\"457\":1,\"463\":2,\"466\":1,\"467\":1,\"482\":2,\"488\":2}}],[\"furthermore\",{\"1\":{\"101\":1,\"103\":1,\"139\":1,\"146\":1,\"148\":1,\"165\":1,\"191\":1,\"270\":1,\"289\":1,\"303\":1,\"309\":1,\"324\":1,\"329\":1,\"333\":1,\"336\":1,\"344\":1,\"361\":1,\"404\":1,\"410\":1,\"418\":1,\"424\":1,\"437\":1,\"471\":1,\"482\":1,\"488\":1}}],[\"fundraising\",{\"1\":{\"443\":1}}],[\"fundamentally\",{\"1\":{\"227\":1}}],[\"fundamental\",{\"1\":{\"132\":1,\"137\":1,\"205\":1,\"207\":1,\"229\":1,\"324\":1,\"330\":1,\"455\":1,\"475\":1}}],[\"functionalities\",{\"1\":{\"477\":1,\"495\":1}}],[\"functionality\",{\"1\":{\"119\":1,\"128\":1,\"148\":1,\"172\":1,\"192\":1,\"370\":1}}],[\"functional\",{\"1\":{\"223\":1,\"256\":1,\"399\":1}}],[\"function\",{\"1\":{\"151\":1,\"166\":1,\"223\":1,\"301\":1,\"347\":1,\"348\":1}}],[\"functions\",{\"0\":{\"488\":1},\"1\":{\"112\":1,\"166\":1,\"222\":1,\"224\":1,\"309\":1,\"317\":1,\"326\":1,\"398\":1,\"427\":1,\"477\":1,\"488\":5}}],[\"fungible\",{\"1\":{\"40\":1}}],[\"futures\",{\"0\":{\"248\":1},\"1\":{\"248\":2,\"533\":1}}],[\"future\",{\"0\":{\"169\":1},\"1\":{\"40\":1,\"97\":1,\"98\":1,\"100\":2,\"105\":1,\"108\":1,\"125\":1,\"132\":1,\"140\":1,\"152\":1,\"159\":2,\"164\":1,\"166\":1,\"169\":1,\"173\":1,\"181\":1,\"182\":1,\"186\":1,\"198\":1,\"206\":1,\"207\":1,\"212\":1,\"214\":1,\"220\":1,\"228\":1,\"229\":1,\"239\":1,\"243\":1,\"248\":3,\"251\":1,\"252\":1,\"253\":1,\"259\":1,\"265\":1,\"271\":1,\"272\":1,\"294\":1,\"296\":1,\"303\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":2,\"312\":1,\"326\":1,\"330\":1,\"333\":1,\"334\":1,\"358\":1,\"364\":2,\"379\":1,\"387\":1,\"412\":1,\"415\":1,\"423\":1,\"426\":1,\"429\":1,\"431\":1,\"437\":2,\"439\":1,\"457\":1,\"458\":1,\"470\":1,\"475\":1,\"491\":1,\"529\":2}}],[\"tl=zh\",{\"1\":{\"539\":1}}],[\"tm\",{\"0\":{\"439\":1}}],[\"tml\",{\"1\":{\"409\":1}}],[\"tgif\",{\"1\":{\"393\":1}}],[\"tf\",{\"1\":{\"365\":1}}],[\"tvqa\",{\"1\":{\"393\":1}}],[\"tvesha\",{\"1\":{\"302\":1}}],[\"tv\",{\"1\":{\"282\":1}}],[\"tps\",{\"1\":{\"235\":1}}],[\"tts\",{\"1\":{\"190\":2}}],[\"tsarfaty\",{\"1\":{\"340\":1}}],[\"tsort\",{\"1\":{\"337\":1}}],[\"tsu\",{\"1\":{\"271\":1,\"457\":1}}],[\"tsung\",{\"1\":{\"270\":1,\"319\":1}}],[\"tseng\",{\"1\":{\"197\":1}}],[\"tschorsch\",{\"1\":{\"187\":1}}],[\"tsvetkov\",{\"1\":{\"133\":1,\"331\":1}}],[\"twin\",{\"1\":{\"519\":1}}],[\"twitch\",{\"1\":{\"192\":1}}],[\"tweaking\",{\"1\":{\"224\":1}}],[\"twenty\",{\"1\":{\"203\":1}}],[\"twelve\",{\"1\":{\"201\":1,\"242\":1}}],[\"tweya\",{\"1\":{\"178\":1}}],[\"two\",{\"0\":{\"335\":1,\"445\":1},\"1\":{\"74\":1,\"96\":1,\"97\":1,\"103\":2,\"113\":1,\"114\":1,\"117\":2,\"122\":1,\"123\":1,\"126\":2,\"141\":1,\"143\":2,\"148\":1,\"152\":1,\"153\":1,\"158\":1,\"160\":1,\"165\":1,\"176\":1,\"179\":1,\"186\":1,\"189\":2,\"209\":1,\"210\":1,\"213\":1,\"214\":1,\"218\":2,\"220\":1,\"224\":1,\"226\":1,\"231\":1,\"234\":1,\"235\":1,\"239\":1,\"241\":1,\"245\":1,\"249\":1,\"258\":1,\"260\":1,\"263\":2,\"275\":1,\"278\":1,\"281\":1,\"284\":1,\"288\":1,\"291\":1,\"297\":2,\"298\":1,\"301\":4,\"304\":1,\"306\":1,\"312\":1,\"313\":2,\"315\":1,\"335\":1,\"337\":1,\"338\":2,\"342\":1,\"353\":1,\"355\":2,\"362\":1,\"366\":1,\"368\":1,\"376\":1,\"379\":1,\"390\":1,\"395\":1,\"402\":2,\"411\":1,\"417\":1,\"424\":1,\"433\":1,\"435\":1,\"438\":2,\"444\":1,\"445\":2,\"458\":1,\"462\":2,\"468\":2,\"470\":2,\"475\":2,\"476\":1,\"487\":1,\"490\":1,\"492\":1,\"497\":1}}],[\"tyagi\",{\"1\":{\"172\":1,\"370\":1}}],[\"typed\",{\"1\":{\"163\":3,\"455\":1}}],[\"type\",{\"1\":{\"155\":1,\"158\":1,\"203\":1,\"228\":2,\"236\":1,\"273\":1,\"339\":1,\"422\":1,\"435\":1,\"497\":1}}],[\"types\",{\"1\":{\"137\":1,\"189\":1,\"220\":1,\"240\":1,\"251\":1,\"257\":1,\"292\":1,\"327\":1,\"339\":1,\"374\":1,\"391\":1,\"407\":1,\"413\":4,\"423\":1,\"424\":1,\"427\":1,\"437\":2,\"439\":1,\"470\":1}}],[\"typical\",{\"1\":{\"126\":1,\"251\":1,\"270\":1,\"423\":1,\"488\":1}}],[\"typically\",{\"1\":{\"100\":1,\"112\":1,\"140\":1,\"151\":1,\"180\":1,\"221\":1,\"240\":1,\"244\":1,\"284\":1,\"292\":1,\"297\":1,\"315\":1,\"324\":1,\"348\":1,\"363\":1,\"405\":1,\"467\":1,\"490\":1}}],[\"t\",{\"0\":{\"170\":1,\"241\":1,\"244\":1,\"301\":1,\"494\":1},\"1\":{\"161\":1,\"176\":1,\"220\":1,\"244\":1,\"268\":1,\"301\":2,\"354\":1,\"419\":1,\"420\":1,\"452\":1}}],[\"tzimiropoulos\",{\"1\":{\"123\":1}}],[\"tu\",{\"1\":{\"478\":1}}],[\"turbo\",{\"1\":{\"334\":1,\"408\":1,\"411\":1,\"422\":2,\"486\":1}}],[\"turjo\",{\"1\":{\"298\":1}}],[\"turning\",{\"1\":{\"439\":1,\"447\":1}}],[\"turn\",{\"0\":{\"418\":1},\"1\":{\"218\":1,\"231\":1,\"252\":1,\"411\":1,\"418\":1,\"426\":1}}],[\"tune\",{\"1\":{\"344\":1,\"453\":1,\"487\":1}}],[\"tuned\",{\"0\":{\"416\":1},\"1\":{\"240\":1,\"258\":1,\"351\":1,\"374\":1,\"416\":2,\"433\":1,\"455\":1,\"458\":1}}],[\"tuning\",{\"0\":{\"322\":1,\"374\":1,\"425\":1,\"471\":1,\"472\":1},\"1\":{\"136\":1,\"318\":1,\"319\":2,\"349\":1,\"374\":3,\"382\":1,\"392\":1,\"404\":1,\"405\":1,\"411\":1,\"425\":1,\"436\":1,\"442\":1,\"455\":1,\"468\":1,\"470\":4,\"471\":3,\"472\":3}}],[\"tue\",{\"1\":{\"200\":1}}],[\"tumor\",{\"1\":{\"176\":1}}],[\"tuong\",{\"1\":{\"101\":1}}],[\"tutorial\",{\"1\":{\"122\":1,\"222\":1}}],[\"tutorials\",{\"0\":{\"122\":1},\"1\":{\"104\":1,\"110\":2,\"122\":5,\"321\":2}}],[\"tutoring\",{\"0\":{\"129\":1,\"130\":1},\"1\":{\"99\":1,\"129\":1,\"130\":1,\"169\":1}}],[\"tutors\",{\"0\":{\"99\":1},\"1\":{\"99\":1}}],[\"tutor\",{\"0\":{\"99\":1},\"1\":{\"99\":5}}],[\"teschke\",{\"1\":{\"458\":1}}],[\"testers\",{\"1\":{\"254\":1}}],[\"tested\",{\"1\":{\"119\":1,\"196\":1,\"485\":1}}],[\"tests\",{\"1\":{\"126\":1,\"215\":1,\"237\":1,\"282\":1,\"360\":3,\"402\":1,\"411\":1,\"441\":1}}],[\"test\",{\"0\":{\"360\":1},\"1\":{\"126\":3,\"159\":1,\"165\":1,\"172\":1,\"179\":2,\"199\":1,\"210\":1,\"220\":1,\"245\":1,\"273\":1,\"275\":1,\"337\":3,\"360\":3,\"365\":2,\"370\":1,\"372\":1,\"374\":1,\"379\":1,\"422\":1,\"430\":1,\"441\":1,\"442\":1,\"497\":1}}],[\"testing\",{\"0\":{\"144\":1},\"1\":{\"114\":1,\"128\":1,\"144\":2,\"184\":1,\"234\":1,\"236\":1,\"251\":1,\"257\":1,\"271\":1,\"304\":1,\"333\":1,\"340\":1,\"350\":1,\"360\":1,\"380\":1,\"423\":1,\"457\":1,\"468\":1,\"497\":1,\"539\":1}}],[\"tevfik\",{\"1\":{\"422\":1}}],[\"te\",{\"1\":{\"255\":3,\"424\":2}}],[\"teja\",{\"1\":{\"254\":1}}],[\"temperature\",{\"1\":{\"468\":1}}],[\"templates\",{\"1\":{\"328\":1,\"381\":1,\"409\":1}}],[\"template\",{\"1\":{\"246\":1,\"409\":1}}],[\"temporally\",{\"1\":{\"235\":1,\"301\":2}}],[\"temporal\",{\"0\":{\"301\":1,\"390\":1,\"424\":1},\"1\":{\"137\":1,\"143\":1,\"146\":3,\"229\":1,\"235\":1,\"255\":1,\"264\":3,\"272\":1,\"295\":1,\"301\":5,\"390\":1,\"393\":1,\"414\":1,\"424\":5,\"448\":3}}],[\"teblunthuis\",{\"1\":{\"230\":1}}],[\"teen\",{\"1\":{\"206\":2}}],[\"teens\",{\"1\":{\"206\":2}}],[\"teenagers\",{\"0\":{\"148\":1}}],[\"telpa\",{\"1\":{\"360\":4}}],[\"tell\",{\"0\":{\"240\":1,\"241\":1}}],[\"tellis\",{\"1\":{\"233\":1}}],[\"telukunta\",{\"1\":{\"196\":1}}],[\"telepresence\",{\"0\":{\"173\":1},\"1\":{\"173\":3}}],[\"teleaware\",{\"0\":{\"173\":1},\"1\":{\"173\":2}}],[\"teams\",{\"0\":{\"254\":1},\"1\":{\"244\":3,\"254\":2,\"278\":2,\"308\":1}}],[\"teamwork\",{\"1\":{\"170\":1,\"274\":1}}],[\"team\",{\"0\":{\"159\":1},\"1\":{\"159\":1,\"244\":1,\"269\":1,\"278\":3,\"388\":1,\"399\":1}}],[\"teaming\",{\"0\":{\"133\":1,\"331\":1},\"1\":{\"133\":2,\"244\":4,\"331\":2}}],[\"teach\",{\"1\":{\"241\":1,\"308\":1}}],[\"teachers\",{\"1\":{\"132\":1,\"330\":1,\"447\":1,\"486\":1}}],[\"teacher\",{\"1\":{\"130\":2,\"216\":1,\"273\":1}}],[\"teaching\",{\"0\":{\"241\":1,\"307\":1,\"329\":1},\"1\":{\"99\":1,\"130\":1,\"140\":1,\"160\":1,\"172\":1,\"254\":1,\"292\":1,\"363\":1,\"370\":1}}],[\"terrain\",{\"1\":{\"407\":2}}],[\"terrains\",{\"0\":{\"407\":1},\"1\":{\"407\":2}}],[\"terry\",{\"1\":{\"114\":2}}],[\"teruhisa\",{\"1\":{\"303\":1}}],[\"terminology\",{\"1\":{\"335\":1,\"396\":2,\"398\":1}}],[\"termed\",{\"1\":{\"132\":2,\"324\":1,\"330\":2}}],[\"term\",{\"1\":{\"117\":1,\"194\":1,\"235\":1,\"246\":1,\"283\":1,\"295\":1,\"376\":2,\"454\":1,\"468\":1,\"471\":1,\"475\":2}}],[\"terms\",{\"1\":{\"103\":1,\"182\":1,\"237\":1,\"257\":1,\"301\":2,\"335\":2,\"340\":1,\"360\":1,\"392\":1,\"405\":1,\"415\":1}}],[\"tech\",{\"1\":{\"106\":1}}],[\"techno\",{\"1\":{\"212\":1}}],[\"technological\",{\"1\":{\"179\":1,\"201\":1,\"216\":1,\"253\":1,\"284\":3,\"498\":1}}],[\"technologies`\",{\"1\":{\"253\":1}}],[\"technologies\",{\"0\":{\"246\":1},\"1\":{\"101\":1,\"110\":1,\"119\":1,\"144\":4,\"178\":2,\"206\":1,\"212\":1,\"216\":2,\"239\":1,\"248\":3,\"261\":1,\"282\":1,\"321\":1,\"356\":1,\"358\":1,\"398\":1}}],[\"technology\",{\"0\":{\"268\":1,\"396\":1,\"452\":1},\"1\":{\"166\":1,\"181\":1,\"213\":1,\"216\":1,\"229\":1,\"237\":1,\"244\":1,\"248\":1,\"253\":1,\"263\":1,\"305\":1,\"315\":1,\"396\":1,\"459\":1,\"498\":1,\"539\":1}}],[\"technology等联合举办\",{\"1\":{\"9\":1}}],[\"technical\",{\"0\":{\"101\":1},\"1\":{\"105\":1,\"120\":1,\"174\":1,\"210\":1,\"259\":1,\"261\":2,\"279\":2,\"308\":1,\"371\":1,\"398\":1,\"413\":1,\"498\":1}}],[\"technique\",{\"1\":{\"96\":1,\"113\":1,\"156\":1,\"187\":1,\"203\":1,\"223\":1,\"237\":1,\"255\":1,\"257\":1,\"305\":1,\"360\":1,\"386\":1,\"389\":1,\"390\":1,\"416\":1,\"425\":1,\"434\":2,\"459\":1}}],[\"techniques\",{\"1\":{\"96\":1,\"111\":1,\"112\":1,\"118\":1,\"119\":1,\"131\":1,\"136\":1,\"141\":1,\"152\":2,\"154\":1,\"173\":1,\"174\":1,\"213\":1,\"237\":1,\"246\":1,\"257\":1,\"270\":2,\"287\":1,\"296\":1,\"301\":2,\"306\":1,\"337\":1,\"356\":1,\"359\":1,\"360\":3,\"368\":2,\"371\":1,\"377\":1,\"386\":1,\"403\":2,\"427\":1,\"435\":1,\"460\":1,\"469\":1,\"471\":1,\"491\":1,\"492\":2}}],[\"tensor\",{\"1\":{\"469\":1}}],[\"tension\",{\"0\":{\"453\":1}}],[\"tensions\",{\"0\":{\"189\":1},\"1\":{\"279\":1}}],[\"tens\",{\"1\":{\"410\":1}}],[\"tengxiang\",{\"1\":{\"295\":1}}],[\"teng\",{\"1\":{\"256\":1,\"470\":1}}],[\"tenable\",{\"1\":{\"205\":1}}],[\"tentori\",{\"1\":{\"158\":1}}],[\"tendency\",{\"1\":{\"341\":1,\"368\":1,\"412\":1,\"432\":1,\"453\":1}}],[\"tends\",{\"1\":{\"158\":1,\"280\":1,\"389\":1,\"395\":1,\"424\":1,\"467\":1}}],[\"tend\",{\"1\":{\"146\":1,\"149\":1,\"163\":1,\"233\":1,\"238\":1,\"324\":1}}],[\"ten\",{\"1\":{\"105\":1,\"486\":1}}],[\"tenney\",{\"1\":{\"104\":1}}],[\"texttt\",{\"1\":{\"470\":1}}],[\"textbf\",{\"1\":{\"392\":5,\"470\":2,\"483\":2}}],[\"texture\",{\"1\":{\"355\":1}}],[\"textual\",{\"0\":{\"222\":1,\"393\":1,\"479\":1,\"485\":1,\"496\":1},\"1\":{\"222\":1,\"257\":1,\"259\":1,\"275\":1,\"280\":2,\"319\":1,\"375\":1,\"377\":1,\"393\":2,\"399\":1,\"424\":1,\"454\":1,\"479\":1,\"485\":2}}],[\"textit\",{\"1\":{\"155\":3,\"470\":1}}],[\"textsc\",{\"1\":{\"467\":4,\"470\":2,\"483\":1}}],[\"texts\",{\"1\":{\"104\":1,\"151\":1,\"315\":1,\"324\":2,\"336\":2,\"348\":1,\"352\":1,\"439\":4,\"491\":1}}],[\"text\",{\"0\":{\"101\":1,\"152\":1,\"219\":3,\"226\":1,\"280\":1,\"315\":2,\"324\":1,\"352\":1,\"389\":1,\"401\":3,\"410\":1,\"439\":1,\"467\":1,\"472\":1},\"1\":{\"101\":1,\"102\":1,\"104\":1,\"123\":2,\"124\":1,\"127\":3,\"151\":1,\"152\":3,\"163\":3,\"190\":5,\"195\":1,\"205\":1,\"219\":8,\"224\":1,\"226\":3,\"231\":1,\"251\":2,\"252\":1,\"258\":6,\"266\":3,\"280\":1,\"315\":6,\"324\":1,\"327\":1,\"337\":5,\"340\":1,\"345\":1,\"348\":1,\"349\":2,\"352\":1,\"355\":1,\"358\":1,\"359\":1,\"363\":1,\"369\":1,\"375\":1,\"378\":1,\"382\":3,\"384\":1,\"386\":1,\"389\":8,\"393\":2,\"394\":1,\"401\":8,\"405\":3,\"407\":2,\"410\":4,\"419\":8,\"420\":8,\"423\":2,\"426\":1,\"433\":6,\"438\":3,\"439\":3,\"442\":3,\"449\":2,\"460\":1,\"467\":4,\"472\":1,\"485\":4,\"491\":1,\"492\":3}}],[\"tr\",{\"1\":{\"539\":4}}],[\"try\",{\"1\":{\"322\":1}}],[\"trying\",{\"1\":{\"158\":1}}],[\"trek\",{\"0\":{\"439\":1}}],[\"tree\",{\"0\":{\"462\":2},\"1\":{\"411\":1,\"444\":1,\"462\":6}}],[\"trees\",{\"0\":{\"411\":1},\"1\":{\"305\":1}}],[\"trec\",{\"1\":{\"394\":1,\"481\":2}}],[\"treude\",{\"1\":{\"364\":1}}],[\"trend\",{\"1\":{\"215\":1,\"309\":1}}],[\"trends\",{\"1\":{\"152\":1,\"253\":1,\"309\":1,\"340\":1,\"424\":1,\"498\":1}}],[\"treating\",{\"1\":{\"450\":1}}],[\"treatment\",{\"1\":{\"121\":2,\"220\":1,\"227\":1}}],[\"treat\",{\"1\":{\"107\":1,\"175\":1}}],[\"tritt\",{\"1\":{\"398\":1}}],[\"triplets\",{\"1\":{\"382\":2}}],[\"tripartite\",{\"1\":{\"244\":1}}],[\"trialmaster\",{\"1\":{\"322\":1}}],[\"trials\",{\"0\":{\"358\":1,\"372\":1},\"1\":{\"322\":1,\"372\":1}}],[\"trial\",{\"0\":{\"322\":1},\"1\":{\"322\":3,\"358\":4,\"372\":1}}],[\"triads\",{\"1\":{\"108\":1}}],[\"tristan\",{\"1\":{\"287\":1}}],[\"trivial\",{\"1\":{\"188\":1,\"399\":1}}],[\"trimboli\",{\"1\":{\"171\":1}}],[\"trigger\",{\"1\":{\"130\":1,\"264\":1,\"340\":1,\"448\":1}}],[\"trojan\",{\"1\":{\"409\":3}}],[\"trolled\",{\"1\":{\"302\":1}}],[\"trolling\",{\"1\":{\"302\":1}}],[\"troubleshoot\",{\"1\":{\"172\":1,\"370\":1}}],[\"troubleshooting\",{\"1\":{\"172\":1,\"370\":1}}],[\"troubled\",{\"1\":{\"117\":1}}],[\"troiano\",{\"1\":{\"98\":1}}],[\"trukhanov\",{\"1\":{\"469\":1}}],[\"truth\",{\"1\":{\"199\":1,\"246\":1,\"470\":1}}],[\"truthfulness\",{\"1\":{\"198\":1,\"387\":1,\"390\":1}}],[\"truthful\",{\"1\":{\"198\":1,\"387\":1}}],[\"truly\",{\"0\":{\"333\":1},\"1\":{\"133\":1,\"207\":1,\"331\":1}}],[\"true\",{\"0\":{\"107\":1},\"1\":{\"196\":2,\"231\":1,\"309\":1,\"315\":1,\"424\":1,\"432\":1}}],[\"trustworthiness\",{\"0\":{\"155\":1},\"1\":{\"155\":1,\"258\":1,\"433\":1}}],[\"trust\",{\"0\":{\"135\":1,\"244\":2,\"494\":1},\"1\":{\"102\":1,\"135\":7,\"178\":2,\"226\":1,\"244\":8,\"252\":1,\"288\":1,\"426\":1}}],[\"travis\",{\"1\":{\"407\":1}}],[\"travel\",{\"1\":{\"195\":1}}],[\"tram\",{\"1\":{\"297\":1}}],[\"tran\",{\"1\":{\"297\":1,\"435\":1}}],[\"tranberg\",{\"1\":{\"296\":1}}],[\"transmute\",{\"1\":{\"362\":1}}],[\"transmits\",{\"1\":{\"298\":1}}],[\"transmitted\",{\"1\":{\"298\":1}}],[\"transmitting\",{\"1\":{\"284\":1}}],[\"transmission\",{\"1\":{\"112\":1,\"306\":1}}],[\"transitions\",{\"1\":{\"301\":1,\"417\":2}}],[\"transition\",{\"1\":{\"269\":1,\"288\":1}}],[\"transferring\",{\"1\":{\"449\":1}}],[\"transferred\",{\"1\":{\"194\":1}}],[\"transfers\",{\"1\":{\"313\":1,\"349\":1}}],[\"transferable\",{\"0\":{\"313\":1},\"1\":{\"313\":1}}],[\"transfer\",{\"0\":{\"190\":1,\"255\":1},\"1\":{\"190\":2,\"255\":3,\"409\":1}}],[\"transformed\",{\"1\":{\"243\":1,\"266\":1,\"485\":1}}],[\"transformers\",{\"0\":{\"194\":1},\"1\":{\"137\":1,\"436\":1}}],[\"transformer\",{\"0\":{\"116\":1,\"137\":1},\"1\":{\"116\":1,\"123\":1,\"194\":1,\"213\":1,\"235\":1,\"325\":1,\"335\":1,\"427\":2,\"462\":1}}],[\"transforms\",{\"1\":{\"213\":1,\"253\":1}}],[\"transformative\",{\"0\":{\"227\":1},\"1\":{\"198\":1,\"239\":1,\"359\":1,\"387\":1,\"498\":1}}],[\"transformations\",{\"1\":{\"375\":2,\"434\":1}}],[\"transformation\",{\"1\":{\"141\":1,\"223\":1,\"235\":1,\"329\":1,\"434\":1,\"529\":1}}],[\"transforming\",{\"0\":{\"419\":1,\"420\":1},\"1\":{\"188\":1,\"212\":1,\"399\":1,\"439\":1}}],[\"transform\",{\"1\":{\"124\":1,\"127\":1,\"246\":1,\"386\":1,\"422\":1,\"438\":1}}],[\"transcription\",{\"1\":{\"163\":1}}],[\"translations\",{\"1\":{\"416\":2}}],[\"translation\",{\"0\":{\"365\":1,\"416\":1},\"1\":{\"199\":1,\"333\":1,\"349\":1,\"365\":4,\"416\":3,\"419\":1,\"420\":1,\"438\":2,\"460\":2,\"490\":2}}],[\"translating\",{\"0\":{\"490\":1},\"1\":{\"124\":1,\"252\":1,\"365\":1,\"393\":1,\"407\":1,\"416\":1,\"426\":1}}],[\"translate\",{\"1\":{\"199\":1,\"215\":1,\"284\":2,\"438\":1,\"443\":1,\"494\":1,\"539\":1}}],[\"translated\",{\"1\":{\"187\":1,\"333\":1,\"365\":1,\"407\":1}}],[\"translators\",{\"0\":{\"438\":1}}],[\"translator\",{\"1\":{\"141\":2,\"416\":1}}],[\"transparent\",{\"1\":{\"253\":1}}],[\"transparency\",{\"0\":{\"129\":1},\"1\":{\"288\":1,\"400\":1}}],[\"transport\",{\"1\":{\"221\":1,\"438\":1}}],[\"transportation\",{\"1\":{\"138\":5}}],[\"transactions\",{\"0\":{\"192\":1},\"1\":{\"192\":4}}],[\"transaction\",{\"1\":{\"107\":1,\"192\":2}}],[\"trautwein\",{\"1\":{\"273\":1}}],[\"trap\",{\"1\":{\"251\":2,\"423\":2}}],[\"trapped\",{\"1\":{\"112\":1}}],[\"trade\",{\"1\":{\"189\":1,\"226\":1,\"255\":1,\"465\":2,\"468\":1}}],[\"traditionally\",{\"1\":{\"241\":1}}],[\"traditional\",{\"1\":{\"98\":1,\"113\":1,\"137\":1,\"138\":1,\"152\":1,\"154\":1,\"165\":1,\"203\":1,\"206\":1,\"212\":1,\"234\":1,\"238\":1,\"244\":1,\"246\":1,\"251\":1,\"253\":1,\"259\":1,\"282\":2,\"283\":1,\"294\":1,\"300\":1,\"347\":1,\"355\":1,\"394\":1,\"407\":1,\"419\":1,\"420\":1,\"423\":1,\"430\":1,\"442\":1,\"459\":2,\"460\":2,\"470\":1,\"485\":2}}],[\"trajectory\",{\"1\":{\"182\":2,\"359\":1}}],[\"trajectories\",{\"1\":{\"140\":1,\"182\":3,\"239\":1,\"309\":1,\"411\":1,\"478\":1}}],[\"traffic\",{\"1\":{\"150\":4}}],[\"traits\",{\"0\":{\"461\":1},\"1\":{\"130\":1,\"132\":1,\"218\":2,\"330\":1,\"461\":7}}],[\"trait\",{\"0\":{\"122\":1}}],[\"trains\",{\"1\":{\"489\":1}}],[\"trainable\",{\"1\":{\"432\":1,\"479\":1}}],[\"trained\",{\"0\":{\"194\":1},\"1\":{\"105\":1,\"140\":1,\"190\":1,\"283\":1,\"284\":1,\"289\":1,\"315\":1,\"322\":3,\"327\":2,\"362\":1,\"372\":1,\"376\":1,\"381\":1,\"382\":1,\"389\":1,\"400\":1,\"409\":1,\"419\":2,\"420\":2,\"425\":1,\"427\":1,\"432\":1,\"438\":1,\"444\":1,\"449\":1,\"471\":1,\"479\":1,\"485\":1}}],[\"training\",{\"0\":{\"180\":1,\"259\":1,\"389\":1,\"442\":1,\"489\":1},\"1\":{\"99\":1,\"101\":1,\"121\":1,\"123\":3,\"130\":1,\"137\":1,\"140\":1,\"151\":2,\"159\":1,\"180\":2,\"182\":1,\"188\":1,\"194\":1,\"196\":1,\"224\":1,\"251\":1,\"255\":1,\"258\":2,\"259\":1,\"274\":1,\"313\":2,\"315\":1,\"322\":2,\"334\":2,\"347\":1,\"348\":2,\"349\":1,\"352\":1,\"355\":1,\"363\":1,\"374\":1,\"377\":3,\"389\":2,\"390\":1,\"396\":1,\"400\":1,\"402\":3,\"408\":1,\"419\":3,\"420\":3,\"423\":1,\"427\":2,\"433\":2,\"436\":3,\"442\":1,\"443\":1,\"449\":2,\"455\":3,\"460\":1,\"475\":1,\"478\":2,\"479\":2,\"480\":2,\"482\":2,\"485\":3,\"487\":2,\"490\":1,\"494\":1,\"496\":1}}],[\"train\",{\"0\":{\"140\":1},\"1\":{\"99\":1,\"116\":1,\"123\":1,\"140\":2,\"205\":1,\"315\":1,\"349\":1,\"362\":1,\"376\":1,\"381\":1,\"389\":1,\"450\":1}}],[\"tracing\",{\"1\":{\"396\":1}}],[\"traceability\",{\"1\":{\"396\":1}}],[\"trace\",{\"1\":{\"249\":1}}],[\"traces\",{\"0\":{\"475\":1},\"1\":{\"113\":1,\"341\":1,\"350\":1,\"475\":9}}],[\"tracks\",{\"1\":{\"325\":1,\"388\":1}}],[\"track\",{\"1\":{\"146\":1,\"293\":3,\"309\":1,\"338\":1,\"352\":1,\"388\":1,\"443\":2}}],[\"tracking\",{\"0\":{\"139\":1,\"152\":1,\"185\":1,\"203\":1,\"290\":1,\"293\":1},\"1\":{\"116\":3,\"126\":1,\"127\":3,\"139\":3,\"152\":1,\"185\":1,\"203\":4,\"290\":3,\"293\":7,\"475\":2}}],[\"tractable\",{\"1\":{\"104\":1}}],[\"tien\",{\"1\":{\"491\":1}}],[\"tier\",{\"1\":{\"249\":1}}],[\"tions\",{\"1\":{\"414\":2}}],[\"tilman\",{\"1\":{\"290\":1}}],[\"tianle\",{\"1\":{\"412\":1}}],[\"tianyu\",{\"1\":{\"377\":1}}],[\"tianyang\",{\"1\":{\"353\":1}}],[\"tianjun\",{\"1\":{\"326\":1}}],[\"tianshi\",{\"1\":{\"286\":1,\"474\":1}}],[\"tian\",{\"1\":{\"222\":1,\"403\":1}}],[\"timor\",{\"1\":{\"365\":1}}],[\"timothy\",{\"1\":{\"156\":1}}],[\"tim\",{\"1\":{\"157\":1,\"166\":1}}],[\"timely\",{\"1\":{\"416\":1,\"486\":1,\"498\":1}}],[\"timeline\",{\"1\":{\"182\":1}}],[\"times\",{\"1\":{\"366\":1,\"408\":1,\"479\":1}}],[\"timeslicing\",{\"0\":{\"143\":1},\"1\":{\"143\":2}}],[\"times$\",{\"1\":{\"131\":1,\"414\":2,\"427\":1,\"462\":2}}],[\"time\",{\"0\":{\"146\":1,\"170\":1,\"214\":1,\"223\":1,\"268\":1,\"279\":1,\"298\":1,\"452\":1},\"1\":{\"98\":1,\"99\":2,\"105\":1,\"120\":2,\"127\":1,\"128\":1,\"137\":1,\"140\":1,\"146\":1,\"152\":1,\"154\":1,\"158\":2,\"159\":1,\"170\":2,\"183\":2,\"188\":4,\"200\":1,\"214\":6,\"215\":1,\"223\":1,\"244\":1,\"249\":5,\"260\":1,\"264\":1,\"265\":1,\"268\":1,\"269\":1,\"272\":1,\"279\":2,\"283\":1,\"301\":2,\"305\":2,\"355\":2,\"356\":1,\"363\":1,\"395\":1,\"396\":1,\"419\":1,\"420\":1,\"428\":2,\"435\":1,\"436\":1,\"441\":2,\"443\":1,\"448\":1,\"452\":1,\"466\":1,\"469\":1,\"475\":1,\"479\":1,\"486\":2}}],[\"tiny\",{\"0\":{\"377\":1},\"1\":{\"377\":1}}],[\"tinghao\",{\"1\":{\"472\":1}}],[\"ting\",{\"1\":{\"185\":1,\"212\":1,\"342\":1,\"429\":1}}],[\"tin\",{\"1\":{\"155\":1}}],[\"tiktok\",{\"1\":{\"122\":1}}],[\"titus\",{\"1\":{\"110\":1,\"321\":1}}],[\"title\",{\"1\":{\"74\":1}}],[\"taghizadeh\",{\"1\":{\"492\":1}}],[\"tabular\",{\"0\":{\"480\":1},\"1\":{\"480\":2}}],[\"tablellm\",{\"0\":{\"480\":1},\"1\":{\"480\":3}}],[\"tables\",{\"1\":{\"252\":1,\"426\":1}}],[\"tableware\",{\"1\":{\"142\":1}}],[\"tabletop\",{\"1\":{\"140\":1,\"381\":1}}],[\"tablet\",{\"1\":{\"126\":2}}],[\"table\",{\"1\":{\"105\":1}}],[\"tai\",{\"1\":{\"394\":1}}],[\"tailors\",{\"1\":{\"359\":1}}],[\"tailored\",{\"0\":{\"341\":1},\"1\":{\"104\":1,\"151\":1,\"174\":1,\"180\":1,\"263\":1,\"314\":1,\"333\":1,\"336\":1,\"348\":1,\"353\":1,\"371\":1,\"379\":1,\"442\":1,\"480\":1}}],[\"taejin\",{\"1\":{\"477\":1}}],[\"tae\",{\"1\":{\"356\":1}}],[\"taelin\",{\"1\":{\"328\":1}}],[\"taesiri2022visual\",{\"1\":{\"159\":1}}],[\"taesiri\",{\"1\":{\"159\":1}}],[\"tayo\",{\"1\":{\"281\":1}}],[\"tahiya\",{\"1\":{\"272\":1}}],[\"tamim\",{\"1\":{\"298\":1}}],[\"tamil\",{\"1\":{\"108\":1}}],[\"tamoto\",{\"1\":{\"264\":1,\"448\":1}}],[\"tamara\",{\"1\":{\"249\":1,\"250\":1}}],[\"tau\",{\"1\":{\"227\":2}}],[\"taught\",{\"1\":{\"156\":1,\"307\":1}}],[\"taouk\",{\"1\":{\"336\":1}}],[\"tao\",{\"1\":{\"223\":1,\"314\":1,\"394\":1,\"402\":1,\"429\":1,\"462\":1,\"472\":1,\"495\":1}}],[\"tal\",{\"1\":{\"441\":1}}],[\"talie\",{\"1\":{\"268\":1,\"452\":1}}],[\"talwalkar\",{\"1\":{\"215\":1}}],[\"talaria\",{\"0\":{\"210\":1},\"1\":{\"210\":5}}],[\"talents\",{\"1\":{\"6\":1}}],[\"tarpit\",{\"1\":{\"257\":1}}],[\"tarpits\",{\"0\":{\"257\":1},\"1\":{\"257\":4}}],[\"tarassenko\",{\"1\":{\"194\":1}}],[\"targeting\",{\"1\":{\"339\":1}}],[\"targeted\",{\"0\":{\"468\":1,\"496\":1},\"1\":{\"191\":1,\"237\":1,\"302\":4,\"482\":1}}],[\"target\",{\"1\":{\"191\":1,\"209\":1,\"275\":1,\"292\":1,\"349\":1,\"360\":2,\"375\":1,\"382\":1,\"388\":1,\"409\":2,\"442\":1,\"444\":2}}],[\"targets\",{\"1\":{\"167\":1,\"405\":1,\"434\":1,\"467\":1}}],[\"tan\",{\"1\":{\"363\":1,\"376\":1,\"485\":1}}],[\"tanay\",{\"1\":{\"308\":1}}],[\"tang\",{\"1\":{\"318\":1,\"432\":1,\"480\":1}}],[\"tangible\",{\"1\":{\"291\":1,\"476\":1}}],[\"tangi\",{\"1\":{\"271\":1,\"457\":1}}],[\"tangermann\",{\"1\":{\"188\":1}}],[\"tanja\",{\"1\":{\"203\":1}}],[\"tanjila\",{\"1\":{\"154\":1}}],[\"tackling\",{\"1\":{\"477\":1}}],[\"tackle\",{\"1\":{\"167\":1,\"172\":1,\"234\":1,\"370\":1,\"396\":1,\"402\":1,\"415\":1}}],[\"tactic\",{\"1\":{\"322\":1}}],[\"tactics\",{\"1\":{\"322\":3,\"491\":1}}],[\"tactile\",{\"1\":{\"142\":1,\"207\":2}}],[\"taste\",{\"1\":{\"142\":5}}],[\"task1\",{\"1\":{\"458\":1}}],[\"task~3\",{\"1\":{\"351\":1}}],[\"tasked\",{\"1\":{\"245\":1}}],[\"tasks\",{\"0\":{\"100\":1,\"185\":1,\"240\":1,\"277\":1,\"317\":1,\"455\":1},\"1\":{\"97\":2,\"100\":1,\"116\":2,\"118\":1,\"119\":1,\"137\":1,\"140\":1,\"141\":3,\"148\":1,\"151\":4,\"159\":1,\"160\":1,\"161\":1,\"176\":1,\"185\":1,\"195\":1,\"222\":2,\"241\":1,\"244\":1,\"246\":1,\"269\":4,\"271\":1,\"275\":4,\"283\":2,\"290\":1,\"294\":1,\"300\":1,\"308\":1,\"312\":2,\"314\":2,\"316\":1,\"317\":2,\"318\":4,\"319\":4,\"323\":1,\"324\":1,\"325\":1,\"333\":1,\"335\":1,\"337\":1,\"338\":1,\"341\":1,\"345\":5,\"348\":4,\"353\":2,\"354\":1,\"359\":1,\"369\":1,\"372\":1,\"374\":1,\"376\":2,\"377\":1,\"378\":1,\"379\":4,\"384\":1,\"386\":1,\"391\":4,\"394\":1,\"396\":1,\"403\":2,\"405\":1,\"411\":4,\"412\":1,\"413\":1,\"417\":1,\"418\":1,\"425\":2,\"427\":1,\"432\":1,\"437\":1,\"441\":1,\"449\":1,\"450\":1,\"454\":1,\"455\":3,\"457\":1,\"458\":1,\"462\":1,\"466\":2,\"467\":1,\"471\":3,\"475\":4,\"480\":1,\"482\":1,\"483\":2,\"485\":1,\"486\":1,\"496\":2}}],[\"task\",{\"0\":{\"273\":1,\"304\":1,\"325\":1,\"351\":1,\"352\":1,\"372\":1,\"381\":1,\"388\":1,\"404\":1,\"439\":1,\"449\":1},\"1\":{\"96\":2,\"102\":2,\"106\":2,\"116\":1,\"118\":1,\"123\":2,\"125\":1,\"137\":1,\"140\":1,\"141\":4,\"146\":1,\"158\":1,\"160\":1,\"161\":1,\"169\":1,\"173\":1,\"176\":1,\"184\":1,\"185\":3,\"186\":1,\"188\":1,\"189\":2,\"191\":2,\"195\":1,\"222\":2,\"232\":2,\"244\":1,\"269\":4,\"270\":1,\"275\":1,\"283\":2,\"304\":1,\"314\":1,\"317\":1,\"318\":3,\"323\":1,\"324\":1,\"325\":3,\"336\":1,\"338\":2,\"340\":2,\"350\":1,\"352\":1,\"353\":1,\"354\":1,\"369\":2,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"380\":1,\"381\":6,\"384\":1,\"385\":1,\"386\":1,\"388\":3,\"394\":1,\"395\":2,\"404\":3,\"409\":1,\"412\":1,\"417\":3,\"418\":1,\"434\":1,\"435\":2,\"438\":1,\"439\":3,\"444\":1,\"449\":5,\"455\":2,\"471\":2,\"472\":3,\"488\":1,\"490\":2,\"495\":1,\"497\":5}}],[\"taxonomy\",{\"0\":{\"271\":1,\"457\":1},\"1\":{\"122\":1,\"271\":3,\"344\":2,\"437\":1,\"444\":1,\"457\":3,\"486\":1}}],[\"tak\",{\"1\":{\"461\":1}}],[\"takacs\",{\"1\":{\"537\":1}}],[\"takafumi\",{\"1\":{\"459\":1}}],[\"takato\",{\"1\":{\"165\":1}}],[\"taking\",{\"0\":{\"227\":1},\"1\":{\"123\":1,\"159\":1,\"227\":3,\"278\":1,\"366\":1,\"386\":1}}],[\"takeaways\",{\"1\":{\"254\":1}}],[\"takes\",{\"1\":{\"224\":1,\"240\":1,\"292\":1,\"422\":1}}],[\"taken\",{\"1\":{\"158\":1}}],[\"take\",{\"1\":{\"117\":1,\"178\":1,\"257\":1,\"265\":1,\"301\":1,\"402\":1}}],[\"takuji\",{\"1\":{\"108\":1,\"165\":1}}],[\"thuhcsi\",{\"1\":{\"235\":1}}],[\"thus\",{\"1\":{\"99\":1,\"112\":1,\"133\":1,\"163\":1,\"243\":1,\"264\":1,\"265\":1,\"272\":1,\"289\":1,\"315\":1,\"331\":1,\"362\":1,\"391\":1,\"448\":1,\"469\":1,\"482\":1,\"491\":1}}],[\"thread\",{\"1\":{\"519\":2}}],[\"threat\",{\"1\":{\"491\":1}}],[\"threats\",{\"0\":{\"453\":1}}],[\"threshold\",{\"1\":{\"298\":1}}],[\"thresholds\",{\"1\":{\"240\":2}}],[\"thresholding\",{\"1\":{\"240\":1}}],[\"three\",{\"1\":{\"105\":1,\"123\":1,\"132\":1,\"139\":1,\"140\":2,\"170\":1,\"171\":1,\"175\":1,\"176\":1,\"187\":1,\"199\":1,\"210\":1,\"218\":1,\"231\":1,\"244\":1,\"249\":1,\"250\":1,\"253\":1,\"261\":1,\"269\":1,\"272\":1,\"279\":1,\"281\":1,\"284\":1,\"293\":1,\"295\":1,\"296\":1,\"316\":1,\"317\":1,\"327\":1,\"329\":1,\"330\":1,\"336\":1,\"363\":1,\"366\":1,\"378\":1,\"382\":2,\"391\":1,\"396\":1,\"404\":1,\"442\":1,\"461\":1,\"481\":1,\"496\":1,\"497\":1}}],[\"thrive\",{\"1\":{\"180\":1}}],[\"throughput\",{\"1\":{\"366\":1,\"414\":1,\"465\":1}}],[\"throughout\",{\"1\":{\"254\":1,\"286\":1,\"474\":1}}],[\"through\",{\"0\":{\"123\":1,\"190\":1,\"226\":1,\"236\":1,\"239\":1,\"266\":1,\"291\":1,\"303\":1,\"306\":1,\"347\":1,\"356\":1,\"359\":1,\"365\":1,\"471\":1,\"476\":1,\"487\":1},\"1\":{\"103\":1,\"110\":1,\"115\":1,\"120\":1,\"124\":1,\"126\":1,\"132\":1,\"133\":1,\"136\":1,\"138\":1,\"142\":1,\"150\":2,\"157\":1,\"163\":1,\"170\":1,\"171\":1,\"173\":1,\"178\":1,\"181\":1,\"201\":1,\"203\":1,\"209\":1,\"211\":2,\"213\":2,\"215\":1,\"218\":2,\"224\":2,\"236\":1,\"238\":1,\"239\":1,\"246\":1,\"248\":2,\"249\":1,\"250\":1,\"252\":2,\"257\":1,\"271\":1,\"273\":1,\"286\":1,\"288\":1,\"289\":1,\"290\":1,\"293\":3,\"296\":2,\"297\":1,\"298\":2,\"300\":1,\"303\":1,\"309\":1,\"314\":2,\"318\":1,\"321\":1,\"322\":1,\"323\":1,\"330\":1,\"331\":1,\"334\":1,\"338\":1,\"339\":2,\"347\":1,\"351\":1,\"358\":1,\"364\":1,\"365\":1,\"368\":1,\"375\":1,\"377\":2,\"388\":1,\"396\":1,\"398\":2,\"399\":1,\"403\":2,\"404\":1,\"407\":1,\"411\":1,\"417\":1,\"426\":2,\"432\":1,\"442\":2,\"445\":2,\"457\":1,\"458\":2,\"459\":1,\"471\":1,\"474\":1,\"478\":1,\"479\":2,\"487\":1}}],[\"thoroughly\",{\"1\":{\"317\":1,\"396\":1}}],[\"thorough\",{\"1\":{\"280\":1,\"379\":1,\"470\":1,\"480\":1}}],[\"thousands\",{\"1\":{\"159\":1,\"410\":1,\"454\":1}}],[\"though\",{\"1\":{\"108\":1,\"226\":1,\"233\":1,\"282\":1,\"413\":1}}],[\"thoughts\",{\"1\":{\"404\":1}}],[\"thoughtful\",{\"0\":{\"236\":1}}],[\"thought\",{\"1\":{\"104\":1,\"125\":1,\"275\":1,\"381\":1,\"454\":1,\"470\":1}}],[\"thomas\",{\"1\":{\"149\":1,\"220\":1,\"232\":1,\"271\":1,\"335\":1,\"457\":1}}],[\"those\",{\"1\":{\"74\":1,\"100\":1,\"140\":1,\"141\":1,\"156\":1,\"160\":2,\"189\":1,\"204\":1,\"221\":1,\"256\":1,\"286\":1,\"313\":1,\"355\":1,\"385\":1,\"388\":1,\"445\":1,\"458\":1,\"469\":2,\"470\":1,\"474\":1}}],[\"thawani\",{\"1\":{\"178\":1}}],[\"thaqi\",{\"1\":{\"127\":1}}],[\"thanks\",{\"1\":{\"192\":1,\"258\":1,\"433\":1,\"462\":1}}],[\"than\",{\"0\":{\"302\":1},\"1\":{\"102\":1,\"103\":1,\"107\":1,\"108\":1,\"137\":1,\"151\":1,\"165\":1,\"197\":2,\"224\":1,\"229\":1,\"246\":3,\"249\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1,\"263\":1,\"283\":1,\"300\":1,\"302\":1,\"303\":1,\"306\":1,\"326\":1,\"334\":1,\"335\":1,\"348\":1,\"385\":2,\"388\":1,\"389\":1,\"391\":1,\"394\":1,\"405\":2,\"411\":1,\"423\":1,\"428\":1,\"441\":1,\"470\":2,\"485\":1,\"486\":3,\"494\":2}}],[\"that\",{\"0\":{\"236\":1,\"241\":1,\"418\":1},\"1\":{\"96\":7,\"97\":1,\"98\":1,\"99\":4,\"100\":2,\"101\":1,\"102\":1,\"103\":1,\"105\":1,\"107\":5,\"110\":3,\"111\":1,\"112\":1,\"113\":3,\"116\":2,\"117\":3,\"121\":2,\"122\":3,\"123\":2,\"124\":1,\"125\":2,\"127\":1,\"128\":2,\"129\":1,\"130\":1,\"131\":1,\"133\":4,\"135\":5,\"137\":1,\"139\":1,\"140\":1,\"142\":1,\"143\":2,\"144\":1,\"148\":2,\"149\":3,\"150\":2,\"151\":1,\"152\":2,\"153\":1,\"154\":3,\"155\":3,\"156\":2,\"157\":1,\"158\":4,\"159\":4,\"160\":1,\"161\":4,\"163\":3,\"164\":1,\"165\":1,\"166\":3,\"167\":1,\"169\":1,\"170\":1,\"172\":2,\"173\":1,\"175\":2,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":3,\"182\":1,\"183\":2,\"184\":1,\"185\":1,\"187\":2,\"188\":2,\"189\":2,\"190\":1,\"191\":2,\"194\":1,\"195\":4,\"196\":1,\"197\":3,\"198\":2,\"199\":1,\"200\":4,\"201\":2,\"202\":2,\"203\":1,\"204\":4,\"207\":4,\"209\":3,\"211\":1,\"213\":4,\"214\":2,\"215\":5,\"216\":1,\"218\":2,\"219\":1,\"220\":3,\"221\":2,\"222\":1,\"223\":1,\"224\":4,\"227\":2,\"228\":1,\"229\":1,\"230\":1,\"231\":2,\"232\":1,\"233\":4,\"234\":2,\"235\":1,\"236\":1,\"237\":1,\"238\":3,\"239\":2,\"240\":2,\"241\":5,\"242\":2,\"243\":2,\"244\":1,\"245\":1,\"249\":1,\"251\":3,\"252\":2,\"254\":1,\"255\":2,\"256\":3,\"257\":10,\"258\":1,\"259\":1,\"260\":1,\"261\":4,\"263\":2,\"264\":2,\"265\":6,\"266\":1,\"268\":3,\"269\":1,\"271\":1,\"272\":2,\"273\":1,\"274\":1,\"275\":3,\"278\":2,\"280\":1,\"282\":1,\"283\":1,\"284\":2,\"287\":1,\"289\":3,\"290\":1,\"291\":3,\"292\":4,\"293\":1,\"294\":2,\"295\":2,\"296\":1,\"298\":1,\"300\":1,\"301\":3,\"302\":1,\"303\":1,\"304\":1,\"306\":2,\"307\":2,\"308\":1,\"309\":2,\"312\":1,\"313\":1,\"314\":2,\"315\":2,\"317\":2,\"319\":1,\"321\":3,\"322\":6,\"323\":1,\"324\":3,\"325\":1,\"326\":2,\"328\":1,\"329\":2,\"331\":4,\"333\":3,\"334\":1,\"335\":4,\"336\":3,\"337\":1,\"338\":2,\"339\":1,\"340\":1,\"342\":2,\"344\":2,\"348\":1,\"349\":2,\"350\":5,\"352\":1,\"354\":4,\"356\":1,\"358\":1,\"359\":2,\"360\":2,\"361\":1,\"362\":2,\"363\":1,\"365\":1,\"366\":1,\"368\":3,\"369\":1,\"370\":2,\"374\":1,\"375\":3,\"376\":4,\"377\":1,\"378\":3,\"380\":1,\"381\":2,\"382\":4,\"384\":2,\"385\":1,\"386\":2,\"387\":2,\"388\":3,\"389\":6,\"390\":1,\"391\":1,\"392\":1,\"394\":1,\"395\":3,\"396\":2,\"398\":1,\"399\":5,\"400\":3,\"401\":1,\"402\":3,\"403\":3,\"404\":1,\"405\":1,\"409\":6,\"410\":4,\"411\":1,\"412\":2,\"413\":1,\"414\":1,\"415\":2,\"416\":3,\"417\":3,\"418\":2,\"419\":1,\"420\":1,\"422\":10,\"423\":3,\"424\":1,\"425\":5,\"426\":2,\"427\":2,\"428\":3,\"429\":2,\"431\":1,\"432\":1,\"433\":1,\"434\":4,\"435\":3,\"436\":3,\"437\":1,\"438\":2,\"439\":2,\"441\":2,\"442\":2,\"443\":1,\"444\":2,\"445\":1,\"447\":1,\"448\":2,\"449\":2,\"450\":1,\"452\":3,\"453\":1,\"454\":1,\"455\":3,\"456\":1,\"457\":1,\"458\":4,\"459\":1,\"460\":1,\"461\":2,\"462\":1,\"463\":2,\"465\":1,\"466\":3,\"467\":4,\"468\":2,\"469\":2,\"470\":1,\"471\":2,\"472\":1,\"475\":2,\"476\":3,\"478\":1,\"479\":3,\"481\":1,\"482\":1,\"483\":1,\"485\":3,\"486\":1,\"487\":2,\"488\":1,\"490\":3,\"494\":2,\"495\":1,\"496\":2,\"497\":3,\"498\":1}}],[\"thi\",{\"1\":{\"297\":1}}],[\"thickness\",{\"1\":{\"295\":1}}],[\"thierry\",{\"1\":{\"232\":1}}],[\"things\",{\"1\":{\"239\":1}}],[\"thin\",{\"1\":{\"113\":1}}],[\"thinkers\",{\"0\":{\"404\":1},\"1\":{\"175\":1}}],[\"thinking\",{\"0\":{\"122\":1,\"175\":1,\"292\":1},\"1\":{\"125\":2,\"175\":8,\"292\":11,\"340\":1,\"404\":5,\"519\":1,\"524\":1,\"533\":1}}],[\"think\",{\"1\":{\"98\":1,\"106\":1,\"209\":1,\"228\":1,\"265\":1}}],[\"this\",{\"0\":{\"122\":1},\"1\":{\"96\":1,\"97\":2,\"100\":2,\"101\":1,\"102\":2,\"103\":2,\"104\":1,\"106\":1,\"107\":5,\"110\":1,\"111\":1,\"112\":2,\"113\":3,\"114\":1,\"115\":2,\"116\":2,\"117\":1,\"118\":3,\"119\":1,\"120\":2,\"123\":2,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":3,\"130\":1,\"131\":1,\"132\":2,\"135\":1,\"136\":2,\"137\":3,\"138\":1,\"139\":1,\"140\":1,\"143\":2,\"147\":1,\"148\":2,\"149\":1,\"150\":1,\"151\":1,\"153\":2,\"154\":2,\"157\":2,\"158\":3,\"159\":3,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":2,\"167\":4,\"169\":2,\"170\":1,\"171\":1,\"172\":4,\"173\":2,\"174\":3,\"175\":1,\"176\":4,\"179\":3,\"180\":2,\"181\":1,\"182\":2,\"183\":1,\"184\":1,\"186\":2,\"187\":1,\"188\":1,\"189\":2,\"190\":2,\"191\":1,\"192\":1,\"194\":2,\"196\":1,\"198\":2,\"199\":3,\"200\":1,\"201\":2,\"202\":1,\"203\":2,\"206\":1,\"207\":4,\"209\":1,\"211\":1,\"212\":1,\"213\":3,\"214\":1,\"216\":1,\"217\":2,\"218\":1,\"220\":1,\"221\":1,\"222\":1,\"224\":4,\"226\":1,\"229\":2,\"231\":3,\"232\":3,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":3,\"239\":4,\"240\":2,\"241\":2,\"242\":3,\"243\":1,\"244\":1,\"245\":1,\"246\":2,\"248\":1,\"250\":2,\"251\":4,\"252\":1,\"253\":2,\"254\":1,\"256\":1,\"257\":2,\"259\":3,\"260\":2,\"261\":1,\"263\":2,\"264\":2,\"265\":3,\"266\":5,\"268\":1,\"269\":4,\"270\":2,\"271\":3,\"272\":2,\"273\":1,\"274\":1,\"275\":1,\"277\":3,\"278\":3,\"279\":1,\"281\":4,\"282\":2,\"283\":1,\"284\":2,\"286\":2,\"287\":1,\"288\":1,\"289\":2,\"290\":2,\"291\":2,\"292\":4,\"293\":1,\"294\":3,\"296\":1,\"297\":4,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"304\":3,\"305\":2,\"306\":2,\"307\":2,\"308\":2,\"309\":1,\"312\":2,\"313\":1,\"314\":1,\"315\":4,\"316\":1,\"317\":2,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"325\":2,\"326\":4,\"328\":2,\"329\":2,\"330\":2,\"333\":1,\"334\":3,\"335\":4,\"336\":2,\"337\":1,\"339\":3,\"340\":3,\"341\":1,\"343\":1,\"344\":2,\"345\":1,\"347\":2,\"348\":1,\"349\":3,\"350\":2,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":3,\"358\":3,\"359\":3,\"360\":1,\"361\":1,\"362\":2,\"363\":5,\"364\":2,\"365\":2,\"366\":1,\"368\":1,\"369\":1,\"370\":4,\"371\":3,\"372\":1,\"375\":1,\"376\":1,\"377\":3,\"378\":2,\"379\":1,\"380\":1,\"381\":2,\"382\":3,\"384\":2,\"385\":1,\"386\":1,\"387\":2,\"388\":1,\"389\":5,\"390\":2,\"391\":1,\"392\":1,\"393\":2,\"394\":1,\"396\":1,\"398\":1,\"399\":2,\"400\":1,\"403\":2,\"404\":2,\"405\":1,\"407\":2,\"408\":2,\"409\":1,\"410\":1,\"411\":1,\"412\":2,\"413\":2,\"415\":2,\"416\":1,\"417\":1,\"418\":2,\"422\":3,\"423\":4,\"424\":2,\"425\":3,\"426\":1,\"427\":3,\"428\":2,\"429\":3,\"430\":1,\"431\":1,\"432\":1,\"434\":3,\"435\":2,\"438\":1,\"439\":3,\"441\":2,\"442\":1,\"443\":2,\"444\":2,\"445\":1,\"446\":1,\"448\":2,\"449\":1,\"450\":3,\"452\":1,\"453\":3,\"454\":3,\"455\":2,\"456\":1,\"457\":3,\"458\":2,\"459\":5,\"460\":1,\"462\":1,\"463\":2,\"465\":1,\"466\":1,\"469\":2,\"470\":2,\"472\":1,\"474\":2,\"475\":2,\"476\":2,\"477\":1,\"478\":2,\"479\":1,\"481\":3,\"482\":1,\"483\":2,\"485\":1,\"486\":2,\"487\":1,\"488\":2,\"489\":1,\"490\":3,\"491\":2,\"494\":2,\"495\":1,\"496\":1,\"497\":4,\"498\":3}}],[\"third\",{\"1\":{\"74\":1,\"128\":1,\"179\":1,\"270\":1,\"296\":1,\"430\":1}}],[\"thelxino\",{\"1\":{\"305\":1}}],[\"thelxinoë\",{\"0\":{\"305\":1}}],[\"therapies\",{\"1\":{\"120\":1}}],[\"therapeutic\",{\"1\":{\"120\":1}}],[\"therapy\",{\"1\":{\"120\":1,\"178\":1,\"220\":1,\"227\":2}}],[\"thereby\",{\"1\":{\"183\":1,\"195\":1,\"218\":1,\"222\":1,\"284\":1,\"303\":1,\"329\":3,\"347\":1,\"349\":1,\"362\":1,\"442\":1,\"458\":1,\"459\":1,\"460\":1,\"465\":1,\"467\":1,\"479\":1,\"489\":1}}],[\"there\",{\"1\":{\"98\":1,\"108\":1,\"119\":1,\"154\":1,\"161\":1,\"166\":1,\"172\":1,\"176\":1,\"212\":1,\"213\":1,\"223\":1,\"235\":1,\"240\":1,\"243\":1,\"255\":1,\"257\":1,\"261\":2,\"272\":1,\"291\":1,\"293\":1,\"308\":2,\"350\":1,\"354\":1,\"370\":1,\"379\":1,\"403\":1,\"417\":1,\"428\":1,\"434\":1,\"439\":1,\"443\":1,\"465\":1,\"472\":1,\"476\":1,\"482\":1}}],[\"therefore\",{\"1\":{\"97\":1,\"171\":1,\"172\":1,\"204\":1,\"206\":1,\"223\":1,\"245\":1,\"284\":1,\"308\":1,\"312\":1,\"324\":1,\"338\":1,\"350\":1,\"370\":1,\"432\":1,\"463\":1,\"491\":1}}],[\"theatre\",{\"1\":{\"117\":2}}],[\"theme\",{\"1\":{\"409\":1}}],[\"themes\",{\"1\":{\"229\":1,\"308\":1}}],[\"thematic\",{\"1\":{\"339\":1,\"437\":1}}],[\"themselves\",{\"1\":{\"133\":1,\"158\":1,\"207\":1,\"209\":1,\"293\":3,\"331\":1,\"494\":1}}],[\"them\",{\"1\":{\"102\":1,\"103\":1,\"107\":1,\"108\":1,\"111\":1,\"114\":1,\"122\":1,\"144\":1,\"148\":2,\"149\":1,\"156\":1,\"158\":1,\"159\":1,\"160\":1,\"195\":1,\"198\":1,\"207\":1,\"213\":1,\"219\":1,\"221\":1,\"226\":1,\"243\":1,\"249\":1,\"255\":1,\"257\":1,\"265\":1,\"266\":1,\"282\":1,\"284\":2,\"292\":1,\"303\":1,\"317\":1,\"326\":1,\"335\":2,\"359\":1,\"376\":1,\"377\":1,\"381\":1,\"382\":1,\"387\":1,\"395\":1,\"401\":1,\"408\":1,\"414\":1,\"432\":1,\"450\":1,\"463\":1,\"469\":1,\"470\":1,\"471\":1,\"479\":1,\"487\":1}}],[\"then\",{\"0\":{\"481\":1},\"1\":{\"99\":1,\"107\":1,\"108\":1,\"124\":1,\"143\":1,\"149\":1,\"159\":1,\"180\":1,\"181\":1,\"194\":1,\"229\":1,\"232\":1,\"235\":1,\"240\":1,\"242\":1,\"250\":1,\"257\":1,\"266\":1,\"270\":1,\"272\":1,\"284\":1,\"289\":1,\"317\":1,\"318\":1,\"335\":1,\"342\":1,\"355\":1,\"360\":1,\"363\":1,\"366\":1,\"369\":1,\"377\":1,\"385\":1,\"399\":1,\"402\":1,\"408\":1,\"409\":1,\"418\":1,\"435\":1,\"442\":1,\"444\":1,\"462\":1,\"467\":1,\"485\":1,\"487\":1,\"490\":1,\"492\":1,\"496\":1}}],[\"these\",{\"1\":{\"98\":1,\"99\":1,\"100\":1,\"101\":2,\"102\":1,\"103\":1,\"106\":1,\"110\":1,\"112\":1,\"117\":1,\"121\":3,\"122\":1,\"123\":1,\"125\":1,\"127\":1,\"135\":1,\"137\":2,\"141\":1,\"142\":1,\"143\":1,\"146\":1,\"150\":1,\"151\":2,\"154\":2,\"155\":2,\"158\":1,\"163\":1,\"164\":1,\"165\":2,\"167\":3,\"171\":1,\"176\":1,\"179\":2,\"185\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"196\":1,\"199\":2,\"204\":1,\"206\":1,\"207\":1,\"209\":2,\"213\":2,\"214\":1,\"217\":2,\"218\":2,\"219\":2,\"221\":1,\"226\":1,\"227\":1,\"229\":1,\"234\":2,\"235\":1,\"238\":2,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":2,\"248\":2,\"250\":1,\"253\":2,\"256\":1,\"257\":1,\"260\":2,\"261\":2,\"263\":1,\"265\":1,\"266\":2,\"269\":1,\"270\":2,\"272\":2,\"278\":1,\"279\":3,\"280\":1,\"286\":1,\"288\":1,\"290\":1,\"292\":2,\"297\":1,\"298\":2,\"301\":1,\"306\":1,\"308\":3,\"309\":2,\"315\":2,\"318\":1,\"319\":1,\"321\":1,\"328\":1,\"333\":1,\"334\":2,\"337\":2,\"338\":1,\"340\":1,\"348\":2,\"349\":1,\"350\":2,\"356\":3,\"360\":4,\"362\":1,\"363\":2,\"365\":1,\"374\":1,\"379\":1,\"382\":1,\"385\":1,\"394\":3,\"395\":1,\"396\":2,\"400\":1,\"401\":2,\"408\":2,\"409\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"424\":2,\"427\":1,\"430\":1,\"434\":1,\"438\":1,\"445\":2,\"447\":2,\"458\":1,\"463\":1,\"465\":5,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"471\":1,\"474\":1,\"477\":1,\"479\":1,\"481\":2,\"483\":1,\"489\":1,\"495\":1,\"498\":1}}],[\"they\",{\"0\":{\"107\":1,\"395\":1},\"1\":{\"96\":2,\"101\":3,\"102\":1,\"107\":3,\"110\":1,\"119\":1,\"122\":3,\"126\":1,\"133\":2,\"135\":1,\"138\":1,\"149\":2,\"156\":1,\"158\":1,\"160\":1,\"161\":1,\"164\":2,\"167\":1,\"170\":1,\"172\":1,\"175\":2,\"192\":1,\"197\":2,\"199\":1,\"200\":1,\"202\":1,\"209\":2,\"224\":1,\"231\":1,\"241\":1,\"248\":1,\"251\":1,\"265\":1,\"274\":1,\"275\":1,\"277\":1,\"282\":2,\"286\":1,\"307\":2,\"308\":1,\"309\":1,\"315\":2,\"321\":1,\"331\":2,\"334\":1,\"337\":1,\"340\":2,\"354\":1,\"355\":1,\"363\":2,\"370\":1,\"372\":1,\"386\":1,\"395\":3,\"410\":1,\"413\":2,\"417\":2,\"423\":1,\"427\":2,\"432\":1,\"446\":1,\"453\":2,\"463\":3,\"470\":1,\"474\":1,\"480\":1,\"483\":1,\"494\":2}}],[\"theoremqa\",{\"1\":{\"411\":1}}],[\"theorems\",{\"1\":{\"322\":2}}],[\"theorem\",{\"1\":{\"322\":2,\"494\":1}}],[\"theoretic\",{\"1\":{\"255\":1}}],[\"theoretical\",{\"0\":{\"292\":1},\"1\":{\"125\":1,\"153\":1,\"171\":1,\"172\":1,\"344\":1,\"370\":1,\"400\":1}}],[\"theoretically\",{\"1\":{\"96\":1,\"461\":1}}],[\"theorize\",{\"1\":{\"156\":1}}],[\"theory\",{\"0\":{\"187\":1},\"1\":{\"96\":1,\"122\":1,\"167\":1,\"189\":2,\"204\":1}}],[\"theodor\",{\"1\":{\"48\":1}}],[\"their\",{\"0\":{\"164\":1,\"214\":1},\"1\":{\"96\":1,\"98\":1,\"99\":2,\"101\":1,\"107\":1,\"110\":3,\"113\":1,\"114\":1,\"115\":1,\"117\":1,\"118\":1,\"121\":1,\"122\":2,\"124\":3,\"125\":1,\"126\":1,\"128\":1,\"132\":2,\"135\":2,\"136\":2,\"138\":2,\"140\":1,\"143\":1,\"144\":2,\"146\":2,\"148\":2,\"149\":3,\"151\":1,\"153\":2,\"156\":3,\"160\":4,\"161\":2,\"164\":2,\"166\":1,\"167\":1,\"170\":1,\"171\":2,\"172\":2,\"173\":3,\"175\":1,\"178\":3,\"179\":1,\"183\":1,\"187\":1,\"188\":1,\"191\":1,\"192\":2,\"195\":1,\"198\":1,\"199\":1,\"200\":2,\"203\":1,\"206\":3,\"207\":1,\"209\":2,\"210\":1,\"211\":2,\"212\":1,\"213\":1,\"214\":3,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"222\":1,\"223\":1,\"227\":2,\"233\":3,\"234\":2,\"238\":1,\"239\":1,\"242\":2,\"246\":1,\"248\":1,\"252\":3,\"254\":3,\"258\":2,\"259\":1,\"264\":1,\"265\":2,\"266\":1,\"272\":2,\"274\":2,\"275\":1,\"279\":2,\"282\":2,\"283\":1,\"284\":3,\"286\":1,\"288\":1,\"292\":1,\"300\":1,\"301\":2,\"303\":1,\"304\":1,\"307\":1,\"308\":5,\"313\":1,\"316\":1,\"321\":3,\"323\":1,\"326\":1,\"327\":1,\"328\":1,\"330\":2,\"333\":2,\"334\":1,\"339\":1,\"340\":1,\"341\":1,\"344\":1,\"345\":1,\"348\":1,\"354\":2,\"355\":1,\"356\":2,\"359\":1,\"368\":1,\"370\":2,\"372\":1,\"374\":1,\"376\":2,\"379\":1,\"381\":1,\"385\":1,\"386\":1,\"387\":1,\"390\":1,\"391\":1,\"394\":2,\"395\":1,\"398\":1,\"399\":1,\"404\":1,\"409\":1,\"411\":1,\"412\":3,\"413\":2,\"414\":1,\"417\":1,\"419\":1,\"420\":1,\"424\":1,\"425\":1,\"426\":3,\"427\":1,\"428\":1,\"429\":1,\"433\":2,\"435\":1,\"437\":2,\"443\":1,\"446\":1,\"448\":1,\"450\":2,\"453\":1,\"458\":1,\"461\":4,\"462\":1,\"463\":2,\"467\":1,\"468\":3,\"469\":1,\"470\":2,\"471\":1,\"472\":1,\"474\":1,\"478\":1,\"488\":1,\"489\":2,\"490\":1,\"491\":1,\"494\":1}}],[\"the\",{\"0\":{\"100\":2,\"102\":2,\"112\":1,\"116\":1,\"121\":1,\"122\":1,\"146\":1,\"169\":3,\"171\":1,\"191\":1,\"200\":1,\"212\":1,\"215\":1,\"216\":1,\"230\":2,\"236\":1,\"237\":1,\"239\":1,\"241\":1,\"245\":3,\"251\":2,\"265\":1,\"268\":1,\"273\":1,\"279\":1,\"282\":1,\"287\":1,\"289\":1,\"292\":1,\"293\":2,\"304\":1,\"315\":1,\"333\":1,\"342\":1,\"350\":1,\"352\":1,\"355\":1,\"364\":1,\"365\":1,\"369\":1,\"372\":1,\"390\":1,\"394\":1,\"398\":1,\"407\":1,\"408\":1,\"416\":1,\"418\":1,\"423\":2,\"428\":1,\"441\":2,\"443\":1,\"449\":1,\"452\":1,\"453\":1,\"461\":1,\"465\":1,\"466\":1,\"468\":1,\"471\":1,\"479\":1,\"488\":1,\"491\":1,\"492\":1,\"500\":1,\"519\":1},\"1\":{\"16\":1,\"74\":3,\"96\":17,\"97\":7,\"98\":3,\"99\":5,\"100\":8,\"101\":6,\"102\":7,\"103\":4,\"104\":3,\"105\":3,\"106\":6,\"107\":6,\"108\":10,\"110\":3,\"111\":5,\"112\":18,\"113\":4,\"114\":6,\"115\":3,\"116\":6,\"117\":7,\"118\":9,\"119\":9,\"120\":11,\"121\":3,\"122\":3,\"123\":17,\"124\":6,\"125\":6,\"126\":15,\"127\":10,\"128\":10,\"129\":2,\"130\":5,\"131\":3,\"132\":12,\"133\":7,\"135\":10,\"136\":3,\"137\":8,\"138\":2,\"139\":8,\"140\":4,\"141\":11,\"142\":6,\"143\":9,\"144\":3,\"146\":9,\"147\":1,\"148\":3,\"149\":10,\"150\":6,\"151\":6,\"152\":6,\"153\":18,\"154\":4,\"155\":7,\"156\":16,\"157\":8,\"158\":10,\"159\":11,\"160\":6,\"161\":9,\"163\":1,\"164\":6,\"165\":7,\"166\":7,\"167\":8,\"169\":9,\"170\":4,\"171\":14,\"172\":7,\"173\":9,\"174\":11,\"175\":5,\"176\":6,\"178\":3,\"179\":11,\"180\":1,\"181\":4,\"182\":5,\"183\":7,\"184\":2,\"185\":4,\"186\":14,\"187\":8,\"188\":7,\"189\":6,\"190\":13,\"191\":15,\"192\":7,\"194\":7,\"195\":3,\"196\":10,\"198\":7,\"199\":4,\"200\":8,\"201\":9,\"202\":6,\"203\":3,\"204\":11,\"205\":3,\"206\":2,\"207\":6,\"209\":6,\"210\":4,\"211\":5,\"212\":3,\"213\":7,\"214\":4,\"215\":6,\"216\":11,\"217\":10,\"218\":8,\"219\":10,\"220\":5,\"221\":4,\"222\":5,\"223\":8,\"224\":9,\"226\":3,\"227\":5,\"228\":2,\"229\":5,\"230\":9,\"231\":3,\"232\":14,\"233\":6,\"234\":17,\"235\":5,\"236\":6,\"237\":6,\"238\":5,\"239\":8,\"240\":11,\"241\":6,\"242\":6,\"243\":12,\"244\":12,\"245\":20,\"246\":11,\"248\":8,\"249\":2,\"250\":7,\"251\":14,\"252\":12,\"253\":13,\"254\":6,\"255\":13,\"256\":9,\"257\":11,\"258\":4,\"259\":12,\"260\":9,\"261\":7,\"263\":9,\"264\":6,\"265\":14,\"266\":5,\"268\":4,\"269\":10,\"270\":2,\"271\":7,\"272\":12,\"273\":4,\"274\":7,\"275\":5,\"277\":12,\"278\":7,\"279\":2,\"280\":4,\"281\":11,\"282\":19,\"283\":10,\"284\":7,\"286\":7,\"287\":4,\"288\":4,\"289\":18,\"290\":2,\"291\":2,\"292\":11,\"293\":6,\"294\":4,\"295\":4,\"296\":4,\"297\":6,\"298\":22,\"300\":3,\"301\":15,\"302\":2,\"303\":1,\"304\":18,\"305\":4,\"306\":9,\"307\":8,\"308\":11,\"309\":4,\"312\":7,\"313\":11,\"314\":10,\"315\":10,\"316\":4,\"317\":7,\"318\":9,\"319\":1,\"321\":3,\"322\":9,\"323\":8,\"324\":10,\"325\":5,\"326\":15,\"327\":2,\"328\":10,\"329\":12,\"330\":12,\"331\":7,\"333\":8,\"334\":13,\"335\":20,\"336\":9,\"337\":9,\"338\":6,\"339\":13,\"340\":11,\"341\":2,\"342\":6,\"343\":2,\"344\":10,\"345\":7,\"347\":4,\"348\":6,\"349\":4,\"350\":17,\"351\":5,\"352\":4,\"353\":12,\"354\":9,\"355\":14,\"356\":9,\"358\":9,\"359\":11,\"360\":9,\"361\":4,\"362\":10,\"363\":4,\"364\":4,\"365\":6,\"366\":14,\"368\":5,\"369\":18,\"370\":7,\"371\":11,\"372\":5,\"374\":2,\"375\":5,\"376\":2,\"377\":10,\"378\":13,\"379\":5,\"380\":2,\"381\":6,\"382\":6,\"384\":7,\"385\":4,\"386\":4,\"387\":7,\"388\":10,\"389\":7,\"390\":16,\"391\":3,\"392\":3,\"393\":9,\"394\":8,\"395\":5,\"396\":8,\"398\":16,\"399\":4,\"400\":7,\"401\":10,\"402\":11,\"403\":17,\"404\":3,\"405\":3,\"407\":5,\"408\":11,\"409\":12,\"410\":15,\"411\":4,\"412\":13,\"413\":5,\"414\":4,\"415\":10,\"416\":4,\"417\":9,\"418\":9,\"419\":3,\"420\":3,\"422\":12,\"423\":14,\"424\":10,\"425\":8,\"426\":12,\"427\":4,\"428\":9,\"429\":9,\"430\":5,\"431\":3,\"432\":9,\"433\":4,\"434\":7,\"435\":9,\"436\":2,\"437\":17,\"438\":8,\"439\":11,\"441\":8,\"442\":7,\"443\":9,\"444\":11,\"445\":16,\"446\":4,\"447\":8,\"448\":6,\"449\":5,\"450\":4,\"452\":4,\"453\":12,\"454\":10,\"455\":12,\"456\":10,\"457\":7,\"458\":13,\"459\":7,\"460\":9,\"461\":12,\"462\":7,\"463\":10,\"465\":16,\"466\":11,\"467\":11,\"468\":7,\"469\":30,\"470\":5,\"471\":11,\"472\":2,\"474\":7,\"475\":9,\"476\":2,\"477\":8,\"478\":4,\"479\":15,\"480\":5,\"481\":11,\"482\":11,\"483\":4,\"485\":13,\"486\":11,\"487\":9,\"488\":7,\"489\":8,\"490\":6,\"491\":7,\"492\":14,\"494\":5,\"495\":15,\"496\":9,\"497\":13,\"498\":4,\"502\":1,\"510\":1,\"533\":1}}],[\"tong\",{\"1\":{\"470\":1}}],[\"toni\",{\"1\":{\"158\":1}}],[\"torrellas\",{\"1\":{\"465\":1}}],[\"torsten\",{\"1\":{\"189\":1,\"456\":1}}],[\"tosin\",{\"1\":{\"369\":1}}],[\"toshihiro\",{\"1\":{\"341\":1}}],[\"tot\",{\"1\":{\"353\":1}}],[\"total\",{\"1\":{\"128\":1,\"149\":2,\"156\":1,\"194\":2,\"200\":1,\"250\":1,\"462\":1}}],[\"tofu\",{\"0\":{\"328\":1}}],[\"toy\",{\"1\":{\"301\":1}}],[\"toby\",{\"1\":{\"271\":1,\"286\":1,\"457\":1,\"474\":1}}],[\"tomography\",{\"1\":{\"238\":1}}],[\"tod\",{\"1\":{\"449\":2}}],[\"today\",{\"1\":{\"231\":1,\"326\":1}}],[\"todd\",{\"1\":{\"176\":1}}],[\"tolerant\",{\"1\":{\"378\":1}}],[\"tolerance\",{\"0\":{\"378\":1},\"1\":{\"195\":1,\"378\":1}}],[\"tolosana\",{\"1\":{\"126\":1}}],[\"together\",{\"1\":{\"170\":2,\"173\":1,\"240\":1,\"275\":1,\"335\":1,\"337\":1,\"338\":1,\"411\":1}}],[\"toes\",{\"0\":{\"170\":1}}],[\"top$\",{\"1\":{\"462\":1}}],[\"topology\",{\"1\":{\"462\":1}}],[\"topic\",{\"0\":{\"402\":1,\"410\":1},\"1\":{\"315\":1,\"402\":3,\"410\":2,\"431\":2}}],[\"topics\",{\"1\":{\"160\":1,\"179\":1,\"211\":5,\"217\":2,\"253\":1,\"307\":1,\"333\":1,\"410\":1,\"418\":1}}],[\"top\",{\"0\":{\"482\":2},\"1\":{\"151\":1,\"159\":1,\"252\":1,\"348\":1,\"369\":1,\"391\":1,\"398\":1,\"426\":1,\"439\":1,\"465\":1}}],[\"touchless\",{\"1\":{\"149\":1}}],[\"touch\",{\"1\":{\"126\":1,\"149\":1,\"207\":2,\"220\":1,\"305\":2}}],[\"toxicity\",{\"0\":{\"230\":1},\"1\":{\"230\":2}}],[\"toxic\",{\"0\":{\"122\":1},\"1\":{\"374\":1,\"483\":1}}],[\"towards\",{\"0\":{\"114\":1,\"166\":1,\"206\":1,\"253\":1,\"278\":1,\"300\":1,\"326\":1,\"396\":1,\"416\":1,\"439\":1,\"459\":1,\"465\":1,\"485\":1},\"1\":{\"114\":1,\"131\":1,\"164\":1,\"172\":1,\"178\":1,\"196\":1,\"217\":1,\"221\":1,\"269\":1,\"277\":1,\"278\":2,\"317\":1,\"326\":1,\"328\":1,\"334\":1,\"359\":1,\"370\":1,\"377\":1,\"379\":1,\"390\":1,\"395\":1,\"407\":1,\"412\":1,\"424\":2,\"434\":1,\"477\":1}}],[\"toward\",{\"0\":{\"211\":1,\"415\":1},\"1\":{\"107\":1,\"125\":1,\"158\":2,\"181\":1,\"257\":1,\"284\":1,\"303\":1,\"313\":1,\"424\":1}}],[\"too\",{\"0\":{\"107\":1},\"1\":{\"282\":1,\"350\":1}}],[\"toolchains\",{\"1\":{\"363\":1}}],[\"tooling\",{\"1\":{\"363\":1}}],[\"toolkit\",{\"1\":{\"117\":1,\"231\":1,\"361\":1}}],[\"tool\",{\"0\":{\"117\":1,\"146\":1,\"286\":1,\"474\":1},\"1\":{\"98\":1,\"99\":1,\"104\":2,\"111\":1,\"113\":1,\"114\":1,\"117\":1,\"119\":1,\"120\":1,\"124\":1,\"125\":1,\"126\":1,\"135\":5,\"146\":3,\"159\":1,\"167\":1,\"212\":1,\"258\":1,\"259\":1,\"260\":1,\"263\":2,\"290\":1,\"292\":1,\"315\":1,\"418\":1,\"428\":1,\"433\":1,\"434\":1,\"445\":2,\"467\":1}}],[\"tools\",{\"0\":{\"175\":1,\"229\":1,\"277\":1,\"308\":1},\"1\":{\"97\":5,\"105\":1,\"106\":1,\"124\":1,\"125\":1,\"128\":1,\"136\":2,\"153\":1,\"154\":1,\"171\":1,\"172\":1,\"175\":3,\"182\":1,\"189\":1,\"198\":1,\"199\":1,\"212\":1,\"214\":1,\"216\":1,\"217\":1,\"229\":7,\"231\":3,\"257\":4,\"259\":1,\"272\":1,\"302\":2,\"308\":2,\"312\":5,\"326\":1,\"361\":2,\"363\":2,\"370\":1,\"387\":1,\"408\":1,\"410\":1,\"471\":1,\"482\":1,\"483\":2}}],[\"tochilnikova\",{\"1\":{\"98\":1}}],[\"to\",{\"0\":{\"97\":1,\"99\":1,\"101\":1,\"107\":1,\"129\":1,\"136\":1,\"140\":1,\"156\":1,\"159\":2,\"160\":1,\"169\":1,\"171\":1,\"181\":1,\"187\":1,\"192\":1,\"201\":1,\"211\":1,\"215\":1,\"223\":1,\"226\":1,\"237\":1,\"240\":2,\"251\":1,\"268\":1,\"274\":1,\"296\":1,\"297\":1,\"298\":1,\"312\":1,\"315\":1,\"318\":1,\"319\":1,\"324\":1,\"333\":1,\"338\":1,\"341\":1,\"355\":1,\"360\":1,\"362\":2,\"400\":1,\"403\":1,\"417\":1,\"423\":1,\"431\":1,\"445\":1,\"452\":1,\"465\":1,\"468\":1,\"470\":1,\"489\":1,\"496\":1},\"1\":{\"74\":1,\"96\":5,\"97\":9,\"98\":3,\"99\":3,\"100\":2,\"101\":4,\"102\":4,\"103\":4,\"104\":2,\"105\":3,\"106\":2,\"107\":9,\"108\":5,\"110\":4,\"111\":6,\"112\":2,\"113\":4,\"114\":6,\"115\":1,\"116\":5,\"117\":4,\"118\":4,\"119\":4,\"120\":9,\"121\":5,\"122\":4,\"123\":7,\"124\":7,\"125\":6,\"126\":8,\"127\":3,\"128\":7,\"129\":1,\"130\":3,\"131\":5,\"132\":8,\"133\":6,\"135\":7,\"136\":1,\"137\":4,\"138\":2,\"139\":1,\"140\":6,\"141\":8,\"142\":5,\"143\":4,\"144\":3,\"146\":9,\"148\":3,\"149\":4,\"150\":3,\"151\":6,\"152\":4,\"153\":2,\"154\":3,\"155\":8,\"156\":9,\"157\":3,\"158\":6,\"159\":6,\"160\":2,\"161\":2,\"163\":4,\"164\":6,\"165\":6,\"166\":6,\"167\":3,\"169\":5,\"170\":3,\"171\":4,\"172\":7,\"173\":6,\"174\":5,\"175\":4,\"176\":6,\"178\":5,\"179\":8,\"180\":4,\"181\":5,\"182\":2,\"183\":2,\"184\":7,\"185\":2,\"186\":4,\"187\":3,\"188\":3,\"189\":1,\"190\":3,\"191\":7,\"192\":5,\"194\":5,\"195\":4,\"196\":4,\"197\":1,\"198\":5,\"199\":8,\"200\":4,\"201\":2,\"202\":2,\"203\":6,\"204\":7,\"205\":3,\"206\":3,\"207\":5,\"209\":5,\"210\":6,\"211\":6,\"212\":3,\"213\":10,\"214\":4,\"215\":11,\"216\":3,\"217\":2,\"218\":5,\"219\":9,\"220\":5,\"221\":1,\"222\":3,\"223\":7,\"224\":3,\"226\":6,\"227\":5,\"228\":4,\"229\":3,\"230\":4,\"231\":5,\"232\":5,\"233\":5,\"234\":8,\"235\":6,\"236\":6,\"237\":7,\"238\":6,\"239\":2,\"240\":5,\"241\":3,\"242\":5,\"243\":5,\"244\":4,\"245\":4,\"246\":6,\"248\":3,\"249\":3,\"250\":3,\"251\":6,\"252\":7,\"253\":4,\"254\":4,\"255\":6,\"256\":8,\"257\":10,\"258\":6,\"259\":1,\"260\":4,\"261\":4,\"263\":6,\"264\":4,\"265\":11,\"266\":6,\"268\":6,\"269\":5,\"270\":2,\"271\":2,\"272\":6,\"273\":2,\"274\":7,\"275\":6,\"277\":3,\"278\":4,\"279\":3,\"280\":4,\"281\":6,\"282\":11,\"283\":4,\"284\":10,\"286\":5,\"287\":1,\"288\":5,\"289\":6,\"290\":1,\"291\":3,\"292\":8,\"293\":6,\"294\":6,\"296\":4,\"297\":1,\"298\":6,\"300\":2,\"301\":4,\"302\":7,\"303\":8,\"304\":3,\"305\":5,\"306\":8,\"307\":2,\"308\":12,\"309\":4,\"312\":9,\"313\":7,\"314\":1,\"315\":6,\"316\":4,\"317\":4,\"318\":6,\"319\":1,\"321\":4,\"322\":2,\"323\":2,\"324\":3,\"326\":6,\"327\":3,\"328\":4,\"329\":7,\"330\":8,\"331\":6,\"333\":5,\"334\":10,\"335\":7,\"336\":2,\"337\":5,\"338\":6,\"339\":4,\"340\":10,\"341\":3,\"342\":4,\"343\":1,\"344\":8,\"345\":6,\"347\":1,\"348\":6,\"349\":7,\"350\":10,\"351\":2,\"352\":1,\"353\":4,\"354\":2,\"355\":10,\"356\":4,\"358\":4,\"359\":2,\"360\":10,\"361\":3,\"362\":3,\"363\":6,\"364\":4,\"365\":2,\"366\":6,\"368\":4,\"369\":4,\"370\":7,\"371\":5,\"372\":1,\"374\":5,\"375\":5,\"376\":6,\"377\":2,\"378\":9,\"379\":8,\"380\":7,\"381\":11,\"382\":10,\"384\":5,\"385\":3,\"386\":8,\"387\":5,\"388\":1,\"389\":7,\"390\":5,\"391\":5,\"392\":4,\"393\":3,\"394\":2,\"395\":7,\"396\":8,\"398\":2,\"399\":4,\"400\":2,\"401\":9,\"402\":3,\"403\":11,\"404\":1,\"405\":4,\"407\":3,\"408\":6,\"409\":11,\"410\":6,\"411\":5,\"412\":9,\"413\":4,\"414\":9,\"415\":8,\"416\":6,\"417\":6,\"418\":7,\"419\":3,\"420\":3,\"422\":5,\"423\":6,\"424\":1,\"425\":4,\"426\":7,\"427\":7,\"428\":5,\"429\":2,\"430\":4,\"431\":6,\"432\":4,\"433\":6,\"434\":11,\"435\":7,\"436\":3,\"437\":4,\"438\":5,\"439\":3,\"441\":2,\"442\":5,\"443\":4,\"444\":4,\"445\":11,\"446\":1,\"447\":6,\"448\":4,\"449\":4,\"450\":5,\"452\":6,\"453\":10,\"454\":6,\"455\":6,\"456\":5,\"457\":2,\"458\":3,\"459\":5,\"460\":3,\"462\":9,\"463\":3,\"465\":5,\"466\":8,\"467\":3,\"468\":6,\"469\":3,\"470\":7,\"471\":5,\"474\":5,\"475\":6,\"476\":3,\"477\":2,\"478\":4,\"479\":7,\"480\":5,\"481\":3,\"482\":12,\"483\":3,\"485\":6,\"486\":10,\"487\":4,\"488\":10,\"489\":6,\"490\":6,\"491\":5,\"492\":5,\"494\":3,\"495\":5,\"496\":4,\"497\":6,\"498\":4}}],[\"tokenizers\",{\"1\":{\"376\":1,\"389\":3}}],[\"tokens\",{\"0\":{\"362\":1,\"393\":1,\"487\":1},\"1\":{\"337\":2,\"362\":2,\"366\":2,\"376\":1,\"377\":4,\"409\":1,\"410\":2,\"412\":1,\"438\":3,\"485\":1,\"487\":5}}],[\"token\",{\"1\":{\"40\":1,\"104\":2,\"313\":1,\"361\":1,\"366\":1,\"409\":2,\"412\":1,\"439\":1,\"483\":1}}],[\"武汉第三届毕业生艺术博览会参展艺术家\",{\"1\":{\"9\":1}}],[\"5️⃣通过数据驱动设计\",{\"1\":{\"527\":1}}],[\"58\",{\"1\":{\"492\":1}}],[\"5x\",{\"1\":{\"487\":1}}],[\"5$\",{\"1\":{\"462\":1}}],[\"534\",{\"1\":{\"431\":1}}],[\"52\",{\"1\":{\"395\":1,\"488\":1}}],[\"5k\",{\"1\":{\"345\":1}}],[\"5mm\",{\"1\":{\"295\":1}}],[\"5w1h\",{\"1\":{\"271\":1,\"457\":1}}],[\"54\",{\"1\":{\"250\":1,\"486\":1}}],[\"57\",{\"1\":{\"246\":1}}],[\"50k\",{\"1\":{\"412\":1}}],[\"501\",{\"1\":{\"219\":1,\"401\":1}}],[\"50\",{\"1\":{\"194\":1,\"260\":1,\"286\":1,\"327\":1,\"427\":1,\"474\":1,\"479\":1}}],[\"500多篇论文\",{\"1\":{\"79\":1}}],[\"500\",{\"1\":{\"49\":1,\"477\":1}}],[\"56\",{\"1\":{\"149\":1,\"190\":1,\"492\":1}}],[\"51\",{\"1\":{\"114\":1,\"135\":1,\"482\":1}}],[\"5\",{\"1\":{\"9\":2,\"74\":1,\"103\":1,\"156\":1,\"223\":1,\"246\":1,\"272\":1,\"283\":1,\"313\":1,\"319\":1,\"327\":1,\"334\":1,\"351\":1,\"372\":1,\"381\":2,\"386\":1,\"395\":2,\"404\":1,\"409\":1,\"411\":1,\"415\":1,\"422\":2,\"424\":5,\"428\":1,\"437\":1,\"447\":1,\"460\":1,\"461\":1,\"481\":1,\"486\":1,\"492\":2,\"495\":1,\"497\":2,\"505\":1}}],[\"艺术思维能够激发团队成员的创造力\",{\"1\":{\"533\":1}}],[\"艺术思维也可以帮助团队或组织应对未来的挑战\",{\"1\":{\"533\":1}}],[\"艺术思维可以通过引入新颖的视角和方法来激发创新\",{\"1\":{\"533\":1}}],[\"艺术思维是由\",{\"1\":{\"533\":1}}],[\"艺术与技术的融合\",{\"1\":{\"533\":1}}],[\"艺术与城市的关系正在增长\",{\"1\":{\"80\":1}}],[\"艺术的作用越来越重要\",{\"1\":{\"533\":1}}],[\"艺术的价值被重新赋予\",{\"1\":{\"33\":1}}],[\"艺术等其他学科相结合\",{\"1\":{\"510\":1}}],[\"艺术装置思考和择校思考\",{\"0\":{\"509\":1}}],[\"艺术装置和择校思考\",{\"1\":{\"503\":1}}],[\"艺术\",{\"1\":{\"77\":1,\"78\":1,\"499\":1}}],[\"艺术文化\",{\"1\":{\"30\":1}}],[\"艺术品投资人\",{\"1\":{\"28\":1}}],[\"艺术ip运营\",{\"0\":{\"22\":1}}],[\"艺术家等跨界合作\",{\"1\":{\"531\":1}}],[\"艺术家\",{\"1\":{\"80\":1}}],[\"艺术家成员\",{\"1\":{\"9\":1}}],[\"艺术家自传\",{\"0\":{\"9\":1}}],[\"艺人合作\",{\"1\":{\"6\":1}}],[\"cn\",{\"1\":{\"539\":1}}],[\"cnn\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"cnns\",{\"1\":{\"137\":2,\"246\":1}}],[\"cps\",{\"1\":{\"519\":1}}],[\"cphos\",{\"1\":{\"447\":3}}],[\"cpm\",{\"1\":{\"204\":2}}],[\"cărbune\",{\"1\":{\"435\":1}}],[\"cve\",{\"1\":{\"413\":1}}],[\"cwe\",{\"1\":{\"413\":4}}],[\"cbr\",{\"0\":{\"384\":1},\"1\":{\"384\":5}}],[\"cc\",{\"1\":{\"377\":1}}],[\"cci\",{\"1\":{\"126\":2}}],[\"ctr\",{\"0\":{\"479\":1},\"1\":{\"479\":7}}],[\"ctrs\",{\"1\":{\"372\":1}}],[\"ct\",{\"1\":{\"377\":4}}],[\"czerwinski\",{\"1\":{\"268\":1,\"452\":1}}],[\"css\",{\"1\":{\"241\":1}}],[\"cycle\",{\"1\":{\"361\":1,\"384\":1}}],[\"cyclical\",{\"1\":{\"314\":1}}],[\"cybercrime\",{\"0\":{\"416\":1},\"1\":{\"416\":2}}],[\"cybersecurity\",{\"1\":{\"416\":1}}],[\"cybersickness\",{\"0\":{\"304\":1},\"1\":{\"304\":3}}],[\"cyberstalking\",{\"1\":{\"302\":1}}],[\"cyberflashing\",{\"1\":{\"302\":1}}],[\"cyber\",{\"1\":{\"253\":1,\"519\":3}}],[\"cybernetics\",{\"1\":{\"223\":2}}],[\"cymatics\",{\"0\":{\"142\":2},\"1\":{\"142\":3}}],[\"cmaf\",{\"1\":{\"355\":1}}],[\"cmsc\",{\"1\":{\"307\":1}}],[\"cmcd\",{\"1\":{\"190\":1}}],[\"cmu\",{\"1\":{\"184\":1,\"380\":1}}],[\"cárdenas\",{\"1\":{\"147\":1}}],[\"cetera\",{\"1\":{\"462\":1}}],[\"cees\",{\"1\":{\"442\":1}}],[\"cer\",{\"1\":{\"419\":1,\"420\":1}}],[\"certain\",{\"1\":{\"112\":1,\"214\":1,\"235\":1,\"250\":1,\"360\":1,\"410\":1,\"413\":2,\"458\":1}}],[\"celia\",{\"1\":{\"291\":1,\"476\":1}}],[\"cell\",{\"1\":{\"213\":1}}],[\"censorship\",{\"1\":{\"241\":1}}],[\"censor\",{\"0\":{\"241\":1}}],[\"center\",{\"1\":{\"356\":1,\"465\":2}}],[\"centered\",{\"0\":{\"114\":1,\"144\":1,\"158\":1,\"206\":1,\"229\":1,\"300\":1,\"324\":2},\"1\":{\"114\":1,\"144\":2,\"176\":1,\"178\":1,\"206\":2,\"218\":1,\"229\":4,\"231\":2,\"300\":1,\"324\":2,\"328\":1}}],[\"central\",{\"1\":{\"293\":1,\"306\":1}}],[\"centres\",{\"1\":{\"282\":1}}],[\"centred\",{\"1\":{\"154\":1,\"166\":2}}],[\"centroids\",{\"0\":{\"281\":1},\"1\":{\"281\":2}}],[\"centric\",{\"0\":{\"272\":1,\"377\":1},\"1\":{\"110\":1,\"166\":1,\"184\":1,\"212\":1,\"215\":2,\"321\":1,\"380\":1}}],[\"cecile\",{\"1\":{\"210\":1,\"244\":1}}],[\"cesare\",{\"1\":{\"120\":1}}],[\"clue\",{\"0\":{\"379\":1},\"1\":{\"379\":4}}],[\"cluttered\",{\"1\":{\"301\":2}}],[\"clustering\",{\"1\":{\"146\":2,\"174\":1,\"368\":1,\"371\":1}}],[\"cluster\",{\"1\":{\"146\":1}}],[\"clusterradar\",{\"0\":{\"146\":1},\"1\":{\"146\":3}}],[\"clusters\",{\"0\":{\"146\":1},\"1\":{\"111\":1,\"146\":3}}],[\"cloud\",{\"1\":{\"210\":1,\"475\":2,\"519\":1}}],[\"closed\",{\"0\":{\"313\":1},\"1\":{\"313\":2,\"337\":1,\"363\":1,\"443\":1}}],[\"closely\",{\"1\":{\"251\":1,\"423\":1}}],[\"closer\",{\"1\":{\"173\":1,\"415\":1}}],[\"close\",{\"0\":{\"227\":1},\"1\":{\"105\":1,\"132\":1,\"227\":3,\"283\":1,\"330\":1,\"353\":1}}],[\"clear\",{\"1\":{\"221\":1,\"319\":1,\"443\":1}}],[\"clearly\",{\"1\":{\"174\":1,\"371\":1}}],[\"cleophas\",{\"1\":{\"183\":1}}],[\"clifford\",{\"1\":{\"294\":1}}],[\"clickdiffusion\",{\"0\":{\"375\":1},\"1\":{\"375\":2}}],[\"click\",{\"1\":{\"238\":1,\"479\":1}}],[\"climate\",{\"0\":{\"217\":1},\"1\":{\"217\":3,\"390\":1}}],[\"clip\",{\"1\":{\"213\":2,\"442\":1,\"444\":1}}],[\"clients\",{\"1\":{\"178\":4}}],[\"client\",{\"0\":{\"178\":1},\"1\":{\"178\":2}}],[\"clinic\",{\"1\":{\"261\":1}}],[\"clinician\",{\"1\":{\"121\":2}}],[\"clinicians\",{\"0\":{\"121\":1},\"1\":{\"121\":1,\"171\":1,\"309\":1}}],[\"clinically\",{\"1\":{\"309\":3}}],[\"clinical\",{\"0\":{\"323\":1,\"335\":1,\"356\":1,\"358\":1,\"372\":1,\"379\":1},\"1\":{\"120\":1,\"148\":1,\"194\":1,\"309\":4,\"323\":2,\"335\":3,\"356\":1,\"358\":3,\"372\":2,\"379\":7}}],[\"claude\",{\"1\":{\"291\":2,\"334\":1,\"345\":1,\"391\":1,\"409\":2,\"443\":1,\"476\":2,\"495\":1}}],[\"claudio\",{\"1\":{\"275\":1}}],[\"claims\",{\"1\":{\"390\":1}}],[\"claim\",{\"1\":{\"255\":1,\"337\":1,\"390\":1}}],[\"clarification\",{\"1\":{\"240\":1}}],[\"clarify\",{\"1\":{\"175\":1,\"243\":1,\"429\":1}}],[\"classes\",{\"1\":{\"199\":1,\"207\":1,\"355\":1,\"375\":1,\"412\":1}}],[\"classroom\",{\"0\":{\"132\":1,\"330\":1},\"1\":{\"132\":1,\"149\":1,\"308\":1,\"330\":1}}],[\"class\",{\"0\":{\"172\":1,\"281\":1,\"370\":1,\"444\":1},\"1\":{\"118\":1,\"194\":1,\"197\":1,\"199\":1,\"268\":1,\"281\":2,\"307\":1,\"340\":1,\"378\":1,\"405\":1,\"442\":3,\"444\":2,\"452\":1,\"459\":2}}],[\"classifiers\",{\"1\":{\"327\":1,\"388\":1,\"405\":1}}],[\"classifier\",{\"1\":{\"257\":1,\"405\":1,\"408\":1}}],[\"classification\",{\"0\":{\"159\":1,\"351\":1,\"355\":1,\"388\":1,\"405\":1,\"444\":1},\"1\":{\"104\":1,\"123\":2,\"159\":2,\"194\":1,\"231\":1,\"237\":1,\"257\":1,\"294\":1,\"351\":1,\"355\":4,\"388\":1,\"396\":1,\"402\":1,\"405\":2,\"412\":1,\"427\":1,\"439\":1,\"442\":1,\"443\":1,\"444\":2,\"460\":1,\"496\":2}}],[\"classifying\",{\"1\":{\"237\":1,\"396\":1,\"439\":1}}],[\"classical\",{\"1\":{\"112\":1,\"269\":1,\"326\":1}}],[\"classic\",{\"1\":{\"98\":1}}],[\"cryptographic\",{\"0\":{\"488\":1},\"1\":{\"488\":6}}],[\"craft\",{\"1\":{\"327\":1}}],[\"crafted\",{\"1\":{\"268\":1,\"452\":1,\"470\":1,\"480\":1,\"490\":1}}],[\"croci\",{\"1\":{\"456\":1}}],[\"croce\",{\"1\":{\"409\":1}}],[\"crowded\",{\"1\":{\"428\":1}}],[\"crowdsourced\",{\"1\":{\"197\":1,\"226\":1}}],[\"crowdsourcing\",{\"0\":{\"233\":1},\"1\":{\"196\":1,\"233\":1}}],[\"crowd\",{\"1\":{\"196\":1,\"388\":1}}],[\"cross\",{\"0\":{\"103\":1,\"163\":1,\"419\":2,\"420\":2},\"1\":{\"103\":1,\"137\":2,\"157\":1,\"163\":1,\"180\":1,\"190\":1,\"302\":1,\"355\":1,\"364\":1,\"398\":1,\"419\":1,\"420\":1,\"477\":1,\"480\":1}}],[\"crescendomation\",{\"1\":{\"418\":1}}],[\"crescendo\",{\"0\":{\"418\":1},\"1\":{\"418\":5}}],[\"credible\",{\"1\":{\"245\":1}}],[\"credibility\",{\"0\":{\"329\":1},\"1\":{\"155\":1,\"246\":1,\"329\":5}}],[\"creator\",{\"0\":{\"200\":1},\"1\":{\"200\":3}}],[\"creators\",{\"0\":{\"200\":1},\"1\":{\"122\":3,\"200\":1,\"224\":1}}],[\"creates\",{\"1\":{\"110\":1,\"321\":1,\"340\":1,\"353\":1}}],[\"create\",{\"0\":{\"99\":1},\"1\":{\"119\":1,\"133\":1,\"138\":1,\"156\":1,\"191\":1,\"192\":1,\"210\":1,\"252\":1,\"265\":1,\"268\":1,\"282\":1,\"331\":1,\"334\":2,\"336\":1,\"404\":1,\"405\":1,\"426\":1,\"452\":1,\"488\":2}}],[\"created\",{\"1\":{\"98\":1,\"101\":1,\"105\":1,\"190\":1,\"250\":1,\"251\":1,\"265\":1,\"269\":1,\"278\":1,\"423\":1,\"482\":1,\"491\":1}}],[\"creativity\",{\"0\":{\"174\":1,\"371\":1},\"1\":{\"133\":2,\"271\":1,\"331\":2,\"457\":1,\"524\":1}}],[\"creative\",{\"0\":{\"98\":1},\"1\":{\"98\":4,\"114\":1,\"122\":1,\"212\":2,\"229\":1,\"233\":2,\"244\":1,\"289\":2,\"524\":1}}],[\"creating\",{\"0\":{\"287\":1},\"1\":{\"98\":1,\"113\":1,\"122\":1,\"133\":1,\"144\":1,\"218\":1,\"229\":1,\"244\":1,\"265\":1,\"331\":1,\"339\":1,\"356\":1,\"470\":1,\"533\":1}}],[\"creation\",{\"1\":{\"98\":2,\"99\":2,\"133\":1,\"163\":2,\"167\":1,\"205\":1,\"212\":2,\"222\":1,\"243\":2,\"282\":1,\"331\":1,\"345\":1,\"356\":1,\"382\":1,\"398\":1,\"491\":1}}],[\"cruz\",{\"1\":{\"147\":1,\"220\":1}}],[\"crucially\",{\"1\":{\"340\":1}}],[\"crucial\",{\"1\":{\"135\":1,\"148\":1,\"160\":1,\"183\":1,\"190\":1,\"226\":1,\"235\":1,\"238\":1,\"241\":1,\"245\":1,\"248\":1,\"265\":1,\"309\":1,\"356\":1,\"359\":1,\"391\":1,\"400\":1,\"407\":1,\"409\":2,\"425\":1,\"429\":1,\"430\":2,\"453\":1}}],[\"crisis\",{\"1\":{\"217\":2}}],[\"cristina\",{\"1\":{\"149\":1,\"234\":1}}],[\"cristofolini\",{\"1\":{\"120\":1}}],[\"crimes\",{\"1\":{\"181\":1}}],[\"critique\",{\"1\":{\"411\":1}}],[\"criticalities\",{\"1\":{\"149\":1}}],[\"critically\",{\"1\":{\"114\":1,\"117\":1,\"136\":1}}],[\"critical\",{\"0\":{\"125\":1,\"175\":1,\"186\":1,\"446\":1},\"1\":{\"105\":1,\"125\":3,\"161\":1,\"175\":7,\"179\":1,\"186\":3,\"201\":1,\"233\":1,\"243\":1,\"245\":1,\"257\":1,\"260\":1,\"297\":1,\"326\":1,\"329\":1,\"344\":2,\"350\":1,\"354\":1,\"360\":1,\"366\":1,\"392\":1,\"394\":1,\"396\":1,\"427\":1,\"429\":1,\"443\":2,\"459\":1,\"461\":1,\"469\":1,\"479\":1}}],[\"criteria\",{\"1\":{\"160\":1,\"179\":1,\"236\":1,\"353\":2}}],[\"cuofano提出了商业工程师这一角色\",{\"1\":{\"525\":1}}],[\"custodian\",{\"1\":{\"495\":1}}],[\"custom\",{\"1\":{\"306\":1}}],[\"customers\",{\"1\":{\"232\":1}}],[\"customer\",{\"0\":{\"447\":2},\"1\":{\"222\":1,\"232\":2,\"447\":6}}],[\"customizable\",{\"1\":{\"287\":1}}],[\"customization\",{\"1\":{\"99\":1,\"358\":1}}],[\"customizing\",{\"1\":{\"228\":1}}],[\"customized\",{\"1\":{\"127\":1,\"207\":1,\"329\":1,\"495\":1}}],[\"cuan\",{\"1\":{\"269\":1}}],[\"cue\",{\"1\":{\"264\":1,\"448\":1}}],[\"cues\",{\"1\":{\"108\":2,\"123\":1,\"124\":1}}],[\"cui\",{\"1\":{\"227\":1,\"411\":1,\"491\":1}}],[\"cumulative\",{\"1\":{\"220\":1}}],[\"cumbersome\",{\"1\":{\"194\":1,\"246\":1}}],[\"cut\",{\"1\":{\"221\":1}}],[\"cutting\",{\"1\":{\"213\":1,\"266\":1,\"307\":2}}],[\"cuts\",{\"1\":{\"164\":1}}],[\"culminating\",{\"1\":{\"174\":1,\"184\":1,\"371\":1,\"380\":1}}],[\"cultivating\",{\"0\":{\"303\":1}}],[\"cultivate\",{\"1\":{\"100\":1,\"274\":1,\"303\":1}}],[\"culture\",{\"1\":{\"221\":2,\"524\":1}}],[\"culturally\",{\"1\":{\"328\":2}}],[\"culturalbench\",{\"1\":{\"133\":1,\"331\":1}}],[\"cultural\",{\"0\":{\"221\":1,\"328\":1},\"1\":{\"133\":4,\"221\":1,\"328\":9,\"331\":4,\"333\":1}}],[\"culturalteaming\",{\"0\":{\"133\":1,\"331\":1},\"1\":{\"133\":2,\"331\":2}}],[\"cub\",{\"1\":{\"159\":1}}],[\"curriculum\",{\"1\":{\"308\":1,\"403\":1}}],[\"currently\",{\"1\":{\"114\":1,\"163\":1,\"227\":1,\"236\":1,\"284\":1,\"315\":1,\"335\":1}}],[\"current\",{\"0\":{\"171\":1},\"1\":{\"100\":1,\"112\":1,\"114\":1,\"124\":1,\"133\":1,\"171\":1,\"174\":3,\"175\":1,\"211\":1,\"216\":1,\"217\":1,\"224\":1,\"229\":1,\"234\":1,\"240\":1,\"242\":1,\"261\":1,\"277\":1,\"282\":1,\"284\":1,\"306\":1,\"315\":1,\"322\":1,\"331\":1,\"337\":1,\"340\":1,\"343\":1,\"345\":1,\"371\":3,\"410\":2,\"412\":1,\"429\":1,\"447\":1,\"449\":1,\"458\":2,\"462\":1,\"463\":2,\"467\":1,\"479\":1,\"485\":1,\"490\":1,\"491\":2}}],[\"curation\",{\"1\":{\"398\":2}}],[\"curating\",{\"1\":{\"249\":1}}],[\"curate\",{\"1\":{\"322\":1,\"344\":1}}],[\"curated\",{\"1\":{\"151\":1,\"249\":1,\"333\":1,\"345\":1,\"348\":1,\"411\":1}}],[\"curves\",{\"1\":{\"309\":1}}],[\"curve\",{\"1\":{\"190\":1,\"259\":1}}],[\"curb\",{\"1\":{\"164\":1}}],[\"curiosity\",{\"0\":{\"155\":1},\"1\":{\"155\":2,\"175\":1}}],[\"cup\",{\"0\":{\"142\":1},\"1\":{\"284\":1}}],[\"chc\",{\"1\":{\"377\":2}}],[\"chlorella\",{\"1\":{\"207\":1}}],[\"chmcorrplusplus\",{\"1\":{\"159\":1}}],[\"chm\",{\"1\":{\"159\":4}}],[\"chu\",{\"1\":{\"478\":1}}],[\"chuanyi\",{\"1\":{\"342\":1}}],[\"chuang\",{\"1\":{\"103\":1}}],[\"chul\",{\"1\":{\"213\":1}}],[\"chunqiu\",{\"1\":{\"482\":1}}],[\"chunshan\",{\"1\":{\"478\":1}}],[\"chunks\",{\"1\":{\"369\":6}}],[\"chung\",{\"1\":{\"355\":1}}],[\"chunyang\",{\"1\":{\"219\":1,\"401\":1}}],[\"chunxu\",{\"1\":{\"176\":1}}],[\"chun\",{\"1\":{\"140\":1,\"222\":1,\"355\":1}}],[\"chrome\",{\"0\":{\"128\":1},\"1\":{\"128\":1}}],[\"christian\",{\"1\":{\"189\":1,\"494\":1}}],[\"christine\",{\"1\":{\"178\":1}}],[\"christoph\",{\"1\":{\"107\":1,\"233\":1,\"364\":1}}],[\"christopher\",{\"1\":{\"99\":1,\"170\":1,\"344\":1,\"398\":1,\"428\":2}}],[\"chris\",{\"1\":{\"119\":1,\"207\":1,\"213\":1}}],[\"choukse\",{\"1\":{\"465\":1}}],[\"choshen\",{\"1\":{\"454\":1}}],[\"chosen\",{\"1\":{\"174\":1,\"371\":1,\"377\":1,\"408\":1}}],[\"chops\",{\"0\":{\"447\":1},\"1\":{\"447\":2}}],[\"chong\",{\"1\":{\"436\":1}}],[\"chonghua\",{\"1\":{\"337\":1}}],[\"choo\",{\"1\":{\"271\":1,\"457\":1}}],[\"choosing\",{\"1\":{\"146\":1}}],[\"choose\",{\"1\":{\"124\":1,\"342\":1}}],[\"chowdhury\",{\"1\":{\"221\":1,\"272\":1}}],[\"choreographer\",{\"1\":{\"269\":2}}],[\"choreographed\",{\"1\":{\"182\":2}}],[\"choreographies\",{\"0\":{\"182\":1},\"1\":{\"182\":2}}],[\"choreovis\",{\"0\":{\"182\":1}}],[\"choe\",{\"1\":{\"156\":1}}],[\"choi\",{\"1\":{\"133\":1,\"200\":1,\"268\":1,\"287\":1,\"331\":1,\"356\":1,\"381\":2,\"452\":1}}],[\"choice\",{\"1\":{\"107\":1,\"233\":3,\"333\":1,\"466\":1,\"482\":1}}],[\"choices\",{\"0\":{\"214\":1},\"1\":{\"100\":1,\"178\":1,\"214\":1,\"250\":2}}],[\"chia\",{\"1\":{\"355\":1}}],[\"chih\",{\"1\":{\"355\":1}}],[\"chicago\",{\"1\":{\"307\":1}}],[\"chip\",{\"1\":{\"207\":1,\"462\":1,\"469\":1}}],[\"chinese\",{\"0\":{\"377\":2,\"472\":1},\"1\":{\"377\":9,\"472\":1}}],[\"chin\",{\"1\":{\"166\":1,\"197\":1}}],[\"chiu\",{\"1\":{\"133\":1,\"331\":1}}],[\"child\",{\"0\":{\"126\":2,\"144\":1},\"1\":{\"126\":2,\"144\":2,\"148\":1}}],[\"childcidblong\",{\"0\":{\"126\":1},\"1\":{\"126\":5}}],[\"children\",{\"1\":{\"105\":1,\"126\":5,\"144\":4,\"248\":2,\"261\":1}}],[\"chi\",{\"0\":{\"147\":1},\"1\":{\"108\":1,\"147\":1,\"303\":1,\"427\":1,\"492\":1}}],[\"chesky\",{\"1\":{\"533\":1}}],[\"chesler\",{\"1\":{\"220\":1}}],[\"chew\",{\"1\":{\"436\":1}}],[\"cheol\",{\"1\":{\"274\":1}}],[\"cheikh\",{\"1\":{\"232\":1}}],[\"chelsea\",{\"1\":{\"226\":1}}],[\"chevalier\",{\"1\":{\"217\":1}}],[\"checkpoints\",{\"1\":{\"478\":1}}],[\"checkpoint\",{\"0\":{\"478\":1},\"1\":{\"478\":1,\"480\":1}}],[\"checkedc\",{\"1\":{\"434\":1}}],[\"checking\",{\"1\":{\"336\":1,\"396\":1,\"477\":1}}],[\"checklist\",{\"1\":{\"336\":2}}],[\"checklists\",{\"1\":{\"105\":1}}],[\"check\",{\"1\":{\"172\":1,\"322\":1,\"370\":1,\"417\":2}}],[\"chetan\",{\"1\":{\"154\":1,\"339\":1,\"396\":1}}],[\"chenyang\",{\"1\":{\"322\":1}}],[\"chenshu\",{\"1\":{\"260\":1}}],[\"chen\",{\"1\":{\"124\":1,\"130\":1,\"135\":1,\"138\":1,\"142\":1,\"176\":1,\"185\":1,\"192\":2,\"213\":2,\"215\":2,\"218\":1,\"219\":2,\"223\":1,\"235\":1,\"236\":1,\"242\":1,\"251\":1,\"275\":1,\"291\":1,\"318\":2,\"322\":1,\"328\":1,\"337\":1,\"360\":2,\"361\":1,\"377\":1,\"378\":1,\"401\":2,\"405\":1,\"411\":1,\"412\":1,\"417\":1,\"423\":1,\"425\":1,\"427\":1,\"432\":1,\"443\":1,\"462\":1,\"476\":1,\"478\":1,\"488\":1,\"489\":1}}],[\"chengbo\",{\"1\":{\"487\":1}}],[\"chengcheng\",{\"1\":{\"472\":1}}],[\"chengwei\",{\"1\":{\"443\":1}}],[\"chenghua\",{\"1\":{\"432\":1}}],[\"chengyuan\",{\"1\":{\"425\":1}}],[\"chengqian\",{\"1\":{\"359\":1}}],[\"cheng2019explaining\",{\"1\":{\"159\":1}}],[\"cheng\",{\"1\":{\"105\":1,\"110\":1,\"140\":1,\"241\":1,\"321\":1,\"324\":1,\"361\":1,\"377\":1,\"453\":1,\"463\":1,\"488\":1,\"495\":1}}],[\"chemical\",{\"1\":{\"112\":1}}],[\"chameleons\",{\"1\":{\"453\":1}}],[\"chaojie\",{\"1\":{\"465\":1}}],[\"chao\",{\"1\":{\"378\":1,\"413\":1}}],[\"chaoqun\",{\"1\":{\"210\":1,\"316\":1}}],[\"chaudhury\",{\"1\":{\"266\":1}}],[\"chau\",{\"1\":{\"258\":1,\"375\":1,\"433\":1}}],[\"chakravarthy\",{\"1\":{\"258\":1,\"433\":1}}],[\"chakraborti\",{\"1\":{\"205\":1}}],[\"chase\",{\"1\":{\"226\":1,\"280\":1}}],[\"chats\",{\"1\":{\"416\":1}}],[\"chatbots\",{\"1\":{\"374\":1}}],[\"chatbot\",{\"0\":{\"282\":1},\"1\":{\"282\":3,\"486\":1}}],[\"chat\",{\"0\":{\"259\":1,\"447\":1},\"1\":{\"201\":1,\"215\":1,\"242\":1,\"252\":1,\"259\":1,\"313\":1,\"334\":1,\"409\":1,\"414\":1,\"418\":2,\"426\":1,\"447\":2,\"481\":1}}],[\"chatgpt\",{\"1\":{\"102\":1,\"138\":1,\"154\":3,\"216\":1,\"271\":1,\"308\":2,\"325\":1,\"336\":1,\"339\":3,\"340\":1,\"417\":1,\"418\":1,\"428\":1,\"457\":1,\"471\":1,\"488\":1,\"533\":1}}],[\"chalkias\",{\"1\":{\"192\":1}}],[\"challenging\",{\"0\":{\"133\":1,\"331\":1},\"1\":{\"100\":1,\"108\":1,\"118\":1,\"124\":1,\"130\":1,\"133\":1,\"188\":1,\"190\":1,\"250\":1,\"265\":1,\"273\":1,\"275\":1,\"290\":1,\"313\":1,\"324\":1,\"331\":1,\"333\":1,\"337\":2,\"369\":1,\"376\":1,\"391\":3,\"407\":1,\"411\":1,\"412\":1,\"438\":1,\"445\":1,\"466\":1,\"488\":1}}],[\"challenged\",{\"1\":{\"459\":1}}],[\"challenge\",{\"0\":{\"333\":1},\"1\":{\"100\":1,\"125\":1,\"175\":1,\"210\":1,\"222\":1,\"250\":1,\"259\":1,\"279\":2,\"281\":2,\"290\":1,\"333\":3,\"334\":1,\"340\":1,\"345\":1,\"353\":1,\"363\":2,\"396\":1,\"415\":1,\"424\":1,\"453\":1,\"458\":1,\"465\":1,\"477\":1,\"495\":1}}],[\"challenges\",{\"0\":{\"101\":1,\"212\":1,\"491\":1},\"1\":{\"97\":2,\"101\":1,\"103\":1,\"106\":1,\"110\":1,\"111\":1,\"116\":1,\"124\":2,\"127\":1,\"136\":3,\"138\":1,\"144\":1,\"146\":1,\"159\":1,\"164\":1,\"169\":1,\"170\":1,\"174\":1,\"176\":1,\"179\":1,\"181\":2,\"192\":2,\"199\":1,\"206\":2,\"207\":1,\"211\":1,\"212\":2,\"219\":1,\"235\":1,\"238\":1,\"239\":1,\"243\":1,\"254\":1,\"257\":1,\"261\":3,\"272\":1,\"273\":1,\"277\":1,\"279\":1,\"283\":1,\"288\":1,\"294\":1,\"306\":2,\"307\":1,\"312\":2,\"321\":1,\"323\":1,\"326\":1,\"333\":1,\"334\":1,\"339\":1,\"343\":1,\"345\":1,\"358\":1,\"359\":1,\"360\":2,\"364\":2,\"371\":1,\"377\":1,\"379\":1,\"401\":1,\"402\":1,\"408\":1,\"414\":1,\"424\":1,\"430\":2,\"432\":1,\"437\":1,\"443\":1,\"445\":2,\"460\":1,\"466\":1,\"478\":1,\"492\":1,\"495\":1,\"498\":2}}],[\"chair\",{\"1\":{\"304\":1}}],[\"chai\",{\"0\":{\"244\":1},\"1\":{\"244\":1}}],[\"chaitanya\",{\"1\":{\"172\":1,\"370\":1}}],[\"chains\",{\"1\":{\"353\":2,\"372\":1,\"381\":2,\"411\":1}}],[\"chain\",{\"1\":{\"104\":1,\"275\":1,\"286\":1,\"353\":1,\"381\":1,\"404\":1,\"454\":1,\"470\":1,\"474\":1}}],[\"charles\",{\"1\":{\"369\":1,\"494\":1}}],[\"charlotte\",{\"1\":{\"200\":1}}],[\"chart\",{\"0\":{\"163\":2},\"1\":{\"163\":5}}],[\"characters\",{\"1\":{\"132\":2,\"330\":2}}],[\"character\",{\"0\":{\"132\":1,\"330\":1},\"1\":{\"132\":2,\"218\":1,\"274\":2,\"330\":2,\"438\":2}}],[\"characterizations\",{\"1\":{\"496\":1}}],[\"characterization\",{\"1\":{\"269\":1}}],[\"characterizing\",{\"1\":{\"130\":1}}],[\"characterized\",{\"1\":{\"304\":1,\"316\":1,\"469\":1}}],[\"characterize\",{\"1\":{\"100\":1,\"175\":1,\"207\":1,\"465\":1}}],[\"characteristics\",{\"1\":{\"130\":1,\"132\":1,\"146\":1,\"163\":1,\"171\":1,\"216\":1,\"244\":2,\"245\":1,\"263\":2,\"278\":3,\"292\":1,\"330\":1,\"345\":1,\"414\":1,\"438\":1,\"442\":1,\"449\":1}}],[\"characteristic\",{\"1\":{\"107\":1,\"266\":1,\"278\":2}}],[\"channels\",{\"1\":{\"456\":1,\"469\":1}}],[\"channel\",{\"1\":{\"282\":1,\"469\":1}}],[\"chance\",{\"0\":{\"280\":1},\"1\":{\"350\":1}}],[\"chandrasekharan\",{\"1\":{\"200\":1}}],[\"chan\",{\"1\":{\"133\":1,\"331\":1}}],[\"changan\",{\"1\":{\"342\":1}}],[\"changed\",{\"1\":{\"253\":1,\"308\":1}}],[\"change\",{\"0\":{\"227\":1},\"1\":{\"216\":1,\"327\":1,\"375\":1,\"488\":1,\"495\":1}}],[\"changes\",{\"1\":{\"121\":1,\"142\":1,\"143\":1,\"159\":1,\"166\":1,\"170\":1,\"260\":1,\"290\":1,\"309\":1,\"336\":1,\"391\":1,\"461\":1,\"482\":2,\"488\":1}}],[\"changing\",{\"0\":{\"142\":1},\"1\":{\"142\":1,\"158\":1,\"303\":1,\"422\":1,\"456\":1,\"482\":1,\"487\":1}}],[\"chang\",{\"1\":{\"111\":1,\"140\":1,\"323\":1,\"491\":1}}],[\"cosine\",{\"1\":{\"460\":2}}],[\"costing\",{\"1\":{\"355\":1}}],[\"costly\",{\"1\":{\"284\":2,\"369\":1,\"408\":1,\"427\":1}}],[\"costs\",{\"1\":{\"103\":2,\"107\":4,\"138\":1,\"139\":1,\"408\":1,\"416\":1,\"435\":1,\"459\":1,\"470\":1,\"478\":1,\"490\":1}}],[\"cost\",{\"1\":{\"96\":2,\"136\":1,\"154\":1,\"165\":1,\"238\":1,\"284\":3,\"318\":2,\"366\":1,\"381\":1,\"408\":1,\"427\":1,\"436\":1,\"447\":1,\"459\":2,\"460\":1,\"465\":1,\"470\":1,\"478\":1,\"496\":1}}],[\"coerced\",{\"1\":{\"497\":1}}],[\"coefficient\",{\"1\":{\"467\":1}}],[\"coefficients\",{\"1\":{\"400\":1,\"422\":1}}],[\"coexist\",{\"1\":{\"230\":1}}],[\"coterrorset\",{\"1\":{\"470\":2}}],[\"cot\",{\"1\":{\"353\":1,\"381\":1,\"404\":1,\"454\":1,\"470\":1}}],[\"coffee\",{\"0\":{\"328\":1}}],[\"cohan\",{\"1\":{\"391\":1}}],[\"cohorts\",{\"1\":{\"290\":1}}],[\"coherence\",{\"1\":{\"301\":1,\"327\":1,\"491\":1}}],[\"coherent\",{\"0\":{\"327\":1},\"1\":{\"235\":1,\"356\":1}}],[\"cohen\",{\"1\":{\"194\":2,\"335\":1}}],[\"cognidot\",{\"0\":{\"295\":1},\"1\":{\"295\":2}}],[\"cognition\",{\"1\":{\"218\":1,\"227\":1,\"263\":1,\"264\":1,\"404\":1,\"448\":1,\"470\":1}}],[\"cognitive\",{\"0\":{\"185\":1,\"295\":1},\"1\":{\"120\":1,\"126\":3,\"130\":1,\"131\":1,\"169\":1,\"175\":1,\"185\":3,\"195\":1,\"216\":1,\"222\":1,\"223\":1,\"242\":1,\"251\":2,\"261\":1,\"263\":2,\"264\":1,\"295\":2,\"314\":1,\"364\":1,\"423\":2,\"429\":1,\"448\":1}}],[\"cogley\",{\"1\":{\"226\":1}}],[\"corner\",{\"1\":{\"338\":1}}],[\"corpora\",{\"0\":{\"327\":1},\"1\":{\"365\":1,\"377\":1,\"438\":1}}],[\"corpus\",{\"1\":{\"250\":2,\"315\":2,\"365\":2,\"377\":2,\"428\":1,\"494\":1}}],[\"core\",{\"1\":{\"260\":1,\"307\":1,\"316\":1,\"326\":1,\"329\":1,\"402\":1,\"487\":1}}],[\"cortex\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"corr++\",{\"1\":{\"159\":1}}],[\"corr\",{\"1\":{\"159\":3}}],[\"correctly\",{\"1\":{\"350\":1,\"431\":1}}],[\"correctness\",{\"0\":{\"483\":1},\"1\":{\"322\":1,\"326\":2,\"329\":1,\"339\":1,\"342\":1,\"363\":1,\"368\":1,\"391\":1,\"437\":1,\"489\":1}}],[\"correct\",{\"0\":{\"385\":1,\"466\":1},\"1\":{\"251\":1,\"275\":1,\"322\":1,\"362\":1,\"363\":1,\"395\":1,\"412\":1,\"423\":1,\"466\":2,\"470\":4,\"483\":2,\"494\":1}}],[\"corrected\",{\"1\":{\"112\":1}}],[\"correlating\",{\"1\":{\"467\":1}}],[\"correlation\",{\"1\":{\"235\":1,\"304\":2,\"327\":1,\"369\":2,\"400\":1,\"437\":1}}],[\"correlations\",{\"0\":{\"304\":1},\"1\":{\"120\":2,\"304\":1,\"432\":1}}],[\"correlate\",{\"1\":{\"215\":1,\"273\":1}}],[\"correlated\",{\"1\":{\"102\":1,\"368\":1}}],[\"correspondence\",{\"1\":{\"190\":1}}],[\"correspondences\",{\"1\":{\"159\":1}}],[\"correspondingly\",{\"1\":{\"479\":1}}],[\"corresponding\",{\"1\":{\"111\":1,\"222\":1,\"259\":1,\"266\":1,\"381\":1,\"396\":1,\"489\":1}}],[\"correspond\",{\"1\":{\"96\":1}}],[\"coding\",{\"0\":{\"482\":2},\"1\":{\"215\":2,\"256\":1,\"389\":1,\"482\":1,\"495\":1}}],[\"codebook\",{\"1\":{\"438\":1}}],[\"codebases\",{\"1\":{\"415\":1,\"434\":1}}],[\"codebase\",{\"1\":{\"170\":1,\"415\":1}}],[\"codellama\",{\"1\":{\"411\":1,\"487\":1}}],[\"codes\",{\"1\":{\"260\":1,\"369\":1}}],[\"code\",{\"0\":{\"110\":1,\"321\":1,\"341\":1,\"342\":1,\"415\":1,\"422\":1,\"437\":1,\"441\":1},\"1\":{\"104\":1,\"106\":1,\"110\":8,\"123\":1,\"128\":1,\"139\":1,\"159\":1,\"170\":1,\"172\":5,\"184\":1,\"215\":4,\"222\":1,\"228\":2,\"235\":1,\"242\":1,\"250\":1,\"257\":1,\"259\":3,\"290\":1,\"321\":8,\"323\":1,\"326\":2,\"337\":1,\"341\":1,\"342\":5,\"350\":1,\"361\":3,\"363\":1,\"366\":1,\"370\":5,\"375\":1,\"377\":1,\"380\":1,\"381\":1,\"388\":1,\"391\":1,\"393\":1,\"409\":1,\"411\":1,\"413\":1,\"415\":7,\"422\":9,\"427\":1,\"432\":1,\"434\":6,\"437\":8,\"441\":1,\"445\":1,\"447\":1,\"456\":1,\"458\":1,\"480\":1,\"482\":4,\"485\":1,\"488\":3,\"490\":1,\"494\":2,\"495\":3}}],[\"coders\",{\"1\":{\"97\":1,\"312\":1}}],[\"cobos\",{\"1\":{\"176\":1}}],[\"copies\",{\"1\":{\"490\":1}}],[\"coping\",{\"1\":{\"445\":1}}],[\"copilot\",{\"1\":{\"163\":1,\"308\":2}}],[\"copy\",{\"1\":{\"172\":1,\"215\":1,\"370\":1,\"539\":1}}],[\"covariates\",{\"1\":{\"183\":2}}],[\"covariate\",{\"1\":{\"183\":3}}],[\"covid\",{\"1\":{\"169\":1}}],[\"covers\",{\"1\":{\"379\":1}}],[\"coverage\",{\"1\":{\"257\":3,\"333\":1,\"360\":1,\"460\":1}}],[\"covering\",{\"1\":{\"179\":1,\"307\":1,\"329\":1,\"345\":1,\"411\":2,\"412\":1,\"413\":1}}],[\"cover\",{\"0\":{\"360\":1},\"1\":{\"163\":1,\"308\":1,\"337\":1,\"344\":1,\"360\":4}}],[\"cooperative\",{\"1\":{\"269\":1}}],[\"cooperate\",{\"1\":{\"186\":1}}],[\"coordinating\",{\"1\":{\"477\":1}}],[\"coordination\",{\"1\":{\"126\":1,\"275\":1,\"279\":1}}],[\"coordinate\",{\"1\":{\"319\":2}}],[\"coordinates\",{\"1\":{\"281\":2}}],[\"coordinated\",{\"1\":{\"182\":1,\"213\":1}}],[\"cooney\",{\"1\":{\"181\":1}}],[\"cookie\",{\"1\":{\"107\":2}}],[\"colbertv2\",{\"1\":{\"347\":1}}],[\"colin\",{\"1\":{\"268\":1,\"452\":1,\"461\":1}}],[\"colglazier\",{\"1\":{\"230\":1}}],[\"colossal\",{\"1\":{\"469\":1}}],[\"coloca\",{\"1\":{\"414\":1}}],[\"colocate\",{\"1\":{\"414\":2}}],[\"colormaps\",{\"1\":{\"197\":1}}],[\"color\",{\"0\":{\"197\":1},\"1\":{\"197\":2,\"295\":1}}],[\"colonial\",{\"1\":{\"117\":1}}],[\"collier\",{\"1\":{\"467\":1}}],[\"collin\",{\"1\":{\"236\":1}}],[\"college\",{\"0\":{\"237\":1},\"1\":{\"268\":2,\"274\":1,\"452\":2}}],[\"collen\",{\"1\":{\"232\":1}}],[\"collecting\",{\"0\":{\"294\":1},\"1\":{\"122\":1,\"355\":1,\"391\":1}}],[\"collection\",{\"0\":{\"253\":1},\"1\":{\"120\":1,\"128\":1,\"140\":1,\"253\":1,\"272\":1,\"296\":3,\"306\":1,\"333\":1,\"407\":1,\"491\":1}}],[\"collective\",{\"0\":{\"108\":1},\"1\":{\"108\":1,\"205\":1}}],[\"collect\",{\"1\":{\"118\":1,\"140\":1,\"245\":1,\"249\":1,\"431\":1}}],[\"collected\",{\"1\":{\"102\":2,\"140\":1,\"154\":1,\"172\":1,\"174\":1,\"185\":1,\"196\":1,\"204\":1,\"272\":1,\"294\":1,\"298\":1,\"308\":1,\"369\":1,\"370\":1,\"371\":1,\"447\":1,\"485\":1}}],[\"collaborates\",{\"1\":{\"449\":1}}],[\"collaborate\",{\"1\":{\"201\":1,\"275\":1,\"326\":1}}],[\"collaborated\",{\"1\":{\"106\":1}}],[\"collaborators\",{\"1\":{\"170\":2}}],[\"collaborating\",{\"1\":{\"100\":1,\"176\":1,\"415\":1}}],[\"collaborations\",{\"0\":{\"164\":1}}],[\"collaboration\",{\"0\":{\"170\":1,\"174\":1,\"244\":1,\"263\":1,\"371\":1},\"1\":{\"97\":3,\"100\":1,\"106\":1,\"120\":1,\"133\":1,\"154\":1,\"182\":1,\"186\":1,\"201\":1,\"206\":2,\"216\":1,\"240\":1,\"244\":1,\"263\":4,\"274\":1,\"275\":2,\"279\":3,\"300\":1,\"307\":1,\"312\":3,\"331\":1,\"428\":1,\"495\":1}}],[\"collaboratively\",{\"1\":{\"117\":1,\"252\":1,\"297\":1,\"426\":1,\"435\":1}}],[\"collaborative\",{\"0\":{\"97\":1,\"173\":1,\"201\":1,\"206\":1,\"231\":1,\"244\":1,\"275\":1,\"289\":1,\"312\":1},\"1\":{\"97\":1,\"98\":1,\"132\":1,\"170\":2,\"173\":2,\"206\":2,\"216\":1,\"231\":1,\"232\":1,\"244\":5,\"279\":1,\"289\":4,\"290\":1,\"297\":1,\"300\":2,\"307\":1,\"312\":1,\"330\":1,\"364\":1,\"477\":1}}],[\"coupled\",{\"1\":{\"284\":1,\"356\":1,\"362\":1}}],[\"coupling\",{\"1\":{\"229\":1}}],[\"courses\",{\"1\":{\"172\":3,\"291\":1,\"308\":1,\"370\":3,\"476\":1}}],[\"course\",{\"0\":{\"307\":1},\"1\":{\"153\":1,\"156\":2,\"172\":1,\"307\":5,\"308\":3,\"370\":1}}],[\"could\",{\"1\":{\"114\":1,\"126\":1,\"155\":1,\"163\":1,\"175\":1,\"181\":2,\"213\":1,\"216\":1,\"218\":2,\"250\":1,\"256\":1,\"265\":1,\"319\":1,\"366\":1,\"412\":1,\"435\":1,\"443\":1}}],[\"count\",{\"1\":{\"389\":1,\"454\":1}}],[\"countries\",{\"1\":{\"178\":2,\"254\":1}}],[\"counter\",{\"1\":{\"317\":1,\"360\":3,\"403\":1}}],[\"counterfactuals\",{\"1\":{\"288\":1}}],[\"counterfactual\",{\"0\":{\"288\":1},\"1\":{\"294\":1}}],[\"counterspeech\",{\"0\":{\"241\":1},\"1\":{\"241\":3}}],[\"counterpart\",{\"1\":{\"188\":1}}],[\"counterparts\",{\"1\":{\"108\":1}}],[\"counterexample\",{\"1\":{\"171\":1}}],[\"counting\",{\"1\":{\"96\":2,\"324\":1}}],[\"comes\",{\"1\":{\"363\":1,\"427\":1}}],[\"come\",{\"1\":{\"257\":1,\"286\":1,\"288\":1,\"424\":1,\"465\":1,\"474\":1}}],[\"comfortable\",{\"0\":{\"302\":1},\"1\":{\"300\":1,\"302\":1}}],[\"comfort\",{\"1\":{\"186\":1,\"302\":5}}],[\"com\",{\"1\":{\"123\":1,\"137\":1,\"184\":1,\"235\":1,\"258\":1,\"260\":1,\"316\":1,\"326\":1,\"337\":1,\"338\":1,\"366\":1,\"375\":1,\"379\":1,\"380\":1,\"381\":1,\"391\":1,\"398\":1,\"409\":1,\"427\":1,\"432\":1,\"433\":1,\"443\":1,\"456\":1,\"458\":1,\"482\":1,\"485\":1,\"487\":1,\"494\":1,\"539\":1}}],[\"commercial\",{\"1\":{\"269\":1,\"436\":1,\"471\":1}}],[\"commentary\",{\"1\":{\"155\":1}}],[\"commenting\",{\"1\":{\"155\":1}}],[\"comment\",{\"0\":{\"200\":1},\"1\":{\"122\":1,\"155\":4,\"200\":2}}],[\"comments\",{\"0\":{\"155\":1},\"1\":{\"122\":1,\"155\":3,\"200\":2,\"209\":1,\"228\":1,\"400\":1}}],[\"commands\",{\"1\":{\"184\":1,\"380\":1,\"381\":3}}],[\"command\",{\"1\":{\"163\":1,\"201\":1}}],[\"commonsense\",{\"1\":{\"404\":1}}],[\"commonly\",{\"1\":{\"155\":1,\"432\":1}}],[\"common\",{\"0\":{\"123\":1},\"1\":{\"135\":1,\"140\":1,\"144\":1,\"159\":1,\"164\":1,\"189\":1,\"209\":1,\"229\":1,\"251\":2,\"257\":2,\"283\":2,\"338\":1,\"375\":1,\"382\":1,\"390\":1,\"409\":1,\"413\":1,\"423\":2,\"430\":1,\"435\":1,\"441\":1,\"469\":1,\"477\":1}}],[\"communities\",{\"1\":{\"200\":1,\"205\":2,\"230\":4,\"431\":2}}],[\"community\",{\"1\":{\"97\":2,\"174\":1,\"205\":1,\"236\":3,\"243\":1,\"274\":1,\"277\":2,\"312\":2,\"337\":1,\"338\":1,\"344\":1,\"345\":1,\"371\":1,\"443\":1}}],[\"communicating\",{\"1\":{\"226\":2,\"240\":1}}],[\"communications\",{\"1\":{\"124\":1,\"416\":2}}],[\"communication\",{\"0\":{\"108\":1,\"124\":1,\"239\":1,\"278\":1,\"290\":1,\"297\":1},\"1\":{\"106\":1,\"108\":3,\"124\":3,\"138\":1,\"139\":2,\"140\":1,\"144\":1,\"180\":1,\"201\":2,\"217\":2,\"227\":2,\"230\":1,\"231\":1,\"239\":1,\"240\":1,\"266\":2,\"275\":1,\"278\":2,\"284\":1,\"290\":6,\"297\":1,\"339\":1,\"356\":1,\"399\":1}}],[\"communicate\",{\"0\":{\"240\":1},\"1\":{\"124\":1,\"169\":1,\"173\":1,\"180\":1,\"184\":1,\"232\":1,\"275\":1,\"380\":1}}],[\"combines\",{\"1\":{\"207\":1,\"244\":1,\"375\":1,\"432\":1}}],[\"combine\",{\"1\":{\"158\":1,\"207\":1,\"240\":1,\"355\":1}}],[\"combined\",{\"1\":{\"120\":1,\"234\":1,\"237\":1,\"333\":1,\"395\":1}}],[\"combining\",{\"0\":{\"240\":1,\"400\":1},\"1\":{\"137\":1,\"143\":1,\"240\":1,\"282\":2,\"290\":1,\"351\":1,\"352\":1,\"362\":1}}],[\"combinational\",{\"0\":{\"143\":1}}],[\"combinations\",{\"1\":{\"112\":1,\"350\":1}}],[\"combination\",{\"1\":{\"98\":1,\"150\":1,\"158\":1,\"246\":1,\"252\":1,\"316\":1,\"426\":1,\"439\":1,\"447\":1,\"492\":1}}],[\"compiling\",{\"1\":{\"315\":1}}],[\"compiled\",{\"1\":{\"250\":1}}],[\"compile\",{\"1\":{\"151\":1,\"210\":1,\"348\":1,\"363\":1,\"469\":1}}],[\"compel\",{\"1\":{\"291\":1,\"476\":1}}],[\"compelling\",{\"1\":{\"220\":1,\"269\":1}}],[\"compensating\",{\"1\":{\"279\":1}}],[\"competition\",{\"1\":{\"351\":1,\"388\":1,\"409\":1,\"439\":1}}],[\"competitively\",{\"1\":{\"344\":1}}],[\"competitive\",{\"1\":{\"183\":1,\"315\":2,\"364\":1,\"405\":2}}],[\"competing\",{\"1\":{\"286\":1,\"474\":1}}],[\"competencies\",{\"1\":{\"216\":1,\"292\":4}}],[\"compete\",{\"1\":{\"164\":1}}],[\"composed\",{\"1\":{\"425\":1}}],[\"composition\",{\"1\":{\"377\":1,\"398\":1,\"482\":1}}],[\"compositions\",{\"1\":{\"290\":1}}],[\"composability\",{\"1\":{\"316\":1}}],[\"compostable\",{\"1\":{\"207\":1}}],[\"componential\",{\"0\":{\"204\":1}}],[\"component\",{\"1\":{\"169\":1,\"204\":2,\"219\":1,\"229\":1,\"301\":1,\"382\":1,\"401\":1}}],[\"components\",{\"0\":{\"306\":1},\"1\":{\"119\":1,\"141\":1,\"153\":1,\"172\":1,\"183\":1,\"204\":4,\"219\":2,\"240\":1,\"256\":1,\"292\":1,\"298\":1,\"306\":5,\"316\":1,\"347\":1,\"353\":1,\"361\":3,\"370\":1,\"384\":1,\"393\":1,\"395\":1,\"401\":2,\"415\":1,\"456\":1}}],[\"compatibility\",{\"1\":{\"438\":1}}],[\"compatible\",{\"1\":{\"295\":1}}],[\"companion\",{\"0\":{\"300\":1},\"1\":{\"291\":1,\"300\":1,\"476\":1}}],[\"companions\",{\"0\":{\"291\":1,\"476\":1},\"1\":{\"291\":1,\"476\":1}}],[\"companionship\",{\"1\":{\"282\":1}}],[\"companies\",{\"1\":{\"164\":1,\"179\":3,\"279\":1,\"445\":1}}],[\"company\",{\"1\":{\"232\":1,\"279\":1}}],[\"compass\",{\"1\":{\"216\":1,\"337\":1}}],[\"compact\",{\"1\":{\"133\":1,\"213\":1,\"295\":1,\"331\":1}}],[\"comparatively\",{\"1\":{\"334\":1}}],[\"comparative\",{\"0\":{\"187\":1},\"1\":{\"195\":1,\"205\":1,\"207\":1,\"239\":1,\"260\":1,\"372\":1,\"460\":1}}],[\"comparable\",{\"1\":{\"123\":1,\"140\":1,\"195\":1,\"246\":1,\"283\":1,\"362\":1,\"381\":1,\"405\":1,\"410\":1,\"436\":1,\"485\":1}}],[\"comparing\",{\"0\":{\"335\":1},\"1\":{\"146\":1,\"161\":1,\"289\":1,\"325\":1,\"327\":1,\"354\":1,\"445\":1}}],[\"comparisons\",{\"1\":{\"163\":1,\"273\":1,\"378\":1,\"441\":1}}],[\"comparison\",{\"0\":{\"119\":1,\"139\":1,\"195\":1},\"1\":{\"103\":1,\"119\":3,\"139\":1,\"146\":1,\"154\":1,\"182\":1,\"186\":1,\"239\":1,\"249\":1,\"265\":1,\"353\":1,\"379\":1,\"384\":1,\"428\":1}}],[\"compares\",{\"1\":{\"291\":1,\"476\":1}}],[\"compare\",{\"1\":{\"126\":1,\"152\":1,\"154\":1,\"158\":1,\"163\":1,\"187\":1,\"258\":1,\"304\":1,\"322\":1,\"381\":1,\"385\":1,\"433\":1,\"495\":1,\"496\":1}}],[\"compared\",{\"1\":{\"102\":1,\"107\":1,\"119\":1,\"123\":1,\"165\":2,\"173\":1,\"176\":1,\"190\":1,\"203\":1,\"218\":1,\"220\":2,\"223\":1,\"227\":1,\"238\":2,\"246\":1,\"251\":1,\"256\":1,\"260\":1,\"302\":1,\"305\":1,\"347\":1,\"368\":1,\"382\":1,\"403\":1,\"408\":1,\"411\":1,\"415\":1,\"416\":1,\"423\":1,\"432\":1,\"434\":1,\"435\":1,\"449\":1,\"459\":1,\"463\":1,\"480\":1,\"482\":1,\"486\":1,\"488\":1}}],[\"computation\",{\"1\":{\"210\":1,\"390\":1,\"414\":1,\"462\":1}}],[\"computationally\",{\"1\":{\"188\":1,\"241\":1}}],[\"computational\",{\"0\":{\"110\":1,\"119\":1,\"170\":1,\"205\":1,\"283\":1,\"292\":1,\"321\":1},\"1\":{\"110\":2,\"118\":1,\"119\":3,\"139\":1,\"170\":2,\"205\":2,\"207\":1,\"231\":2,\"258\":1,\"283\":1,\"292\":11,\"321\":2,\"359\":2,\"379\":1,\"408\":1,\"417\":1,\"427\":1,\"433\":1,\"456\":1,\"478\":1,\"479\":2,\"494\":1}}],[\"computing\",{\"0\":{\"172\":1,\"308\":1,\"370\":1},\"1\":{\"119\":1,\"172\":4,\"253\":1,\"308\":3,\"370\":4}}],[\"compute\",{\"1\":{\"284\":2,\"338\":1,\"441\":2,\"465\":1,\"469\":3,\"479\":1}}],[\"computed\",{\"1\":{\"196\":1,\"376\":1,\"400\":1}}],[\"computerization\",{\"1\":{\"253\":1}}],[\"computer\",{\"0\":{\"126\":1,\"166\":1,\"290\":1},\"1\":{\"105\":2,\"126\":1,\"137\":1,\"142\":1,\"148\":1,\"153\":1,\"159\":1,\"165\":1,\"166\":1,\"181\":1,\"244\":1,\"246\":1,\"265\":1,\"284\":1,\"286\":1,\"290\":2,\"294\":1,\"308\":1,\"400\":1,\"428\":1,\"474\":1,\"492\":1}}],[\"computes\",{\"1\":{\"96\":1}}],[\"compromising\",{\"1\":{\"465\":1,\"469\":1,\"487\":1}}],[\"compromised\",{\"1\":{\"234\":1,\"355\":1}}],[\"comprise\",{\"1\":{\"141\":1}}],[\"comprises\",{\"1\":{\"126\":2,\"218\":1,\"298\":1,\"345\":1,\"438\":1,\"480\":1}}],[\"comprising\",{\"1\":{\"106\":1,\"333\":1,\"344\":1}}],[\"compressors\",{\"1\":{\"389\":1}}],[\"compression\",{\"0\":{\"359\":1},\"1\":{\"255\":2,\"359\":2,\"366\":1,\"389\":4,\"436\":1}}],[\"compress\",{\"1\":{\"255\":1,\"366\":1,\"389\":2}}],[\"compressed\",{\"0\":{\"389\":1},\"1\":{\"103\":1,\"255\":1,\"389\":4,\"404\":1}}],[\"compresses\",{\"1\":{\"103\":1}}],[\"comprehending\",{\"1\":{\"392\":1,\"429\":1}}],[\"comprehend\",{\"1\":{\"151\":1,\"213\":1,\"222\":1,\"266\":1,\"291\":1,\"348\":1,\"393\":1,\"412\":1,\"417\":1,\"475\":1,\"476\":1,\"481\":1}}],[\"comprehensible\",{\"1\":{\"187\":1}}],[\"comprehensibility\",{\"1\":{\"187\":1}}],[\"comprehension\",{\"0\":{\"127\":1,\"187\":1,\"340\":1},\"1\":{\"126\":1,\"127\":1,\"151\":1,\"187\":4,\"291\":1,\"326\":1,\"333\":2,\"340\":2,\"348\":1,\"359\":1,\"363\":1,\"424\":1,\"476\":1}}],[\"comprehensively\",{\"1\":{\"328\":1,\"344\":1}}],[\"comprehensiveness\",{\"1\":{\"316\":1}}],[\"comprehensive\",{\"0\":{\"205\":1,\"316\":1,\"343\":1,\"443\":1,\"460\":1},\"1\":{\"120\":1,\"126\":1,\"137\":1,\"139\":1,\"151\":1,\"169\":1,\"172\":1,\"174\":2,\"186\":1,\"246\":1,\"292\":1,\"315\":1,\"316\":1,\"329\":1,\"333\":2,\"345\":1,\"348\":1,\"358\":1,\"361\":1,\"370\":1,\"371\":2,\"398\":1,\"400\":1,\"411\":1,\"429\":1,\"437\":1,\"442\":2,\"455\":1,\"471\":1,\"477\":1,\"479\":1,\"482\":3,\"489\":1,\"492\":1}}],[\"compliat\",{\"1\":{\"396\":5}}],[\"compliant\",{\"0\":{\"396\":1}}],[\"compliance\",{\"1\":{\"212\":1,\"254\":1,\"396\":4}}],[\"complicated\",{\"1\":{\"223\":1,\"362\":1}}],[\"completing\",{\"1\":{\"195\":1}}],[\"completion\",{\"1\":{\"140\":1,\"152\":1,\"234\":1,\"283\":2,\"308\":1,\"362\":1,\"368\":1,\"374\":1}}],[\"completeness\",{\"1\":{\"339\":1,\"356\":1}}],[\"completed\",{\"1\":{\"226\":1,\"342\":1}}],[\"completely\",{\"1\":{\"209\":1,\"485\":1}}],[\"complete\",{\"1\":{\"184\":1,\"215\":1,\"275\":1,\"344\":1,\"363\":1,\"380\":1,\"482\":1}}],[\"complementing\",{\"1\":{\"339\":1}}],[\"complements\",{\"1\":{\"214\":1,\"261\":1,\"399\":1}}],[\"complement\",{\"1\":{\"172\":2,\"260\":1,\"294\":1,\"370\":2}}],[\"complementarity\",{\"0\":{\"171\":1},\"1\":{\"171\":1}}],[\"complementary\",{\"1\":{\"123\":1,\"137\":1,\"301\":1}}],[\"complexities\",{\"1\":{\"212\":1,\"281\":1,\"323\":1,\"333\":1,\"393\":1}}],[\"complexity\",{\"0\":{\"304\":1},\"1\":{\"106\":1,\"110\":1,\"113\":1,\"116\":1,\"123\":1,\"128\":1,\"146\":1,\"186\":1,\"204\":1,\"259\":1,\"300\":1,\"304\":2,\"321\":1,\"328\":1,\"379\":1,\"415\":1,\"424\":1,\"471\":1,\"479\":1,\"488\":1}}],[\"complex\",{\"0\":{\"294\":1},\"1\":{\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"104\":2,\"111\":1,\"123\":2,\"125\":1,\"127\":1,\"141\":1,\"143\":1,\"146\":1,\"163\":1,\"172\":1,\"179\":2,\"189\":1,\"201\":1,\"204\":2,\"212\":1,\"217\":1,\"235\":1,\"242\":1,\"257\":1,\"259\":1,\"271\":1,\"275\":1,\"278\":1,\"291\":1,\"304\":3,\"312\":1,\"316\":1,\"318\":1,\"353\":1,\"356\":1,\"360\":2,\"363\":2,\"364\":2,\"370\":1,\"375\":1,\"381\":1,\"382\":1,\"407\":1,\"411\":1,\"415\":1,\"434\":1,\"445\":1,\"457\":1,\"458\":1,\"471\":1,\"475\":2,\"476\":1,\"490\":1,\"491\":1,\"495\":1,\"498\":1}}],[\"comply\",{\"1\":{\"96\":2,\"498\":1}}],[\"connell\",{\"1\":{\"261\":1}}],[\"connects\",{\"1\":{\"281\":1}}],[\"connect\",{\"1\":{\"277\":1,\"293\":1}}],[\"connection\",{\"1\":{\"202\":1}}],[\"connecting\",{\"1\":{\"156\":1,\"301\":1}}],[\"connected\",{\"1\":{\"129\":1,\"249\":1}}],[\"convincing\",{\"1\":{\"258\":1,\"433\":1}}],[\"convolutional\",{\"1\":{\"137\":1,\"223\":1,\"246\":1}}],[\"convey\",{\"1\":{\"241\":1,\"463\":1}}],[\"convene\",{\"1\":{\"205\":1}}],[\"conventions\",{\"1\":{\"197\":1}}],[\"conventionally\",{\"1\":{\"165\":1}}],[\"conventional\",{\"1\":{\"113\":1,\"119\":1,\"207\":1,\"239\":2,\"309\":1,\"324\":1,\"355\":1,\"377\":1}}],[\"convenings\",{\"1\":{\"164\":1}}],[\"converts\",{\"1\":{\"438\":2}}],[\"converted\",{\"1\":{\"407\":1,\"490\":1}}],[\"converting\",{\"1\":{\"113\":1,\"259\":1,\"362\":1,\"490\":1}}],[\"converse\",{\"0\":{\"490\":1}}],[\"conversely\",{\"1\":{\"158\":1,\"165\":1,\"340\":1}}],[\"conversion\",{\"1\":{\"477\":1,\"497\":3}}],[\"conversations\",{\"0\":{\"351\":1},\"1\":{\"151\":1,\"172\":1,\"242\":1,\"243\":1,\"348\":1,\"351\":1,\"370\":1,\"393\":1,\"411\":1,\"449\":1}}],[\"conversational\",{\"0\":{\"130\":1,\"211\":1,\"242\":1,\"336\":1,\"481\":1},\"1\":{\"130\":2,\"132\":3,\"155\":1,\"201\":1,\"242\":2,\"266\":1,\"268\":1,\"271\":1,\"282\":2,\"330\":3,\"336\":2,\"350\":1,\"452\":1,\"457\":1,\"481\":1}}],[\"conversation\",{\"1\":{\"108\":2,\"144\":1,\"211\":1,\"236\":1,\"252\":1,\"335\":2,\"426\":1,\"449\":1}}],[\"confusing\",{\"1\":{\"444\":1}}],[\"confusion\",{\"1\":{\"288\":1}}],[\"confers\",{\"1\":{\"436\":1}}],[\"confer\",{\"1\":{\"389\":1}}],[\"conferencing\",{\"0\":{\"108\":1},\"1\":{\"108\":2}}],[\"confronts\",{\"1\":{\"359\":1}}],[\"confronting\",{\"1\":{\"314\":1}}],[\"conform\",{\"1\":{\"339\":1,\"378\":1}}],[\"conflicting\",{\"1\":{\"255\":1,\"437\":1,\"498\":1}}],[\"conflict\",{\"0\":{\"227\":1},\"1\":{\"170\":1,\"180\":1,\"227\":3}}],[\"conflicts\",{\"0\":{\"170\":1},\"1\":{\"206\":1,\"340\":3,\"453\":3}}],[\"configurable\",{\"1\":{\"445\":1}}],[\"configuration\",{\"0\":{\"445\":1},\"1\":{\"350\":1,\"445\":6}}],[\"configurations\",{\"1\":{\"158\":1,\"328\":1}}],[\"confined\",{\"1\":{\"412\":1}}],[\"confinement\",{\"1\":{\"326\":1}}],[\"confirms\",{\"1\":{\"219\":1,\"400\":1,\"401\":1}}],[\"confirm\",{\"1\":{\"187\":1,\"197\":1,\"341\":1,\"356\":1,\"455\":1}}],[\"confirmed\",{\"1\":{\"132\":1,\"330\":1}}],[\"confident\",{\"1\":{\"326\":1,\"463\":1}}],[\"confidentiality\",{\"1\":{\"178\":1}}],[\"confidence\",{\"0\":{\"368\":1,\"496\":1},\"1\":{\"161\":1,\"226\":1,\"294\":1,\"354\":1,\"368\":2,\"369\":1,\"467\":3,\"483\":1,\"496\":4,\"524\":1}}],[\"conjunction\",{\"1\":{\"120\":1,\"126\":1,\"479\":1,\"490\":1}}],[\"concatenated\",{\"1\":{\"402\":1}}],[\"concurrent\",{\"1\":{\"260\":1}}],[\"concurrently\",{\"1\":{\"252\":1,\"426\":1}}],[\"conclusive\",{\"1\":{\"304\":1}}],[\"conclusion\",{\"1\":{\"237\":1}}],[\"concludes\",{\"1\":{\"306\":1}}],[\"conclude\",{\"1\":{\"100\":1,\"114\":1,\"140\":1,\"144\":1,\"198\":1,\"231\":1,\"364\":1,\"387\":1}}],[\"concise\",{\"1\":{\"131\":1,\"356\":1,\"359\":1,\"485\":2}}],[\"concern\",{\"1\":{\"196\":1,\"254\":1,\"439\":1}}],[\"concerns\",{\"1\":{\"139\":1,\"178\":2,\"198\":1,\"213\":1,\"252\":1,\"258\":1,\"261\":1,\"286\":1,\"308\":1,\"387\":1,\"426\":1,\"433\":1,\"434\":1,\"460\":1,\"474\":1}}],[\"concepts\",{\"1\":{\"171\":1,\"172\":1,\"175\":1,\"199\":5,\"254\":1,\"307\":1,\"370\":1,\"398\":3,\"399\":1,\"475\":1}}],[\"concept\",{\"0\":{\"199\":1,\"232\":1,\"398\":1},\"1\":{\"157\":1,\"187\":1,\"199\":4,\"232\":2,\"239\":1,\"251\":1,\"282\":1,\"326\":1,\"388\":1,\"410\":1,\"422\":1,\"423\":1}}],[\"conceptual\",{\"0\":{\"157\":1,\"171\":1},\"1\":{\"157\":1,\"172\":1,\"370\":1,\"399\":1,\"486\":1}}],[\"conceptually\",{\"1\":{\"156\":1}}],[\"conceptualization\",{\"1\":{\"148\":1}}],[\"conceptualizing\",{\"1\":{\"125\":1,\"227\":1}}],[\"concentrate\",{\"1\":{\"118\":1,\"174\":1,\"371\":1,\"489\":1}}],[\"concentrated\",{\"1\":{\"112\":1,\"191\":1}}],[\"concretely\",{\"1\":{\"318\":1}}],[\"concrete\",{\"1\":{\"114\":1,\"228\":1,\"249\":1,\"389\":1}}],[\"continually\",{\"1\":{\"385\":1}}],[\"continuity\",{\"1\":{\"356\":2}}],[\"continues\",{\"1\":{\"434\":1}}],[\"continue\",{\"1\":{\"302\":1}}],[\"continued\",{\"1\":{\"288\":1,\"291\":1,\"476\":1}}],[\"continuous\",{\"1\":{\"191\":1}}],[\"continuously\",{\"1\":{\"126\":1,\"222\":1}}],[\"contaminated\",{\"0\":{\"443\":1}}],[\"contamination\",{\"1\":{\"333\":1,\"443\":7}}],[\"contacts\",{\"1\":{\"298\":2}}],[\"contactless\",{\"0\":{\"246\":1}}],[\"contact\",{\"1\":{\"194\":3}}],[\"contained\",{\"1\":{\"492\":1,\"494\":1}}],[\"containers\",{\"1\":{\"384\":1}}],[\"contain\",{\"0\":{\"328\":1},\"1\":{\"151\":1,\"301\":1,\"348\":1,\"439\":1,\"482\":1}}],[\"contains\",{\"1\":{\"149\":1,\"391\":1}}],[\"containing\",{\"1\":{\"118\":1,\"132\":1,\"197\":1,\"251\":1,\"290\":1,\"330\":1,\"378\":1,\"395\":1,\"403\":1,\"423\":1,\"491\":1}}],[\"contest\",{\"1\":{\"233\":1}}],[\"contests\",{\"0\":{\"233\":1},\"1\":{\"233\":1}}],[\"contemporary\",{\"1\":{\"216\":1,\"266\":1,\"460\":1}}],[\"contend\",{\"1\":{\"161\":1,\"354\":1,\"392\":1}}],[\"content\",{\"0\":{\"344\":1},\"1\":{\"120\":1,\"154\":2,\"155\":2,\"167\":1,\"198\":4,\"211\":1,\"212\":1,\"219\":2,\"224\":1,\"228\":1,\"230\":1,\"241\":1,\"260\":1,\"264\":1,\"272\":1,\"273\":1,\"274\":4,\"282\":1,\"291\":1,\"302\":2,\"327\":1,\"344\":5,\"359\":1,\"374\":1,\"387\":4,\"393\":1,\"394\":1,\"398\":1,\"401\":2,\"428\":1,\"432\":2,\"439\":1,\"448\":1,\"467\":2,\"476\":1,\"483\":1}}],[\"contexts\",{\"0\":{\"242\":1,\"249\":1,\"329\":1},\"1\":{\"156\":2,\"165\":1,\"172\":1,\"228\":1,\"233\":2,\"242\":2,\"244\":2,\"246\":1,\"249\":2,\"292\":1,\"309\":1,\"340\":3,\"359\":3,\"370\":1,\"425\":1,\"432\":1,\"453\":1,\"468\":1}}],[\"contextualizing\",{\"1\":{\"404\":1}}],[\"contextualize\",{\"1\":{\"293\":1}}],[\"contextualized\",{\"1\":{\"97\":2,\"156\":1,\"312\":2}}],[\"contextually\",{\"0\":{\"300\":1},\"1\":{\"384\":1,\"430\":1}}],[\"contextual\",{\"0\":{\"165\":1,\"268\":1,\"452\":1},\"1\":{\"122\":1,\"123\":3,\"137\":1,\"264\":1,\"268\":4,\"300\":1,\"319\":1,\"359\":1,\"448\":1,\"452\":4,\"460\":4,\"491\":1}}],[\"context\",{\"0\":{\"123\":1,\"317\":1,\"337\":1,\"341\":1,\"359\":1,\"412\":2},\"1\":{\"97\":2,\"100\":1,\"101\":2,\"110\":1,\"111\":1,\"123\":3,\"127\":1,\"129\":1,\"149\":1,\"156\":1,\"165\":6,\"172\":1,\"190\":1,\"196\":1,\"204\":1,\"217\":1,\"219\":1,\"227\":1,\"230\":1,\"233\":2,\"240\":1,\"249\":1,\"253\":1,\"255\":1,\"264\":2,\"271\":1,\"280\":1,\"291\":1,\"292\":1,\"312\":2,\"317\":1,\"321\":1,\"323\":1,\"324\":4,\"328\":2,\"337\":3,\"340\":3,\"341\":1,\"343\":1,\"350\":3,\"359\":2,\"365\":1,\"370\":1,\"372\":1,\"382\":1,\"384\":2,\"386\":1,\"388\":2,\"391\":1,\"395\":1,\"401\":1,\"404\":1,\"405\":1,\"409\":1,\"412\":8,\"413\":1,\"415\":1,\"417\":1,\"425\":2,\"431\":1,\"432\":2,\"437\":1,\"448\":2,\"449\":1,\"453\":6,\"455\":1,\"457\":1,\"476\":1,\"481\":1,\"491\":1,\"497\":1,\"498\":1}}],[\"contreras\",{\"1\":{\"379\":1}}],[\"contriever\",{\"1\":{\"347\":1}}],[\"contributor\",{\"1\":{\"425\":1}}],[\"contributing\",{\"1\":{\"171\":1,\"353\":1,\"394\":1,\"418\":1,\"466\":1}}],[\"contribution\",{\"1\":{\"129\":2,\"183\":1,\"204\":1,\"236\":1,\"244\":1,\"356\":1,\"446\":1}}],[\"contributions\",{\"1\":{\"126\":1,\"179\":1,\"204\":1,\"269\":1,\"270\":1,\"296\":1,\"317\":1,\"400\":1}}],[\"contribute\",{\"1\":{\"165\":1,\"182\":1,\"218\":1,\"226\":1,\"252\":1,\"271\":1,\"341\":1,\"345\":1,\"379\":1,\"389\":1,\"426\":1,\"457\":1}}],[\"contributes\",{\"1\":{\"131\":1,\"136\":1,\"148\":1,\"157\":1,\"159\":1,\"172\":1,\"186\":1,\"291\":1,\"292\":1,\"305\":1,\"370\":1,\"476\":1,\"492\":1}}],[\"controversial\",{\"1\":{\"243\":1}}],[\"controls\",{\"1\":{\"230\":1}}],[\"controllability\",{\"1\":{\"260\":1,\"462\":1}}],[\"controllable\",{\"1\":{\"104\":1}}],[\"controllers\",{\"1\":{\"203\":2}}],[\"controller\",{\"0\":{\"203\":1},\"1\":{\"201\":1,\"407\":1}}],[\"controlled\",{\"1\":{\"108\":1,\"120\":1,\"152\":1,\"173\":1,\"185\":1,\"273\":1,\"284\":2,\"444\":1}}],[\"controlling\",{\"1\":{\"112\":1,\"468\":1}}],[\"control\",{\"0\":{\"186\":1,\"203\":1},\"1\":{\"103\":1,\"141\":1,\"150\":3,\"155\":1,\"186\":9,\"203\":3,\"218\":1,\"224\":1,\"260\":1,\"265\":1,\"275\":1,\"286\":1,\"289\":1,\"291\":1,\"474\":1,\"476\":1}}],[\"contradicts\",{\"1\":{\"449\":1}}],[\"contradicted\",{\"1\":{\"203\":1}}],[\"contrary\",{\"1\":{\"263\":1}}],[\"contrast\",{\"1\":{\"215\":1,\"255\":1,\"260\":1,\"263\":1,\"273\":1,\"353\":1,\"365\":1,\"485\":1,\"492\":1}}],[\"contrastive\",{\"0\":{\"288\":1},\"1\":{\"137\":2,\"288\":1}}],[\"contrasting\",{\"1\":{\"111\":1,\"288\":1}}],[\"conducing\",{\"1\":{\"307\":1}}],[\"conduct\",{\"1\":{\"128\":2,\"130\":1,\"227\":1,\"238\":1,\"242\":1,\"245\":1,\"254\":1,\"261\":1,\"292\":1,\"300\":1,\"327\":2,\"339\":1,\"342\":1,\"353\":1,\"408\":1,\"411\":1,\"417\":1,\"425\":1,\"428\":1,\"445\":1,\"463\":1,\"470\":1,\"472\":1,\"486\":1,\"489\":1,\"490\":1,\"496\":1,\"498\":1}}],[\"conductive\",{\"1\":{\"113\":1}}],[\"conducting\",{\"1\":{\"108\":1,\"128\":1,\"172\":2,\"370\":2,\"468\":2}}],[\"conducted\",{\"1\":{\"98\":1,\"99\":1,\"120\":1,\"122\":1,\"150\":1,\"166\":1,\"174\":1,\"176\":1,\"185\":1,\"186\":1,\"197\":1,\"212\":1,\"214\":1,\"215\":1,\"220\":1,\"228\":1,\"270\":1,\"274\":1,\"282\":1,\"286\":1,\"293\":1,\"306\":1,\"350\":1,\"362\":1,\"371\":1,\"372\":1,\"391\":1,\"437\":1,\"447\":1,\"458\":1,\"474\":1,\"487\":1,\"491\":1,\"492\":1}}],[\"condensed\",{\"1\":{\"234\":3}}],[\"conditional\",{\"1\":{\"286\":1,\"340\":2,\"474\":1,\"491\":1}}],[\"condition\",{\"1\":{\"155\":1,\"166\":1,\"233\":1,\"376\":1}}],[\"conditioned\",{\"0\":{\"118\":1,\"376\":1},\"1\":{\"335\":1,\"376\":1}}],[\"conditions\",{\"1\":{\"107\":1,\"155\":1,\"220\":1,\"224\":2,\"269\":1,\"360\":1,\"459\":1}}],[\"conditioning\",{\"1\":{\"101\":1,\"224\":1}}],[\"consciousness\",{\"0\":{\"283\":1},\"1\":{\"283\":2}}],[\"consolidation\",{\"0\":{\"264\":1,\"448\":1},\"1\":{\"264\":1,\"448\":1,\"477\":1}}],[\"consolidate\",{\"1\":{\"217\":1}}],[\"cons\",{\"1\":{\"189\":1}}],[\"consists\",{\"1\":{\"218\":1,\"298\":1,\"345\":1}}],[\"consistency\",{\"0\":{\"335\":1},\"1\":{\"175\":1,\"283\":1,\"335\":4,\"372\":2,\"391\":1,\"396\":2,\"408\":1,\"491\":1,\"494\":1}}],[\"consistently\",{\"1\":{\"238\":1,\"246\":1,\"298\":1,\"458\":1,\"459\":1,\"485\":1,\"494\":2}}],[\"consistent\",{\"0\":{\"121\":1},\"1\":{\"235\":1,\"324\":1,\"388\":1,\"424\":1,\"430\":1,\"441\":1}}],[\"consisting\",{\"1\":{\"140\":1,\"381\":1,\"391\":1,\"411\":1,\"495\":1}}],[\"consider\",{\"1\":{\"240\":1,\"280\":1,\"316\":1,\"381\":1,\"393\":1,\"417\":2,\"444\":1,\"454\":1,\"466\":1,\"468\":1}}],[\"considers\",{\"1\":{\"204\":1}}],[\"consideration\",{\"0\":{\"280\":1},\"1\":{\"280\":2,\"489\":1}}],[\"considerations\",{\"1\":{\"98\":1,\"140\":1,\"144\":1,\"169\":1,\"184\":1,\"291\":1,\"380\":1,\"398\":1,\"476\":1}}],[\"considerably\",{\"1\":{\"191\":1,\"431\":1}}],[\"considerable\",{\"1\":{\"174\":1,\"356\":1,\"371\":1,\"427\":1}}],[\"considering\",{\"1\":{\"166\":1,\"219\":1,\"240\":1,\"251\":1,\"264\":1,\"283\":1,\"350\":1,\"401\":1,\"410\":2,\"414\":1,\"423\":1,\"448\":1,\"458\":1}}],[\"considered\",{\"1\":{\"126\":1,\"132\":1,\"189\":1,\"274\":1,\"282\":1,\"330\":1,\"366\":1}}],[\"consequences\",{\"1\":{\"237\":1}}],[\"consequently\",{\"1\":{\"111\":1,\"140\":1,\"167\":1,\"213\":1,\"266\":1,\"298\":1,\"334\":1,\"355\":1,\"390\":1}}],[\"consensus\",{\"1\":{\"174\":1,\"203\":1,\"371\":1,\"443\":1}}],[\"conservative\",{\"1\":{\"102\":1,\"378\":1}}],[\"constitutes\",{\"1\":{\"435\":1}}],[\"constitute\",{\"1\":{\"291\":1,\"476\":1}}],[\"constitutional\",{\"1\":{\"104\":1}}],[\"constantin\",{\"1\":{\"379\":1}}],[\"constant\",{\"1\":{\"165\":3,\"389\":1,\"415\":1,\"436\":1}}],[\"constrained\",{\"0\":{\"407\":1},\"1\":{\"222\":1,\"407\":1,\"459\":1}}],[\"constraint\",{\"1\":{\"114\":1}}],[\"constraints\",{\"0\":{\"114\":1},\"1\":{\"96\":6,\"114\":4,\"209\":1,\"294\":1,\"359\":1,\"360\":1,\"379\":1,\"399\":1,\"415\":1}}],[\"constrain\",{\"1\":{\"114\":1}}],[\"constructed\",{\"1\":{\"184\":1,\"380\":1,\"399\":1,\"471\":1,\"480\":1}}],[\"constructing\",{\"1\":{\"301\":1,\"360\":1,\"427\":1,\"471\":2}}],[\"constructiveness\",{\"0\":{\"155\":1},\"1\":{\"155\":4}}],[\"construction\",{\"0\":{\"201\":1,\"300\":1,\"382\":1,\"398\":1},\"1\":{\"153\":1,\"201\":5,\"252\":1,\"280\":1,\"300\":5,\"426\":1,\"434\":1,\"497\":1}}],[\"constructs\",{\"1\":{\"153\":1,\"382\":1,\"453\":1}}],[\"construct\",{\"1\":{\"112\":1,\"121\":1,\"130\":1,\"329\":1,\"337\":1,\"360\":1,\"402\":1,\"403\":1,\"415\":1,\"455\":1,\"488\":1}}],[\"consumer\",{\"1\":{\"203\":1,\"272\":1}}],[\"consumption\",{\"1\":{\"103\":1,\"128\":1,\"167\":1,\"239\":1,\"361\":1,\"483\":1}}],[\"consuming\",{\"1\":{\"99\":1,\"137\":1,\"154\":1,\"356\":1}}],[\"co\",{\"0\":{\"98\":1,\"164\":1,\"224\":1,\"235\":1,\"275\":1},\"1\":{\"97\":1,\"98\":6,\"164\":1,\"171\":1,\"212\":2,\"224\":3,\"235\":3,\"312\":1,\"395\":1,\"483\":1}}],[\"caufield\",{\"1\":{\"398\":1}}],[\"causing\",{\"1\":{\"260\":1}}],[\"causal\",{\"1\":{\"255\":1,\"454\":1}}],[\"cause\",{\"0\":{\"351\":1},\"1\":{\"257\":1,\"351\":2,\"445\":2}}],[\"causes\",{\"1\":{\"230\":1,\"351\":1,\"425\":1}}],[\"caused\",{\"1\":{\"128\":1,\"219\":1,\"329\":1,\"401\":1,\"469\":1}}],[\"cache\",{\"0\":{\"366\":1},\"1\":{\"366\":6,\"456\":2,\"462\":3,\"469\":2}}],[\"caching\",{\"1\":{\"224\":1}}],[\"cag\",{\"1\":{\"329\":4}}],[\"cabrio\",{\"1\":{\"315\":1}}],[\"cazacu\",{\"1\":{\"297\":1}}],[\"campuses\",{\"1\":{\"268\":1,\"452\":1}}],[\"campbell\",{\"1\":{\"268\":2,\"452\":2}}],[\"cama\",{\"1\":{\"232\":1}}],[\"camino\",{\"1\":{\"220\":1}}],[\"camera\",{\"1\":{\"194\":3,\"203\":1,\"407\":1}}],[\"cameras\",{\"1\":{\"181\":2,\"486\":1}}],[\"camelia\",{\"1\":{\"111\":1}}],[\"cavs\",{\"1\":{\"199\":4}}],[\"catering\",{\"1\":{\"480\":1}}],[\"category\",{\"1\":{\"444\":1,\"497\":1}}],[\"categorical\",{\"0\":{\"197\":2},\"1\":{\"197\":5}}],[\"categorizing\",{\"0\":{\"286\":1,\"474\":1}}],[\"categorization\",{\"1\":{\"174\":1,\"371\":1}}],[\"categorize\",{\"1\":{\"174\":1,\"371\":1,\"396\":1,\"437\":1}}],[\"categorized\",{\"1\":{\"143\":1,\"339\":1}}],[\"categories\",{\"1\":{\"128\":1,\"197\":1,\"237\":1,\"244\":1,\"256\":3,\"257\":1,\"297\":1,\"344\":2,\"391\":1,\"431\":1,\"437\":1}}],[\"catania\",{\"1\":{\"416\":1}}],[\"catalog\",{\"1\":{\"398\":1}}],[\"catalogue\",{\"1\":{\"292\":1}}],[\"cataron\",{\"1\":{\"255\":1}}],[\"catie\",{\"1\":{\"269\":1}}],[\"catherine\",{\"1\":{\"183\":1}}],[\"cao\",{\"1\":{\"138\":1,\"207\":1,\"328\":1,\"329\":1,\"347\":1,\"428\":1}}],[\"calibration\",{\"1\":{\"368\":2}}],[\"calibrate\",{\"1\":{\"135\":1}}],[\"caliber\",{\"1\":{\"359\":1}}],[\"calculates\",{\"1\":{\"400\":1}}],[\"calculate\",{\"1\":{\"270\":1,\"281\":1,\"400\":1,\"462\":1}}],[\"calculation\",{\"1\":{\"224\":1,\"462\":2}}],[\"caleb\",{\"1\":{\"180\":1}}],[\"calling\",{\"1\":{\"241\":2}}],[\"callbacks\",{\"1\":{\"231\":1}}],[\"calls\",{\"1\":{\"180\":1,\"316\":1,\"361\":1}}],[\"called\",{\"1\":{\"126\":1,\"165\":1,\"188\":1,\"206\":1,\"209\":1,\"219\":1,\"229\":1,\"251\":1,\"257\":1,\"277\":1,\"284\":1,\"296\":1,\"334\":1,\"401\":1,\"418\":1,\"423\":1,\"434\":1,\"436\":1,\"449\":1,\"467\":1,\"481\":1,\"488\":1,\"489\":1,\"491\":1}}],[\"call\",{\"0\":{\"161\":1,\"354\":1},\"1\":{\"122\":1,\"137\":1,\"144\":1,\"211\":1,\"280\":1,\"282\":1,\"427\":1}}],[\"caer\",{\"1\":{\"123\":1}}],[\"capitalized\",{\"1\":{\"358\":1}}],[\"capturing\",{\"1\":{\"137\":2,\"143\":1,\"328\":1}}],[\"captured\",{\"1\":{\"272\":1,\"335\":1,\"339\":1}}],[\"captures\",{\"1\":{\"244\":1,\"259\":1,\"313\":1}}],[\"capture\",{\"0\":{\"352\":1},\"1\":{\"133\":1,\"137\":2,\"199\":1,\"237\":1,\"244\":1,\"255\":1,\"309\":1,\"331\":1,\"345\":1,\"412\":1,\"416\":2,\"497\":1}}],[\"captions\",{\"1\":{\"123\":1}}],[\"capacities\",{\"1\":{\"275\":1}}],[\"capacity\",{\"1\":{\"131\":1,\"167\":1,\"198\":1,\"245\":1,\"387\":1,\"390\":1,\"478\":1}}],[\"capability\",{\"1\":{\"116\":1,\"119\":1,\"132\":1,\"151\":1,\"258\":1,\"317\":2,\"329\":1,\"330\":1,\"337\":1,\"339\":1,\"348\":1,\"355\":1,\"356\":1,\"377\":1,\"385\":1,\"417\":1,\"429\":1,\"433\":1,\"439\":1,\"450\":1,\"467\":1}}],[\"capabilities\",{\"0\":{\"275\":1,\"282\":1,\"372\":1,\"471\":1},\"1\":{\"100\":1,\"123\":1,\"125\":1,\"133\":1,\"138\":1,\"141\":1,\"151\":1,\"161\":1,\"163\":1,\"169\":1,\"174\":1,\"205\":2,\"212\":2,\"224\":3,\"239\":1,\"242\":1,\"244\":1,\"261\":1,\"282\":2,\"303\":2,\"314\":1,\"316\":1,\"329\":1,\"331\":1,\"334\":1,\"337\":3,\"340\":1,\"342\":1,\"345\":1,\"348\":1,\"349\":1,\"350\":1,\"354\":1,\"358\":2,\"361\":1,\"363\":1,\"371\":1,\"372\":1,\"376\":1,\"393\":1,\"403\":2,\"412\":1,\"413\":1,\"419\":1,\"420\":1,\"434\":1,\"438\":1,\"447\":1,\"458\":2,\"459\":1,\"470\":1,\"471\":3,\"472\":1,\"475\":1,\"478\":1,\"481\":1,\"486\":1,\"487\":1,\"490\":2}}],[\"capable\",{\"1\":{\"100\":1,\"288\":1,\"291\":1,\"381\":1,\"393\":1,\"419\":1,\"420\":1,\"469\":1,\"476\":1,\"480\":1,\"494\":1}}],[\"caiqi\",{\"1\":{\"467\":1}}],[\"caiming\",{\"1\":{\"443\":1}}],[\"caishun\",{\"1\":{\"417\":1}}],[\"cair\",{\"1\":{\"393\":1}}],[\"cai\",{\"1\":{\"114\":1,\"329\":1,\"359\":1,\"450\":1}}],[\"cardoso\",{\"1\":{\"300\":1}}],[\"cardinaux\",{\"1\":{\"444\":1}}],[\"cardiology\",{\"1\":{\"356\":1}}],[\"cardio\",{\"1\":{\"194\":2}}],[\"cardiac\",{\"0\":{\"356\":1},\"1\":{\"194\":1,\"356\":1}}],[\"carpentry\",{\"0\":{\"300\":1},\"1\":{\"300\":1}}],[\"carpendale\",{\"1\":{\"243\":1}}],[\"carlo\",{\"1\":{\"390\":1}}],[\"carlos\",{\"1\":{\"126\":1,\"416\":1}}],[\"carla\",{\"1\":{\"362\":1}}],[\"carl\",{\"1\":{\"230\":1}}],[\"carolina\",{\"1\":{\"533\":2}}],[\"carolin\",{\"1\":{\"338\":1}}],[\"carol\",{\"1\":{\"201\":1}}],[\"carter\",{\"1\":{\"194\":1}}],[\"carmel\",{\"1\":{\"184\":1,\"380\":1}}],[\"caryl\",{\"1\":{\"178\":1}}],[\"car\",{\"1\":{\"150\":2}}],[\"carsten\",{\"1\":{\"220\":1}}],[\"cars\",{\"1\":{\"137\":1}}],[\"carer\",{\"1\":{\"166\":1}}],[\"care\",{\"1\":{\"121\":1,\"178\":4,\"309\":2,\"356\":5,\"379\":1}}],[\"carefully\",{\"1\":{\"261\":1,\"395\":1}}],[\"careful\",{\"1\":{\"101\":1}}],[\"carry\",{\"1\":{\"117\":1,\"149\":1,\"413\":1,\"446\":1,\"447\":1}}],[\"carried\",{\"1\":{\"434\":1}}],[\"carrie\",{\"1\":{\"114\":1,\"176\":1}}],[\"cascade\",{\"1\":{\"435\":3}}],[\"cascades\",{\"1\":{\"435\":2}}],[\"casado\",{\"1\":{\"326\":1}}],[\"castellucci\",{\"1\":{\"405\":1}}],[\"castello\",{\"1\":{\"120\":1}}],[\"castiñeira\",{\"1\":{\"282\":1}}],[\"castaño\",{\"1\":{\"282\":1}}],[\"cash\",{\"0\":{\"233\":2},\"1\":{\"233\":7}}],[\"cas\",{\"1\":{\"211\":7}}],[\"casey\",{\"1\":{\"196\":1}}],[\"cases\",{\"1\":{\"114\":1,\"131\":1,\"172\":1,\"189\":1,\"224\":1,\"256\":1,\"272\":2,\"308\":1,\"326\":1,\"337\":1,\"338\":1,\"363\":1,\"370\":1,\"374\":2,\"384\":1,\"414\":1,\"430\":1}}],[\"case\",{\"0\":{\"135\":1,\"175\":1,\"384\":1,\"431\":1},\"1\":{\"104\":1,\"112\":1,\"161\":1,\"171\":2,\"175\":1,\"201\":1,\"239\":1,\"272\":2,\"288\":1,\"289\":1,\"354\":1,\"361\":1,\"377\":1,\"382\":1,\"384\":2,\"445\":1,\"458\":3,\"498\":1}}],[\"casper\",{\"1\":{\"98\":1}}],[\"candice\",{\"1\":{\"468\":1}}],[\"candidate\",{\"1\":{\"417\":1}}],[\"candidates\",{\"1\":{\"154\":1,\"234\":1,\"263\":1,\"441\":1}}],[\"canonicalization\",{\"1\":{\"382\":2}}],[\"canonicalize\",{\"0\":{\"382\":1},\"1\":{\"382\":1}}],[\"cancer\",{\"1\":{\"372\":1}}],[\"canvas\",{\"1\":{\"361\":1,\"524\":1}}],[\"cangqing\",{\"1\":{\"359\":1}}],[\"cans\",{\"1\":{\"289\":1}}],[\"canfield\",{\"1\":{\"196\":1}}],[\"cannot\",{\"1\":{\"133\":1,\"331\":1,\"355\":2,\"417\":2,\"455\":1,\"463\":1,\"490\":1}}],[\"can\",{\"0\":{\"156\":1,\"209\":1,\"241\":1,\"261\":1,\"352\":1,\"381\":1,\"404\":1,\"435\":1,\"458\":1,\"466\":1,\"470\":1,\"490\":1},\"1\":{\"96\":1,\"97\":2,\"98\":1,\"99\":3,\"101\":4,\"102\":1,\"103\":1,\"104\":2,\"107\":2,\"108\":1,\"111\":1,\"112\":2,\"114\":1,\"117\":1,\"118\":1,\"119\":1,\"122\":1,\"124\":2,\"125\":2,\"128\":1,\"130\":3,\"131\":1,\"132\":2,\"137\":1,\"140\":1,\"141\":1,\"142\":1,\"149\":1,\"150\":1,\"154\":1,\"156\":1,\"163\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":2,\"175\":1,\"176\":1,\"180\":1,\"182\":1,\"183\":1,\"184\":1,\"192\":2,\"194\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"207\":2,\"211\":1,\"217\":1,\"218\":1,\"219\":2,\"220\":1,\"222\":1,\"223\":2,\"224\":3,\"226\":1,\"228\":3,\"229\":2,\"231\":1,\"233\":2,\"234\":1,\"235\":1,\"237\":1,\"238\":2,\"242\":1,\"243\":1,\"248\":1,\"250\":1,\"251\":1,\"252\":1,\"255\":2,\"256\":1,\"257\":2,\"258\":1,\"261\":1,\"263\":1,\"270\":1,\"272\":2,\"275\":1,\"279\":1,\"281\":1,\"289\":1,\"292\":1,\"295\":1,\"296\":1,\"301\":2,\"303\":1,\"304\":2,\"306\":1,\"308\":1,\"312\":2,\"313\":1,\"315\":1,\"322\":1,\"323\":2,\"324\":1,\"326\":2,\"327\":1,\"329\":1,\"330\":2,\"334\":1,\"335\":3,\"337\":1,\"338\":2,\"339\":2,\"340\":1,\"342\":1,\"355\":1,\"360\":1,\"362\":1,\"363\":1,\"368\":1,\"369\":2,\"370\":1,\"380\":1,\"381\":3,\"382\":1,\"385\":1,\"386\":1,\"389\":1,\"395\":1,\"400\":1,\"401\":2,\"402\":3,\"403\":1,\"407\":1,\"410\":1,\"411\":2,\"414\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"425\":1,\"426\":1,\"427\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"441\":1,\"447\":1,\"449\":1,\"454\":1,\"455\":3,\"458\":1,\"462\":2,\"466\":2,\"468\":1,\"470\":3,\"471\":2,\"475\":4,\"482\":3,\"486\":2,\"487\":1,\"490\":1,\"491\":1,\"494\":3,\"495\":1,\"497\":1,\"498\":1}}],[\"canada\",{\"1\":{\"74\":1}}],[\"cad\",{\"1\":{\"8\":1}}],[\"cj\",{\"1\":{\"74\":1}}],[\"cis\",{\"1\":{\"481\":1}}],[\"circle\",{\"0\":{\"279\":1}}],[\"circular\",{\"1\":{\"217\":1}}],[\"circuit\",{\"1\":{\"113\":7,\"306\":1}}],[\"circuits\",{\"0\":{\"113\":1},\"1\":{\"113\":3}}],[\"circumstances\",{\"1\":{\"100\":1,\"128\":1}}],[\"civic\",{\"0\":{\"248\":1},\"1\":{\"248\":2}}],[\"cihualpilli\",{\"1\":{\"220\":1}}],[\"citep\",{\"1\":{\"313\":1}}],[\"cite\",{\"1\":{\"159\":4}}],[\"city\",{\"0\":{\"500\":1},\"1\":{\"16\":1,\"40\":2,\"248\":1,\"520\":1}}],[\"cinthya\",{\"1\":{\"147\":1}}],[\"cinema\",{\"1\":{\"8\":1}}],[\"c++\",{\"1\":{\"8\":1}}],[\"c\",{\"0\":{\"434\":1},\"1\":{\"8\":1,\"137\":1,\"201\":1,\"220\":1,\"281\":1,\"304\":1,\"308\":1,\"326\":1,\"434\":3}}],[\"a3\",{\"1\":{\"539\":1}}],[\"a2\",{\"1\":{\"524\":1,\"539\":1}}],[\"a1\",{\"1\":{\"524\":1,\"539\":1}}],[\"a100\",{\"1\":{\"479\":1}}],[\"aqua\",{\"0\":{\"400\":1},\"1\":{\"400\":3}}],[\"aoi\",{\"1\":{\"355\":1}}],[\"aegissafetyexperts\",{\"1\":{\"344\":1}}],[\"aegissafetydataset\",{\"1\":{\"344\":2}}],[\"aegis\",{\"0\":{\"344\":1},\"1\":{\"344\":1}}],[\"aesthetic\",{\"0\":{\"287\":1},\"1\":{\"289\":2}}],[\"aesthetically\",{\"1\":{\"189\":2}}],[\"axes\",{\"1\":{\"281\":1}}],[\"aae\",{\"1\":{\"361\":1}}],[\"aaen\",{\"1\":{\"296\":1}}],[\"aakash\",{\"1\":{\"266\":1}}],[\"aaron\",{\"1\":{\"156\":1,\"230\":1,\"326\":1,\"368\":1,\"436\":1}}],[\"ahead\",{\"0\":{\"364\":1}}],[\"ahmed\",{\"1\":{\"349\":1,\"418\":1}}],[\"ahmadou\",{\"1\":{\"232\":1}}],[\"ahsanul\",{\"1\":{\"298\":1}}],[\"ahn\",{\"1\":{\"150\":1,\"381\":1,\"391\":1}}],[\"akin\",{\"1\":{\"478\":1}}],[\"akiko\",{\"1\":{\"458\":1}}],[\"aka\",{\"1\":{\"232\":1}}],[\"akash\",{\"1\":{\"205\":1,\"303\":1,\"434\":1}}],[\"akter\",{\"1\":{\"206\":1}}],[\"akhil\",{\"1\":{\"184\":1,\"380\":1}}],[\"ayday\",{\"1\":{\"410\":1}}],[\"ayoub\",{\"1\":{\"171\":1}}],[\"aythami\",{\"1\":{\"126\":1}}],[\"apis\",{\"1\":{\"338\":1,\"409\":1}}],[\"api\",{\"1\":{\"287\":1,\"337\":1,\"361\":1,\"447\":1}}],[\"april\",{\"1\":{\"170\":1}}],[\"appearances\",{\"1\":{\"270\":1}}],[\"appearance\",{\"1\":{\"235\":3,\"270\":1,\"375\":1}}],[\"apps\",{\"0\":{\"219\":1,\"401\":1},\"1\":{\"219\":2,\"257\":3,\"401\":2}}],[\"app\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"140\":1,\"157\":1,\"214\":1,\"257\":5,\"268\":1,\"296\":1,\"452\":1}}],[\"apply\",{\"1\":{\"283\":1,\"409\":1,\"416\":1,\"442\":1,\"462\":1}}],[\"applying\",{\"1\":{\"114\":1,\"174\":1,\"261\":1,\"283\":1,\"305\":1,\"371\":1,\"382\":1,\"435\":2}}],[\"applies\",{\"1\":{\"257\":1,\"382\":1}}],[\"applied\",{\"1\":{\"113\":1,\"176\":1,\"251\":1,\"256\":2,\"281\":1,\"282\":1,\"286\":1,\"296\":1,\"334\":1,\"340\":1,\"382\":1,\"423\":1,\"427\":1,\"456\":1,\"474\":1}}],[\"applicable\",{\"1\":{\"329\":1}}],[\"applicability\",{\"1\":{\"116\":1,\"139\":1,\"359\":1,\"379\":2,\"454\":1,\"477\":1}}],[\"application\",{\"1\":{\"99\":1,\"105\":2,\"136\":1,\"161\":1,\"182\":1,\"192\":1,\"218\":1,\"223\":1,\"243\":1,\"268\":1,\"296\":1,\"323\":1,\"341\":1,\"344\":1,\"354\":1,\"379\":1,\"452\":1,\"469\":1,\"480\":1,\"495\":1}}],[\"applications\",{\"0\":{\"148\":1,\"192\":1,\"234\":1,\"324\":1,\"326\":1,\"361\":1,\"436\":1},\"1\":{\"97\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"137\":1,\"146\":1,\"148\":4,\"154\":1,\"165\":1,\"166\":2,\"178\":1,\"191\":1,\"192\":1,\"207\":1,\"222\":1,\"234\":2,\"239\":2,\"245\":1,\"253\":1,\"261\":1,\"282\":1,\"295\":2,\"300\":1,\"306\":1,\"307\":1,\"308\":1,\"312\":1,\"315\":1,\"324\":2,\"326\":3,\"329\":1,\"345\":1,\"358\":1,\"359\":2,\"361\":1,\"364\":1,\"379\":1,\"382\":1,\"418\":1,\"429\":1,\"430\":1,\"436\":2,\"437\":2,\"443\":1,\"453\":1,\"459\":1,\"467\":1,\"468\":1,\"475\":1,\"498\":1}}],[\"appleby\",{\"1\":{\"111\":1,\"252\":1,\"426\":1}}],[\"apparent\",{\"1\":{\"107\":1,\"123\":2}}],[\"appraisal\",{\"1\":{\"204\":1}}],[\"approved\",{\"1\":{\"249\":1}}],[\"approximation\",{\"1\":{\"188\":1}}],[\"approximate\",{\"0\":{\"188\":1},\"1\":{\"188\":2}}],[\"approximately\",{\"1\":{\"176\":1,\"344\":1,\"361\":1,\"363\":1,\"365\":1}}],[\"appropriateness\",{\"0\":{\"176\":1},\"1\":{\"176\":2,\"326\":1}}],[\"appropriate\",{\"1\":{\"114\":1,\"176\":3,\"191\":1,\"240\":1,\"335\":1,\"377\":1,\"449\":1,\"481\":1,\"498\":1}}],[\"approach\",{\"0\":{\"246\":1,\"274\":1,\"425\":1},\"1\":{\"97\":1,\"112\":1,\"113\":2,\"115\":1,\"116\":1,\"118\":1,\"120\":1,\"121\":1,\"123\":2,\"127\":1,\"132\":1,\"137\":4,\"138\":1,\"141\":2,\"143\":3,\"152\":1,\"154\":1,\"155\":2,\"158\":1,\"159\":1,\"161\":1,\"167\":1,\"171\":1,\"174\":2,\"176\":1,\"179\":1,\"181\":1,\"182\":2,\"183\":1,\"186\":1,\"190\":2,\"204\":1,\"207\":2,\"216\":2,\"229\":1,\"230\":1,\"234\":2,\"239\":1,\"253\":1,\"254\":1,\"255\":1,\"259\":1,\"260\":1,\"266\":1,\"275\":1,\"278\":1,\"281\":1,\"282\":2,\"284\":1,\"288\":1,\"289\":3,\"292\":1,\"297\":1,\"300\":1,\"301\":1,\"305\":1,\"306\":2,\"312\":1,\"318\":2,\"330\":1,\"335\":1,\"349\":1,\"351\":2,\"352\":2,\"354\":1,\"371\":2,\"376\":4,\"379\":1,\"386\":1,\"388\":3,\"392\":1,\"396\":1,\"398\":1,\"403\":2,\"405\":1,\"407\":1,\"408\":1,\"422\":1,\"425\":2,\"430\":1,\"439\":1,\"442\":1,\"454\":1,\"458\":2,\"459\":2,\"467\":1,\"469\":1,\"477\":1,\"483\":1,\"485\":1,\"490\":2,\"494\":1,\"496\":2}}],[\"approaches\",{\"1\":{\"96\":3,\"105\":1,\"112\":1,\"119\":1,\"123\":2,\"139\":1,\"143\":2,\"158\":1,\"167\":1,\"179\":1,\"182\":1,\"183\":1,\"186\":1,\"189\":1,\"190\":2,\"206\":3,\"230\":1,\"231\":1,\"235\":1,\"236\":1,\"238\":1,\"240\":2,\"250\":3,\"257\":1,\"259\":1,\"263\":1,\"289\":1,\"308\":1,\"335\":1,\"347\":1,\"353\":3,\"355\":1,\"378\":1,\"391\":1,\"402\":1,\"414\":1,\"415\":1,\"429\":1,\"441\":1,\"460\":1,\"468\":1,\"475\":1,\"481\":1}}],[\"apprentices\",{\"0\":{\"136\":1}}],[\"apprentice\",{\"0\":{\"99\":1},\"1\":{\"99\":1}}],[\"ajayi\",{\"1\":{\"281\":1}}],[\"ajanie\",{\"1\":{\"125\":1}}],[\"ajit\",{\"1\":{\"156\":1}}],[\"aframe\",{\"0\":{\"157\":1},\"1\":{\"157\":1}}],[\"aforementioned\",{\"1\":{\"151\":1,\"326\":1,\"348\":1,\"445\":1}}],[\"after\",{\"1\":{\"148\":1,\"151\":1,\"164\":1,\"169\":1,\"194\":1,\"200\":1,\"217\":1,\"218\":1,\"234\":1,\"257\":1,\"326\":1,\"348\":1,\"403\":1,\"412\":1,\"424\":1,\"466\":1}}],[\"affirm\",{\"1\":{\"489\":1}}],[\"affirmative\",{\"1\":{\"340\":2,\"385\":1}}],[\"affiliation\",{\"1\":{\"461\":1}}],[\"affordable\",{\"1\":{\"295\":1}}],[\"affordability\",{\"1\":{\"140\":1}}],[\"affordances\",{\"1\":{\"163\":1,\"226\":1}}],[\"affecting\",{\"1\":{\"198\":1,\"283\":1,\"387\":1}}],[\"affective\",{\"1\":{\"117\":1,\"261\":1}}],[\"affect\",{\"1\":{\"135\":1,\"156\":1,\"178\":1,\"181\":1,\"199\":1,\"203\":1,\"265\":1,\"469\":1,\"487\":1}}],[\"affects\",{\"0\":{\"198\":1,\"387\":1},\"1\":{\"106\":1,\"233\":1,\"497\":1}}],[\"affected\",{\"0\":{\"340\":1},\"1\":{\"98\":1,\"230\":1,\"263\":1,\"265\":1,\"302\":1}}],[\"awad\",{\"1\":{\"152\":1}}],[\"awareness\",{\"0\":{\"173\":1,\"462\":1},\"1\":{\"160\":1,\"173\":6,\"190\":1,\"204\":1,\"217\":1,\"219\":1,\"240\":1,\"319\":3,\"401\":1}}],[\"aware\",{\"0\":{\"130\":1,\"301\":1,\"329\":1,\"425\":1},\"1\":{\"130\":1,\"160\":1,\"254\":1,\"301\":1,\"325\":1,\"329\":1,\"388\":1,\"425\":1,\"430\":1,\"462\":2}}],[\"awards\",{\"0\":{\"7\":1}}],[\"awkward\",{\"1\":{\"124\":1}}],[\"amr\",{\"1\":{\"432\":2}}],[\"amsterdam\",{\"1\":{\"388\":1}}],[\"amd\",{\"1\":{\"363\":1}}],[\"amir\",{\"1\":{\"498\":1}}],[\"amirkeivan\",{\"1\":{\"456\":1}}],[\"amirhossein\",{\"1\":{\"404\":1}}],[\"amiruzzaman\",{\"1\":{\"270\":1}}],[\"amidst\",{\"1\":{\"398\":1}}],[\"amin\",{\"1\":{\"298\":1,\"379\":1}}],[\"am\",{\"0\":{\"293\":1}}],[\"amplegcg\",{\"0\":{\"313\":1},\"1\":{\"313\":3}}],[\"amplifies\",{\"1\":{\"313\":1}}],[\"amplified\",{\"1\":{\"150\":1}}],[\"amplify\",{\"1\":{\"142\":1,\"229\":1}}],[\"amputations\",{\"1\":{\"220\":1}}],[\"amounts\",{\"1\":{\"315\":1}}],[\"amount\",{\"1\":{\"200\":1,\"255\":1,\"256\":1,\"408\":1,\"415\":1}}],[\"amongst\",{\"1\":{\"458\":1,\"482\":1}}],[\"among\",{\"1\":{\"108\":1,\"124\":1,\"126\":1,\"226\":1,\"230\":1,\"237\":2,\"270\":1,\"291\":1,\"293\":1,\"298\":1,\"304\":1,\"356\":1,\"359\":1,\"366\":1,\"385\":1,\"411\":1,\"412\":1,\"413\":1,\"429\":1,\"437\":1,\"450\":1,\"476\":1}}],[\"ameliorate\",{\"1\":{\"459\":1}}],[\"amenable\",{\"1\":{\"288\":1,\"434\":1}}],[\"ameet\",{\"1\":{\"215\":1}}],[\"ameer\",{\"1\":{\"176\":1}}],[\"amer\",{\"1\":{\"185\":1}}],[\"amalgamating\",{\"1\":{\"359\":1}}],[\"amalgamates\",{\"1\":{\"359\":1}}],[\"amazon\",{\"1\":{\"163\":1}}],[\"amanda\",{\"1\":{\"151\":1,\"348\":1,\"379\":1}}],[\"amy\",{\"1\":{\"147\":1,\"175\":1,\"261\":1}}],[\"ambiguity\",{\"1\":{\"116\":1,\"141\":1,\"339\":1}}],[\"austronesian\",{\"1\":{\"365\":1}}],[\"aurora\",{\"0\":{\"257\":1},\"1\":{\"257\":6}}],[\"auc\",{\"1\":{\"190\":1,\"223\":1}}],[\"aumap\",{\"1\":{\"188\":1}}],[\"audio\",{\"1\":{\"190\":3,\"224\":1,\"232\":1,\"235\":1,\"287\":2,\"335\":1,\"512\":2,\"514\":1,\"515\":1,\"516\":2}}],[\"auditory\",{\"1\":{\"287\":2,\"305\":1}}],[\"auditors\",{\"0\":{\"148\":1},\"1\":{\"148\":1}}],[\"audited\",{\"1\":{\"148\":1}}],[\"auditing\",{\"0\":{\"148\":1},\"1\":{\"148\":5}}],[\"audiences\",{\"1\":{\"117\":1}}],[\"audience\",{\"1\":{\"117\":1,\"398\":1}}],[\"augmentation\",{\"0\":{\"402\":1,\"460\":1},\"1\":{\"329\":1,\"347\":1,\"425\":1,\"460\":5,\"496\":1}}],[\"augmenting\",{\"0\":{\"459\":1},\"1\":{\"142\":1,\"359\":2,\"460\":1}}],[\"augment\",{\"0\":{\"355\":1},\"1\":{\"142\":1,\"151\":1,\"229\":1,\"348\":1,\"478\":1}}],[\"augments\",{\"1\":{\"110\":1,\"321\":1,\"384\":1}}],[\"augmented\",{\"0\":{\"105\":1,\"140\":1,\"147\":1,\"173\":1,\"195\":1,\"347\":1,\"365\":1,\"384\":1},\"1\":{\"105\":1,\"140\":1,\"147\":1,\"195\":2,\"282\":1,\"323\":1,\"329\":1,\"347\":1,\"359\":1,\"372\":1,\"382\":1,\"384\":1,\"392\":1,\"396\":1,\"404\":1,\"430\":1,\"453\":1}}],[\"autoformalize\",{\"1\":{\"494\":1}}],[\"autoformalization\",{\"0\":{\"494\":1}}],[\"autoencoder\",{\"1\":{\"491\":1}}],[\"autoregressive\",{\"1\":{\"389\":1}}],[\"autorace\",{\"1\":{\"353\":2}}],[\"auto\",{\"1\":{\"374\":1}}],[\"automotive\",{\"1\":{\"297\":2}}],[\"automating\",{\"1\":{\"175\":1,\"222\":1,\"257\":1,\"275\":1,\"300\":1,\"396\":1,\"400\":1}}],[\"automation\",{\"0\":{\"222\":1},\"1\":{\"133\":1,\"150\":1,\"186\":1,\"222\":1,\"331\":1,\"356\":1,\"363\":1,\"408\":2}}],[\"automatic\",{\"0\":{\"439\":1},\"1\":{\"126\":1,\"232\":1,\"259\":1,\"282\":1,\"289\":2,\"298\":1,\"335\":1,\"353\":1,\"360\":1,\"369\":1,\"415\":2,\"432\":1}}],[\"automatically\",{\"0\":{\"490\":1},\"1\":{\"105\":1,\"131\":1,\"213\":1,\"224\":1,\"242\":1,\"257\":1,\"353\":1,\"382\":1,\"388\":1,\"480\":1,\"490\":2,\"494\":2}}],[\"automates\",{\"1\":{\"418\":1}}],[\"automate\",{\"1\":{\"116\":1,\"140\":1}}],[\"automated\",{\"0\":{\"112\":1,\"150\":1,\"257\":1,\"278\":1,\"297\":1,\"459\":1},\"1\":{\"112\":2,\"150\":1,\"158\":1,\"186\":2,\"219\":1,\"257\":2,\"278\":3,\"297\":1,\"315\":1,\"322\":1,\"339\":1,\"353\":1,\"358\":1,\"382\":1,\"388\":1,\"401\":1,\"415\":1,\"477\":1,\"488\":1,\"496\":1}}],[\"autonomously\",{\"1\":{\"264\":1,\"448\":1}}],[\"autonomous\",{\"0\":{\"221\":1,\"326\":1,\"362\":1},\"1\":{\"221\":4,\"288\":1,\"313\":1,\"326\":1,\"362\":1,\"364\":2,\"477\":1}}],[\"autonomic\",{\"1\":{\"166\":1}}],[\"autocomplete\",{\"1\":{\"215\":1}}],[\"autism\",{\"0\":{\"120\":1},\"1\":{\"120\":1,\"261\":1}}],[\"author\",{\"1\":{\"336\":1,\"369\":2}}],[\"authoring\",{\"0\":{\"163\":1,\"358\":1},\"1\":{\"163\":3,\"182\":1,\"259\":1,\"358\":1}}],[\"authors\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":2,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":2,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":4,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"authentic\",{\"1\":{\"132\":1,\"213\":1,\"266\":1,\"330\":1}}],[\"authenticity\",{\"1\":{\"101\":1,\"213\":2}}],[\"authentication\",{\"1\":{\"101\":1}}],[\"averaged\",{\"1\":{\"419\":1,\"420\":1}}],[\"average\",{\"1\":{\"222\":1,\"246\":1,\"351\":1,\"360\":1,\"369\":1,\"395\":1,\"445\":1,\"482\":1}}],[\"avenues\",{\"1\":{\"200\":1,\"305\":1,\"339\":1}}],[\"avenue\",{\"1\":{\"74\":1,\"120\":1}}],[\"avoided\",{\"1\":{\"460\":1}}],[\"avoiding\",{\"1\":{\"447\":1}}],[\"avoidance\",{\"1\":{\"447\":1}}],[\"avoid\",{\"0\":{\"403\":1},\"1\":{\"170\":1,\"176\":1,\"257\":1,\"333\":1,\"418\":1}}],[\"avoids\",{\"1\":{\"153\":1}}],[\"avs\",{\"1\":{\"150\":2,\"297\":2}}],[\"availability\",{\"1\":{\"212\":1,\"396\":1,\"465\":1}}],[\"available\",{\"1\":{\"104\":1,\"123\":1,\"137\":1,\"139\":1,\"159\":1,\"180\":1,\"220\":1,\"231\":1,\"235\":1,\"238\":1,\"249\":1,\"258\":1,\"259\":1,\"260\":1,\"316\":1,\"336\":1,\"337\":1,\"338\":1,\"365\":1,\"366\":1,\"375\":1,\"382\":1,\"388\":1,\"393\":1,\"398\":1,\"405\":1,\"413\":1,\"419\":1,\"420\":1,\"427\":1,\"432\":1,\"433\":1,\"444\":1,\"456\":1,\"458\":1,\"465\":1,\"469\":1}}],[\"avatars\",{\"1\":{\"224\":1}}],[\"avatar\",{\"0\":{\"108\":1},\"1\":{\"108\":6}}],[\"agiresearch\",{\"1\":{\"485\":1}}],[\"agichtein\",{\"1\":{\"386\":1}}],[\"aggravated\",{\"1\":{\"469\":1}}],[\"aggregate\",{\"1\":{\"428\":1}}],[\"aggregated\",{\"1\":{\"182\":1,\"479\":1}}],[\"aggregation\",{\"1\":{\"104\":1,\"223\":1}}],[\"aguera\",{\"1\":{\"435\":1}}],[\"agarwal\",{\"1\":{\"374\":1}}],[\"again\",{\"1\":{\"340\":1}}],[\"against\",{\"1\":{\"131\":1,\"183\":1,\"188\":1,\"255\":1,\"302\":1,\"325\":1,\"329\":1,\"334\":1,\"350\":1,\"355\":1,\"396\":1,\"409\":1,\"410\":3,\"418\":1,\"468\":1}}],[\"agnostic\",{\"1\":{\"325\":1,\"327\":1,\"347\":1,\"381\":1,\"388\":1,\"485\":1}}],[\"agness\",{\"1\":{\"178\":1}}],[\"agree\",{\"1\":{\"263\":1}}],[\"agreements\",{\"1\":{\"465\":1}}],[\"agreement\",{\"1\":{\"112\":1,\"263\":1,\"335\":1,\"408\":1}}],[\"ago\",{\"1\":{\"210\":1}}],[\"agora\",{\"0\":{\"202\":1}}],[\"agha\",{\"1\":{\"206\":1}}],[\"agerri\",{\"1\":{\"315\":1}}],[\"agendas\",{\"1\":{\"243\":1}}],[\"agenda\",{\"1\":{\"227\":1,\"364\":1}}],[\"agentbench\",{\"1\":{\"471\":1}}],[\"agentquest\",{\"0\":{\"338\":1},\"1\":{\"338\":3}}],[\"agent\",{\"0\":{\"167\":1,\"264\":1,\"283\":1,\"314\":1,\"361\":1,\"364\":1,\"403\":1,\"415\":1,\"448\":1,\"471\":1,\"477\":1,\"495\":1},\"1\":{\"99\":2,\"116\":1,\"118\":2,\"167\":1,\"218\":3,\"242\":1,\"245\":1,\"264\":1,\"271\":1,\"283\":1,\"288\":1,\"291\":1,\"314\":2,\"318\":4,\"338\":2,\"345\":1,\"361\":4,\"364\":1,\"403\":2,\"415\":8,\"429\":1,\"447\":1,\"448\":1,\"457\":1,\"471\":5,\"476\":1,\"477\":1,\"495\":1}}],[\"agents\",{\"0\":{\"211\":1,\"218\":1,\"264\":1,\"318\":1,\"338\":1,\"415\":1,\"448\":1},\"1\":{\"96\":1,\"99\":1,\"107\":2,\"211\":1,\"218\":11,\"222\":2,\"245\":1,\"264\":3,\"269\":1,\"275\":4,\"283\":2,\"318\":1,\"326\":1,\"338\":1,\"364\":1,\"374\":1,\"399\":1,\"403\":1,\"415\":3,\"429\":1,\"447\":1,\"448\":3,\"471\":3,\"477\":2,\"495\":3}}],[\"ages\",{\"1\":{\"148\":1,\"239\":1,\"333\":1}}],[\"aged\",{\"1\":{\"126\":1}}],[\"age\",{\"0\":{\"519\":1},\"1\":{\"105\":1,\"125\":1,\"126\":1,\"221\":2,\"243\":1,\"248\":1,\"335\":1,\"461\":2,\"482\":1,\"502\":1,\"510\":1}}],[\"achintya\",{\"1\":{\"422\":1,\"436\":1}}],[\"achievable\",{\"1\":{\"115\":1}}],[\"achievements\",{\"1\":{\"273\":1}}],[\"achieve\",{\"1\":{\"123\":1,\"135\":1,\"137\":1,\"194\":1,\"235\":1,\"257\":1,\"265\":1,\"283\":1,\"305\":1,\"337\":1,\"389\":1,\"392\":1,\"403\":1,\"405\":1,\"409\":1,\"411\":1,\"416\":1,\"419\":1,\"420\":1,\"438\":1,\"447\":1,\"462\":1,\"471\":2,\"475\":1}}],[\"achieves\",{\"1\":{\"103\":1,\"161\":1,\"260\":1,\"283\":2,\"284\":1,\"313\":1,\"325\":1,\"354\":1,\"366\":1,\"405\":1,\"411\":1,\"414\":1,\"430\":1,\"459\":1,\"495\":1}}],[\"achieved\",{\"1\":{\"99\":1,\"126\":1,\"176\":1,\"223\":1,\"232\":1,\"314\":1,\"319\":1,\"388\":1,\"393\":1,\"417\":1,\"439\":1,\"449\":1,\"485\":1}}],[\"achieving\",{\"1\":{\"101\":1,\"118\":1,\"137\":1,\"192\":1,\"194\":1,\"234\":1,\"313\":1,\"314\":1,\"347\":1,\"352\":1,\"360\":1,\"418\":1,\"458\":1,\"459\":1}}],[\"academia\",{\"1\":{\"243\":1,\"252\":1,\"272\":1,\"377\":1,\"426\":1,\"472\":1}}],[\"academics\",{\"1\":{\"250\":1}}],[\"academic\",{\"0\":{\"291\":1,\"476\":1},\"1\":{\"126\":1,\"237\":1,\"291\":1,\"307\":1,\"324\":1,\"428\":2,\"439\":1,\"476\":1,\"482\":1}}],[\"acknowledging\",{\"1\":{\"239\":1,\"293\":1}}],[\"acosta\",{\"1\":{\"155\":1}}],[\"acquiring\",{\"1\":{\"284\":1}}],[\"acquire\",{\"1\":{\"137\":1,\"487\":1}}],[\"acquired\",{\"1\":{\"126\":1,\"190\":1,\"251\":1,\"318\":2,\"423\":1}}],[\"acquisitions\",{\"1\":{\"126\":1}}],[\"acquisition\",{\"1\":{\"112\":2,\"120\":1}}],[\"across\",{\"0\":{\"249\":1},\"1\":{\"105\":2,\"107\":1,\"108\":1,\"122\":1,\"123\":1,\"137\":1,\"155\":1,\"164\":1,\"174\":1,\"178\":1,\"185\":1,\"190\":2,\"213\":1,\"215\":1,\"216\":1,\"217\":1,\"237\":1,\"246\":1,\"249\":2,\"254\":1,\"258\":1,\"272\":1,\"290\":2,\"291\":1,\"293\":1,\"302\":1,\"317\":1,\"318\":1,\"319\":2,\"323\":1,\"327\":2,\"328\":2,\"333\":1,\"337\":1,\"344\":1,\"345\":1,\"352\":1,\"359\":1,\"365\":1,\"368\":2,\"371\":1,\"372\":1,\"376\":1,\"390\":1,\"391\":1,\"395\":1,\"399\":1,\"404\":1,\"408\":1,\"411\":1,\"418\":2,\"419\":1,\"420\":1,\"425\":1,\"427\":1,\"428\":1,\"430\":1,\"431\":1,\"433\":1,\"441\":1,\"459\":2,\"460\":1,\"461\":1,\"462\":1,\"465\":1,\"466\":1,\"476\":1,\"478\":1,\"483\":2,\"485\":1,\"494\":1}}],[\"accidents\",{\"1\":{\"298\":6}}],[\"accident\",{\"0\":{\"298\":1},\"1\":{\"298\":1}}],[\"accumulation\",{\"1\":{\"150\":1}}],[\"accumulated\",{\"1\":{\"222\":1}}],[\"accumulate\",{\"1\":{\"131\":1}}],[\"accurate\",{\"0\":{\"469\":1},\"1\":{\"169\":1,\"227\":1,\"237\":1,\"264\":1,\"353\":2,\"416\":1,\"417\":1,\"430\":1,\"442\":1,\"447\":1,\"448\":1,\"450\":1,\"466\":2,\"469\":1,\"489\":1,\"492\":1}}],[\"accurately\",{\"1\":{\"116\":1,\"124\":1,\"228\":1,\"246\":1,\"295\":1,\"298\":1,\"329\":1,\"356\":1,\"376\":1,\"396\":1,\"413\":1,\"416\":1,\"417\":1,\"430\":1,\"435\":1,\"458\":1,\"468\":1,\"487\":1}}],[\"accuracy\",{\"0\":{\"159\":1},\"1\":{\"103\":1,\"113\":1,\"118\":1,\"123\":1,\"133\":1,\"135\":1,\"138\":1,\"159\":2,\"161\":2,\"181\":1,\"183\":1,\"191\":3,\"194\":1,\"198\":1,\"234\":5,\"237\":1,\"295\":1,\"305\":1,\"314\":1,\"331\":1,\"340\":1,\"352\":1,\"354\":2,\"358\":2,\"365\":1,\"368\":1,\"369\":2,\"376\":2,\"387\":1,\"388\":1,\"395\":2,\"405\":2,\"411\":1,\"413\":2,\"415\":1,\"442\":1,\"445\":1,\"458\":1,\"463\":1,\"466\":2,\"467\":1,\"469\":4,\"475\":1,\"477\":1,\"487\":1,\"490\":1,\"492\":2,\"496\":1}}],[\"accompaniment\",{\"0\":{\"269\":1}}],[\"accompanied\",{\"1\":{\"118\":1}}],[\"accompanying\",{\"1\":{\"161\":1,\"354\":1}}],[\"accordingly\",{\"1\":{\"161\":1,\"257\":1,\"354\":1,\"429\":1}}],[\"according\",{\"1\":{\"130\":1,\"143\":1,\"149\":1,\"183\":1,\"255\":1,\"265\":2,\"282\":1,\"335\":1,\"336\":1,\"369\":1,\"396\":1,\"409\":1,\"415\":1}}],[\"accountabilities\",{\"1\":{\"293\":1}}],[\"accountability\",{\"0\":{\"293\":1},\"1\":{\"293\":1}}],[\"accountable\",{\"0\":{\"293\":1}}],[\"accounts\",{\"1\":{\"192\":2,\"230\":1,\"244\":1}}],[\"accounting\",{\"1\":{\"144\":1,\"280\":1}}],[\"account\",{\"1\":{\"123\":1,\"166\":1,\"301\":1}}],[\"accelerometer\",{\"1\":{\"298\":2}}],[\"accelerates\",{\"1\":{\"361\":1}}],[\"accelerated\",{\"1\":{\"260\":1}}],[\"acceleration\",{\"1\":{\"316\":1,\"487\":1}}],[\"accelerating\",{\"0\":{\"487\":1,\"498\":1},\"1\":{\"140\":1,\"175\":1,\"217\":1,\"238\":1,\"294\":1}}],[\"accepting\",{\"1\":{\"221\":1}}],[\"acceptable\",{\"1\":{\"191\":1}}],[\"acceptance\",{\"0\":{\"221\":1},\"1\":{\"178\":1,\"183\":1,\"186\":1,\"196\":2,\"215\":1,\"221\":1,\"293\":1,\"303\":1}}],[\"accepted\",{\"1\":{\"107\":1,\"236\":1}}],[\"accept\",{\"1\":{\"107\":4}}],[\"accessing\",{\"1\":{\"219\":1,\"282\":1,\"401\":1,\"447\":1}}],[\"accessible\",{\"0\":{\"164\":1},\"1\":{\"115\":1,\"139\":1,\"144\":1,\"157\":1,\"180\":1,\"192\":1,\"205\":1,\"214\":1,\"261\":1,\"282\":1,\"284\":2,\"315\":1,\"361\":1,\"399\":1,\"428\":1,\"445\":1}}],[\"accessibility\",{\"0\":{\"105\":1,\"115\":1,\"277\":1},\"1\":{\"105\":3,\"164\":5,\"203\":1,\"277\":1,\"396\":1}}],[\"access\",{\"1\":{\"105\":1,\"140\":1,\"157\":1,\"192\":1,\"258\":1,\"277\":2,\"328\":1,\"349\":1,\"409\":1,\"433\":1,\"435\":1,\"445\":1,\"447\":1,\"462\":1}}],[\"actors\",{\"1\":{\"334\":1}}],[\"actnetformer\",{\"0\":{\"137\":1},\"1\":{\"137\":3}}],[\"activating\",{\"1\":{\"292\":1}}],[\"activations\",{\"1\":{\"456\":2,\"469\":2}}],[\"activation\",{\"0\":{\"199\":1},\"1\":{\"199\":1,\"224\":1,\"390\":1,\"427\":2}}],[\"activated\",{\"1\":{\"292\":1}}],[\"activate\",{\"1\":{\"125\":1,\"292\":1,\"355\":1,\"471\":1}}],[\"actively\",{\"1\":{\"186\":1,\"212\":1,\"326\":1}}],[\"active\",{\"0\":{\"244\":1,\"408\":1},\"1\":{\"182\":1,\"210\":1,\"244\":1,\"390\":1,\"408\":2}}],[\"activities\",{\"1\":{\"118\":1,\"128\":1,\"132\":1,\"140\":1,\"173\":1,\"182\":1,\"184\":1,\"205\":1,\"214\":5,\"243\":1,\"284\":1,\"292\":2,\"295\":1,\"330\":1,\"376\":1,\"380\":1}}],[\"activity\",{\"1\":{\"116\":1,\"137\":1,\"202\":1,\"230\":1,\"277\":1}}],[\"actionable\",{\"1\":{\"158\":1,\"217\":1,\"381\":1}}],[\"action\",{\"0\":{\"118\":1,\"137\":1,\"217\":1,\"450\":1},\"1\":{\"118\":2,\"121\":1,\"137\":7,\"149\":1,\"178\":1,\"217\":3,\"218\":1,\"240\":1,\"284\":1,\"286\":1,\"314\":2,\"326\":1,\"376\":1,\"450\":5,\"474\":1}}],[\"actions\",{\"1\":{\"100\":1,\"118\":1,\"121\":3,\"149\":3,\"186\":1,\"200\":1,\"259\":1,\"284\":1,\"291\":1,\"303\":1,\"326\":4,\"381\":1,\"429\":1,\"476\":1}}],[\"acts\",{\"1\":{\"116\":1}}],[\"actual\",{\"1\":{\"112\":1,\"156\":1,\"172\":1,\"215\":1,\"370\":1,\"454\":1}}],[\"act\",{\"0\":{\"446\":1},\"1\":{\"104\":1,\"122\":1,\"275\":1}}],[\"alperen\",{\"1\":{\"379\":1}}],[\"alkhaled\",{\"1\":{\"369\":1}}],[\"ala\",{\"1\":{\"461\":1}}],[\"alarming\",{\"1\":{\"327\":1}}],[\"alan\",{\"1\":{\"110\":1,\"321\":1}}],[\"alfworld\",{\"1\":{\"283\":1}}],[\"almost\",{\"1\":{\"266\":1}}],[\"almeida\",{\"1\":{\"146\":1}}],[\"alyssa\",{\"1\":{\"257\":1}}],[\"al\",{\"1\":{\"215\":1,\"298\":1,\"404\":1}}],[\"algae\",{\"1\":{\"207\":1}}],[\"algorithmic\",{\"0\":{\"158\":1,\"296\":1},\"1\":{\"148\":3,\"158\":1,\"196\":1}}],[\"algorithm\",{\"0\":{\"148\":1},\"1\":{\"116\":1,\"118\":1,\"139\":2,\"148\":2,\"196\":2,\"269\":2,\"273\":1,\"313\":1,\"399\":1,\"409\":1,\"410\":5,\"414\":1,\"462\":1}}],[\"algorithms\",{\"1\":{\"103\":1,\"118\":1,\"143\":1,\"167\":3,\"186\":1,\"188\":1,\"269\":1,\"343\":1,\"353\":1,\"366\":2,\"368\":1,\"394\":1,\"403\":1,\"410\":3,\"411\":1,\"443\":1,\"462\":1,\"488\":1}}],[\"aliannejadi\",{\"1\":{\"481\":1}}],[\"alistarh\",{\"1\":{\"456\":1}}],[\"alison\",{\"1\":{\"199\":1,\"278\":1}}],[\"alireza\",{\"1\":{\"333\":1,\"461\":1}}],[\"ali\",{\"1\":{\"206\":1,\"257\":1,\"453\":1}}],[\"alike\",{\"1\":{\"205\":1}}],[\"aligning\",{\"1\":{\"334\":1,\"396\":1,\"399\":2,\"468\":1}}],[\"aligns\",{\"1\":{\"136\":1,\"157\":1,\"172\":1,\"195\":1,\"340\":1,\"370\":1,\"386\":1,\"396\":1,\"400\":1}}],[\"alignment\",{\"0\":{\"163\":1,\"355\":1,\"453\":1,\"485\":1},\"1\":{\"127\":1,\"132\":2,\"216\":1,\"313\":1,\"330\":2,\"344\":1,\"355\":2,\"374\":1,\"377\":1,\"378\":4,\"403\":1,\"411\":1,\"418\":1,\"438\":2,\"453\":5,\"462\":1,\"463\":1,\"489\":1}}],[\"aligned\",{\"0\":{\"409\":1},\"1\":{\"124\":1,\"132\":1,\"169\":1,\"235\":1,\"313\":2,\"330\":1,\"344\":1,\"409\":1,\"418\":1,\"445\":1}}],[\"align\",{\"1\":{\"97\":1,\"189\":1,\"309\":1,\"312\":1,\"379\":1,\"485\":1}}],[\"alberto\",{\"1\":{\"292\":1,\"315\":1,\"365\":1}}],[\"albers\",{\"1\":{\"197\":1}}],[\"albeit\",{\"1\":{\"257\":1}}],[\"albaba\",{\"1\":{\"116\":1,\"118\":1}}],[\"altiok\",{\"1\":{\"336\":1}}],[\"alteration\",{\"1\":{\"460\":3}}],[\"alterations\",{\"1\":{\"260\":1}}],[\"altera\",{\"1\":{\"363\":1}}],[\"alternate\",{\"1\":{\"284\":1}}],[\"alternatives\",{\"1\":{\"385\":1,\"444\":1,\"447\":1}}],[\"alternative\",{\"1\":{\"122\":1,\"155\":1,\"158\":1,\"176\":1,\"194\":1,\"216\":1,\"280\":1,\"340\":1}}],[\"alter\",{\"1\":{\"243\":1,\"460\":1}}],[\"altering\",{\"1\":{\"170\":1,\"191\":1,\"390\":1}}],[\"although\",{\"1\":{\"160\":2,\"173\":1,\"196\":1,\"251\":1,\"293\":1,\"339\":1,\"341\":1,\"413\":1,\"417\":1,\"423\":1,\"461\":1}}],[\"always\",{\"0\":{\"159\":1,\"482\":1},\"1\":{\"132\":1,\"189\":1,\"330\":1,\"402\":1}}],[\"alerts\",{\"1\":{\"477\":1}}],[\"alerting\",{\"1\":{\"298\":1}}],[\"alemi\",{\"1\":{\"389\":1}}],[\"aleksander\",{\"1\":{\"263\":1}}],[\"alec\",{\"1\":{\"258\":1,\"375\":1,\"433\":1}}],[\"alex\",{\"1\":{\"231\":1,\"291\":1,\"389\":1,\"468\":1,\"476\":1,\"533\":1}}],[\"alexandros\",{\"1\":{\"123\":1}}],[\"alexander\",{\"1\":{\"101\":1,\"114\":1,\"182\":1,\"220\":1,\"294\":1,\"410\":1}}],[\"alessio\",{\"1\":{\"339\":1}}],[\"alessia\",{\"1\":{\"120\":1}}],[\"alessandro\",{\"1\":{\"171\":1,\"343\":1}}],[\"alesandra\",{\"1\":{\"164\":1}}],[\"alva\",{\"1\":{\"261\":1}}],[\"alvari\",{\"1\":{\"120\":1}}],[\"alvitta\",{\"1\":{\"100\":1}}],[\"alone\",{\"1\":{\"228\":1,\"375\":2}}],[\"alonso\",{\"1\":{\"181\":1}}],[\"along\",{\"1\":{\"117\":1,\"122\":1,\"123\":1,\"150\":1,\"205\":1,\"242\":1,\"292\":1,\"381\":1,\"392\":1,\"462\":1,\"489\":1}}],[\"alongside\",{\"1\":{\"114\":1,\"252\":1,\"269\":1,\"317\":1,\"426\":1,\"485\":1,\"498\":1}}],[\"aloud\",{\"1\":{\"98\":1,\"106\":1,\"228\":1}}],[\"already\",{\"1\":{\"101\":1,\"153\":1,\"179\":1,\"229\":1,\"286\":1,\"474\":1}}],[\"allen\",{\"1\":{\"388\":1}}],[\"alleviating\",{\"1\":{\"362\":1}}],[\"alleviates\",{\"1\":{\"196\":1}}],[\"alleviate\",{\"1\":{\"101\":1,\"223\":1,\"294\":1,\"329\":1,\"355\":1,\"478\":1}}],[\"allison\",{\"1\":{\"220\":1,\"269\":1}}],[\"allocation\",{\"1\":{\"366\":1,\"495\":1}}],[\"allocations\",{\"0\":{\"185\":1},\"1\":{\"185\":1}}],[\"allowed\",{\"1\":{\"183\":1}}],[\"allow\",{\"1\":{\"128\":1,\"173\":1,\"230\":1,\"282\":1,\"306\":1,\"381\":1,\"469\":1}}],[\"allowing\",{\"0\":{\"159\":1},\"1\":{\"111\":1,\"159\":1,\"186\":1,\"233\":1,\"237\":1,\"266\":1,\"289\":1,\"294\":1,\"393\":1,\"415\":1,\"479\":1,\"487\":1}}],[\"allows\",{\"0\":{\"188\":1},\"1\":{\"111\":1,\"113\":1,\"141\":1,\"146\":1,\"190\":1,\"192\":1,\"213\":1,\"226\":1,\"246\":1,\"264\":1,\"282\":1,\"296\":1,\"316\":1,\"335\":1,\"381\":1,\"385\":1,\"399\":2,\"403\":1,\"411\":1,\"448\":1,\"479\":1}}],[\"all\",{\"0\":{\"329\":1,\"455\":1},\"1\":{\"98\":1,\"112\":1,\"123\":1,\"148\":1,\"149\":1,\"151\":2,\"166\":1,\"204\":1,\"216\":1,\"218\":1,\"251\":1,\"259\":1,\"284\":2,\"298\":1,\"302\":1,\"315\":1,\"323\":1,\"327\":1,\"333\":1,\"335\":2,\"340\":1,\"348\":2,\"350\":1,\"355\":1,\"361\":1,\"376\":1,\"391\":1,\"396\":1,\"409\":1,\"418\":1,\"419\":1,\"420\":1,\"423\":1,\"427\":1,\"443\":1,\"455\":2,\"456\":2,\"458\":1,\"463\":1,\"494\":1,\"496\":1,\"498\":1}}],[\"alsobay\",{\"1\":{\"215\":1}}],[\"alsoubai\",{\"1\":{\"206\":1}}],[\"also\",{\"1\":{\"96\":1,\"100\":1,\"107\":2,\"114\":1,\"124\":1,\"128\":1,\"142\":1,\"148\":1,\"149\":1,\"151\":1,\"152\":1,\"153\":2,\"155\":1,\"158\":1,\"172\":1,\"191\":1,\"197\":1,\"200\":1,\"204\":2,\"211\":1,\"215\":1,\"218\":2,\"219\":1,\"227\":1,\"230\":1,\"231\":1,\"236\":2,\"238\":1,\"240\":1,\"245\":1,\"252\":1,\"253\":2,\"256\":2,\"266\":1,\"268\":1,\"275\":1,\"277\":1,\"279\":1,\"280\":1,\"288\":1,\"289\":1,\"295\":1,\"302\":1,\"304\":1,\"306\":1,\"307\":1,\"313\":1,\"318\":1,\"324\":1,\"325\":1,\"327\":1,\"333\":1,\"340\":1,\"343\":1,\"344\":3,\"347\":1,\"348\":1,\"356\":1,\"358\":1,\"368\":1,\"370\":1,\"376\":2,\"381\":1,\"393\":1,\"394\":1,\"398\":1,\"401\":1,\"409\":1,\"417\":2,\"426\":1,\"434\":2,\"441\":1,\"443\":1,\"445\":1,\"450\":1,\"452\":1,\"459\":1,\"460\":1,\"461\":1,\"463\":1,\"466\":1,\"475\":1,\"477\":1,\"488\":1,\"492\":1,\"495\":2,\"497\":1,\"498\":1}}],[\"atop\",{\"1\":{\"487\":1}}],[\"atomic\",{\"1\":{\"112\":2,\"141\":1,\"479\":1}}],[\"ata\",{\"1\":{\"399\":1}}],[\"ataallah\",{\"1\":{\"393\":1}}],[\"atutxa\",{\"1\":{\"315\":1}}],[\"atypical\",{\"1\":{\"131\":2}}],[\"attainment\",{\"1\":{\"414\":1}}],[\"attain\",{\"1\":{\"362\":1,\"496\":1}}],[\"attacked\",{\"1\":{\"453\":2}}],[\"attacker\",{\"0\":{\"403\":1},\"1\":{\"403\":3,\"410\":1}}],[\"attackers\",{\"1\":{\"334\":2,\"403\":4}}],[\"attacks\",{\"0\":{\"409\":1},\"1\":{\"327\":1,\"374\":3,\"403\":1,\"409\":4,\"410\":2,\"418\":2,\"491\":2}}],[\"attacking\",{\"1\":{\"313\":1}}],[\"attack\",{\"0\":{\"334\":2,\"418\":1},\"1\":{\"313\":3,\"334\":6,\"344\":1,\"403\":3,\"409\":3,\"418\":3,\"453\":2,\"491\":1}}],[\"attitudes\",{\"1\":{\"164\":1,\"221\":1}}],[\"attributor\",{\"0\":{\"258\":1,\"433\":1},\"1\":{\"258\":3,\"433\":3}}],[\"attributes\",{\"1\":{\"221\":1,\"272\":1,\"394\":2}}],[\"attribute\",{\"1\":{\"219\":1,\"258\":1,\"401\":1,\"433\":1}}],[\"attributed\",{\"1\":{\"203\":1,\"257\":1,\"411\":1,\"454\":1}}],[\"attributions\",{\"1\":{\"258\":1,\"433\":1}}],[\"attribution\",{\"0\":{\"258\":1,\"369\":1,\"433\":1},\"1\":{\"159\":2,\"258\":2,\"369\":2,\"433\":2}}],[\"attracted\",{\"1\":{\"432\":1,\"450\":1}}],[\"attractive\",{\"1\":{\"289\":1}}],[\"attractiveness\",{\"1\":{\"158\":2}}],[\"attract\",{\"1\":{\"117\":1,\"127\":1}}],[\"attentive\",{\"1\":{\"273\":2}}],[\"attention\",{\"0\":{\"273\":1,\"279\":1,\"462\":1},\"1\":{\"103\":1,\"127\":1,\"129\":1,\"143\":1,\"159\":2,\"174\":1,\"181\":1,\"204\":1,\"213\":1,\"223\":1,\"273\":7,\"279\":4,\"280\":1,\"298\":1,\"322\":1,\"349\":1,\"355\":1,\"362\":1,\"366\":1,\"371\":1,\"425\":1,\"432\":1,\"450\":1,\"456\":1,\"462\":6,\"472\":1}}],[\"attentional\",{\"0\":{\"103\":1},\"1\":{\"103\":2}}],[\"attending\",{\"1\":{\"206\":1}}],[\"attempted\",{\"1\":{\"366\":1}}],[\"attempting\",{\"1\":{\"279\":1}}],[\"attempts\",{\"1\":{\"133\":1,\"196\":1,\"283\":1,\"322\":1,\"331\":1,\"334\":1}}],[\"attempt\",{\"1\":{\"122\":1,\"302\":1,\"486\":1}}],[\"atb\",{\"1\":{\"99\":3}}],[\"at\",{\"0\":{\"179\":1,\"213\":1,\"293\":1,\"351\":1,\"352\":1,\"372\":1,\"388\":1,\"391\":1,\"404\":1,\"439\":1},\"1\":{\"97\":1,\"102\":1,\"104\":1,\"114\":1,\"132\":1,\"133\":1,\"135\":1,\"137\":3,\"156\":1,\"158\":1,\"159\":1,\"160\":1,\"171\":1,\"172\":2,\"179\":2,\"184\":1,\"186\":1,\"188\":1,\"191\":1,\"200\":1,\"204\":1,\"213\":1,\"227\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"241\":1,\"243\":1,\"249\":2,\"258\":2,\"259\":1,\"260\":1,\"261\":1,\"265\":2,\"277\":1,\"279\":1,\"302\":1,\"307\":1,\"308\":1,\"312\":1,\"316\":1,\"319\":1,\"322\":2,\"325\":1,\"326\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"337\":1,\"351\":1,\"355\":1,\"360\":1,\"366\":1,\"370\":2,\"372\":1,\"375\":1,\"380\":1,\"385\":1,\"391\":2,\"393\":1,\"395\":2,\"396\":5,\"408\":1,\"409\":1,\"412\":1,\"418\":1,\"419\":1,\"420\":1,\"427\":5,\"428\":1,\"431\":1,\"432\":1,\"433\":2,\"435\":1,\"436\":1,\"443\":3,\"444\":1,\"446\":1,\"447\":1,\"456\":2,\"460\":1,\"469\":1,\"478\":1,\"482\":1,\"485\":1,\"494\":1,\"495\":1}}],[\"asif\",{\"1\":{\"453\":1}}],[\"asier\",{\"1\":{\"186\":1}}],[\"aso\",{\"1\":{\"365\":1}}],[\"astonishing\",{\"1\":{\"355\":1}}],[\"aseem\",{\"1\":{\"434\":1}}],[\"ase\",{\"0\":{\"355\":1},\"1\":{\"355\":3}}],[\"asgari\",{\"1\":{\"333\":1}}],[\"asymmetries\",{\"1\":{\"293\":1}}],[\"asr\",{\"1\":{\"232\":1,\"313\":2}}],[\"aspires\",{\"1\":{\"216\":1}}],[\"aspect\",{\"1\":{\"130\":1,\"151\":2,\"297\":1,\"328\":1,\"348\":2,\"470\":1,\"478\":1}}],[\"aspects\",{\"1\":{\"119\":1,\"130\":1,\"137\":1,\"142\":1,\"146\":1,\"242\":1,\"297\":1,\"300\":1,\"301\":1,\"335\":1,\"362\":1,\"363\":1,\"396\":1,\"398\":1,\"400\":2,\"413\":2,\"442\":1,\"456\":1}}],[\"asap\",{\"0\":{\"213\":1},\"1\":{\"213\":5}}],[\"ashkboos\",{\"1\":{\"456\":1}}],[\"ashutosh\",{\"1\":{\"372\":1}}],[\"ashwaq\",{\"1\":{\"206\":1}}],[\"ashley\",{\"1\":{\"166\":1,\"248\":1,\"252\":1,\"426\":1}}],[\"asks\",{\"1\":{\"368\":1}}],[\"asking\",{\"1\":{\"234\":1,\"368\":1}}],[\"ask\",{\"1\":{\"172\":1,\"217\":1,\"370\":1,\"422\":1,\"431\":1}}],[\"asked\",{\"1\":{\"107\":1,\"263\":1,\"369\":1}}],[\"asd\",{\"1\":{\"120\":4,\"261\":1}}],[\"assembly\",{\"1\":{\"442\":1}}],[\"assets\",{\"1\":{\"399\":1}}],[\"assessable\",{\"1\":{\"391\":1}}],[\"assesses\",{\"1\":{\"245\":1,\"490\":1}}],[\"assessed\",{\"1\":{\"133\":1,\"273\":1,\"331\":1,\"356\":1}}],[\"assessing\",{\"0\":{\"182\":1,\"490\":1},\"1\":{\"133\":1,\"182\":2,\"190\":1,\"210\":1,\"212\":1,\"280\":2,\"291\":1,\"331\":1,\"333\":1,\"365\":1,\"404\":1,\"476\":1}}],[\"assessments\",{\"1\":{\"120\":1,\"129\":1,\"289\":1,\"356\":1,\"446\":1}}],[\"assessment\",{\"0\":{\"125\":1,\"246\":1},\"1\":{\"105\":2,\"127\":1,\"156\":1,\"160\":1,\"166\":1,\"446\":1,\"524\":1}}],[\"assess\",{\"0\":{\"400\":1},\"1\":{\"96\":1,\"118\":1,\"152\":1,\"166\":2,\"236\":2,\"238\":1,\"245\":2,\"268\":1,\"270\":1,\"289\":1,\"292\":2,\"303\":1,\"337\":1,\"340\":1,\"345\":1,\"390\":1,\"394\":1,\"400\":1,\"408\":1,\"417\":1,\"452\":1,\"491\":1}}],[\"assurance\",{\"1\":{\"360\":1,\"495\":1}}],[\"assumption\",{\"1\":{\"209\":1}}],[\"assumptions\",{\"1\":{\"175\":1}}],[\"assumed\",{\"1\":{\"112\":1}}],[\"assaf\",{\"1\":{\"292\":1,\"454\":1}}],[\"assign\",{\"1\":{\"378\":1,\"432\":1}}],[\"assigned\",{\"1\":{\"281\":1}}],[\"assignment\",{\"1\":{\"172\":1,\"370\":1}}],[\"assignments\",{\"1\":{\"172\":1,\"370\":1}}],[\"assi\",{\"1\":{\"176\":1}}],[\"assist\",{\"1\":{\"215\":1,\"252\":1,\"272\":1,\"274\":1,\"300\":1,\"403\":1,\"426\":1,\"445\":1,\"466\":1,\"486\":1}}],[\"assistive\",{\"0\":{\"184\":1,\"261\":1,\"380\":1,\"396\":1},\"1\":{\"184\":4,\"261\":1,\"380\":4,\"396\":1}}],[\"assisting\",{\"0\":{\"300\":1},\"1\":{\"172\":1,\"292\":1,\"370\":1}}],[\"assistance\",{\"1\":{\"127\":1,\"133\":3,\"166\":1,\"176\":1,\"222\":1,\"228\":1,\"324\":1,\"331\":3,\"339\":1,\"398\":1,\"447\":1,\"486\":2,\"491\":1}}],[\"assistants\",{\"0\":{\"136\":1,\"252\":1,\"426\":1},\"1\":{\"201\":2,\"215\":1,\"282\":1,\"461\":1,\"481\":1,\"486\":1}}],[\"assistant\",{\"0\":{\"127\":1},\"1\":{\"232\":1,\"291\":1,\"476\":1,\"486\":2}}],[\"assisted\",{\"0\":{\"133\":1,\"163\":1,\"331\":1,\"398\":1},\"1\":{\"124\":2,\"165\":1,\"342\":1,\"361\":1}}],[\"associate\",{\"1\":{\"242\":1}}],[\"associated\",{\"1\":{\"111\":1,\"149\":1,\"166\":1,\"178\":1,\"198\":1,\"257\":1,\"259\":1,\"270\":1,\"288\":1,\"298\":1,\"306\":1,\"326\":1,\"344\":1,\"387\":1,\"408\":1,\"428\":1}}],[\"associations\",{\"1\":{\"302\":1,\"461\":1}}],[\"association\",{\"1\":{\"146\":1}}],[\"as\",{\"0\":{\"117\":1,\"148\":1,\"157\":1,\"175\":1,\"184\":1,\"252\":1,\"286\":1,\"291\":1,\"308\":1,\"324\":1,\"380\":1,\"394\":1,\"426\":1,\"429\":1,\"474\":1,\"476\":1,\"481\":1,\"496\":1},\"1\":{\"96\":2,\"97\":3,\"98\":3,\"105\":4,\"107\":1,\"110\":1,\"116\":2,\"119\":2,\"120\":1,\"122\":2,\"123\":2,\"126\":3,\"127\":1,\"128\":3,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"135\":2,\"136\":4,\"139\":1,\"143\":1,\"146\":1,\"148\":2,\"149\":3,\"151\":1,\"153\":1,\"155\":2,\"157\":1,\"158\":1,\"159\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":2,\"167\":1,\"172\":1,\"175\":3,\"176\":1,\"179\":1,\"181\":2,\"182\":2,\"184\":3,\"185\":2,\"187\":1,\"189\":2,\"192\":1,\"194\":1,\"195\":1,\"196\":4,\"197\":1,\"198\":2,\"201\":1,\"204\":1,\"206\":2,\"207\":1,\"210\":1,\"211\":1,\"213\":3,\"215\":4,\"216\":2,\"217\":2,\"218\":1,\"220\":1,\"221\":3,\"222\":1,\"223\":1,\"224\":1,\"227\":2,\"231\":2,\"232\":1,\"234\":3,\"236\":2,\"237\":1,\"238\":3,\"239\":2,\"240\":2,\"241\":1,\"242\":1,\"243\":1,\"245\":2,\"249\":2,\"254\":2,\"255\":2,\"256\":3,\"257\":1,\"260\":2,\"261\":2,\"264\":2,\"269\":6,\"270\":1,\"273\":1,\"274\":2,\"275\":1,\"278\":1,\"279\":2,\"280\":2,\"281\":1,\"282\":2,\"284\":1,\"287\":1,\"288\":2,\"289\":2,\"290\":1,\"292\":1,\"293\":1,\"294\":1,\"296\":1,\"297\":1,\"298\":1,\"301\":1,\"302\":2,\"303\":1,\"304\":1,\"307\":3,\"308\":4,\"309\":1,\"312\":3,\"313\":2,\"315\":1,\"317\":1,\"318\":1,\"321\":1,\"322\":1,\"326\":2,\"330\":1,\"333\":3,\"334\":2,\"335\":2,\"336\":3,\"337\":2,\"338\":1,\"339\":1,\"340\":1,\"343\":1,\"344\":1,\"345\":2,\"347\":2,\"348\":1,\"349\":3,\"350\":2,\"354\":1,\"356\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"365\":2,\"369\":3,\"370\":1,\"374\":3,\"376\":1,\"380\":3,\"381\":1,\"384\":2,\"387\":2,\"389\":2,\"392\":1,\"394\":3,\"395\":2,\"399\":2,\"402\":3,\"403\":3,\"405\":2,\"408\":1,\"409\":1,\"412\":1,\"414\":1,\"417\":1,\"418\":2,\"422\":5,\"427\":2,\"429\":1,\"430\":1,\"431\":1,\"434\":6,\"436\":1,\"439\":3,\"441\":1,\"442\":1,\"443\":2,\"444\":1,\"445\":2,\"447\":3,\"448\":2,\"450\":2,\"453\":4,\"454\":2,\"455\":4,\"456\":2,\"458\":1,\"459\":2,\"460\":2,\"461\":2,\"462\":2,\"463\":2,\"465\":3,\"467\":1,\"468\":3,\"470\":1,\"471\":5,\"475\":4,\"478\":1,\"479\":1,\"480\":4,\"482\":2,\"483\":1,\"485\":3,\"486\":1,\"488\":2,\"489\":1,\"490\":1,\"494\":1,\"495\":1,\"496\":2,\"497\":3}}],[\"abbasiantaeb\",{\"1\":{\"481\":1}}],[\"abaskohi\",{\"1\":{\"404\":1}}],[\"abdelrahman\",{\"1\":{\"393\":1}}],[\"abdul\",{\"1\":{\"284\":1}}],[\"abeyratne\",{\"1\":{\"384\":1}}],[\"aberration\",{\"1\":{\"112\":1}}],[\"above\",{\"1\":{\"350\":1,\"402\":1,\"453\":2}}],[\"about\",{\"0\":{\"418\":1},\"1\":{\"74\":1,\"96\":1,\"100\":2,\"117\":1,\"128\":1,\"156\":2,\"164\":1,\"172\":1,\"175\":1,\"178\":1,\"209\":2,\"210\":1,\"212\":1,\"213\":1,\"214\":2,\"232\":1,\"236\":1,\"243\":1,\"248\":1,\"252\":1,\"255\":1,\"258\":1,\"265\":1,\"303\":1,\"307\":1,\"308\":1,\"341\":1,\"353\":1,\"368\":1,\"370\":1,\"376\":2,\"418\":1,\"426\":1,\"428\":1,\"433\":1,\"437\":1,\"439\":1,\"461\":1,\"467\":1}}],[\"abrego\",{\"1\":{\"419\":1,\"420\":1}}],[\"abriele\",{\"1\":{\"350\":1}}],[\"abraham\",{\"1\":{\"234\":1}}],[\"abualhaija\",{\"1\":{\"339\":1}}],[\"abundance\",{\"1\":{\"242\":1}}],[\"abnormal\",{\"1\":{\"223\":1}}],[\"able\",{\"1\":{\"198\":1,\"227\":1,\"257\":2,\"286\":1,\"302\":1,\"347\":1,\"382\":1,\"387\":1,\"416\":1,\"435\":1,\"437\":1,\"456\":1,\"474\":1,\"497\":1}}],[\"ablation\",{\"1\":{\"132\":1,\"218\":1,\"240\":1,\"330\":1}}],[\"abin\",{\"1\":{\"167\":2}}],[\"abilities\",{\"0\":{\"215\":1},\"1\":{\"138\":1,\"197\":1,\"264\":1,\"271\":1,\"292\":1,\"314\":1,\"318\":1,\"340\":1,\"345\":1,\"364\":1,\"402\":1,\"412\":1,\"417\":2,\"448\":1,\"457\":1,\"471\":1,\"475\":1,\"482\":1,\"487\":1,\"490\":1,\"497\":1}}],[\"ability\",{\"1\":{\"119\":1,\"124\":1,\"125\":1,\"130\":1,\"151\":2,\"169\":1,\"173\":1,\"204\":1,\"213\":1,\"215\":2,\"233\":1,\"243\":1,\"252\":2,\"265\":1,\"273\":1,\"274\":1,\"275\":1,\"279\":1,\"283\":1,\"317\":1,\"328\":1,\"329\":1,\"348\":2,\"386\":1,\"403\":1,\"404\":1,\"412\":1,\"413\":2,\"425\":1,\"426\":2,\"430\":1,\"435\":1,\"436\":1,\"439\":1,\"444\":1,\"462\":1,\"466\":1,\"468\":3,\"471\":1,\"482\":2,\"487\":1,\"488\":1,\"489\":1,\"492\":1}}],[\"absolute\",{\"1\":{\"376\":1,\"419\":1,\"420\":1,\"492\":2}}],[\"absent\",{\"1\":{\"198\":1,\"387\":1}}],[\"absence\",{\"1\":{\"124\":1,\"266\":1,\"333\":1,\"408\":1}}],[\"abstraction\",{\"0\":{\"282\":1},\"1\":{\"282\":1}}],[\"abstract\",{\"0\":{\"432\":1},\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":2,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"430\":1,\"431\":1,\"432\":2,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"abs\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"a是art\",{\"1\":{\"77\":1}}],[\"ankit\",{\"1\":{\"458\":1}}],[\"anke\",{\"1\":{\"400\":1}}],[\"anschütz\",{\"1\":{\"327\":1}}],[\"answer\",{\"0\":{\"481\":1},\"1\":{\"141\":1,\"258\":1,\"350\":1,\"376\":1,\"385\":1,\"393\":1,\"433\":1,\"458\":2,\"466\":3,\"475\":1,\"492\":1}}],[\"answers\",{\"0\":{\"431\":1},\"1\":{\"102\":1,\"152\":1,\"234\":1,\"282\":1,\"319\":1,\"333\":1,\"340\":1,\"384\":1,\"431\":5,\"458\":2,\"494\":2}}],[\"answering\",{\"0\":{\"384\":1,\"430\":1},\"1\":{\"102\":1,\"185\":1,\"245\":1,\"319\":1,\"340\":1,\"363\":1,\"368\":1,\"376\":1,\"384\":2,\"424\":1,\"431\":1,\"443\":1,\"458\":2}}],[\"anitamalina\",{\"1\":{\"296\":1}}],[\"animals\",{\"1\":{\"256\":1}}],[\"anil\",{\"1\":{\"190\":1,\"195\":1,\"289\":1}}],[\"ancient\",{\"1\":{\"243\":1}}],[\"anomalies\",{\"1\":{\"477\":1}}],[\"anomaly\",{\"0\":{\"477\":1},\"1\":{\"477\":3}}],[\"anoop\",{\"1\":{\"265\":1}}],[\"another\",{\"1\":{\"230\":1,\"255\":1,\"334\":1,\"355\":1,\"399\":1,\"470\":1}}],[\"anonymization\",{\"1\":{\"254\":1}}],[\"anonymity\",{\"0\":{\"187\":1},\"1\":{\"187\":2,\"470\":1}}],[\"anonymous\",{\"1\":{\"159\":1,\"196\":1}}],[\"angelic\",{\"1\":{\"350\":1}}],[\"angel\",{\"1\":{\"255\":1}}],[\"angus\",{\"1\":{\"199\":1}}],[\"angles\",{\"1\":{\"103\":2}}],[\"angle\",{\"0\":{\"103\":1},\"1\":{\"246\":3}}],[\"anurakt\",{\"1\":{\"374\":1}}],[\"anuradha\",{\"1\":{\"154\":1}}],[\"anubhav\",{\"1\":{\"169\":1}}],[\"anh\",{\"1\":{\"159\":1}}],[\"anna\",{\"1\":{\"416\":1}}],[\"anne\",{\"1\":{\"384\":1,\"395\":1}}],[\"annie\",{\"1\":{\"156\":1}}],[\"annotator\",{\"0\":{\"238\":1},\"1\":{\"238\":1,\"408\":3}}],[\"annotators\",{\"1\":{\"133\":3,\"140\":1,\"238\":6,\"294\":2,\"331\":3,\"335\":1,\"378\":1}}],[\"annotating\",{\"1\":{\"97\":1,\"238\":1,\"312\":1,\"459\":1}}],[\"annotations\",{\"0\":{\"294\":1,\"408\":1},\"1\":{\"133\":1,\"151\":1,\"182\":1,\"294\":1,\"331\":1,\"344\":1,\"348\":1,\"353\":1,\"391\":1,\"400\":3,\"434\":3,\"455\":1,\"459\":2}}],[\"annotation\",{\"0\":{\"97\":1,\"312\":1,\"459\":1},\"1\":{\"97\":3,\"238\":2,\"312\":3,\"368\":1,\"369\":1,\"398\":1,\"407\":1,\"408\":2,\"459\":2}}],[\"annotated\",{\"1\":{\"137\":1,\"156\":1,\"252\":1,\"369\":2,\"391\":1,\"426\":1,\"490\":1}}],[\"annotate\",{\"1\":{\"97\":1,\"312\":1}}],[\"anandghan\",{\"1\":{\"207\":1}}],[\"anastasia\",{\"1\":{\"163\":1}}],[\"ana\",{\"1\":{\"147\":1}}],[\"analytic\",{\"1\":{\"242\":2}}],[\"analytical\",{\"1\":{\"146\":1,\"317\":1,\"477\":1}}],[\"analytics\",{\"0\":{\"100\":1,\"156\":1,\"195\":1},\"1\":{\"100\":2,\"111\":1,\"137\":1,\"156\":11,\"182\":2,\"195\":4,\"217\":1}}],[\"analysing\",{\"1\":{\"292\":1}}],[\"analysis\",{\"0\":{\"126\":1,\"205\":1,\"213\":1,\"242\":1,\"246\":1,\"253\":1,\"255\":1,\"290\":1,\"292\":1,\"353\":1,\"360\":1},\"1\":{\"98\":1,\"101\":1,\"116\":1,\"119\":1,\"120\":1,\"122\":1,\"130\":1,\"136\":1,\"143\":3,\"146\":3,\"149\":1,\"150\":1,\"156\":1,\"172\":3,\"174\":1,\"179\":1,\"182\":1,\"189\":2,\"205\":2,\"207\":2,\"210\":1,\"213\":2,\"216\":1,\"217\":1,\"219\":1,\"229\":1,\"237\":2,\"239\":2,\"242\":5,\"245\":1,\"246\":2,\"249\":1,\"250\":1,\"252\":3,\"253\":1,\"254\":1,\"255\":2,\"271\":1,\"282\":1,\"290\":1,\"301\":1,\"314\":1,\"317\":1,\"325\":1,\"327\":1,\"335\":1,\"339\":2,\"345\":1,\"351\":1,\"353\":1,\"358\":1,\"360\":1,\"370\":3,\"371\":1,\"372\":1,\"385\":1,\"389\":1,\"390\":2,\"394\":1,\"400\":1,\"401\":1,\"412\":1,\"424\":2,\"426\":3,\"428\":2,\"434\":1,\"437\":1,\"445\":1,\"457\":1,\"458\":3,\"459\":1,\"460\":2,\"466\":1,\"470\":1,\"472\":1,\"477\":1,\"488\":3,\"492\":3}}],[\"analyse\",{\"1\":{\"221\":1,\"477\":1}}],[\"analyses\",{\"1\":{\"111\":1,\"139\":1,\"148\":1,\"246\":1,\"333\":1,\"356\":1,\"390\":2,\"455\":1,\"492\":1}}],[\"analysts\",{\"1\":{\"119\":1,\"183\":1,\"242\":2}}],[\"analyzing\",{\"0\":{\"106\":1,\"172\":1,\"252\":1,\"370\":1,\"426\":1},\"1\":{\"122\":1,\"172\":1,\"182\":1,\"190\":1,\"250\":1,\"353\":1,\"370\":1,\"400\":1,\"488\":1}}],[\"analyzes\",{\"1\":{\"219\":1,\"390\":1,\"401\":1}}],[\"analyzed\",{\"1\":{\"102\":1,\"120\":1,\"148\":1,\"149\":1,\"185\":1,\"304\":1,\"308\":1,\"437\":1}}],[\"analyze\",{\"1\":{\"96\":1,\"141\":1,\"146\":1,\"174\":1,\"246\":1,\"288\":1,\"305\":1,\"328\":1,\"335\":1,\"371\":1,\"441\":1,\"466\":1,\"492\":1,\"495\":2}}],[\"anthropic\",{\"1\":{\"291\":1,\"418\":1,\"476\":1}}],[\"anthropomorphism\",{\"0\":{\"263\":1},\"1\":{\"263\":2}}],[\"anthony\",{\"1\":{\"176\":1}}],[\"antiretroviral\",{\"1\":{\"178\":1}}],[\"anticipating\",{\"1\":{\"429\":1}}],[\"anticipation\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"anticipate\",{\"1\":{\"97\":1,\"271\":1,\"312\":1,\"457\":1}}],[\"antecedents\",{\"1\":{\"244\":2}}],[\"ante\",{\"1\":{\"159\":1}}],[\"anton\",{\"1\":{\"399\":1}}],[\"antons\",{\"1\":{\"203\":1}}],[\"antoniak\",{\"1\":{\"133\":1,\"331\":1}}],[\"antonette\",{\"1\":{\"125\":1}}],[\"anymore\",{\"1\":{\"443\":1}}],[\"anything\",{\"1\":{\"430\":1}}],[\"anya\",{\"1\":{\"236\":1}}],[\"anyone\",{\"0\":{\"140\":1},\"1\":{\"381\":1}}],[\"any\",{\"1\":{\"123\":1,\"151\":1,\"234\":1,\"313\":2,\"338\":1,\"348\":1,\"382\":1,\"385\":2,\"403\":1,\"443\":1,\"444\":1,\"453\":1,\"456\":1,\"490\":1}}],[\"anxiety\",{\"1\":{\"106\":1}}],[\"andonie\",{\"1\":{\"255\":1}}],[\"anderson\",{\"1\":{\"178\":1}}],[\"anders\",{\"1\":{\"176\":1,\"296\":1}}],[\"andriushchenko\",{\"1\":{\"409\":1}}],[\"android\",{\"1\":{\"219\":1,\"401\":1}}],[\"andre\",{\"1\":{\"175\":1,\"458\":1}}],[\"andreas\",{\"1\":{\"244\":1}}],[\"andrea\",{\"1\":{\"158\":1,\"315\":1,\"390\":1,\"466\":1}}],[\"andrew\",{\"1\":{\"107\":1,\"122\":1,\"184\":1,\"268\":1,\"325\":1,\"380\":1,\"398\":1,\"452\":1}}],[\"andruid\",{\"1\":{\"156\":1}}],[\"andrés\",{\"1\":{\"147\":1}}],[\"andré\",{\"1\":{\"96\":1}}],[\"and\",{\"0\":{\"99\":1,\"100\":1,\"101\":2,\"102\":1,\"105\":1,\"119\":2,\"126\":1,\"129\":1,\"144\":1,\"155\":2,\"157\":1,\"158\":1,\"164\":1,\"174\":1,\"182\":1,\"185\":1,\"187\":1,\"189\":1,\"191\":1,\"192\":1,\"195\":1,\"197\":1,\"198\":1,\"203\":1,\"206\":1,\"211\":1,\"212\":1,\"213\":1,\"216\":1,\"217\":1,\"218\":1,\"223\":1,\"226\":1,\"230\":1,\"231\":2,\"234\":1,\"237\":1,\"240\":1,\"242\":1,\"246\":2,\"248\":1,\"252\":1,\"253\":1,\"254\":1,\"263\":1,\"264\":1,\"266\":1,\"268\":2,\"269\":1,\"273\":2,\"277\":1,\"279\":1,\"280\":1,\"281\":1,\"292\":1,\"294\":1,\"297\":1,\"298\":2,\"302\":1,\"304\":1,\"305\":1,\"307\":1,\"313\":2,\"314\":1,\"316\":1,\"317\":2,\"322\":1,\"325\":1,\"326\":1,\"338\":1,\"340\":1,\"345\":1,\"347\":1,\"353\":1,\"364\":1,\"371\":1,\"374\":1,\"387\":1,\"388\":1,\"400\":1,\"402\":1,\"405\":1,\"407\":1,\"415\":1,\"419\":1,\"420\":1,\"426\":1,\"437\":1,\"443\":1,\"446\":3,\"448\":1,\"452\":2,\"453\":1,\"459\":1,\"460\":1,\"468\":1,\"471\":1,\"481\":1,\"490\":1,\"492\":1},\"1\":{\"74\":3,\"96\":5,\"97\":9,\"98\":3,\"99\":10,\"100\":9,\"101\":8,\"102\":6,\"103\":9,\"104\":8,\"105\":12,\"106\":6,\"107\":3,\"108\":6,\"110\":7,\"111\":3,\"112\":7,\"113\":4,\"114\":8,\"115\":1,\"116\":3,\"117\":10,\"118\":3,\"119\":7,\"120\":11,\"121\":2,\"122\":8,\"123\":8,\"124\":2,\"125\":5,\"126\":4,\"127\":9,\"128\":6,\"130\":14,\"131\":5,\"132\":8,\"133\":8,\"135\":4,\"136\":7,\"137\":7,\"138\":6,\"139\":5,\"140\":5,\"141\":4,\"142\":7,\"143\":3,\"144\":11,\"146\":6,\"148\":9,\"149\":4,\"150\":3,\"151\":9,\"152\":6,\"153\":2,\"154\":7,\"155\":10,\"156\":7,\"157\":4,\"158\":4,\"159\":7,\"160\":4,\"161\":2,\"163\":8,\"164\":2,\"165\":5,\"166\":8,\"169\":7,\"170\":3,\"171\":9,\"172\":13,\"173\":12,\"174\":4,\"175\":8,\"176\":5,\"178\":8,\"179\":7,\"180\":6,\"181\":3,\"182\":6,\"183\":5,\"184\":7,\"185\":5,\"186\":7,\"187\":2,\"188\":5,\"189\":8,\"190\":3,\"191\":13,\"192\":7,\"194\":5,\"195\":6,\"196\":4,\"197\":2,\"198\":5,\"199\":7,\"200\":3,\"201\":6,\"202\":4,\"203\":7,\"204\":8,\"205\":15,\"206\":7,\"207\":11,\"209\":4,\"210\":7,\"211\":7,\"212\":6,\"213\":8,\"214\":5,\"215\":1,\"216\":5,\"217\":16,\"218\":11,\"219\":4,\"220\":4,\"221\":6,\"222\":5,\"223\":8,\"224\":2,\"226\":9,\"227\":8,\"228\":5,\"229\":5,\"230\":7,\"231\":6,\"232\":2,\"233\":5,\"234\":5,\"235\":7,\"236\":1,\"237\":11,\"238\":8,\"239\":5,\"240\":7,\"241\":5,\"242\":7,\"243\":11,\"244\":12,\"245\":9,\"246\":10,\"248\":8,\"249\":3,\"250\":3,\"251\":3,\"252\":7,\"253\":13,\"254\":7,\"255\":3,\"256\":5,\"257\":5,\"258\":5,\"259\":6,\"260\":5,\"261\":8,\"263\":3,\"264\":5,\"265\":2,\"266\":7,\"268\":5,\"269\":11,\"270\":6,\"271\":6,\"272\":11,\"273\":4,\"274\":6,\"275\":9,\"277\":6,\"278\":3,\"279\":5,\"280\":3,\"281\":7,\"282\":5,\"283\":6,\"284\":8,\"286\":8,\"287\":3,\"288\":4,\"289\":3,\"290\":4,\"291\":5,\"292\":10,\"293\":8,\"294\":1,\"295\":1,\"296\":4,\"297\":5,\"298\":11,\"300\":4,\"301\":6,\"302\":10,\"303\":3,\"304\":15,\"305\":5,\"306\":6,\"307\":8,\"308\":10,\"309\":5,\"312\":9,\"313\":6,\"314\":5,\"315\":5,\"316\":8,\"317\":4,\"318\":6,\"319\":5,\"321\":7,\"322\":5,\"323\":2,\"324\":2,\"325\":6,\"326\":9,\"327\":5,\"328\":7,\"329\":5,\"330\":8,\"331\":8,\"333\":6,\"334\":11,\"335\":9,\"336\":3,\"337\":7,\"338\":8,\"339\":7,\"340\":6,\"342\":3,\"343\":2,\"344\":5,\"345\":7,\"347\":2,\"348\":9,\"349\":5,\"350\":2,\"351\":1,\"352\":4,\"353\":10,\"354\":2,\"355\":6,\"356\":9,\"358\":7,\"359\":9,\"360\":9,\"361\":9,\"362\":2,\"363\":8,\"364\":5,\"365\":7,\"366\":4,\"368\":4,\"369\":10,\"370\":13,\"371\":4,\"372\":5,\"374\":9,\"375\":7,\"376\":5,\"377\":8,\"378\":2,\"379\":8,\"380\":7,\"381\":6,\"382\":5,\"384\":7,\"385\":1,\"386\":3,\"387\":5,\"388\":7,\"389\":5,\"390\":5,\"391\":10,\"392\":2,\"393\":6,\"394\":5,\"395\":10,\"396\":9,\"398\":13,\"399\":10,\"400\":5,\"401\":4,\"402\":9,\"403\":10,\"404\":4,\"405\":3,\"407\":6,\"408\":7,\"409\":4,\"410\":3,\"411\":7,\"412\":4,\"413\":2,\"414\":9,\"415\":5,\"416\":5,\"417\":10,\"418\":6,\"419\":9,\"420\":9,\"422\":5,\"423\":3,\"424\":9,\"425\":1,\"426\":7,\"427\":3,\"428\":9,\"429\":9,\"430\":8,\"431\":3,\"432\":2,\"433\":5,\"434\":1,\"436\":1,\"437\":9,\"438\":4,\"439\":5,\"441\":4,\"442\":1,\"443\":6,\"444\":4,\"445\":8,\"446\":5,\"447\":10,\"448\":5,\"449\":1,\"450\":1,\"452\":5,\"453\":5,\"454\":1,\"455\":6,\"456\":3,\"457\":6,\"458\":6,\"459\":2,\"460\":7,\"461\":6,\"462\":5,\"463\":1,\"465\":4,\"466\":7,\"467\":3,\"468\":6,\"469\":8,\"470\":7,\"471\":9,\"472\":3,\"474\":8,\"475\":9,\"476\":5,\"477\":5,\"478\":3,\"479\":3,\"480\":3,\"481\":6,\"482\":10,\"483\":3,\"485\":3,\"486\":7,\"487\":3,\"488\":6,\"489\":1,\"490\":4,\"491\":6,\"492\":7,\"494\":6,\"495\":5,\"496\":4,\"497\":4,\"498\":5,\"519\":1}}],[\"an\",{\"0\":{\"128\":1,\"132\":1,\"146\":1,\"149\":1,\"157\":1,\"158\":1,\"161\":1,\"172\":1,\"236\":1,\"243\":1,\"271\":1,\"274\":1,\"278\":1,\"294\":1,\"300\":1,\"308\":1,\"315\":1,\"330\":1,\"335\":1,\"339\":1,\"351\":1,\"354\":1,\"370\":1,\"382\":1,\"394\":1,\"418\":1,\"445\":1,\"457\":1,\"472\":1,\"477\":1,\"486\":1},\"1\":{\"74\":1,\"96\":2,\"100\":1,\"104\":1,\"105\":1,\"110\":3,\"111\":1,\"114\":2,\"116\":2,\"117\":1,\"118\":1,\"123\":1,\"124\":3,\"126\":1,\"128\":4,\"132\":1,\"133\":1,\"135\":1,\"137\":1,\"138\":1,\"140\":1,\"141\":1,\"143\":1,\"146\":2,\"149\":1,\"151\":1,\"155\":1,\"157\":3,\"158\":2,\"159\":3,\"160\":1,\"161\":4,\"163\":1,\"166\":1,\"167\":2,\"169\":1,\"172\":2,\"173\":3,\"174\":1,\"176\":1,\"179\":3,\"181\":1,\"182\":1,\"183\":2,\"184\":3,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"190\":2,\"195\":1,\"196\":1,\"205\":1,\"206\":1,\"207\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"222\":1,\"223\":2,\"226\":1,\"227\":2,\"230\":1,\"231\":1,\"234\":2,\"235\":1,\"236\":1,\"238\":2,\"242\":3,\"243\":2,\"244\":1,\"245\":1,\"246\":2,\"249\":2,\"251\":1,\"252\":5,\"258\":2,\"259\":3,\"260\":1,\"261\":1,\"263\":1,\"266\":1,\"269\":1,\"270\":2,\"273\":1,\"274\":1,\"278\":1,\"279\":2,\"281\":1,\"283\":1,\"284\":2,\"289\":4,\"290\":1,\"291\":6,\"292\":1,\"295\":1,\"296\":4,\"297\":2,\"298\":2,\"300\":2,\"302\":1,\"303\":1,\"305\":2,\"307\":1,\"317\":1,\"318\":1,\"321\":3,\"322\":1,\"325\":1,\"326\":2,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"335\":2,\"336\":1,\"344\":1,\"348\":1,\"349\":4,\"352\":1,\"353\":1,\"354\":4,\"355\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"365\":1,\"368\":1,\"369\":2,\"370\":2,\"371\":1,\"372\":1,\"375\":3,\"377\":1,\"380\":3,\"384\":2,\"385\":1,\"386\":1,\"390\":2,\"394\":2,\"395\":1,\"396\":1,\"399\":2,\"400\":1,\"401\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":2,\"411\":1,\"413\":1,\"422\":3,\"423\":1,\"425\":1,\"426\":5,\"427\":1,\"428\":2,\"430\":3,\"433\":2,\"434\":1,\"438\":1,\"442\":3,\"443\":1,\"445\":2,\"446\":1,\"447\":2,\"450\":3,\"454\":1,\"455\":2,\"459\":1,\"460\":1,\"462\":2,\"466\":1,\"472\":1,\"475\":2,\"476\":6,\"478\":2,\"481\":1,\"482\":1,\"483\":1,\"486\":1,\"487\":1,\"488\":1,\"490\":4,\"492\":2,\"495\":1,\"496\":3}}],[\"adi\",{\"1\":{\"441\":1}}],[\"adithya\",{\"1\":{\"353\":1}}],[\"aditi\",{\"1\":{\"213\":1}}],[\"adit\",{\"1\":{\"99\":1}}],[\"adequately\",{\"1\":{\"408\":1,\"490\":1}}],[\"adept\",{\"1\":{\"393\":1}}],[\"adeptness\",{\"1\":{\"372\":1,\"377\":1,\"424\":1}}],[\"adeptly\",{\"1\":{\"100\":1,\"266\":1,\"481\":1,\"489\":1}}],[\"adewumi\",{\"1\":{\"369\":1}}],[\"adjacent\",{\"1\":{\"301\":1}}],[\"adjustment\",{\"1\":{\"454\":1}}],[\"adjusting\",{\"1\":{\"429\":1}}],[\"adjust\",{\"1\":{\"120\":1,\"219\":1,\"401\":1}}],[\"adhesive\",{\"1\":{\"295\":1}}],[\"adhering\",{\"1\":{\"128\":1,\"344\":1}}],[\"adhere\",{\"1\":{\"114\":1}}],[\"ad\",{\"1\":{\"282\":1}}],[\"adrian\",{\"1\":{\"255\":1,\"269\":1,\"429\":1}}],[\"adncorp\",{\"1\":{\"232\":1}}],[\"administrative\",{\"1\":{\"253\":1,\"379\":1}}],[\"administrators\",{\"1\":{\"230\":1}}],[\"administer\",{\"1\":{\"194\":1}}],[\"admoni\",{\"1\":{\"184\":1,\"380\":1}}],[\"adults\",{\"1\":{\"184\":1,\"214\":1,\"261\":1,\"302\":1,\"380\":1}}],[\"ada\",{\"0\":{\"337\":1},\"1\":{\"326\":1,\"337\":4}}],[\"adam\",{\"1\":{\"121\":1,\"169\":1,\"195\":1,\"389\":1}}],[\"adapters\",{\"1\":{\"400\":1}}],[\"adapter\",{\"0\":{\"349\":1},\"1\":{\"400\":1,\"425\":1}}],[\"adapted\",{\"1\":{\"213\":1,\"315\":1,\"409\":1,\"444\":1,\"455\":1}}],[\"adaptability\",{\"1\":{\"529\":1}}],[\"adaptable\",{\"0\":{\"337\":1},\"1\":{\"144\":1,\"337\":1,\"353\":1}}],[\"adaptation\",{\"1\":{\"253\":1,\"344\":1,\"407\":1,\"455\":1}}],[\"adapts\",{\"1\":{\"143\":1,\"282\":1}}],[\"adaptivity\",{\"1\":{\"409\":1}}],[\"adaptive\",{\"0\":{\"154\":1,\"234\":1,\"334\":1,\"344\":1,\"409\":1,\"487\":1},\"1\":{\"130\":1,\"154\":4,\"228\":1,\"234\":1,\"240\":1,\"244\":1,\"269\":1,\"300\":1,\"409\":2,\"414\":1,\"453\":1,\"487\":2}}],[\"adapting\",{\"0\":{\"359\":1},\"1\":{\"119\":1,\"130\":1,\"308\":1,\"377\":1,\"444\":1}}],[\"adapt\",{\"1\":{\"120\":1,\"279\":1,\"294\":1,\"376\":1,\"403\":1}}],[\"adorni\",{\"1\":{\"292\":1}}],[\"adolescents\",{\"1\":{\"211\":5}}],[\"adolescent\",{\"0\":{\"211\":1},\"1\":{\"206\":1,\"211\":1}}],[\"adopt\",{\"1\":{\"264\":1,\"275\":1,\"448\":1}}],[\"adopts\",{\"1\":{\"244\":1}}],[\"adoption\",{\"1\":{\"161\":1,\"171\":1,\"192\":1,\"198\":1,\"222\":1,\"288\":1,\"329\":1,\"354\":1,\"387\":1,\"434\":1,\"461\":1}}],[\"adopting\",{\"0\":{\"161\":1,\"354\":1},\"1\":{\"110\":1,\"321\":1,\"362\":1}}],[\"adopted\",{\"1\":{\"128\":1,\"161\":1,\"256\":1,\"354\":1,\"418\":1,\"458\":1}}],[\"adobe\",{\"1\":{\"8\":2}}],[\"adobeillustrator\",{\"1\":{\"8\":1}}],[\"adding\",{\"1\":{\"228\":1,\"251\":1,\"423\":1}}],[\"additive\",{\"0\":{\"183\":1},\"1\":{\"183\":3,\"400\":1}}],[\"addition\",{\"1\":{\"126\":1,\"215\":1,\"245\":1,\"281\":1,\"308\":1,\"313\":1,\"325\":1,\"347\":1,\"409\":1,\"410\":1,\"413\":1,\"481\":1,\"488\":1}}],[\"additionally\",{\"1\":{\"135\":1,\"136\":1,\"155\":1,\"166\":1,\"172\":1,\"184\":1,\"190\":1,\"192\":1,\"202\":1,\"234\":1,\"246\":1,\"249\":1,\"259\":1,\"271\":1,\"303\":1,\"304\":1,\"315\":1,\"316\":1,\"317\":1,\"319\":1,\"322\":1,\"323\":1,\"327\":1,\"333\":1,\"344\":1,\"370\":1,\"379\":1,\"380\":1,\"395\":1,\"399\":1,\"419\":1,\"420\":1,\"439\":1,\"442\":1,\"445\":1,\"457\":1,\"459\":1,\"460\":1,\"468\":1,\"482\":1}}],[\"additional\",{\"1\":{\"99\":1,\"110\":1,\"127\":1,\"146\":1,\"163\":1,\"183\":1,\"197\":1,\"318\":1,\"321\":1,\"365\":1,\"395\":1,\"398\":1,\"453\":1,\"462\":1,\"490\":1,\"491\":1}}],[\"adds\",{\"1\":{\"146\":1,\"186\":1}}],[\"added\",{\"1\":{\"119\":1,\"454\":1}}],[\"addresses\",{\"1\":{\"281\":1,\"336\":1,\"356\":1,\"396\":1,\"430\":1,\"446\":1,\"459\":1}}],[\"addressed\",{\"1\":{\"136\":1,\"171\":1,\"179\":1,\"261\":1,\"279\":1,\"327\":1,\"334\":1,\"358\":1}}],[\"addressing\",{\"1\":{\"129\":1,\"136\":1,\"156\":1,\"160\":1,\"164\":1,\"183\":1,\"192\":1,\"217\":1,\"228\":1,\"239\":1,\"264\":1,\"318\":1,\"349\":1,\"360\":1,\"364\":1,\"434\":1,\"448\":1,\"489\":1}}],[\"address\",{\"1\":{\"103\":1,\"106\":1,\"113\":1,\"118\":1,\"122\":2,\"135\":1,\"146\":1,\"154\":1,\"155\":1,\"159\":1,\"165\":1,\"179\":1,\"180\":1,\"190\":1,\"222\":1,\"231\":1,\"234\":1,\"237\":1,\"238\":1,\"241\":1,\"251\":1,\"254\":1,\"259\":1,\"266\":1,\"284\":1,\"286\":1,\"293\":1,\"301\":1,\"314\":1,\"315\":1,\"340\":1,\"344\":1,\"353\":1,\"363\":1,\"376\":1,\"378\":1,\"379\":1,\"382\":1,\"398\":1,\"408\":1,\"417\":1,\"423\":1,\"425\":1,\"427\":1,\"428\":1,\"441\":1,\"443\":1,\"447\":1,\"453\":1,\"454\":1,\"460\":1,\"462\":1,\"463\":1,\"474\":1,\"480\":1,\"482\":1,\"498\":1}}],[\"add\",{\"1\":{\"97\":1,\"173\":1,\"312\":1}}],[\"advocate\",{\"1\":{\"254\":1}}],[\"advocating\",{\"0\":{\"280\":1},\"1\":{\"172\":1,\"300\":1,\"370\":1}}],[\"advice\",{\"0\":{\"234\":1},\"1\":{\"211\":1,\"234\":1}}],[\"advent\",{\"1\":{\"339\":1}}],[\"adventure\",{\"1\":{\"74\":2}}],[\"adverbs\",{\"0\":{\"407\":1},\"1\":{\"407\":1}}],[\"adversary\",{\"1\":{\"429\":1}}],[\"adversaries\",{\"1\":{\"334\":1}}],[\"adversarially\",{\"1\":{\"409\":1}}],[\"adversarial\",{\"0\":{\"313\":1},\"1\":{\"289\":2,\"313\":3,\"327\":1,\"374\":1,\"409\":1,\"432\":3,\"496\":1}}],[\"adversely\",{\"1\":{\"318\":1}}],[\"adverse\",{\"1\":{\"176\":1,\"309\":1}}],[\"advertising\",{\"1\":{\"153\":1}}],[\"advait\",{\"1\":{\"122\":1}}],[\"advantage\",{\"1\":{\"199\":1,\"224\":1,\"366\":1,\"454\":1}}],[\"advantages\",{\"1\":{\"96\":1,\"389\":1,\"399\":1,\"480\":1}}],[\"advancing\",{\"0\":{\"136\":1,\"351\":1,\"393\":1,\"411\":1},\"1\":{\"356\":1}}],[\"advanced\",{\"0\":{\"172\":1,\"370\":1},\"1\":{\"141\":1,\"151\":1,\"171\":1,\"172\":4,\"246\":1,\"261\":1,\"348\":1,\"353\":1,\"356\":1,\"358\":2,\"362\":1,\"370\":4,\"430\":1,\"437\":1,\"442\":2}}],[\"advancement\",{\"1\":{\"132\":1,\"166\":1,\"201\":1,\"223\":1,\"330\":1,\"359\":1}}],[\"advancements\",{\"1\":{\"124\":1,\"151\":1,\"176\":1,\"305\":1,\"348\":1,\"349\":1,\"398\":1,\"400\":1,\"410\":1,\"415\":1,\"417\":1,\"442\":1,\"458\":2,\"491\":1}}],[\"advances\",{\"1\":{\"105\":1,\"112\":1,\"194\":1,\"243\":1,\"261\":1,\"300\":1,\"322\":1,\"338\":1,\"360\":1,\"363\":1,\"422\":1,\"425\":1,\"482\":1}}],[\"advance\",{\"1\":{\"103\":1,\"178\":1,\"201\":1}}],[\"ai不仅限于电子邮件\",{\"1\":{\"539\":1}}],[\"ai电子邮件生成器能根据客户数据创建定制的电子邮件\",{\"1\":{\"539\":1}}],[\"ai电子邮件生成器帮助用户节省时间\",{\"1\":{\"539\":1}}],[\"ai带来的挑战和行业裁员让设计师陷入一个黑暗期\",{\"1\":{\"536\":1}}],[\"ai带来的管理变革\",{\"1\":{\"518\":1}}],[\"ai预测能力结合\",{\"1\":{\"529\":1}}],[\"ai与ux的融合\",{\"1\":{\"529\":1}}],[\"ai时代的问题复杂多变\",{\"1\":{\"518\":1}}],[\"ai时代项目管理的关键在于如何利用ai工具优化流程\",{\"1\":{\"518\":1}}],[\"ai可能加剧团队隔阂\",{\"1\":{\"518\":1}}],[\"ai可以更加高效建立团队管理流程\",{\"1\":{\"518\":1}}],[\"ai可以处理大量数据和执行重复性任务\",{\"1\":{\"518\":1}}],[\"ai的发展可能侵蚀传统客户服务岗位\",{\"1\":{\"518\":1}}],[\"aizawa\",{\"1\":{\"458\":1}}],[\"aio\",{\"1\":{\"398\":6}}],[\"ai2apps\",{\"0\":{\"361\":1},\"1\":{\"361\":6}}],[\"aitziber\",{\"1\":{\"315\":1}}],[\"aishwarya\",{\"1\":{\"258\":1,\"433\":1}}],[\"aissatou\",{\"1\":{\"232\":1}}],[\"aig\",{\"1\":{\"257\":2}}],[\"aigc\",{\"1\":{\"29\":1}}],[\"aiping\",{\"1\":{\"198\":1,\"387\":1}}],[\"airbnb\",{\"1\":{\"533\":2}}],[\"airbnb提供详细的图片和房东信息\",{\"1\":{\"532\":1}}],[\"air\",{\"0\":{\"149\":1},\"1\":{\"149\":4}}],[\"aiming\",{\"0\":{\"309\":1},\"1\":{\"169\":1,\"185\":1,\"234\":1,\"246\":1,\"300\":1,\"305\":1,\"365\":1,\"394\":1}}],[\"aim\",{\"1\":{\"122\":1,\"153\":1,\"179\":1,\"181\":1,\"214\":1,\"239\":1,\"261\":1,\"269\":2,\"294\":1,\"315\":1,\"377\":1,\"418\":1,\"427\":1,\"438\":1,\"439\":1,\"447\":1,\"481\":1}}],[\"aimed\",{\"1\":{\"119\":1,\"158\":1,\"160\":1,\"166\":2,\"237\":1,\"333\":1,\"358\":1,\"360\":1}}],[\"aims\",{\"1\":{\"96\":1,\"107\":1,\"117\":1,\"138\":1,\"158\":1,\"164\":1,\"171\":1,\"188\":1,\"198\":1,\"217\":1,\"226\":1,\"231\":1,\"268\":1,\"269\":2,\"288\":1,\"289\":1,\"328\":1,\"329\":1,\"334\":1,\"353\":1,\"378\":1,\"387\":1,\"396\":1,\"398\":1,\"429\":1,\"438\":1,\"452\":1,\"459\":1,\"491\":1}}],[\"aiding\",{\"1\":{\"216\":1,\"467\":1,\"480\":1}}],[\"aids\",{\"0\":{\"135\":1},\"1\":{\"135\":2,\"350\":1}}],[\"aid\",{\"1\":{\"110\":1,\"198\":1,\"205\":1,\"213\":1,\"217\":1,\"248\":1,\"308\":1,\"321\":1,\"369\":1,\"387\":1}}],[\"ai\",{\"0\":{\"97\":1,\"102\":1,\"124\":1,\"125\":1,\"127\":1,\"131\":1,\"133\":1,\"135\":1,\"144\":1,\"154\":1,\"159\":1,\"163\":1,\"176\":1,\"209\":1,\"211\":1,\"212\":1,\"213\":1,\"216\":1,\"229\":1,\"239\":1,\"243\":1,\"244\":2,\"248\":1,\"263\":2,\"268\":1,\"270\":1,\"312\":1,\"331\":1,\"344\":1,\"361\":1,\"398\":1,\"446\":1,\"452\":1},\"1\":{\"97\":4,\"98\":1,\"99\":2,\"102\":6,\"105\":1,\"124\":2,\"125\":8,\"131\":2,\"133\":4,\"135\":5,\"138\":1,\"144\":5,\"148\":1,\"154\":1,\"156\":3,\"159\":4,\"160\":4,\"169\":1,\"171\":3,\"174\":4,\"176\":8,\"180\":2,\"209\":1,\"211\":3,\"213\":3,\"216\":1,\"229\":7,\"231\":3,\"239\":2,\"243\":4,\"244\":4,\"245\":1,\"248\":3,\"251\":1,\"253\":2,\"263\":8,\"266\":1,\"268\":4,\"270\":1,\"291\":4,\"312\":4,\"314\":1,\"315\":1,\"324\":1,\"331\":4,\"344\":1,\"356\":3,\"358\":3,\"361\":2,\"371\":4,\"378\":1,\"385\":1,\"398\":12,\"407\":1,\"408\":2,\"418\":1,\"423\":1,\"443\":1,\"452\":4,\"453\":2,\"458\":1,\"476\":4,\"477\":2,\"505\":1,\"529\":1,\"533\":1,\"535\":1,\"539\":3}}],[\"ai生成技术等的出现也让未来元宇宙看似更加清晰了\",{\"1\":{\"31\":1}}],[\"ai创作的确有时候已经非常接近软件渲染了\",{\"1\":{\"28\":1}}],[\"a\",{\"0\":{\"99\":1,\"108\":1,\"116\":1,\"117\":1,\"120\":1,\"129\":2,\"135\":1,\"141\":1,\"149\":1,\"156\":1,\"157\":3,\"159\":1,\"161\":1,\"171\":1,\"174\":1,\"175\":1,\"187\":1,\"195\":1,\"202\":1,\"205\":1,\"223\":2,\"224\":1,\"227\":2,\"240\":1,\"244\":1,\"246\":1,\"249\":1,\"250\":1,\"252\":1,\"253\":1,\"261\":1,\"265\":1,\"271\":1,\"272\":1,\"277\":1,\"280\":1,\"283\":2,\"286\":1,\"292\":1,\"295\":1,\"297\":1,\"303\":1,\"304\":1,\"308\":1,\"313\":1,\"316\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"335\":1,\"338\":1,\"341\":1,\"343\":1,\"354\":1,\"361\":1,\"365\":1,\"371\":1,\"377\":1,\"379\":1,\"390\":1,\"403\":1,\"415\":1,\"426\":1,\"429\":2,\"431\":1,\"443\":1,\"455\":1,\"457\":1,\"460\":1,\"463\":1,\"474\":1,\"491\":1,\"496\":1},\"1\":{\"22\":2,\"74\":1,\"96\":4,\"97\":2,\"98\":7,\"99\":3,\"100\":5,\"101\":2,\"102\":3,\"103\":3,\"104\":2,\"105\":5,\"106\":2,\"107\":12,\"108\":3,\"110\":2,\"111\":3,\"112\":2,\"113\":3,\"114\":4,\"116\":4,\"117\":2,\"118\":7,\"119\":3,\"120\":8,\"121\":1,\"122\":2,\"123\":2,\"124\":1,\"125\":4,\"126\":8,\"127\":1,\"128\":2,\"129\":2,\"130\":2,\"132\":4,\"133\":4,\"135\":4,\"136\":2,\"137\":4,\"138\":1,\"139\":5,\"140\":5,\"141\":6,\"142\":3,\"143\":3,\"146\":8,\"147\":1,\"148\":2,\"149\":5,\"150\":4,\"151\":4,\"153\":4,\"154\":5,\"155\":7,\"156\":5,\"157\":2,\"158\":5,\"159\":2,\"160\":2,\"161\":4,\"163\":1,\"164\":1,\"165\":3,\"166\":6,\"169\":2,\"170\":2,\"171\":4,\"172\":2,\"173\":5,\"174\":4,\"175\":2,\"176\":5,\"179\":5,\"180\":2,\"181\":5,\"182\":4,\"183\":4,\"184\":2,\"185\":3,\"186\":3,\"187\":5,\"188\":2,\"189\":6,\"190\":3,\"192\":2,\"194\":6,\"195\":2,\"196\":6,\"197\":2,\"198\":3,\"199\":7,\"200\":2,\"201\":7,\"203\":1,\"204\":3,\"205\":1,\"207\":2,\"209\":1,\"210\":5,\"211\":1,\"212\":3,\"213\":5,\"214\":1,\"215\":3,\"216\":4,\"217\":2,\"218\":3,\"219\":2,\"220\":4,\"222\":5,\"223\":6,\"224\":1,\"227\":2,\"228\":3,\"229\":3,\"230\":3,\"231\":3,\"232\":3,\"233\":4,\"234\":4,\"235\":5,\"236\":3,\"237\":1,\"238\":3,\"239\":5,\"240\":4,\"241\":2,\"242\":3,\"243\":1,\"244\":1,\"245\":7,\"246\":2,\"248\":3,\"249\":3,\"250\":4,\"251\":8,\"252\":6,\"253\":2,\"254\":2,\"255\":4,\"256\":2,\"257\":11,\"258\":2,\"259\":7,\"260\":3,\"261\":1,\"263\":3,\"264\":6,\"265\":3,\"266\":5,\"268\":4,\"269\":11,\"270\":5,\"271\":3,\"272\":5,\"273\":1,\"274\":5,\"275\":2,\"277\":4,\"278\":2,\"279\":3,\"280\":3,\"281\":3,\"282\":5,\"283\":2,\"284\":7,\"286\":1,\"287\":4,\"288\":3,\"289\":1,\"290\":1,\"291\":2,\"292\":6,\"293\":1,\"294\":2,\"295\":2,\"296\":4,\"297\":3,\"298\":11,\"300\":5,\"301\":2,\"302\":5,\"304\":5,\"305\":3,\"306\":1,\"307\":3,\"308\":7,\"309\":1,\"312\":2,\"313\":5,\"314\":3,\"315\":6,\"316\":5,\"317\":4,\"318\":3,\"319\":1,\"321\":2,\"322\":5,\"323\":2,\"324\":2,\"325\":5,\"326\":4,\"327\":1,\"328\":4,\"329\":3,\"330\":4,\"331\":4,\"333\":4,\"334\":5,\"335\":7,\"336\":2,\"337\":2,\"338\":3,\"339\":4,\"340\":3,\"341\":2,\"342\":2,\"343\":1,\"344\":6,\"345\":4,\"347\":1,\"348\":4,\"349\":6,\"350\":7,\"351\":1,\"352\":2,\"353\":4,\"354\":4,\"355\":2,\"356\":3,\"358\":5,\"359\":5,\"360\":4,\"361\":9,\"362\":5,\"363\":1,\"364\":3,\"365\":8,\"366\":1,\"369\":3,\"370\":2,\"371\":4,\"372\":2,\"375\":8,\"376\":5,\"377\":6,\"378\":2,\"379\":3,\"380\":2,\"381\":3,\"382\":7,\"384\":1,\"385\":1,\"386\":5,\"387\":3,\"388\":3,\"389\":3,\"390\":6,\"392\":2,\"393\":2,\"394\":1,\"395\":6,\"396\":4,\"398\":5,\"399\":5,\"400\":3,\"401\":2,\"402\":2,\"403\":4,\"404\":3,\"405\":4,\"407\":4,\"408\":3,\"409\":5,\"410\":7,\"411\":8,\"412\":6,\"413\":2,\"414\":3,\"415\":3,\"416\":2,\"417\":2,\"418\":8,\"419\":1,\"420\":1,\"422\":8,\"423\":8,\"424\":6,\"425\":6,\"426\":6,\"427\":7,\"428\":3,\"429\":4,\"430\":3,\"431\":1,\"432\":6,\"433\":2,\"434\":6,\"435\":7,\"436\":4,\"437\":5,\"438\":5,\"439\":2,\"441\":6,\"442\":5,\"443\":6,\"444\":5,\"445\":4,\"447\":4,\"448\":6,\"449\":1,\"450\":3,\"452\":4,\"453\":3,\"454\":4,\"455\":6,\"456\":3,\"457\":3,\"458\":3,\"459\":7,\"461\":1,\"462\":5,\"463\":2,\"466\":1,\"467\":5,\"468\":8,\"469\":4,\"470\":4,\"471\":3,\"472\":1,\"474\":1,\"475\":3,\"476\":2,\"477\":4,\"478\":2,\"479\":3,\"480\":7,\"481\":2,\"482\":4,\"483\":2,\"485\":6,\"486\":5,\"487\":1,\"488\":7,\"489\":4,\"490\":2,\"491\":2,\"492\":4,\"494\":3,\"495\":2,\"496\":1,\"497\":9,\"498\":3,\"539\":2}}],[\"ar中\",{\"1\":{\"517\":1}}],[\"ar的应用\",{\"1\":{\"517\":1}}],[\"ar的交互主要基于语音命令\",{\"1\":{\"515\":1}}],[\"ar的核心是将音频元素融入到用户的现实环境中\",{\"1\":{\"514\":1}}],[\"ar的概念并不是全新的\",{\"1\":{\"512\":1}}],[\"arning\",{\"1\":{\"483\":1}}],[\"arnold\",{\"1\":{\"144\":1}}],[\"arhab\",{\"1\":{\"460\":1}}],[\"arqmath3\",{\"1\":{\"458\":1}}],[\"arbelle\",{\"1\":{\"454\":1}}],[\"arbitrary\",{\"1\":{\"235\":1,\"482\":1}}],[\"arbitration\",{\"1\":{\"186\":1}}],[\"arcas\",{\"1\":{\"435\":1}}],[\"architectural\",{\"1\":{\"129\":2}}],[\"architecture\",{\"0\":{\"129\":1,\"157\":1},\"1\":{\"123\":2,\"137\":2,\"157\":2,\"186\":1,\"207\":1,\"218\":2,\"264\":2,\"289\":1,\"328\":1,\"338\":1,\"447\":1,\"448\":2,\"455\":1,\"479\":1}}],[\"architectures\",{\"1\":{\"101\":1,\"123\":1,\"137\":2,\"213\":1,\"328\":1,\"337\":1,\"395\":1,\"398\":1,\"427\":1,\"450\":2}}],[\"archipelago\",{\"1\":{\"40\":1}}],[\"arithmetic\",{\"1\":{\"389\":1,\"454\":2}}],[\"arinze\",{\"1\":{\"308\":1}}],[\"arie\",{\"1\":{\"308\":1}}],[\"aristide\",{\"1\":{\"232\":1}}],[\"arising\",{\"1\":{\"212\":1}}],[\"arises\",{\"1\":{\"350\":1,\"470\":1}}],[\"arise\",{\"1\":{\"164\":1,\"206\":1,\"253\":1,\"403\":1}}],[\"armeni\",{\"1\":{\"399\":1}}],[\"arman\",{\"1\":{\"391\":1}}],[\"arms\",{\"1\":{\"284\":1}}],[\"armstrong\",{\"1\":{\"108\":1}}],[\"arm\",{\"0\":{\"284\":1},\"1\":{\"284\":3}}],[\"arrows\",{\"1\":{\"301\":2}}],[\"arriba\",{\"1\":{\"282\":1}}],[\"arrays\",{\"1\":{\"363\":1}}],[\"array\",{\"1\":{\"213\":1,\"459\":1}}],[\"arrangement\",{\"1\":{\"399\":1}}],[\"arran\",{\"1\":{\"197\":1}}],[\"araujo\",{\"1\":{\"365\":1}}],[\"arakawa\",{\"1\":{\"314\":1}}],[\"araya\",{\"1\":{\"271\":1,\"457\":1}}],[\"arai\",{\"1\":{\"160\":1}}],[\"argues\",{\"1\":{\"291\":1,\"476\":1}}],[\"argue\",{\"1\":{\"268\":1,\"280\":1,\"326\":1,\"452\":1,\"479\":1}}],[\"arguments\",{\"1\":{\"245\":1}}],[\"argument\",{\"0\":{\"236\":1}}],[\"arvind\",{\"1\":{\"268\":1,\"452\":1}}],[\"aru\",{\"1\":{\"216\":1}}],[\"arp\",{\"1\":{\"196\":2}}],[\"around\",{\"1\":{\"164\":2,\"217\":3,\"248\":3,\"258\":1,\"366\":1,\"398\":1,\"433\":1}}],[\"arousal\",{\"1\":{\"120\":1}}],[\"arora\",{\"1\":{\"154\":1,\"172\":1,\"339\":1,\"370\":1,\"396\":1}}],[\"arechiga\",{\"1\":{\"468\":1}}],[\"arendt\",{\"1\":{\"231\":1}}],[\"are\",{\"0\":{\"121\":1,\"179\":1,\"302\":1,\"329\":1,\"395\":1,\"438\":1,\"443\":1,\"450\":1,\"472\":1},\"1\":{\"96\":2,\"97\":1,\"99\":1,\"101\":4,\"103\":3,\"104\":1,\"105\":1,\"111\":1,\"112\":2,\"117\":1,\"119\":1,\"121\":2,\"122\":1,\"123\":2,\"124\":2,\"125\":2,\"126\":3,\"127\":1,\"128\":1,\"133\":3,\"135\":2,\"137\":2,\"144\":1,\"146\":1,\"150\":2,\"151\":2,\"159\":2,\"160\":1,\"161\":1,\"164\":2,\"165\":1,\"172\":1,\"179\":4,\"180\":1,\"181\":2,\"183\":4,\"184\":3,\"188\":4,\"191\":1,\"192\":1,\"194\":1,\"196\":2,\"197\":2,\"199\":4,\"201\":1,\"202\":1,\"204\":1,\"205\":1,\"209\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":2,\"216\":1,\"219\":2,\"220\":1,\"221\":5,\"227\":1,\"228\":1,\"231\":3,\"232\":1,\"233\":1,\"235\":2,\"236\":1,\"237\":1,\"238\":1,\"240\":1,\"243\":1,\"245\":1,\"248\":4,\"249\":2,\"250\":1,\"251\":1,\"253\":2,\"254\":3,\"255\":1,\"257\":2,\"259\":1,\"260\":1,\"261\":1,\"265\":1,\"266\":2,\"269\":2,\"272\":2,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":2,\"284\":2,\"286\":2,\"288\":1,\"289\":2,\"297\":1,\"298\":1,\"301\":1,\"302\":4,\"306\":2,\"308\":1,\"309\":3,\"312\":1,\"316\":1,\"326\":1,\"331\":3,\"333\":1,\"334\":1,\"335\":2,\"336\":1,\"338\":3,\"340\":1,\"343\":1,\"345\":1,\"348\":2,\"354\":1,\"355\":1,\"362\":1,\"363\":4,\"368\":1,\"370\":1,\"372\":1,\"374\":2,\"376\":1,\"378\":1,\"380\":3,\"381\":3,\"382\":1,\"384\":1,\"389\":1,\"391\":1,\"394\":1,\"395\":4,\"396\":1,\"399\":1,\"401\":2,\"402\":1,\"405\":1,\"409\":2,\"410\":3,\"413\":1,\"414\":1,\"417\":3,\"418\":2,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"427\":1,\"428\":3,\"431\":1,\"432\":3,\"435\":1,\"436\":2,\"437\":2,\"438\":1,\"441\":2,\"443\":1,\"444\":2,\"445\":2,\"447\":1,\"454\":1,\"456\":1,\"459\":1,\"461\":4,\"465\":2,\"466\":1,\"467\":1,\"468\":1,\"474\":2,\"479\":1,\"480\":1,\"482\":1,\"485\":1,\"490\":3,\"494\":2,\"496\":1,\"497\":1}}],[\"areas\",{\"1\":{\"127\":2,\"161\":1,\"216\":2,\"235\":1,\"292\":1,\"344\":1,\"354\":1,\"390\":1,\"392\":1,\"428\":1,\"488\":1}}],[\"area\",{\"1\":{\"40\":1,\"126\":1,\"166\":1,\"174\":1,\"190\":1,\"220\":1,\"292\":1,\"295\":1,\"356\":1,\"371\":1,\"422\":1,\"429\":1,\"439\":1,\"446\":1,\"455\":1,\"466\":1,\"481\":1}}],[\"arxiv\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":1,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":2,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"ar\",{\"1\":{\"81\":1,\"84\":1,\"105\":2,\"117\":1,\"140\":1,\"195\":2,\"450\":1,\"512\":3,\"516\":4,\"531\":1}}],[\"ar技术探究\",{\"1\":{\"81\":1}}],[\"art\",{\"0\":{\"289\":1,\"296\":1},\"1\":{\"101\":1,\"102\":1,\"105\":1,\"118\":1,\"123\":1,\"127\":1,\"130\":1,\"137\":1,\"139\":1,\"140\":1,\"159\":1,\"178\":2,\"183\":1,\"194\":2,\"283\":1,\"289\":3,\"296\":3,\"314\":1,\"315\":1,\"334\":1,\"337\":1,\"344\":1,\"347\":2,\"350\":1,\"360\":1,\"372\":1,\"376\":2,\"386\":1,\"393\":1,\"408\":1,\"411\":1,\"413\":1,\"418\":1,\"432\":1,\"434\":1,\"438\":1,\"492\":1,\"533\":1}}],[\"artifacts\",{\"1\":{\"343\":1}}],[\"artifact\",{\"1\":{\"156\":3}}],[\"artificially\",{\"1\":{\"144\":1}}],[\"artificial\",{\"0\":{\"211\":1,\"398\":1},\"1\":{\"102\":2,\"125\":1,\"135\":1,\"148\":1,\"171\":1,\"176\":1,\"211\":1,\"212\":1,\"216\":1,\"239\":1,\"244\":1,\"248\":1,\"282\":2,\"288\":1,\"356\":1,\"363\":1,\"398\":3,\"446\":1}}],[\"articulate\",{\"1\":{\"236\":1}}],[\"articulating\",{\"1\":{\"114\":1}}],[\"articles\",{\"1\":{\"174\":1,\"258\":1,\"371\":1,\"433\":1}}],[\"article\",{\"0\":{\"418\":1},\"1\":{\"96\":1,\"126\":3,\"136\":1,\"155\":4,\"169\":1,\"218\":1,\"239\":1,\"539\":1}}],[\"artist\",{\"0\":{\"9\":1}}],[\"arts\",{\"1\":{\"78\":1}}],[\"ar工作坊\",{\"1\":{\"17\":1}}],[\"arduino\",{\"1\":{\"8\":1}}],[\"pto=wapp\",{\"1\":{\"539\":1}}],[\"pfa\",{\"1\":{\"355\":1}}],[\"pfc\",{\"1\":{\"223\":2}}],[\"pm\",{\"0\":{\"343\":1},\"1\":{\"343\":3}}],[\"pm4py\",{\"0\":{\"343\":1},\"1\":{\"343\":2}}],[\"pbl\",{\"1\":{\"308\":1}}],[\"pcbs\",{\"1\":{\"306\":2}}],[\"pcb\",{\"0\":{\"306\":1},\"1\":{\"306\":1}}],[\"pérez\",{\"1\":{\"282\":1}}],[\"pytorch\",{\"1\":{\"231\":1}}],[\"python\",{\"1\":{\"231\":2,\"258\":1,\"343\":1,\"350\":1,\"360\":1,\"433\":1,\"443\":1}}],[\"python热度持续\",{\"1\":{\"78\":1}}],[\"python筹又成为热名\",{\"1\":{\"78\":1}}],[\"pwmd\",{\"1\":{\"164\":4}}],[\"p\",{\"1\":{\"140\":1,\"210\":1,\"388\":1,\"392\":1,\"398\":1,\"458\":2,\"477\":1,\"486\":2}}],[\"ps\",{\"1\":{\"521\":1}}],[\"psunlpgroup\",{\"1\":{\"391\":1}}],[\"psychological\",{\"0\":{\"209\":1,\"278\":1},\"1\":{\"209\":5,\"223\":1,\"244\":1,\"278\":4,\"295\":1,\"302\":2,\"461\":1}}],[\"psychology\",{\"1\":{\"107\":1,\"180\":1,\"251\":1,\"423\":1}}],[\"psychophysical\",{\"1\":{\"191\":1}}],[\"pseudo\",{\"1\":{\"137\":2,\"319\":1,\"386\":2}}],[\"peft\",{\"0\":{\"405\":1},\"1\":{\"392\":1}}],[\"pearson\",{\"1\":{\"369\":1}}],[\"pearce\",{\"1\":{\"363\":1}}],[\"pegasus\",{\"1\":{\"335\":1}}],[\"penny\",{\"1\":{\"436\":1}}],[\"pennington\",{\"1\":{\"389\":1}}],[\"pendulum\",{\"1\":{\"304\":1}}],[\"penalizes\",{\"1\":{\"301\":1}}],[\"pengda\",{\"1\":{\"251\":1,\"423\":1}}],[\"peng\",{\"1\":{\"103\":1,\"222\":1,\"258\":1,\"287\":1,\"306\":1,\"322\":1,\"411\":1,\"433\":1,\"455\":1,\"494\":1}}],[\"peddinti\",{\"1\":{\"254\":1}}],[\"petter\",{\"1\":{\"458\":1}}],[\"petukhova\",{\"1\":{\"351\":1,\"352\":1}}],[\"petkaz\",{\"0\":{\"351\":1,\"352\":1}}],[\"petrenko\",{\"1\":{\"281\":1}}],[\"petr\",{\"1\":{\"240\":1}}],[\"pet\",{\"0\":{\"238\":1},\"1\":{\"238\":1}}],[\"peter\",{\"1\":{\"188\":1,\"273\":1,\"296\":1,\"435\":1}}],[\"pesch\",{\"1\":{\"237\":1}}],[\"peers\",{\"1\":{\"148\":1}}],[\"peer\",{\"0\":{\"148\":1},\"1\":{\"307\":1}}],[\"peoples\",{\"1\":{\"303\":1}}],[\"people\",{\"0\":{\"107\":1,\"160\":1,\"164\":1,\"166\":1,\"214\":1,\"282\":1},\"1\":{\"107\":10,\"108\":1,\"144\":1,\"149\":1,\"156\":1,\"164\":1,\"166\":4,\"173\":1,\"180\":2,\"181\":1,\"202\":1,\"214\":3,\"217\":1,\"219\":1,\"265\":1,\"282\":2,\"284\":1,\"401\":1,\"428\":1,\"519\":1}}],[\"perplexing\",{\"0\":{\"431\":1}}],[\"perplexity\",{\"1\":{\"389\":2,\"412\":1,\"456\":1}}],[\"perhaps\",{\"1\":{\"302\":1}}],[\"permits\",{\"1\":{\"282\":1}}],[\"permission\",{\"0\":{\"147\":1},\"1\":{\"147\":1}}],[\"period\",{\"1\":{\"265\":1}}],[\"periods\",{\"1\":{\"249\":1}}],[\"perfectly\",{\"1\":{\"463\":1}}],[\"perfect\",{\"1\":{\"252\":1,\"336\":1,\"426\":1}}],[\"performing\",{\"1\":{\"222\":1,\"326\":1,\"372\":1,\"454\":1,\"475\":1}}],[\"performed\",{\"1\":{\"159\":1,\"160\":1,\"199\":1,\"269\":1,\"456\":1,\"492\":1}}],[\"performs\",{\"1\":{\"113\":1,\"137\":1,\"235\":1,\"458\":1,\"497\":1}}],[\"perform\",{\"1\":{\"111\":1,\"133\":1,\"146\":1,\"207\":2,\"223\":1,\"242\":1,\"248\":1,\"255\":1,\"284\":2,\"317\":1,\"318\":1,\"325\":1,\"331\":1,\"335\":2,\"336\":1,\"344\":2,\"363\":1,\"375\":1,\"381\":1,\"391\":1,\"405\":1,\"412\":1,\"413\":1,\"444\":1,\"497\":1}}],[\"performances\",{\"1\":{\"117\":2,\"182\":1,\"248\":1,\"458\":1}}],[\"performance\",{\"0\":{\"117\":1,\"119\":1,\"128\":1,\"185\":1,\"189\":1,\"191\":1,\"223\":1},\"1\":{\"102\":2,\"105\":2,\"118\":1,\"121\":1,\"128\":6,\"137\":2,\"139\":1,\"174\":1,\"183\":2,\"185\":4,\"189\":1,\"190\":2,\"191\":5,\"194\":1,\"195\":1,\"215\":4,\"216\":1,\"222\":2,\"223\":2,\"229\":1,\"233\":2,\"238\":2,\"240\":1,\"244\":1,\"248\":1,\"251\":1,\"278\":1,\"283\":2,\"288\":1,\"293\":1,\"294\":1,\"306\":1,\"309\":1,\"314\":1,\"315\":1,\"317\":2,\"319\":2,\"323\":1,\"328\":2,\"329\":1,\"334\":1,\"338\":1,\"342\":1,\"344\":1,\"345\":3,\"350\":2,\"355\":3,\"362\":1,\"365\":1,\"368\":1,\"371\":1,\"372\":1,\"374\":1,\"377\":1,\"379\":1,\"382\":2,\"385\":1,\"386\":1,\"388\":1,\"389\":1,\"391\":1,\"394\":1,\"402\":1,\"403\":1,\"404\":4,\"408\":1,\"411\":1,\"412\":2,\"413\":2,\"414\":1,\"423\":1,\"424\":2,\"425\":1,\"427\":1,\"429\":1,\"432\":1,\"434\":1,\"435\":2,\"437\":2,\"441\":1,\"443\":2,\"447\":2,\"454\":1,\"456\":1,\"458\":2,\"459\":3,\"460\":1,\"463\":1,\"465\":2,\"468\":1,\"471\":3,\"472\":1,\"475\":1,\"479\":1,\"480\":1,\"482\":4,\"483\":3,\"485\":2,\"486\":2,\"491\":1,\"492\":1,\"496\":2,\"497\":1}}],[\"per\",{\"1\":{\"238\":1,\"405\":1,\"410\":1}}],[\"perrault\",{\"1\":{\"161\":1,\"271\":1,\"354\":1,\"457\":1}}],[\"perrine\",{\"1\":{\"156\":1}}],[\"pervasive\",{\"0\":{\"139\":1},\"1\":{\"139\":1,\"148\":1}}],[\"perer\",{\"1\":{\"121\":1}}],[\"pereira\",{\"1\":{\"96\":2}}],[\"persistent\",{\"1\":{\"424\":1}}],[\"persian\",{\"0\":{\"333\":1},\"1\":{\"333\":4}}],[\"persianmmlu\",{\"0\":{\"333\":1},\"1\":{\"333\":1}}],[\"perspicuity\",{\"1\":{\"158\":2}}],[\"perspectives\",{\"0\":{\"178\":1,\"326\":1},\"1\":{\"130\":1,\"148\":1,\"174\":1,\"231\":1,\"275\":1,\"297\":1,\"308\":1,\"336\":1,\"371\":1,\"394\":1,\"470\":1}}],[\"perspective\",{\"0\":{\"227\":1,\"277\":1},\"1\":{\"114\":1,\"146\":1,\"180\":1,\"212\":1,\"227\":3,\"279\":1,\"292\":1}}],[\"personae\",{\"0\":{\"291\":1,\"476\":1},\"1\":{\"291\":1,\"476\":1}}],[\"personas\",{\"1\":{\"154\":3}}],[\"personal\",{\"0\":{\"461\":1},\"1\":{\"144\":1,\"152\":1,\"155\":2,\"160\":2,\"180\":1,\"210\":1,\"217\":2,\"266\":1,\"446\":1,\"461\":5}}],[\"personalities\",{\"0\":{\"218\":1},\"1\":{\"218\":1,\"461\":1}}],[\"personality\",{\"0\":{\"130\":1},\"1\":{\"130\":2,\"218\":9,\"399\":1,\"461\":1}}],[\"personalizing\",{\"1\":{\"190\":1}}],[\"personalization\",{\"1\":{\"99\":2,\"539\":1}}],[\"personalized\",{\"0\":{\"216\":1,\"349\":1,\"392\":1,\"399\":1},\"1\":{\"120\":2,\"127\":1,\"130\":1,\"167\":1,\"216\":2,\"237\":1,\"261\":1,\"266\":1,\"268\":1,\"349\":2,\"392\":2,\"399\":1,\"452\":1,\"485\":1}}],[\"personalize\",{\"0\":{\"99\":1}}],[\"persona\",{\"1\":{\"130\":1}}],[\"person\",{\"1\":{\"107\":1,\"253\":1}}],[\"percentage\",{\"1\":{\"443\":1}}],[\"percentile\",{\"1\":{\"126\":1}}],[\"perceptually\",{\"1\":{\"197\":1}}],[\"perceptual\",{\"0\":{\"246\":1},\"1\":{\"191\":3,\"220\":2}}],[\"perceptions\",{\"0\":{\"254\":1},\"1\":{\"108\":1,\"155\":2,\"178\":2,\"221\":1,\"254\":1,\"303\":1}}],[\"perception\",{\"0\":{\"167\":1,\"191\":1,\"197\":1,\"198\":1,\"246\":1,\"387\":1},\"1\":{\"98\":1,\"142\":3,\"151\":1,\"167\":1,\"191\":4,\"198\":1,\"235\":1,\"239\":1,\"269\":1,\"308\":1,\"348\":1,\"362\":3,\"387\":1,\"475\":2}}],[\"perceiving\",{\"1\":{\"118\":1,\"204\":1}}],[\"perceive\",{\"1\":{\"107\":1,\"108\":1,\"164\":1,\"207\":1,\"212\":1,\"308\":1,\"392\":1}}],[\"perceived\",{\"0\":{\"155\":1},\"1\":{\"96\":1,\"124\":1,\"128\":1,\"133\":1,\"135\":1,\"155\":3,\"158\":2,\"191\":1,\"198\":2,\"286\":1,\"303\":2,\"331\":1,\"387\":2,\"400\":1,\"474\":1}}],[\"pertinent\",{\"1\":{\"97\":1,\"312\":1,\"466\":1}}],[\"plummer\",{\"1\":{\"376\":1}}],[\"plugged\",{\"1\":{\"432\":1}}],[\"plugin\",{\"1\":{\"361\":2}}],[\"plugins\",{\"1\":{\"128\":1,\"138\":1}}],[\"plug\",{\"1\":{\"349\":1}}],[\"plms\",{\"1\":{\"372\":1}}],[\"pl\",{\"1\":{\"216\":7}}],[\"plotting\",{\"1\":{\"255\":1}}],[\"plots\",{\"0\":{\"281\":1},\"1\":{\"226\":1,\"281\":2}}],[\"plot\",{\"1\":{\"209\":1,\"281\":1}}],[\"please\",{\"0\":{\"251\":1,\"423\":1},\"1\":{\"487\":1}}],[\"pleasing\",{\"1\":{\"189\":2}}],[\"pleroma\",{\"1\":{\"230\":1}}],[\"plethora\",{\"1\":{\"102\":1,\"427\":1}}],[\"placing\",{\"1\":{\"443\":1}}],[\"placed\",{\"1\":{\"280\":1,\"295\":1,\"497\":1}}],[\"placement\",{\"0\":{\"196\":1},\"1\":{\"196\":3,\"399\":1,\"414\":1}}],[\"place\",{\"1\":{\"117\":1,\"173\":1,\"409\":1,\"439\":1,\"454\":1}}],[\"plateaued\",{\"1\":{\"209\":1}}],[\"platformized\",{\"1\":{\"293\":1}}],[\"platforms\",{\"1\":{\"108\":1,\"120\":1,\"155\":1,\"200\":2,\"278\":1,\"282\":1,\"293\":2,\"316\":1,\"447\":1,\"485\":1}}],[\"platform\",{\"0\":{\"99\":1,\"316\":1},\"1\":{\"99\":2,\"104\":1,\"132\":1,\"157\":2,\"196\":1,\"218\":2,\"245\":1,\"257\":1,\"316\":1,\"330\":1,\"342\":1,\"447\":1,\"485\":1,\"519\":1}}],[\"plausible\",{\"1\":{\"183\":1,\"255\":1}}],[\"plain\",{\"1\":{\"152\":1}}],[\"plan\",{\"1\":{\"344\":1,\"396\":1}}],[\"plane\",{\"0\":{\"255\":1},\"1\":{\"255\":2,\"407\":1}}],[\"plants\",{\"0\":{\"223\":1},\"1\":{\"207\":1}}],[\"planned\",{\"1\":{\"396\":1}}],[\"planners\",{\"1\":{\"270\":1,\"362\":1,\"381\":3}}],[\"planner\",{\"1\":{\"132\":1,\"330\":1}}],[\"planning\",{\"0\":{\"182\":1,\"381\":1},\"1\":{\"96\":3,\"126\":1,\"135\":1,\"182\":2,\"183\":1,\"184\":1,\"218\":1,\"270\":1,\"271\":1,\"272\":3,\"356\":1,\"362\":1,\"364\":1,\"380\":1,\"381\":2,\"411\":1,\"457\":1,\"471\":1,\"495\":1}}],[\"plans\",{\"1\":{\"96\":2,\"158\":2}}],[\"plan启动\",{\"1\":{\"23\":1}}],[\"playing\",{\"1\":{\"265\":1,\"403\":1}}],[\"playtime\",{\"1\":{\"265\":1}}],[\"playtest\",{\"1\":{\"98\":1}}],[\"played\",{\"1\":{\"265\":1}}],[\"players\",{\"1\":{\"98\":4,\"245\":1,\"265\":2,\"274\":3}}],[\"player\",{\"0\":{\"265\":1},\"1\":{\"81\":1,\"265\":2}}],[\"playfutures\",{\"0\":{\"248\":1}}],[\"plays\",{\"1\":{\"238\":1,\"360\":1,\"430\":1}}],[\"playstation\",{\"1\":{\"81\":1}}],[\"playground\",{\"1\":{\"110\":1,\"321\":1}}],[\"play\",{\"1\":{\"74\":2,\"98\":1,\"175\":1,\"197\":1,\"248\":2,\"257\":1,\"270\":1,\"349\":1,\"498\":1}}],[\"pubmedqa\",{\"1\":{\"466\":1}}],[\"publishing\",{\"1\":{\"428\":1}}],[\"publish\",{\"1\":{\"379\":1}}],[\"published\",{\"1\":{\"74\":1,\"122\":1,\"153\":1,\"200\":1,\"271\":1,\"428\":1,\"457\":1,\"470\":1}}],[\"publication\",{\"1\":{\"323\":1}}],[\"publications\",{\"1\":{\"174\":2,\"307\":1,\"371\":2,\"398\":1}}],[\"publicly~\",{\"1\":{\"316\":1}}],[\"publicly\",{\"1\":{\"123\":1,\"139\":2,\"238\":1,\"336\":1,\"361\":1,\"369\":1,\"393\":1,\"413\":1,\"458\":1,\"480\":1}}],[\"public\",{\"0\":{\"164\":1},\"1\":{\"117\":1,\"164\":4,\"178\":1,\"196\":2,\"205\":1,\"217\":2,\"248\":1,\"302\":1,\"334\":1,\"416\":1,\"418\":1}}],[\"puzzle\",{\"1\":{\"404\":1}}],[\"pulkit\",{\"1\":{\"490\":1}}],[\"puli\",{\"1\":{\"396\":1}}],[\"pulse\",{\"1\":{\"194\":1,\"298\":2}}],[\"pupil\",{\"1\":{\"305\":1}}],[\"pupillometry\",{\"0\":{\"305\":1},\"1\":{\"305\":1}}],[\"puppet\",{\"1\":{\"248\":1}}],[\"puppets\",{\"0\":{\"248\":1}}],[\"punctuality\",{\"1\":{\"293\":1}}],[\"punitive\",{\"1\":{\"200\":1}}],[\"putting\",{\"1\":{\"326\":1}}],[\"puts\",{\"1\":{\"229\":1}}],[\"put\",{\"1\":{\"179\":1,\"253\":1}}],[\"pushing\",{\"1\":{\"158\":1}}],[\"pure\",{\"1\":{\"362\":1}}],[\"purely\",{\"1\":{\"158\":1}}],[\"pursuit\",{\"1\":{\"338\":2,\"460\":1}}],[\"pursued\",{\"1\":{\"269\":1,\"292\":1}}],[\"pursue\",{\"1\":{\"230\":1,\"307\":1}}],[\"purdue\",{\"1\":{\"308\":1}}],[\"purpose\",{\"1\":{\"117\":1,\"214\":1,\"232\":1,\"265\":1,\"287\":1,\"308\":2,\"317\":1,\"323\":1,\"344\":1,\"480\":2}}],[\"purposes\",{\"1\":{\"97\":1,\"135\":1,\"240\":1,\"249\":1,\"287\":1,\"312\":1}}],[\"potts\",{\"1\":{\"428\":1}}],[\"potency\",{\"1\":{\"359\":1}}],[\"potentials\",{\"1\":{\"261\":1}}],[\"potentially\",{\"1\":{\"127\":1,\"133\":1,\"158\":1,\"283\":1,\"301\":1,\"331\":1,\"409\":1,\"437\":1,\"466\":1,\"470\":1,\"482\":1,\"498\":1}}],[\"potential\",{\"0\":{\"253\":1,\"381\":1},\"1\":{\"97\":1,\"98\":2,\"101\":1,\"103\":2,\"113\":1,\"115\":1,\"117\":1,\"119\":2,\"120\":1,\"124\":1,\"126\":2,\"127\":1,\"136\":1,\"138\":1,\"143\":1,\"146\":1,\"148\":2,\"149\":1,\"156\":2,\"166\":1,\"178\":1,\"181\":2,\"191\":1,\"198\":2,\"200\":1,\"213\":1,\"216\":1,\"223\":1,\"227\":1,\"237\":2,\"239\":1,\"248\":1,\"253\":1,\"256\":1,\"258\":1,\"259\":1,\"261\":2,\"265\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"291\":1,\"295\":1,\"298\":1,\"300\":1,\"306\":1,\"312\":1,\"314\":1,\"318\":1,\"324\":1,\"326\":2,\"329\":1,\"334\":1,\"356\":2,\"364\":1,\"379\":1,\"387\":2,\"392\":1,\"395\":1,\"398\":1,\"403\":1,\"408\":4,\"410\":2,\"413\":1,\"429\":1,\"433\":1,\"441\":1,\"459\":1,\"466\":1,\"476\":1,\"482\":1,\"483\":1,\"485\":2,\"486\":1,\"491\":1,\"495\":1,\"496\":1,\"498\":1}}],[\"pois\",{\"1\":{\"417\":1}}],[\"poisoned\",{\"1\":{\"409\":1,\"453\":5}}],[\"poi\",{\"0\":{\"417\":1},\"1\":{\"417\":3}}],[\"pointing\",{\"1\":{\"241\":1}}],[\"points\",{\"1\":{\"220\":3,\"242\":1,\"246\":1,\"258\":1,\"280\":1,\"301\":1,\"304\":1,\"338\":1,\"407\":1,\"433\":1,\"443\":1}}],[\"point\",{\"0\":{\"260\":1},\"1\":{\"178\":1,\"239\":1,\"260\":1,\"281\":1,\"301\":1,\"359\":1,\"364\":1,\"417\":1,\"439\":1,\"455\":1,\"469\":1,\"489\":1}}],[\"popa\",{\"1\":{\"326\":1}}],[\"populations\",{\"0\":{\"468\":1},\"1\":{\"261\":1,\"468\":1}}],[\"population\",{\"1\":{\"126\":1,\"181\":1,\"237\":1,\"428\":1,\"468\":3}}],[\"popularity\",{\"1\":{\"138\":1,\"169\":1,\"414\":2,\"418\":1,\"482\":1}}],[\"popular\",{\"1\":{\"122\":1,\"126\":1,\"128\":1,\"199\":1,\"203\":1,\"230\":1,\"257\":1,\"271\":1,\"336\":1,\"369\":1,\"374\":1,\"391\":1,\"395\":1,\"443\":1,\"457\":1,\"460\":1,\"467\":1,\"482\":1,\"495\":1}}],[\"pouya\",{\"1\":{\"404\":1}}],[\"poursaeed\",{\"1\":{\"319\":1}}],[\"poupyrev\",{\"1\":{\"81\":1}}],[\"poorly\",{\"1\":{\"316\":1,\"431\":1}}],[\"pool\",{\"1\":{\"153\":1}}],[\"porting\",{\"1\":{\"434\":2}}],[\"port\",{\"1\":{\"434\":1}}],[\"portfolio\",{\"1\":{\"428\":2}}],[\"portraying\",{\"1\":{\"303\":1}}],[\"portrait\",{\"1\":{\"151\":1,\"348\":1}}],[\"porat\",{\"1\":{\"309\":1}}],[\"pomdps\",{\"1\":{\"288\":1}}],[\"pomdp\",{\"0\":{\"288\":1},\"1\":{\"288\":1}}],[\"pods\",{\"1\":{\"243\":1}}],[\"ponochevnyi\",{\"1\":{\"163\":1}}],[\"polat\",{\"1\":{\"388\":1}}],[\"polarized\",{\"1\":{\"155\":1}}],[\"polarization\",{\"1\":{\"155\":1}}],[\"polo\",{\"1\":{\"375\":1}}],[\"poloclub\",{\"1\":{\"258\":1,\"375\":1,\"433\":1}}],[\"polysomnography\",{\"1\":{\"194\":1}}],[\"poličar\",{\"1\":{\"301\":1}}],[\"policy\",{\"0\":{\"205\":1},\"1\":{\"205\":2,\"291\":1,\"476\":1,\"498\":1}}],[\"policies\",{\"0\":{\"288\":1},\"1\":{\"121\":2,\"205\":2,\"261\":1,\"288\":2}}],[\"political\",{\"1\":{\"178\":1,\"234\":2,\"302\":1,\"400\":1,\"461\":1}}],[\"politics\",{\"1\":{\"155\":2}}],[\"posing\",{\"1\":{\"340\":1,\"478\":1}}],[\"positron\",{\"1\":{\"238\":1}}],[\"positive\",{\"0\":{\"200\":1},\"1\":{\"200\":3,\"227\":1,\"263\":1,\"274\":1,\"284\":1,\"304\":1,\"432\":1,\"444\":3}}],[\"positional\",{\"0\":{\"425\":1},\"1\":{\"425\":7}}],[\"positioning\",{\"1\":{\"274\":1}}],[\"positions\",{\"1\":{\"148\":1,\"182\":1,\"197\":1}}],[\"position\",{\"0\":{\"425\":1},\"1\":{\"97\":1,\"211\":1,\"234\":1,\"291\":1,\"312\":1,\"425\":1,\"476\":1}}],[\"pose\",{\"1\":{\"298\":1,\"381\":1,\"445\":1,\"463\":1,\"475\":1}}],[\"posed\",{\"1\":{\"234\":1,\"431\":1,\"460\":1}}],[\"poses\",{\"1\":{\"164\":1,\"182\":1,\"207\":1,\"273\":1,\"326\":1,\"359\":1,\"360\":1,\"414\":1,\"453\":1}}],[\"possess\",{\"1\":{\"196\":1,\"245\":1,\"251\":1,\"260\":1,\"423\":1,\"432\":1,\"487\":1}}],[\"possibilities\",{\"1\":{\"296\":1}}],[\"possibility\",{\"1\":{\"154\":1,\"158\":1,\"255\":1,\"485\":1}}],[\"possible\",{\"1\":{\"105\":1,\"112\":1,\"224\":1,\"253\":1,\"255\":1,\"282\":1,\"304\":1,\"308\":1,\"375\":1,\"389\":1,\"416\":1,\"427\":1,\"436\":1,\"444\":1,\"462\":1}}],[\"posteriorrmse\",{\"1\":{\"234\":1}}],[\"post\",{\"1\":{\"148\":1,\"273\":1,\"326\":3,\"382\":1,\"386\":1,\"400\":1,\"428\":1}}],[\"powering\",{\"1\":{\"230\":1}}],[\"powerful\",{\"1\":{\"136\":1,\"154\":1,\"161\":1,\"244\":1,\"317\":1,\"345\":1,\"354\":1,\"375\":1,\"415\":1}}],[\"powered\",{\"0\":{\"242\":1,\"437\":1,\"486\":1},\"1\":{\"114\":1,\"132\":1,\"148\":2,\"201\":1,\"242\":1,\"259\":1,\"263\":1,\"324\":1,\"330\":1}}],[\"power\",{\"0\":{\"223\":1,\"407\":1,\"449\":1,\"491\":1},\"1\":{\"103\":1,\"163\":1,\"206\":1,\"210\":1,\"217\":1,\"237\":1,\"250\":2,\"273\":1,\"293\":1,\"369\":1,\"404\":1,\"461\":2,\"469\":1,\"491\":1}}],[\"pisa\",{\"1\":{\"524\":1}}],[\"ping\",{\"1\":{\"376\":1}}],[\"pinpointing\",{\"1\":{\"316\":1}}],[\"pinto\",{\"1\":{\"261\":1}}],[\"piatti\",{\"1\":{\"292\":1}}],[\"piaget\",{\"1\":{\"79\":1}}],[\"pica\",{\"1\":{\"302\":1}}],[\"picking\",{\"1\":{\"313\":1}}],[\"pick\",{\"1\":{\"284\":1}}],[\"pictorials\",{\"1\":{\"217\":1}}],[\"pillai\",{\"1\":{\"268\":1,\"452\":1}}],[\"pilot\",{\"1\":{\"226\":1}}],[\"pilots\",{\"1\":{\"179\":1}}],[\"pipeline\",{\"1\":{\"256\":1,\"266\":1,\"286\":1,\"296\":1,\"399\":1,\"404\":1,\"413\":2,\"474\":1,\"480\":1}}],[\"pipelines\",{\"1\":{\"123\":1}}],[\"pioneering\",{\"1\":{\"243\":1,\"396\":1}}],[\"pitfalls\",{\"0\":{\"336\":1},\"1\":{\"243\":1}}],[\"pixel\",{\"1\":{\"213\":1,\"295\":1}}],[\"pierce\",{\"1\":{\"192\":1}}],[\"pierre\",{\"1\":{\"188\":1}}],[\"pierpaolo\",{\"1\":{\"171\":1}}],[\"pieces\",{\"0\":{\"135\":1},\"1\":{\"412\":1}}],[\"pivotal\",{\"1\":{\"96\":1,\"192\":1,\"316\":1,\"359\":1,\"377\":1,\"467\":1,\"478\":1}}],[\"palma\",{\"1\":{\"419\":1,\"420\":1}}],[\"palette\",{\"1\":{\"197\":2,\"436\":1}}],[\"palettes\",{\"0\":{\"197\":1},\"1\":{\"197\":2}}],[\"pays\",{\"1\":{\"405\":1}}],[\"paul\",{\"1\":{\"388\":1,\"498\":1}}],[\"papeft\",{\"1\":{\"425\":1}}],[\"papers\",{\"0\":{\"428\":1},\"1\":{\"159\":1,\"217\":1,\"243\":1,\"271\":1,\"428\":6,\"457\":1,\"492\":2}}],[\"papercad\",{\"1\":{\"113\":1}}],[\"paper\",{\"0\":{\"113\":1,\"115\":1},\"1\":{\"97\":1,\"100\":1,\"111\":1,\"113\":7,\"115\":3,\"117\":1,\"125\":1,\"128\":1,\"138\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"153\":2,\"154\":1,\"158\":1,\"159\":1,\"161\":1,\"167\":1,\"170\":1,\"172\":1,\"173\":1,\"180\":1,\"186\":1,\"189\":1,\"192\":1,\"194\":1,\"196\":1,\"203\":1,\"206\":1,\"211\":1,\"216\":1,\"221\":1,\"229\":1,\"231\":2,\"232\":1,\"240\":1,\"242\":1,\"243\":1,\"244\":1,\"253\":1,\"254\":1,\"257\":1,\"259\":1,\"268\":1,\"270\":1,\"277\":1,\"282\":1,\"283\":1,\"286\":2,\"287\":2,\"291\":1,\"296\":1,\"300\":1,\"306\":2,\"307\":1,\"312\":1,\"315\":1,\"316\":1,\"317\":1,\"318\":1,\"322\":1,\"323\":1,\"325\":1,\"326\":1,\"329\":1,\"334\":2,\"335\":1,\"336\":1,\"337\":1,\"339\":1,\"343\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"362\":1,\"364\":2,\"368\":1,\"370\":1,\"388\":1,\"389\":1,\"393\":2,\"394\":1,\"396\":1,\"403\":2,\"405\":1,\"407\":1,\"413\":1,\"414\":1,\"418\":1,\"422\":1,\"429\":1,\"432\":1,\"434\":1,\"438\":1,\"439\":1,\"442\":1,\"443\":1,\"449\":1,\"452\":1,\"455\":1,\"458\":1,\"465\":1,\"469\":1,\"474\":2,\"475\":1,\"476\":1,\"477\":1,\"481\":1,\"483\":1,\"486\":1,\"488\":1,\"489\":1,\"490\":1,\"492\":1,\"494\":1,\"497\":1,\"498\":2}}],[\"papa\",{\"1\":{\"232\":1}}],[\"pavlin\",{\"1\":{\"301\":1}}],[\"pavlo\",{\"1\":{\"297\":1}}],[\"paves\",{\"1\":{\"192\":1,\"237\":1,\"309\":1}}],[\"pavit\",{\"1\":{\"172\":1,\"370\":1}}],[\"paving\",{\"1\":{\"100\":1,\"305\":1,\"377\":1,\"437\":1,\"465\":1}}],[\"padmanabha\",{\"1\":{\"184\":1,\"380\":1}}],[\"pasdeloup\",{\"1\":{\"444\":1}}],[\"paste\",{\"1\":{\"172\":1,\"370\":1}}],[\"past\",{\"0\":{\"169\":1,\"249\":1},\"1\":{\"229\":1,\"239\":1,\"249\":1,\"264\":1,\"279\":1,\"448\":1}}],[\"pass\",{\"1\":{\"411\":1,\"415\":1}}],[\"passage\",{\"1\":{\"386\":1,\"481\":1}}],[\"passes\",{\"1\":{\"215\":1}}],[\"passerini\",{\"1\":{\"158\":1,\"390\":1,\"466\":1}}],[\"passive\",{\"1\":{\"207\":1,\"237\":2}}],[\"passionate\",{\"1\":{\"74\":1}}],[\"pasquale\",{\"1\":{\"149\":1,\"466\":1}}],[\"pang\",{\"1\":{\"361\":1}}],[\"pandemic\",{\"1\":{\"169\":1}}],[\"pandya\",{\"1\":{\"104\":1}}],[\"panacea\",{\"1\":{\"161\":1,\"354\":1}}],[\"pan\",{\"1\":{\"131\":1,\"317\":1,\"329\":1,\"377\":1}}],[\"pages\",{\"1\":{\"309\":1,\"345\":1}}],[\"page\",{\"0\":{\"345\":1},\"1\":{\"128\":1}}],[\"paediatricians\",{\"1\":{\"126\":1}}],[\"paola\",{\"1\":{\"120\":1}}],[\"paced\",{\"1\":{\"130\":1,\"398\":1}}],[\"pace\",{\"1\":{\"119\":1,\"498\":2}}],[\"patwa\",{\"1\":{\"405\":1}}],[\"patil\",{\"1\":{\"326\":1}}],[\"patient\",{\"1\":{\"309\":2,\"335\":1,\"356\":3,\"379\":1}}],[\"patients\",{\"0\":{\"356\":1},\"1\":{\"121\":1,\"284\":4,\"356\":2}}],[\"patrice\",{\"1\":{\"232\":1}}],[\"patras\",{\"1\":{\"123\":1}}],[\"patel\",{\"1\":{\"207\":1}}],[\"patches\",{\"1\":{\"488\":1}}],[\"patched\",{\"1\":{\"334\":1}}],[\"patching\",{\"1\":{\"390\":1}}],[\"patch\",{\"1\":{\"159\":1}}],[\"pathologist\",{\"1\":{\"176\":1}}],[\"pathologists\",{\"1\":{\"176\":2}}],[\"pathological\",{\"1\":{\"166\":1}}],[\"pathology\",{\"0\":{\"176\":1},\"1\":{\"176\":3}}],[\"paths\",{\"0\":{\"288\":1},\"1\":{\"164\":1,\"322\":3,\"404\":1}}],[\"path\",{\"1\":{\"112\":1,\"322\":1,\"462\":1,\"471\":1}}],[\"pattern\",{\"1\":{\"111\":1,\"158\":2,\"176\":1,\"198\":1,\"213\":1,\"231\":1,\"355\":1,\"387\":1,\"424\":1,\"488\":1}}],[\"patterns\",{\"0\":{\"111\":1,\"174\":1,\"213\":1,\"270\":1,\"371\":1},\"1\":{\"108\":1,\"111\":4,\"115\":1,\"116\":1,\"120\":1,\"142\":1,\"143\":1,\"146\":1,\"172\":2,\"174\":2,\"213\":8,\"218\":2,\"231\":1,\"237\":1,\"246\":1,\"257\":2,\"266\":2,\"268\":1,\"270\":3,\"290\":1,\"301\":2,\"308\":1,\"370\":2,\"371\":2,\"390\":1,\"417\":1,\"424\":1,\"427\":1,\"452\":1,\"461\":4,\"467\":1,\"469\":1,\"480\":1}}],[\"pait\",{\"1\":{\"248\":1}}],[\"paintings\",{\"1\":{\"296\":1}}],[\"pain\",{\"1\":{\"242\":1}}],[\"paillier\",{\"1\":{\"139\":1}}],[\"pai\",{\"1\":{\"108\":1,\"142\":1}}],[\"paired\",{\"1\":{\"419\":1,\"420\":1,\"492\":1}}],[\"pairwise\",{\"1\":{\"411\":1}}],[\"pairing\",{\"1\":{\"290\":1}}],[\"pairs\",{\"1\":{\"106\":5,\"164\":1,\"258\":1,\"290\":1,\"351\":1,\"368\":1,\"378\":3,\"402\":1,\"433\":1,\"447\":1}}],[\"pair\",{\"0\":{\"106\":1,\"290\":1,\"351\":1},\"1\":{\"106\":2,\"196\":1,\"290\":6,\"410\":1}}],[\"pafla\",{\"1\":{\"102\":1}}],[\"pamela\",{\"1\":{\"97\":1,\"206\":1,\"211\":1,\"312\":1}}],[\"parisien\",{\"1\":{\"344\":1}}],[\"paris\",{\"1\":{\"244\":1}}],[\"parent\",{\"1\":{\"206\":2}}],[\"parents\",{\"1\":{\"206\":1}}],[\"parsing\",{\"1\":{\"141\":1}}],[\"parser\",{\"1\":{\"141\":2}}],[\"parasuraman\",{\"1\":{\"486\":1}}],[\"parasaram\",{\"1\":{\"350\":1}}],[\"paramount\",{\"1\":{\"416\":1}}],[\"parametric\",{\"0\":{\"340\":1},\"1\":{\"188\":1,\"340\":1,\"453\":1}}],[\"parameterized\",{\"0\":{\"392\":1},\"1\":{\"391\":1}}],[\"parameters\",{\"1\":{\"191\":2,\"287\":1,\"306\":1,\"317\":1,\"395\":1,\"407\":1,\"427\":1,\"471\":1,\"480\":1}}],[\"parameter\",{\"0\":{\"425\":1,\"471\":1},\"1\":{\"112\":1,\"382\":1,\"389\":1,\"392\":1,\"405\":1,\"425\":2,\"436\":1,\"497\":1}}],[\"paramesran\",{\"1\":{\"137\":1}}],[\"paraphrases\",{\"1\":{\"386\":1}}],[\"parallel\",{\"1\":{\"365\":1,\"487\":1}}],[\"parallels\",{\"1\":{\"229\":1}}],[\"paradigms\",{\"1\":{\"308\":1,\"343\":1}}],[\"paradigm\",{\"0\":{\"171\":1,\"253\":1},\"1\":{\"110\":1,\"112\":1,\"158\":2,\"188\":1,\"239\":1,\"314\":1,\"321\":1,\"329\":1,\"377\":1,\"392\":1,\"408\":1,\"435\":2,\"483\":2,\"485\":2,\"497\":1}}],[\"paragraph\",{\"1\":{\"104\":1}}],[\"parker\",{\"1\":{\"207\":1,\"431\":1}}],[\"park\",{\"1\":{\"97\":1,\"121\":1,\"133\":1,\"201\":1,\"211\":1,\"312\":1,\"331\":1,\"356\":1,\"477\":1}}],[\"partly\",{\"1\":{\"443\":1}}],[\"parts\",{\"1\":{\"430\":1}}],[\"parth\",{\"1\":{\"405\":1}}],[\"partner\",{\"1\":{\"173\":1,\"180\":1,\"227\":1,\"275\":1}}],[\"partners\",{\"1\":{\"173\":3,\"227\":1,\"303\":1}}],[\"partnerships\",{\"1\":{\"100\":1,\"125\":1}}],[\"party\",{\"1\":{\"128\":1}}],[\"parties\",{\"1\":{\"186\":1,\"234\":1,\"498\":1}}],[\"partisans\",{\"1\":{\"155\":1}}],[\"partition\",{\"1\":{\"131\":1,\"250\":1}}],[\"particles\",{\"1\":{\"142\":1}}],[\"particular\",{\"1\":{\"119\":1,\"156\":1,\"237\":1,\"292\":1,\"302\":1,\"340\":1,\"375\":1,\"381\":1,\"389\":1}}],[\"particularly\",{\"1\":{\"97\":1,\"102\":1,\"125\":1,\"139\":1,\"165\":1,\"191\":1,\"192\":1,\"213\":1,\"237\":1,\"238\":1,\"253\":1,\"257\":1,\"266\":1,\"273\":1,\"282\":1,\"298\":1,\"312\":1,\"315\":1,\"319\":1,\"324\":1,\"339\":1,\"356\":1,\"362\":1,\"363\":1,\"425\":1,\"437\":2,\"445\":1,\"491\":1,\"495\":1}}],[\"participating\",{\"1\":{\"219\":1,\"279\":1,\"401\":1,\"404\":1}}],[\"participation\",{\"0\":{\"230\":1},\"1\":{\"108\":1,\"217\":1,\"230\":1,\"302\":2}}],[\"participatory\",{\"0\":{\"297\":1},\"1\":{\"171\":1,\"297\":1}}],[\"participate\",{\"1\":{\"155\":1,\"269\":1,\"302\":1,\"307\":1}}],[\"participant\",{\"1\":{\"102\":2,\"271\":1,\"457\":1}}],[\"participants\",{\"1\":{\"97\":1,\"98\":1,\"102\":1,\"105\":1,\"107\":1,\"120\":3,\"135\":2,\"154\":1,\"155\":1,\"160\":2,\"185\":3,\"187\":1,\"189\":2,\"195\":1,\"196\":2,\"197\":1,\"198\":1,\"203\":1,\"204\":1,\"209\":2,\"214\":1,\"243\":1,\"245\":1,\"248\":1,\"252\":4,\"254\":2,\"273\":1,\"274\":1,\"282\":1,\"289\":1,\"290\":2,\"291\":1,\"295\":1,\"297\":1,\"303\":1,\"304\":1,\"312\":1,\"387\":1,\"426\":4,\"476\":1}}],[\"partially\",{\"1\":{\"286\":1,\"288\":1,\"454\":1,\"474\":1}}],[\"partial\",{\"1\":{\"96\":1,\"118\":1,\"253\":1,\"462\":1}}],[\"part\",{\"1\":{\"96\":1,\"107\":1,\"125\":1,\"153\":3,\"161\":1,\"211\":1,\"335\":1,\"354\":1,\"384\":1,\"402\":1,\"497\":4}}],[\"philip\",{\"1\":{\"496\":1}}],[\"philosophy\",{\"1\":{\"175\":1}}],[\"philosophers\",{\"0\":{\"175\":1},\"1\":{\"175\":3}}],[\"phrase\",{\"1\":{\"329\":1}}],[\"phenotypes\",{\"1\":{\"281\":1}}],[\"phenomenon\",{\"1\":{\"278\":1,\"293\":1}}],[\"phenomena\",{\"1\":{\"112\":1,\"171\":1,\"328\":1,\"340\":2}}],[\"phute\",{\"1\":{\"258\":1,\"433\":1}}],[\"phone\",{\"1\":{\"237\":2}}],[\"photos\",{\"1\":{\"302\":1}}],[\"photoshop\",{\"1\":{\"8\":2}}],[\"photorealistic\",{\"1\":{\"101\":1}}],[\"pharmacological\",{\"1\":{\"220\":1}}],[\"phases\",{\"1\":{\"236\":1,\"244\":1,\"254\":1,\"271\":2,\"414\":1,\"445\":1,\"457\":2}}],[\"phase\",{\"1\":{\"153\":4,\"266\":1,\"344\":1,\"382\":1}}],[\"phan\",{\"1\":{\"137\":1}}],[\"phantom\",{\"1\":{\"107\":4}}],[\"phantasisland\",{\"0\":{\"75\":1},\"1\":{\"9\":2,\"17\":3,\"76\":1,\"78\":1,\"530\":1}}],[\"physio\",{\"1\":{\"298\":1}}],[\"physiology\",{\"1\":{\"204\":1}}],[\"physiological\",{\"0\":{\"120\":1,\"298\":1},\"1\":{\"120\":4,\"194\":1,\"204\":2,\"223\":2,\"284\":2,\"298\":2}}],[\"physician\",{\"1\":{\"356\":1,\"466\":2}}],[\"physicians\",{\"0\":{\"466\":1},\"1\":{\"335\":1,\"466\":3}}],[\"physicsassistant\",{\"0\":{\"486\":1},\"1\":{\"486\":6}}],[\"physics\",{\"0\":{\"486\":1},\"1\":{\"207\":1,\"447\":1,\"486\":2}}],[\"physical\",{\"1\":{\"112\":1,\"126\":1,\"140\":3,\"165\":1,\"202\":1,\"207\":1,\"211\":1,\"239\":1,\"253\":1,\"399\":1,\"519\":3}}],[\"physically\",{\"0\":{\"184\":1,\"380\":1},\"1\":{\"107\":3,\"140\":1,\"184\":2,\"380\":2}}],[\"pruning\",{\"1\":{\"427\":1}}],[\"prybylo\",{\"1\":{\"254\":1}}],[\"prashanth\",{\"1\":{\"374\":1}}],[\"prasoon\",{\"1\":{\"344\":1}}],[\"prasanna\",{\"1\":{\"215\":1}}],[\"pratipati\",{\"1\":{\"200\":1}}],[\"pratiush\",{\"1\":{\"112\":1}}],[\"pragmatic\",{\"1\":{\"179\":1,\"291\":1,\"359\":1,\"476\":1}}],[\"practicality\",{\"1\":{\"410\":1,\"430\":1}}],[\"practical\",{\"1\":{\"146\":1,\"239\":1,\"244\":1,\"245\":1,\"272\":1,\"283\":1,\"297\":1,\"324\":1,\"339\":1,\"358\":1,\"379\":2,\"391\":1,\"445\":1,\"447\":1,\"449\":1,\"462\":1,\"475\":1,\"479\":2,\"488\":1,\"498\":1}}],[\"practicing\",{\"1\":{\"132\":1,\"330\":1}}],[\"practices\",{\"0\":{\"144\":1,\"286\":1,\"474\":1,\"492\":1},\"1\":{\"128\":2,\"144\":1,\"189\":1,\"206\":1,\"231\":1,\"254\":1,\"277\":1,\"286\":1,\"300\":1,\"328\":1,\"364\":1,\"428\":1,\"460\":1,\"474\":1}}],[\"practice\",{\"0\":{\"6\":1,\"307\":1},\"1\":{\"132\":2,\"180\":2,\"202\":1,\"307\":2,\"308\":1,\"330\":2,\"381\":1,\"385\":1,\"427\":1}}],[\"practitioners\",{\"0\":{\"164\":1},\"1\":{\"104\":2,\"128\":1,\"133\":1,\"164\":3,\"205\":1,\"210\":4,\"250\":1,\"297\":1,\"331\":1,\"482\":1}}],[\"printable\",{\"1\":{\"306\":1}}],[\"printed\",{\"0\":{\"306\":1},\"1\":{\"306\":1}}],[\"principal\",{\"1\":{\"382\":1}}],[\"principle\",{\"1\":{\"255\":1,\"475\":1}}],[\"principles\",{\"1\":{\"104\":1,\"142\":1,\"453\":1}}],[\"principe\",{\"1\":{\"147\":1}}],[\"privileges\",{\"1\":{\"205\":1}}],[\"private\",{\"0\":{\"435\":1},\"1\":{\"139\":1,\"192\":1,\"435\":1}}],[\"privacy\",{\"0\":{\"139\":1,\"187\":1,\"206\":1,\"254\":1,\"446\":1},\"1\":{\"128\":1,\"139\":4,\"146\":1,\"178\":4,\"187\":14,\"206\":6,\"210\":1,\"254\":8,\"334\":1,\"343\":1,\"374\":1,\"379\":1,\"403\":1,\"435\":4,\"446\":2,\"475\":1}}],[\"prior\",{\"1\":{\"142\":1,\"169\":1,\"206\":1,\"212\":1,\"220\":1,\"257\":3,\"293\":1,\"382\":2,\"384\":1,\"394\":1,\"395\":1,\"409\":1,\"453\":1,\"460\":1,\"461\":1,\"472\":1,\"482\":1}}],[\"priorities\",{\"1\":{\"286\":1,\"474\":1}}],[\"prioritizing\",{\"1\":{\"146\":1,\"290\":1,\"291\":1,\"377\":1,\"476\":1}}],[\"prioritizes\",{\"1\":{\"361\":1,\"483\":1}}],[\"prioritize\",{\"1\":{\"98\":1,\"142\":1,\"155\":1}}],[\"priori\",{\"1\":{\"112\":1}}],[\"prime\",{\"1\":{\"363\":2}}],[\"primarily\",{\"1\":{\"133\":1,\"186\":1,\"215\":1,\"300\":1,\"324\":1,\"331\":1,\"377\":1,\"379\":1,\"411\":1,\"439\":1,\"458\":1,\"472\":1,\"489\":1}}],[\"primary\",{\"1\":{\"116\":2,\"200\":1,\"244\":1,\"254\":1,\"257\":1,\"271\":1,\"280\":1,\"333\":2,\"340\":1,\"398\":1,\"425\":1,\"437\":1,\"457\":1,\"465\":1,\"487\":1}}],[\"primitive\",{\"1\":{\"119\":1}}],[\"pro\",{\"1\":{\"334\":1,\"345\":1,\"372\":1,\"418\":1,\"467\":1}}],[\"pronouns\",{\"1\":{\"395\":3}}],[\"pronoun\",{\"0\":{\"395\":1},\"1\":{\"395\":6}}],[\"pronounced\",{\"1\":{\"328\":1,\"339\":1}}],[\"prone\",{\"1\":{\"160\":1,\"227\":1,\"340\":1,\"356\":1,\"372\":1,\"437\":1,\"445\":1,\"467\":1,\"482\":1}}],[\"proliferation\",{\"1\":{\"242\":1,\"478\":1}}],[\"prolific\",{\"1\":{\"196\":3,\"198\":1,\"387\":1}}],[\"proofs\",{\"1\":{\"322\":1}}],[\"proof\",{\"1\":{\"232\":2,\"322\":4}}],[\"proxy\",{\"1\":{\"215\":1,\"245\":1}}],[\"proxies\",{\"1\":{\"215\":1}}],[\"proximity\",{\"1\":{\"173\":1}}],[\"progan\",{\"1\":{\"213\":1}}],[\"programs\",{\"0\":{\"434\":1},\"1\":{\"342\":1}}],[\"programme\",{\"1\":{\"524\":1}}],[\"programmer\",{\"1\":{\"215\":4,\"434\":2}}],[\"programmers\",{\"0\":{\"215\":1},\"1\":{\"215\":1}}],[\"programmable\",{\"1\":{\"363\":1}}],[\"programmatically\",{\"1\":{\"207\":1}}],[\"programming\",{\"0\":{\"96\":1,\"106\":1,\"228\":1,\"290\":1},\"1\":{\"96\":1,\"99\":1,\"106\":3,\"172\":3,\"228\":1,\"290\":6,\"342\":2,\"370\":3,\"388\":1,\"414\":1}}],[\"program\",{\"0\":{\"350\":1,\"360\":1},\"1\":{\"170\":1,\"178\":1,\"232\":1,\"350\":1,\"360\":1,\"434\":1,\"482\":2}}],[\"progressively\",{\"1\":{\"418\":1}}],[\"progressive\",{\"0\":{\"355\":1},\"1\":{\"355\":1}}],[\"progression\",{\"0\":{\"121\":1},\"1\":{\"112\":1,\"121\":1}}],[\"progress\",{\"0\":{\"338\":1},\"1\":{\"166\":1,\"170\":1,\"217\":1,\"275\":1,\"338\":2,\"382\":1,\"395\":1,\"443\":1,\"498\":1}}],[\"probability\",{\"1\":{\"368\":1}}],[\"probable\",{\"1\":{\"240\":1}}],[\"probing\",{\"0\":{\"328\":1},\"1\":{\"328\":1}}],[\"probes\",{\"1\":{\"424\":1}}],[\"probe\",{\"1\":{\"191\":1,\"199\":1,\"218\":1}}],[\"problematic\",{\"1\":{\"149\":1,\"241\":1}}],[\"problem\",{\"0\":{\"350\":1},\"1\":{\"132\":1,\"143\":2,\"228\":2,\"237\":1,\"251\":1,\"292\":1,\"301\":1,\"325\":1,\"330\":1,\"350\":2,\"364\":1,\"372\":1,\"378\":1,\"382\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"423\":1,\"439\":1,\"444\":1,\"458\":1,\"468\":1,\"469\":2,\"470\":1,\"471\":1,\"482\":1,\"494\":1}}],[\"problems\",{\"0\":{\"153\":1,\"292\":1},\"1\":{\"99\":1,\"112\":1,\"132\":1,\"164\":1,\"171\":1,\"235\":1,\"251\":1,\"292\":4,\"330\":1,\"342\":1,\"353\":1,\"355\":1,\"402\":1,\"411\":1,\"423\":1,\"471\":1,\"482\":2,\"490\":1,\"494\":1}}],[\"prosthetic\",{\"0\":{\"284\":1},\"1\":{\"284\":5}}],[\"prosthesis\",{\"1\":{\"220\":1}}],[\"prostheses\",{\"0\":{\"220\":1},\"1\":{\"220\":1}}],[\"prosody\",{\"1\":{\"226\":1}}],[\"prosocial\",{\"0\":{\"303\":1},\"1\":{\"155\":2,\"274\":1,\"303\":7}}],[\"prospects\",{\"0\":{\"212\":1},\"1\":{\"212\":1,\"297\":1}}],[\"pros\",{\"1\":{\"189\":1}}],[\"proactivity\",{\"1\":{\"175\":1}}],[\"proactively\",{\"1\":{\"155\":1}}],[\"protracted\",{\"1\":{\"359\":1}}],[\"protected\",{\"1\":{\"394\":1}}],[\"protecting\",{\"1\":{\"210\":1,\"279\":1}}],[\"protection\",{\"1\":{\"170\":1,\"187\":7,\"446\":1}}],[\"protect\",{\"1\":{\"178\":1,\"302\":1}}],[\"prototypical\",{\"1\":{\"292\":1,\"497\":3}}],[\"prototyping\",{\"0\":{\"306\":1},\"1\":{\"114\":1,\"181\":1,\"188\":1,\"306\":2,\"361\":1}}],[\"prototypes\",{\"1\":{\"155\":1,\"189\":4,\"207\":1,\"236\":3,\"306\":1}}],[\"prototype\",{\"1\":{\"153\":2,\"171\":2,\"181\":1,\"189\":1,\"251\":1,\"259\":1,\"270\":1,\"284\":1,\"300\":1,\"423\":1}}],[\"protocols\",{\"1\":{\"358\":2}}],[\"protocol\",{\"0\":{\"358\":1},\"1\":{\"139\":3,\"170\":1,\"358\":3,\"399\":1}}],[\"proceed\",{\"1\":{\"161\":1,\"354\":1}}],[\"proceedings\",{\"0\":{\"147\":1},\"1\":{\"147\":1}}],[\"procedural\",{\"0\":{\"259\":1},\"1\":{\"132\":1,\"259\":1,\"330\":1,\"360\":2,\"486\":1}}],[\"procedures\",{\"1\":{\"133\":1,\"222\":1,\"331\":1}}],[\"procedure\",{\"1\":{\"132\":2,\"174\":1,\"202\":1,\"301\":1,\"318\":1,\"330\":2,\"371\":1,\"377\":1,\"496\":1}}],[\"processed\",{\"1\":{\"232\":1,\"454\":1,\"469\":1,\"479\":1}}],[\"processes\",{\"0\":{\"246\":1},\"1\":{\"144\":1,\"171\":1,\"176\":1,\"195\":2,\"204\":1,\"205\":1,\"216\":1,\"244\":2,\"249\":1,\"288\":1,\"292\":1,\"379\":1,\"390\":1,\"396\":1,\"414\":1,\"478\":1,\"498\":1}}],[\"process\",{\"0\":{\"222\":1,\"244\":1},\"1\":{\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":2,\"122\":1,\"123\":1,\"125\":1,\"127\":1,\"139\":1,\"144\":1,\"154\":2,\"169\":1,\"171\":2,\"176\":1,\"204\":2,\"212\":1,\"213\":1,\"218\":2,\"222\":1,\"236\":2,\"242\":1,\"244\":1,\"248\":2,\"249\":1,\"253\":1,\"259\":2,\"260\":1,\"271\":1,\"277\":1,\"289\":1,\"290\":1,\"296\":1,\"306\":1,\"314\":1,\"318\":1,\"324\":1,\"329\":1,\"339\":1,\"343\":2,\"347\":1,\"356\":2,\"360\":1,\"377\":1,\"384\":1,\"393\":1,\"396\":1,\"403\":1,\"425\":1,\"434\":1,\"450\":2,\"454\":1,\"457\":1,\"461\":2,\"475\":1,\"480\":1,\"492\":1,\"495\":1,\"497\":1,\"498\":1}}],[\"processing\",{\"0\":{\"113\":1,\"359\":1},\"1\":{\"113\":1,\"139\":1,\"144\":1,\"163\":1,\"169\":1,\"253\":1,\"282\":1,\"284\":1,\"314\":1,\"359\":2,\"372\":1,\"377\":2,\"393\":1,\"395\":2,\"403\":1,\"412\":1,\"416\":1,\"422\":1,\"450\":1,\"455\":1,\"459\":1,\"469\":1,\"479\":1,\"496\":1,\"539\":1}}],[\"profession\",{\"1\":{\"431\":1}}],[\"professions\",{\"1\":{\"272\":1}}],[\"professional\",{\"1\":{\"144\":1,\"175\":1,\"217\":1,\"253\":1,\"308\":1}}],[\"professionals\",{\"1\":{\"105\":1,\"114\":1,\"117\":1,\"176\":1,\"252\":1,\"261\":1,\"399\":1,\"426\":1}}],[\"profoundly\",{\"1\":{\"212\":1}}],[\"profile\",{\"0\":{\"447\":1},\"1\":{\"447\":1}}],[\"profiles\",{\"1\":{\"130\":1,\"142\":1,\"447\":1}}],[\"proficient\",{\"1\":{\"431\":1}}],[\"proficiently\",{\"1\":{\"336\":1,\"480\":1}}],[\"proficiency\",{\"0\":{\"482\":1},\"1\":{\"127\":1,\"133\":1,\"251\":1,\"323\":1,\"331\":1,\"359\":1,\"377\":1,\"392\":1,\"423\":1,\"458\":1,\"477\":1}}],[\"profit\",{\"1\":{\"154\":1,\"192\":1,\"233\":4}}],[\"projecting\",{\"1\":{\"188\":1}}],[\"projection\",{\"1\":{\"111\":1,\"188\":5,\"450\":2}}],[\"projections\",{\"1\":{\"111\":2,\"188\":2,\"190\":1}}],[\"project\",{\"0\":{\"308\":1,\"511\":1},\"1\":{\"153\":1,\"184\":1,\"232\":1,\"307\":1,\"308\":3,\"358\":1,\"369\":1,\"380\":1,\"419\":1,\"420\":1,\"450\":1,\"504\":1,\"511\":1}}],[\"projects\",{\"1\":{\"117\":1,\"286\":1,\"308\":2,\"350\":1,\"360\":1,\"364\":1,\"474\":1}}],[\"promoting\",{\"1\":{\"266\":1,\"268\":2,\"301\":1,\"452\":2}}],[\"promotion\",{\"1\":{\"183\":1,\"302\":1}}],[\"promotes\",{\"1\":{\"290\":1}}],[\"promote\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"113\":1,\"120\":1,\"155\":1,\"306\":1}}],[\"promptly\",{\"1\":{\"243\":1,\"298\":1,\"327\":1}}],[\"promptrpa\",{\"0\":{\"222\":1},\"1\":{\"222\":4}}],[\"prompted\",{\"0\":{\"427\":1},\"1\":{\"160\":1,\"382\":1,\"390\":1,\"494\":1}}],[\"prompting\",{\"0\":{\"362\":1,\"365\":1,\"386\":1,\"483\":1},\"1\":{\"104\":1,\"123\":1,\"241\":1,\"271\":2,\"323\":1,\"339\":1,\"355\":1,\"365\":3,\"382\":1,\"386\":2,\"388\":1,\"404\":2,\"409\":1,\"417\":1,\"455\":2,\"457\":2,\"470\":3,\"475\":1,\"491\":2}}],[\"prompts\",{\"0\":{\"209\":1,\"222\":1},\"1\":{\"104\":2,\"110\":1,\"114\":1,\"172\":1,\"209\":4,\"222\":1,\"268\":1,\"275\":1,\"316\":1,\"321\":1,\"323\":1,\"342\":1,\"350\":4,\"353\":1,\"359\":3,\"365\":1,\"370\":1,\"391\":1,\"404\":2,\"409\":2,\"422\":4,\"452\":1,\"471\":1,\"483\":1}}],[\"prompt\",{\"0\":{\"104\":1,\"163\":1,\"359\":1,\"427\":1},\"1\":{\"104\":1,\"110\":1,\"136\":2,\"209\":4,\"246\":1,\"251\":1,\"252\":1,\"318\":1,\"321\":1,\"349\":2,\"350\":2,\"353\":1,\"358\":1,\"359\":2,\"360\":1,\"362\":1,\"368\":2,\"374\":1,\"375\":1,\"381\":1,\"382\":1,\"384\":1,\"388\":1,\"392\":1,\"403\":1,\"404\":1,\"405\":1,\"409\":1,\"410\":1,\"418\":1,\"422\":4,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"432\":1,\"442\":2,\"453\":1,\"466\":3,\"472\":1,\"491\":1}}],[\"promise\",{\"1\":{\"132\":1,\"246\":1,\"330\":1,\"345\":1,\"356\":1,\"360\":1,\"376\":1,\"495\":1}}],[\"promises\",{\"1\":{\"120\":1,\"133\":1,\"182\":1,\"244\":1,\"331\":1}}],[\"promising\",{\"1\":{\"97\":1,\"120\":2,\"121\":1,\"133\":1,\"138\":1,\"144\":1,\"152\":1,\"154\":1,\"155\":1,\"194\":1,\"201\":1,\"213\":1,\"222\":1,\"274\":1,\"278\":1,\"291\":1,\"309\":1,\"312\":1,\"331\":1,\"339\":1,\"356\":1,\"378\":1,\"386\":1,\"408\":1,\"417\":1,\"437\":1,\"476\":1}}],[\"prominent\",{\"1\":{\"98\":1,\"265\":1,\"481\":1}}],[\"products\",{\"1\":{\"396\":3,\"468\":1}}],[\"production\",{\"1\":{\"205\":1}}],[\"productivity\",{\"0\":{\"286\":1,\"474\":1},\"1\":{\"106\":1,\"172\":2,\"214\":2,\"215\":2,\"370\":2}}],[\"productive\",{\"1\":{\"100\":1,\"214\":1}}],[\"product\",{\"0\":{\"396\":1},\"1\":{\"179\":1,\"254\":1,\"263\":1,\"396\":7,\"519\":1}}],[\"produces\",{\"1\":{\"266\":1,\"301\":1,\"422\":1}}],[\"producers\",{\"1\":{\"265\":1}}],[\"produced\",{\"1\":{\"102\":1,\"110\":1,\"149\":1,\"213\":1,\"253\":1,\"289\":1,\"321\":1,\"359\":1,\"428\":1,\"436\":1,\"466\":1}}],[\"produce\",{\"1\":{\"99\":1,\"114\":1,\"130\":1,\"140\":1,\"146\":1,\"198\":1,\"207\":1,\"213\":1,\"235\":1,\"289\":1,\"337\":1,\"349\":1,\"387\":1,\"389\":1,\"427\":1,\"436\":1,\"437\":1,\"466\":1}}],[\"provably\",{\"1\":{\"378\":1}}],[\"provable\",{\"0\":{\"378\":1}}],[\"proving\",{\"0\":{\"322\":1},\"1\":{\"322\":2,\"494\":1}}],[\"providing\",{\"1\":{\"104\":1,\"126\":1,\"127\":1,\"131\":2,\"135\":1,\"160\":1,\"183\":1,\"187\":1,\"188\":1,\"191\":1,\"195\":1,\"224\":1,\"228\":2,\"246\":1,\"261\":1,\"288\":1,\"326\":1,\"349\":1,\"359\":1,\"379\":1,\"384\":2,\"398\":1,\"413\":1,\"429\":1,\"430\":1}}],[\"provider\",{\"1\":{\"465\":1}}],[\"providers\",{\"1\":{\"334\":1}}],[\"provided\",{\"1\":{\"128\":1,\"148\":1,\"159\":1,\"187\":4,\"202\":1,\"258\":1,\"282\":1,\"288\":1,\"365\":1,\"375\":1,\"384\":1,\"388\":1,\"391\":1,\"433\":1}}],[\"provides\",{\"1\":{\"107\":1,\"110\":1,\"111\":1,\"124\":1,\"125\":1,\"126\":1,\"156\":1,\"172\":1,\"176\":1,\"199\":1,\"205\":1,\"220\":1,\"223\":1,\"237\":1,\"258\":1,\"284\":1,\"292\":1,\"316\":1,\"321\":1,\"340\":1,\"365\":1,\"370\":1,\"413\":1,\"417\":1,\"433\":1,\"470\":1,\"475\":1,\"477\":1,\"482\":1,\"494\":2}}],[\"provide\",{\"0\":{\"123\":1},\"1\":{\"96\":2,\"97\":1,\"126\":1,\"128\":2,\"130\":1,\"131\":1,\"141\":1,\"158\":1,\"160\":1,\"173\":2,\"178\":1,\"179\":2,\"184\":1,\"191\":1,\"197\":1,\"199\":2,\"206\":1,\"216\":1,\"227\":2,\"231\":1,\"232\":1,\"233\":1,\"237\":1,\"239\":1,\"261\":1,\"288\":1,\"290\":1,\"292\":1,\"296\":1,\"303\":1,\"306\":1,\"312\":1,\"317\":1,\"324\":1,\"339\":1,\"368\":1,\"369\":1,\"380\":1,\"389\":1,\"409\":1,\"417\":1,\"430\":1,\"439\":1,\"447\":1,\"455\":1,\"459\":1,\"466\":1,\"469\":1,\"486\":4,\"489\":1,\"496\":1}}],[\"provoking\",{\"1\":{\"317\":1}}],[\"proven\",{\"1\":{\"223\":1}}],[\"proved\",{\"1\":{\"167\":1}}],[\"proves\",{\"1\":{\"100\":1}}],[\"prove\",{\"1\":{\"96\":1,\"282\":1,\"340\":1,\"445\":1,\"470\":1}}],[\"propitious\",{\"1\":{\"359\":1}}],[\"proprietary\",{\"1\":{\"257\":1,\"363\":1,\"365\":1,\"497\":1}}],[\"propagation\",{\"0\":{\"260\":1}}],[\"propagating\",{\"1\":{\"133\":1,\"331\":1}}],[\"propagates\",{\"1\":{\"241\":1,\"260\":1}}],[\"propagate\",{\"1\":{\"213\":1}}],[\"propel\",{\"1\":{\"491\":1}}],[\"propelled\",{\"1\":{\"212\":1}}],[\"properly\",{\"1\":{\"257\":1}}],[\"property\",{\"1\":{\"199\":1,\"449\":1}}],[\"properties\",{\"1\":{\"96\":1,\"112\":1,\"132\":2,\"199\":3,\"207\":2,\"240\":1,\"241\":1,\"330\":2,\"389\":1,\"445\":2}}],[\"proper\",{\"1\":{\"128\":1}}],[\"proportion\",{\"1\":{\"362\":1,\"378\":1,\"428\":1}}],[\"proportionate\",{\"1\":{\"191\":1}}],[\"proportional\",{\"1\":{\"103\":1,\"215\":1,\"249\":1,\"351\":1,\"469\":1}}],[\"propositional\",{\"0\":{\"322\":1},\"1\":{\"322\":1}}],[\"proposing\",{\"1\":{\"96\":1}}],[\"proposals\",{\"1\":{\"149\":2}}],[\"proposal\",{\"0\":{\"129\":1}}],[\"proposes\",{\"1\":{\"137\":1,\"157\":1,\"253\":1,\"287\":1,\"313\":1,\"318\":1,\"341\":1,\"368\":1,\"413\":1,\"414\":1,\"479\":1,\"486\":1}}],[\"proposed\",{\"1\":{\"126\":3,\"132\":1,\"138\":1,\"149\":1,\"150\":1,\"165\":1,\"183\":1,\"184\":1,\"186\":1,\"190\":4,\"196\":2,\"201\":3,\"223\":1,\"234\":1,\"235\":2,\"238\":1,\"239\":2,\"240\":1,\"264\":1,\"282\":1,\"289\":1,\"298\":3,\"319\":1,\"323\":1,\"324\":1,\"326\":1,\"330\":1,\"355\":2,\"362\":1,\"375\":1,\"376\":1,\"380\":1,\"393\":1,\"402\":1,\"408\":1,\"410\":3,\"425\":1,\"437\":1,\"444\":1,\"445\":2,\"447\":1,\"448\":1,\"450\":2,\"460\":1,\"469\":1,\"471\":1,\"478\":2,\"481\":2,\"483\":1,\"485\":1}}],[\"propose\",{\"1\":{\"108\":1,\"113\":1,\"121\":1,\"123\":1,\"124\":1,\"126\":1,\"130\":1,\"131\":1,\"141\":2,\"150\":1,\"158\":1,\"169\":1,\"170\":1,\"175\":1,\"191\":1,\"199\":1,\"206\":1,\"207\":1,\"209\":1,\"216\":1,\"220\":1,\"224\":1,\"236\":1,\"238\":1,\"240\":1,\"242\":1,\"264\":1,\"270\":1,\"283\":1,\"284\":1,\"289\":1,\"294\":1,\"298\":1,\"301\":1,\"303\":1,\"325\":1,\"329\":2,\"338\":1,\"342\":1,\"344\":1,\"355\":1,\"360\":1,\"362\":1,\"366\":1,\"375\":1,\"376\":1,\"378\":1,\"382\":1,\"386\":1,\"389\":1,\"390\":1,\"392\":1,\"396\":1,\"399\":1,\"403\":1,\"405\":1,\"407\":1,\"408\":1,\"410\":2,\"415\":1,\"416\":1,\"419\":1,\"420\":1,\"432\":1,\"435\":1,\"436\":1,\"438\":1,\"442\":1,\"444\":2,\"445\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":2,\"453\":1,\"454\":1,\"455\":1,\"460\":1,\"462\":2,\"467\":1,\"468\":1,\"469\":1,\"471\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":2,\"485\":1,\"487\":1,\"488\":1,\"490\":1,\"491\":1,\"495\":1}}],[\"preum\",{\"1\":{\"431\":1}}],[\"preuve\",{\"0\":{\"232\":1}}],[\"preexisting\",{\"1\":{\"408\":1}}],[\"preprints\",{\"1\":{\"428\":1}}],[\"preprocessing\",{\"1\":{\"398\":1}}],[\"preparation\",{\"1\":{\"358\":1,\"462\":1}}],[\"prepare\",{\"1\":{\"192\":1}}],[\"pretraining\",{\"0\":{\"377\":1,\"478\":1},\"1\":{\"377\":1,\"439\":1,\"478\":3}}],[\"pretrained\",{\"1\":{\"376\":2,\"417\":1}}],[\"precious\",{\"1\":{\"243\":1}}],[\"precisely\",{\"1\":{\"271\":1,\"366\":1,\"375\":1,\"457\":1}}],[\"precise\",{\"0\":{\"375\":1},\"1\":{\"151\":1,\"153\":1,\"201\":1,\"244\":1,\"246\":1,\"317\":1,\"333\":1,\"337\":1,\"348\":1,\"375\":2,\"428\":1}}],[\"precision\",{\"1\":{\"120\":1,\"141\":1,\"176\":1,\"191\":1,\"430\":1,\"447\":1,\"456\":1,\"458\":1,\"469\":1}}],[\"prefill\",{\"1\":{\"414\":1}}],[\"prefilling\",{\"1\":{\"409\":2}}],[\"prefrontal\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"prefer\",{\"1\":{\"233\":1,\"252\":2,\"426\":2}}],[\"preferred\",{\"0\":{\"407\":1},\"1\":{\"203\":1,\"233\":1,\"378\":2,\"400\":1,\"407\":1}}],[\"preference\",{\"0\":{\"158\":1,\"189\":1,\"378\":1,\"411\":1},\"1\":{\"140\":1,\"152\":1,\"166\":2,\"167\":2,\"189\":4,\"196\":1,\"215\":1,\"233\":1,\"290\":1,\"378\":4,\"411\":5,\"417\":1,\"424\":2,\"453\":2}}],[\"preferences\",{\"0\":{\"196\":1,\"233\":1,\"468\":1},\"1\":{\"114\":1,\"158\":1,\"166\":2,\"167\":1,\"184\":1,\"195\":1,\"196\":1,\"215\":1,\"233\":5,\"269\":1,\"378\":2,\"380\":1,\"425\":2,\"453\":1,\"468\":4}}],[\"presume\",{\"1\":{\"236\":1}}],[\"pressure\",{\"0\":{\"220\":1},\"1\":{\"220\":3,\"443\":1}}],[\"presence\",{\"0\":{\"304\":1},\"1\":{\"173\":1,\"199\":1,\"200\":1,\"203\":2,\"304\":3,\"328\":1,\"339\":1,\"378\":1,\"395\":1,\"469\":1}}],[\"presenting\",{\"1\":{\"478\":1}}],[\"presentations\",{\"1\":{\"217\":1,\"243\":1}}],[\"presentation\",{\"1\":{\"156\":1,\"249\":1,\"417\":1}}],[\"presented\",{\"1\":{\"127\":1,\"148\":1,\"191\":1,\"235\":1,\"244\":1,\"256\":2,\"263\":1,\"298\":1,\"412\":1,\"492\":1}}],[\"presents\",{\"1\":{\"100\":1,\"116\":1,\"129\":1,\"136\":1,\"146\":1,\"149\":1,\"153\":1,\"154\":1,\"174\":1,\"186\":1,\"210\":1,\"222\":1,\"232\":1,\"246\":1,\"269\":1,\"278\":1,\"359\":1,\"371\":1,\"384\":1,\"394\":1,\"429\":1,\"458\":1}}],[\"present\",{\"1\":{\"98\":1,\"99\":1,\"100\":1,\"104\":1,\"105\":1,\"110\":1,\"122\":1,\"126\":3,\"132\":1,\"142\":1,\"150\":1,\"151\":1,\"155\":1,\"161\":1,\"169\":1,\"180\":1,\"184\":2,\"198\":1,\"199\":1,\"205\":1,\"207\":1,\"216\":2,\"222\":1,\"229\":1,\"235\":1,\"238\":1,\"248\":1,\"249\":1,\"250\":1,\"252\":1,\"258\":1,\"260\":1,\"266\":1,\"270\":1,\"272\":3,\"274\":1,\"278\":2,\"279\":1,\"282\":2,\"286\":1,\"295\":1,\"296\":2,\"305\":1,\"315\":1,\"321\":1,\"326\":1,\"330\":1,\"339\":1,\"348\":1,\"349\":1,\"351\":1,\"352\":1,\"354\":1,\"355\":1,\"379\":1,\"380\":2,\"384\":1,\"387\":1,\"395\":1,\"399\":1,\"400\":1,\"414\":1,\"426\":1,\"431\":1,\"433\":1,\"434\":1,\"437\":1,\"465\":1,\"474\":1,\"489\":1,\"496\":1}}],[\"preservation\",{\"1\":{\"255\":1,\"260\":1,\"349\":1,\"359\":1,\"475\":1}}],[\"preserves\",{\"1\":{\"400\":1}}],[\"preserve\",{\"1\":{\"146\":1,\"435\":1}}],[\"preserving\",{\"0\":{\"139\":1},\"1\":{\"139\":3,\"235\":1,\"238\":1,\"435\":1,\"453\":1,\"463\":1,\"491\":1}}],[\"pre\",{\"0\":{\"194\":1},\"1\":{\"148\":1,\"190\":1,\"194\":1,\"315\":2,\"326\":1,\"327\":2,\"334\":1,\"353\":1,\"362\":1,\"372\":1,\"382\":1,\"392\":1,\"400\":1,\"419\":1,\"420\":1,\"422\":1,\"425\":1,\"444\":1,\"449\":3,\"450\":1,\"455\":3,\"471\":1,\"479\":1,\"485\":1}}],[\"prevalence\",{\"1\":{\"428\":1,\"439\":1}}],[\"prevalent\",{\"1\":{\"174\":1,\"313\":1,\"328\":1,\"333\":1,\"371\":1,\"381\":1,\"460\":1}}],[\"prevailing\",{\"1\":{\"377\":1}}],[\"preventing\",{\"1\":{\"469\":1}}],[\"prevention\",{\"1\":{\"443\":1}}],[\"prevent\",{\"1\":{\"122\":1,\"165\":1,\"334\":1,\"439\":1,\"479\":1}}],[\"previously\",{\"1\":{\"125\":1,\"256\":1,\"335\":1,\"350\":1,\"385\":1,\"395\":1,\"494\":1}}],[\"previous\",{\"0\":{\"470\":1},\"1\":{\"104\":1,\"123\":1,\"139\":1,\"167\":1,\"190\":1,\"191\":1,\"195\":1,\"214\":1,\"221\":1,\"226\":1,\"234\":1,\"235\":1,\"238\":1,\"249\":1,\"256\":2,\"277\":1,\"309\":1,\"333\":1,\"385\":1,\"386\":1,\"388\":1,\"391\":1,\"410\":1,\"419\":1,\"420\":1,\"439\":1,\"443\":1,\"470\":1,\"488\":1,\"489\":1}}],[\"preliminary\",{\"0\":{\"252\":1,\"426\":1},\"1\":{\"120\":1,\"121\":1,\"125\":1,\"220\":1,\"252\":1,\"268\":1,\"298\":1,\"426\":1,\"445\":2,\"452\":1}}],[\"predicates\",{\"1\":{\"390\":1}}],[\"predicate\",{\"0\":{\"111\":1},\"1\":{\"111\":1}}],[\"predicted\",{\"1\":{\"260\":1,\"273\":1}}],[\"predictors\",{\"1\":{\"237\":1,\"461\":1}}],[\"predictor\",{\"1\":{\"196\":1}}],[\"predicts\",{\"1\":{\"159\":1,\"196\":1}}],[\"predict\",{\"1\":{\"103\":1,\"118\":1,\"234\":1,\"369\":1}}],[\"predictive\",{\"1\":{\"234\":1,\"237\":2,\"273\":1,\"461\":3,\"496\":1}}],[\"prediction\",{\"0\":{\"116\":1,\"479\":1},\"1\":{\"104\":1,\"116\":1,\"135\":1,\"183\":1,\"229\":1,\"309\":1,\"327\":1,\"369\":1,\"412\":1,\"479\":2,\"488\":1}}],[\"predictions\",{\"1\":{\"102\":1,\"203\":1,\"309\":1,\"327\":1,\"412\":1,\"417\":1,\"496\":1}}],[\"predicting\",{\"0\":{\"219\":1,\"401\":1},\"1\":{\"100\":1,\"103\":1,\"118\":1,\"121\":1,\"309\":2,\"413\":1,\"417\":1,\"429\":1}}],[\"predictably\",{\"1\":{\"100\":1}}],[\"predictability\",{\"0\":{\"100\":1,\"121\":1},\"1\":{\"100\":1}}],[\"predominantly\",{\"1\":{\"98\":1,\"172\":1,\"362\":1,\"370\":1,\"377\":1,\"467\":1}}],[\"preoperational\",{\"1\":{\"79\":1}}],[\"premiere\",{\"1\":{\"8\":1}}],[\"块\",{\"1\":{\"7\":1}}],[\"色\",{\"1\":{\"7\":1}}],[\"szegedy\",{\"1\":{\"494\":1}}],[\"szafir\",{\"1\":{\"197\":1}}],[\"sdsat\",{\"0\":{\"487\":1},\"1\":{\"487\":2}}],[\"sdlc\",{\"1\":{\"254\":3}}],[\"sft\",{\"1\":{\"377\":1,\"453\":1}}],[\"squeezeattention\",{\"0\":{\"366\":1},\"1\":{\"366\":3}}],[\"square\",{\"1\":{\"40\":1}}],[\"sbst\",{\"1\":{\"360\":2}}],[\"ssd\",{\"1\":{\"510\":1}}],[\"ssq\",{\"1\":{\"304\":2}}],[\"ssae\",{\"1\":{\"103\":2}}],[\"snoek\",{\"1\":{\"442\":1}}],[\"snigdha\",{\"1\":{\"391\":1}}],[\"sne\",{\"0\":{\"301\":1},\"1\":{\"301\":2}}],[\"snake\",{\"0\":{\"98\":1},\"1\":{\"98\":3}}],[\"søs\",{\"1\":{\"296\":1}}],[\"svi\",{\"1\":{\"272\":1}}],[\"svetlana\",{\"1\":{\"227\":1}}],[\"sánchez\",{\"1\":{\"263\":1}}],[\"s2g\",{\"1\":{\"235\":1}}],[\"séga\",{\"1\":{\"232\":1}}],[\"srivastava\",{\"1\":{\"475\":1,\"490\":1}}],[\"sriram\",{\"1\":{\"196\":1}}],[\"sreela\",{\"1\":{\"220\":1}}],[\"swe\",{\"1\":{\"495\":1}}],[\"swearngin\",{\"1\":{\"151\":1,\"348\":1}}],[\"swift\",{\"1\":{\"316\":1}}],[\"swiss\",{\"1\":{\"234\":2}}],[\"switch\",{\"1\":{\"195\":1}}],[\"switching\",{\"1\":{\"165\":1}}],[\"sword\",{\"0\":{\"106\":1}}],[\"sl=auto\",{\"1\":{\"539\":1}}],[\"slightly\",{\"1\":{\"461\":1}}],[\"sliced\",{\"1\":{\"436\":1}}],[\"slicing\",{\"1\":{\"143\":1}}],[\"slt\",{\"1\":{\"438\":3}}],[\"slms\",{\"1\":{\"432\":3}}],[\"slos\",{\"1\":{\"465\":1}}],[\"slow\",{\"1\":{\"416\":1}}],[\"slower\",{\"1\":{\"256\":1}}],[\"slo\",{\"1\":{\"414\":1}}],[\"slu\",{\"1\":{\"232\":1}}],[\"sleiman\",{\"1\":{\"393\":1}}],[\"sleep\",{\"0\":{\"194\":1},\"1\":{\"194\":7,\"268\":1,\"452\":1}}],[\"sleepvst\",{\"0\":{\"194\":1},\"1\":{\"194\":3}}],[\"sledbear的天赋在于运动\",{\"1\":{\"23\":1}}],[\"sledbear\",{\"1\":{\"23\":2}}],[\"slack\",{\"1\":{\"124\":1,\"278\":1}}],[\"slavery\",{\"1\":{\"117\":1}}],[\"smells\",{\"1\":{\"339\":1}}],[\"smd\",{\"1\":{\"306\":3}}],[\"smkexplore\",{\"1\":{\"296\":1}}],[\"smooth\",{\"1\":{\"141\":1,\"361\":1}}],[\"smaranda\",{\"1\":{\"169\":1}}],[\"smartvote\",{\"1\":{\"234\":2}}],[\"smartphones\",{\"0\":{\"222\":1},\"1\":{\"222\":1}}],[\"smart\",{\"0\":{\"127\":1},\"1\":{\"117\":1,\"190\":1,\"222\":1,\"253\":1,\"519\":1}}],[\"smallest\",{\"1\":{\"250\":1}}],[\"smaller\",{\"1\":{\"151\":1,\"348\":1,\"381\":1,\"434\":1,\"436\":1,\"441\":4,\"449\":1}}],[\"small\",{\"0\":{\"381\":1},\"1\":{\"112\":1,\"173\":1,\"223\":1,\"232\":1,\"254\":1,\"341\":1,\"369\":1,\"381\":3,\"382\":1,\"389\":1,\"391\":1,\"447\":1,\"455\":4,\"471\":1,\"475\":1}}],[\"smith\",{\"1\":{\"99\":1,\"379\":1}}],[\"spreadsheet\",{\"1\":{\"480\":1}}],[\"spreadsheets\",{\"1\":{\"122\":1,\"480\":1}}],[\"split\",{\"1\":{\"462\":1}}],[\"spichkova\",{\"1\":{\"221\":1}}],[\"spikformer\",{\"1\":{\"103\":2}}],[\"spiking\",{\"0\":{\"103\":1},\"1\":{\"103\":4}}],[\"spy\",{\"1\":{\"181\":1}}],[\"spotify使用情境数据提供个性化的播放列表和recommendation\",{\"1\":{\"532\":1}}],[\"spotting\",{\"0\":{\"190\":1},\"1\":{\"190\":1}}],[\"spo$\",{\"1\":{\"298\":2}}],[\"sponsorship\",{\"1\":{\"192\":1}}],[\"sponsors\",{\"1\":{\"192\":2}}],[\"sponsored\",{\"0\":{\"192\":1},\"1\":{\"192\":4}}],[\"spoken\",{\"1\":{\"163\":3,\"232\":1,\"365\":1,\"438\":2}}],[\"sports\",{\"1\":{\"128\":1,\"137\":1,\"182\":1}}],[\"spurious\",{\"1\":{\"121\":1}}],[\"spaan\",{\"1\":{\"496\":1}}],[\"spatiotemporal\",{\"0\":{\"475\":1},\"1\":{\"376\":1,\"475\":1}}],[\"spatial\",{\"0\":{\"146\":1,\"220\":1,\"319\":1},\"1\":{\"112\":1,\"137\":1,\"146\":7,\"157\":1,\"220\":2,\"234\":1,\"270\":1,\"272\":1,\"282\":1,\"319\":3,\"414\":1,\"417\":1,\"430\":2}}],[\"spatially\",{\"1\":{\"108\":1,\"199\":2,\"430\":2}}],[\"span\",{\"1\":{\"455\":2}}],[\"spans\",{\"1\":{\"389\":1,\"455\":1}}],[\"spanish\",{\"1\":{\"315\":2}}],[\"spanning\",{\"1\":{\"251\":1,\"327\":1,\"333\":1,\"412\":1,\"423\":1}}],[\"sparsity\",{\"1\":{\"427\":1}}],[\"sparsification\",{\"1\":{\"366\":1}}],[\"sparsify\",{\"1\":{\"366\":1}}],[\"sparse\",{\"1\":{\"103\":1,\"131\":1,\"344\":1,\"347\":1,\"376\":1,\"396\":1}}],[\"sparking\",{\"1\":{\"243\":1}}],[\"sparked\",{\"1\":{\"198\":1,\"387\":1}}],[\"spark\",{\"1\":{\"181\":1}}],[\"spaces\",{\"1\":{\"164\":1,\"217\":1,\"248\":2,\"302\":1,\"412\":1}}],[\"space\",{\"0\":{\"204\":1,\"250\":1,\"289\":1,\"317\":1},\"1\":{\"111\":1,\"112\":1,\"126\":1,\"141\":2,\"156\":1,\"164\":1,\"181\":1,\"188\":1,\"190\":1,\"191\":1,\"199\":1,\"226\":1,\"234\":2,\"235\":1,\"250\":4,\"252\":1,\"253\":1,\"281\":2,\"282\":1,\"289\":2,\"317\":2,\"319\":1,\"363\":1,\"368\":1,\"390\":2,\"393\":1,\"399\":1,\"409\":1,\"419\":1,\"420\":1,\"426\":1,\"445\":1,\"460\":1,\"478\":1}}],[\"spcl\",{\"1\":{\"456\":1}}],[\"spc\",{\"1\":{\"103\":2}}],[\"spend\",{\"1\":{\"214\":1}}],[\"spends\",{\"1\":{\"214\":1}}],[\"spent\",{\"1\":{\"158\":1,\"214\":1,\"215\":1}}],[\"speaker\",{\"1\":{\"241\":1,\"365\":2}}],[\"speakers\",{\"0\":{\"124\":1},\"1\":{\"124\":1,\"142\":1,\"333\":1,\"365\":1}}],[\"speaking\",{\"1\":{\"155\":1,\"416\":1}}],[\"speculation\",{\"1\":{\"428\":1}}],[\"speculative\",{\"0\":{\"487\":1},\"1\":{\"181\":1,\"487\":1}}],[\"spectrums\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"spectrum\",{\"0\":{\"120\":1,\"498\":1},\"1\":{\"120\":1,\"174\":1,\"261\":1,\"333\":1,\"371\":1,\"467\":1,\"498\":4}}],[\"spectroscopic\",{\"1\":{\"112\":1}}],[\"spectroscopy\",{\"1\":{\"112\":1,\"223\":1}}],[\"spectra\",{\"1\":{\"112\":1}}],[\"specialised\",{\"1\":{\"477\":1}}],[\"specializes\",{\"1\":{\"349\":1}}],[\"specialized\",{\"1\":{\"99\":1,\"180\":1,\"223\":1,\"224\":1,\"238\":1,\"323\":1,\"356\":1,\"374\":1,\"412\":1,\"458\":1}}],[\"specializing\",{\"1\":{\"222\":1}}],[\"special\",{\"1\":{\"261\":1,\"333\":1,\"355\":1}}],[\"specify\",{\"1\":{\"153\":3,\"191\":2,\"375\":1,\"410\":1,\"422\":1}}],[\"specifying\",{\"1\":{\"140\":1,\"205\":1}}],[\"specified\",{\"1\":{\"132\":1,\"183\":1,\"298\":1,\"330\":1,\"339\":1,\"395\":1,\"410\":1,\"422\":1,\"492\":1}}],[\"specifics\",{\"0\":{\"352\":1}}],[\"specificity\",{\"1\":{\"244\":1}}],[\"specifications\",{\"0\":{\"396\":1,\"490\":1},\"1\":{\"153\":1,\"396\":4,\"490\":1}}],[\"specification\",{\"0\":{\"153\":1},\"1\":{\"153\":1}}],[\"specifically\",{\"1\":{\"135\":1,\"151\":1,\"174\":1,\"235\":1,\"251\":1,\"298\":1,\"306\":1,\"323\":1,\"335\":1,\"348\":1,\"350\":1,\"351\":1,\"352\":1,\"356\":1,\"358\":1,\"363\":1,\"371\":1,\"393\":1,\"411\":1,\"417\":1,\"423\":1,\"446\":1,\"455\":1,\"461\":1,\"467\":1,\"479\":1,\"482\":1,\"495\":1}}],[\"specific\",{\"0\":{\"147\":1},\"1\":{\"103\":1,\"112\":1,\"114\":1,\"138\":1,\"147\":1,\"154\":1,\"172\":1,\"174\":1,\"181\":1,\"182\":1,\"183\":1,\"187\":1,\"189\":1,\"196\":1,\"199\":1,\"230\":1,\"246\":1,\"256\":1,\"264\":1,\"292\":1,\"300\":1,\"315\":1,\"318\":1,\"328\":2,\"339\":2,\"350\":1,\"358\":1,\"361\":1,\"364\":1,\"370\":1,\"371\":1,\"379\":1,\"381\":1,\"382\":1,\"384\":1,\"388\":1,\"392\":1,\"400\":1,\"417\":1,\"424\":2,\"427\":1,\"430\":1,\"431\":1,\"432\":1,\"442\":1,\"448\":1,\"468\":1,\"471\":2,\"472\":1}}],[\"speech\",{\"0\":{\"184\":1,\"190\":1,\"224\":1,\"226\":1,\"235\":1,\"266\":1,\"380\":1,\"407\":1,\"460\":1},\"1\":{\"108\":1,\"155\":1,\"163\":2,\"184\":3,\"190\":1,\"224\":5,\"226\":2,\"232\":1,\"235\":5,\"241\":1,\"266\":5,\"302\":1,\"380\":3,\"419\":6,\"420\":6,\"460\":2,\"486\":1,\"497\":3}}],[\"speedup\",{\"1\":{\"462\":1}}],[\"speed\",{\"1\":{\"99\":1,\"175\":1,\"188\":1,\"269\":1,\"306\":1,\"389\":1,\"407\":1,\"427\":2,\"487\":1}}],[\"sylvius\",{\"1\":{\"400\":1}}],[\"symptoms\",{\"1\":{\"166\":1}}],[\"symbolic\",{\"1\":{\"132\":1,\"205\":1,\"330\":1,\"434\":1,\"455\":2}}],[\"synonym\",{\"1\":{\"460\":3}}],[\"sync\",{\"0\":{\"273\":1}}],[\"synchrony\",{\"0\":{\"273\":1},\"1\":{\"273\":5}}],[\"synchronization\",{\"1\":{\"231\":1,\"273\":2}}],[\"synchronous\",{\"1\":{\"170\":1}}],[\"synergetic\",{\"1\":{\"263\":1}}],[\"synergies\",{\"1\":{\"359\":1}}],[\"synergizes\",{\"1\":{\"349\":1,\"459\":1}}],[\"synergize\",{\"1\":{\"133\":1,\"331\":1}}],[\"synergistic\",{\"1\":{\"100\":1}}],[\"syntactically\",{\"1\":{\"422\":3}}],[\"syntactic\",{\"0\":{\"422\":1},\"1\":{\"350\":1,\"422\":5,\"497\":3}}],[\"syntax\",{\"1\":{\"228\":1,\"490\":3}}],[\"synthesis\",{\"0\":{\"190\":1,\"266\":1,\"363\":1},\"1\":{\"266\":1,\"287\":1,\"363\":1,\"482\":2}}],[\"synthesizer\",{\"1\":{\"287\":1}}],[\"synthesizers\",{\"1\":{\"287\":1}}],[\"synthesized\",{\"1\":{\"266\":1}}],[\"synthesizes\",{\"1\":{\"143\":1}}],[\"synthesizing\",{\"1\":{\"176\":1}}],[\"synthetic\",{\"0\":{\"101\":1,\"291\":1,\"405\":1,\"476\":1,\"496\":1},\"1\":{\"101\":5,\"199\":1,\"291\":1,\"412\":1,\"476\":1,\"496\":2}}],[\"systematize\",{\"1\":{\"429\":1}}],[\"systematization\",{\"1\":{\"398\":1}}],[\"systematically\",{\"1\":{\"112\":1,\"174\":1,\"198\":1,\"226\":1,\"339\":1,\"353\":1,\"358\":1,\"371\":1,\"387\":1,\"437\":1}}],[\"systematic\",{\"1\":{\"101\":1,\"141\":1,\"271\":1,\"353\":1,\"368\":1,\"428\":1,\"429\":1,\"457\":1}}],[\"system\",{\"0\":{\"284\":1},\"1\":{\"100\":1,\"104\":2,\"112\":1,\"129\":1,\"133\":1,\"138\":1,\"153\":1,\"160\":2,\"169\":1,\"172\":1,\"174\":1,\"178\":1,\"179\":1,\"190\":2,\"201\":4,\"204\":1,\"205\":1,\"210\":1,\"213\":1,\"214\":1,\"216\":1,\"218\":2,\"222\":1,\"227\":1,\"228\":1,\"232\":1,\"234\":1,\"240\":1,\"242\":1,\"249\":1,\"259\":1,\"266\":2,\"269\":3,\"282\":2,\"284\":4,\"286\":1,\"298\":11,\"300\":1,\"326\":1,\"331\":1,\"339\":1,\"355\":1,\"361\":2,\"370\":1,\"371\":1,\"375\":1,\"388\":4,\"414\":1,\"415\":1,\"419\":3,\"420\":3,\"430\":2,\"435\":1,\"447\":2,\"453\":1,\"474\":1,\"477\":1,\"479\":1,\"490\":1,\"492\":2}}],[\"systems\",{\"0\":{\"129\":1,\"130\":1,\"167\":1,\"231\":1,\"241\":1,\"263\":1,\"364\":1,\"419\":1,\"447\":1},\"1\":{\"99\":1,\"100\":2,\"115\":1,\"129\":1,\"130\":1,\"135\":3,\"140\":1,\"148\":1,\"155\":1,\"160\":1,\"163\":4,\"167\":2,\"169\":1,\"171\":2,\"172\":2,\"173\":2,\"178\":1,\"179\":1,\"186\":3,\"196\":1,\"207\":2,\"218\":1,\"221\":1,\"223\":2,\"228\":1,\"231\":1,\"244\":1,\"252\":1,\"253\":1,\"261\":1,\"266\":2,\"269\":1,\"270\":1,\"272\":1,\"288\":3,\"294\":1,\"303\":1,\"313\":1,\"326\":1,\"364\":3,\"370\":2,\"375\":1,\"378\":1,\"385\":1,\"418\":1,\"419\":3,\"420\":3,\"426\":1,\"430\":2,\"435\":3,\"445\":2,\"446\":1,\"447\":2,\"462\":1,\"475\":1,\"481\":1,\"486\":2,\"490\":1,\"519\":3,\"524\":1}}],[\"scoring\",{\"0\":{\"368\":1}}],[\"score\",{\"1\":{\"181\":1,\"232\":1,\"323\":2,\"324\":3,\"351\":1,\"352\":1,\"372\":2,\"400\":5,\"432\":1,\"460\":1,\"488\":1}}],[\"scores\",{\"1\":{\"106\":1,\"159\":1,\"194\":1,\"273\":1,\"304\":1,\"324\":1,\"325\":1,\"344\":1,\"351\":1,\"362\":1,\"365\":1,\"368\":2,\"400\":1,\"467\":1,\"479\":1}}],[\"scopes\",{\"1\":{\"429\":1}}],[\"scope\",{\"1\":{\"272\":1,\"489\":1}}],[\"scripts\",{\"1\":{\"379\":1}}],[\"scripting\",{\"1\":{\"222\":1}}],[\"scrutiny\",{\"1\":{\"246\":1,\"253\":1,\"359\":1}}],[\"scratch\",{\"1\":{\"172\":1,\"370\":1,\"377\":1}}],[\"screencast\",{\"1\":{\"361\":1}}],[\"screens\",{\"1\":{\"151\":6,\"257\":3,\"348\":6}}],[\"screen\",{\"0\":{\"257\":1},\"1\":{\"107\":3,\"139\":1,\"151\":1,\"219\":3,\"257\":7,\"348\":1,\"401\":3}}],[\"sclerosis\",{\"0\":{\"166\":1},\"1\":{\"166\":1}}],[\"schütze\",{\"1\":{\"497\":1}}],[\"schubotz\",{\"1\":{\"458\":1}}],[\"schut\",{\"1\":{\"199\":1}}],[\"schwartz\",{\"1\":{\"441\":1,\"454\":1}}],[\"schindler\",{\"1\":{\"399\":1}}],[\"schlicht\",{\"1\":{\"336\":1}}],[\"schaaf\",{\"1\":{\"335\":1}}],[\"scholarly\",{\"1\":{\"362\":1}}],[\"scholars\",{\"1\":{\"205\":1}}],[\"scholarship\",{\"1\":{\"205\":1}}],[\"school\",{\"1\":{\"153\":1,\"333\":2,\"334\":1,\"447\":1}}],[\"schoop\",{\"1\":{\"151\":1,\"348\":1}}],[\"schreiber\",{\"1\":{\"195\":1}}],[\"schmierer\",{\"1\":{\"182\":1}}],[\"scheduling\",{\"1\":{\"414\":1}}],[\"scheduled\",{\"1\":{\"279\":1}}],[\"schedule\",{\"1\":{\"135\":1}}],[\"schemes\",{\"1\":{\"410\":2}}],[\"scheme\",{\"1\":{\"139\":1,\"239\":1,\"410\":1,\"455\":1,\"456\":1,\"487\":1}}],[\"schemas\",{\"1\":{\"382\":1}}],[\"schema\",{\"1\":{\"132\":1,\"330\":1,\"382\":6}}],[\"scientists\",{\"1\":{\"170\":2}}],[\"scientific\",{\"0\":{\"428\":1},\"1\":{\"142\":1,\"146\":1,\"217\":1,\"259\":1,\"428\":4,\"492\":1}}],[\"sciences\",{\"1\":{\"333\":1}}],[\"science\",{\"0\":{\"236\":1,\"290\":1},\"1\":{\"101\":1,\"153\":1,\"159\":1,\"169\":1,\"217\":2,\"234\":1,\"244\":1,\"290\":2,\"400\":2,\"428\":2,\"486\":1}}],[\"scenario\",{\"1\":{\"124\":1,\"130\":1,\"190\":1,\"355\":1}}],[\"scenarios\",{\"0\":{\"480\":1},\"1\":{\"100\":2,\"103\":1,\"105\":1,\"112\":1,\"114\":1,\"120\":2,\"128\":1,\"158\":1,\"173\":1,\"181\":1,\"186\":1,\"212\":1,\"213\":1,\"217\":1,\"223\":1,\"248\":1,\"258\":1,\"283\":1,\"286\":1,\"290\":1,\"297\":1,\"306\":1,\"324\":1,\"329\":1,\"340\":1,\"355\":1,\"360\":1,\"362\":1,\"402\":1,\"403\":1,\"412\":1,\"417\":1,\"433\":1,\"441\":1,\"447\":1,\"466\":1,\"474\":1,\"480\":2}}],[\"scenes\",{\"1\":{\"304\":1}}],[\"scenery\",{\"0\":{\"227\":1}}],[\"scene\",{\"0\":{\"304\":1},\"1\":{\"123\":3,\"240\":1,\"304\":4,\"399\":2}}],[\"scarce\",{\"1\":{\"416\":1}}],[\"scarcity\",{\"1\":{\"172\":1,\"323\":1,\"370\":1,\"408\":1}}],[\"scattered\",{\"1\":{\"429\":1}}],[\"scatter\",{\"0\":{\"281\":1},\"1\":{\"281\":3}}],[\"scatterplots\",{\"0\":{\"197\":1},\"1\":{\"197\":2}}],[\"scanpaths\",{\"1\":{\"139\":1}}],[\"scanpath\",{\"0\":{\"139\":1},\"1\":{\"139\":1,\"273\":1}}],[\"scanning\",{\"0\":{\"105\":1},\"1\":{\"112\":1}}],[\"scaling\",{\"1\":{\"335\":1,\"382\":1}}],[\"scalable\",{\"1\":{\"197\":1,\"364\":1,\"415\":1,\"496\":1}}],[\"scalability\",{\"1\":{\"133\":1,\"331\":1,\"333\":1,\"359\":1,\"415\":1}}],[\"scaled\",{\"1\":{\"317\":1}}],[\"scales\",{\"1\":{\"112\":1,\"245\":1,\"249\":3,\"395\":1}}],[\"scale\",{\"0\":{\"213\":1,\"250\":1,\"272\":1,\"415\":1},\"1\":{\"97\":2,\"112\":1,\"143\":1,\"156\":1,\"179\":1,\"213\":1,\"238\":2,\"250\":3,\"272\":1,\"312\":2,\"341\":1,\"350\":1,\"389\":1,\"411\":1,\"415\":2,\"417\":1,\"428\":1,\"443\":1,\"444\":1,\"460\":1,\"469\":2,\"486\":1}}],[\"scaffold\",{\"1\":{\"169\":1}}],[\"scaffolds\",{\"1\":{\"110\":1,\"321\":1}}],[\"scaffolding\",{\"0\":{\"110\":1,\"321\":1},\"1\":{\"110\":1,\"130\":1,\"321\":1}}],[\"s\",{\"0\":{\"106\":2,\"159\":1,\"236\":1,\"249\":1,\"403\":1},\"1\":{\"97\":1,\"99\":2,\"105\":1,\"106\":1,\"108\":1,\"117\":1,\"118\":1,\"123\":2,\"124\":1,\"126\":2,\"127\":3,\"130\":2,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"138\":1,\"146\":4,\"150\":1,\"151\":1,\"153\":1,\"156\":1,\"157\":1,\"159\":3,\"166\":1,\"173\":1,\"176\":1,\"178\":2,\"180\":1,\"181\":1,\"192\":2,\"194\":2,\"195\":1,\"196\":1,\"204\":1,\"213\":2,\"220\":1,\"221\":1,\"226\":1,\"227\":1,\"229\":1,\"232\":2,\"234\":3,\"241\":1,\"245\":1,\"246\":3,\"251\":2,\"252\":3,\"255\":1,\"256\":1,\"257\":1,\"258\":3,\"263\":1,\"264\":2,\"265\":3,\"266\":1,\"269\":1,\"271\":1,\"274\":1,\"275\":2,\"282\":1,\"287\":1,\"290\":1,\"296\":2,\"298\":2,\"300\":1,\"308\":2,\"309\":1,\"312\":1,\"314\":1,\"316\":1,\"317\":1,\"318\":1,\"319\":1,\"327\":1,\"330\":1,\"331\":1,\"334\":1,\"336\":1,\"348\":1,\"350\":1,\"355\":1,\"356\":1,\"366\":2,\"369\":1,\"377\":1,\"384\":2,\"386\":2,\"388\":2,\"390\":1,\"393\":1,\"398\":3,\"402\":1,\"403\":1,\"407\":1,\"410\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"423\":2,\"426\":3,\"427\":2,\"430\":3,\"433\":3,\"435\":1,\"442\":1,\"448\":2,\"457\":1,\"467\":2,\"469\":1,\"477\":4,\"479\":1,\"481\":3,\"486\":1,\"487\":2,\"489\":2,\"494\":2,\"529\":1}}],[\"sujeeth\",{\"1\":{\"391\":1}}],[\"suyeon\",{\"1\":{\"356\":1}}],[\"susceptible\",{\"1\":{\"251\":1,\"423\":1}}],[\"sustain\",{\"1\":{\"273\":1}}],[\"sustained\",{\"1\":{\"244\":1}}],[\"sustainability\",{\"0\":{\"217\":1},\"1\":{\"207\":1,\"217\":3}}],[\"sustainable\",{\"1\":{\"192\":1,\"465\":1}}],[\"sultana\",{\"1\":{\"249\":1}}],[\"suleyman\",{\"1\":{\"116\":1,\"118\":1,\"139\":1}}],[\"sumita\",{\"1\":{\"248\":1}}],[\"sumaiya\",{\"1\":{\"298\":1}}],[\"suma\",{\"1\":{\"227\":1}}],[\"summaries\",{\"1\":{\"356\":1}}],[\"summarize\",{\"1\":{\"313\":1,\"437\":1,\"488\":1}}],[\"summarizing\",{\"1\":{\"245\":1,\"252\":1,\"426\":1}}],[\"summarization\",{\"0\":{\"213\":1,\"402\":1},\"1\":{\"337\":1,\"359\":4,\"391\":1,\"475\":2}}],[\"summary\",{\"1\":{\"335\":1,\"402\":2}}],[\"summation\",{\"0\":{\"220\":1},\"1\":{\"220\":2}}],[\"summative\",{\"1\":{\"131\":1}}],[\"sukruth\",{\"1\":{\"196\":1}}],[\"sui\",{\"0\":{\"192\":1},\"1\":{\"192\":3,\"478\":1}}],[\"suit\",{\"1\":{\"462\":1}}],[\"suite\",{\"1\":{\"222\":1,\"411\":1,\"482\":1}}],[\"suited\",{\"0\":{\"185\":1},\"1\":{\"104\":1,\"279\":1,\"389\":1}}],[\"suitably\",{\"1\":{\"233\":1}}],[\"suitable\",{\"1\":{\"146\":1,\"149\":1,\"161\":1,\"235\":1,\"245\":1,\"306\":1,\"354\":1,\"363\":1,\"408\":1,\"411\":1,\"436\":1,\"460\":1}}],[\"suitability\",{\"1\":{\"188\":1,\"282\":1}}],[\"sundnes\",{\"1\":{\"296\":1}}],[\"sunberg\",{\"1\":{\"288\":1}}],[\"sun\",{\"1\":{\"212\":1,\"222\":1,\"313\":1,\"314\":1,\"316\":1,\"329\":1,\"355\":1,\"359\":1,\"376\":1,\"411\":1,\"442\":1,\"491\":1}}],[\"sun2022exploring\",{\"1\":{\"159\":1}}],[\"sunnie\",{\"1\":{\"159\":1}}],[\"sung\",{\"1\":{\"419\":1,\"420\":1}}],[\"sungbok\",{\"1\":{\"229\":1}}],[\"sungkajun\",{\"1\":{\"156\":1}}],[\"sungwon\",{\"1\":{\"119\":1}}],[\"supratim\",{\"1\":{\"248\":1}}],[\"super\",{\"1\":{\"436\":1}}],[\"supernet\",{\"1\":{\"436\":2}}],[\"supervise\",{\"1\":{\"326\":1}}],[\"supervised\",{\"0\":{\"137\":1,\"472\":1},\"1\":{\"137\":3,\"376\":1,\"411\":1,\"442\":3,\"460\":1,\"471\":2,\"472\":1,\"485\":1}}],[\"supervision\",{\"0\":{\"442\":1},\"1\":{\"294\":1,\"326\":1,\"442\":1,\"480\":1}}],[\"superiority\",{\"1\":{\"283\":1,\"432\":1}}],[\"superior\",{\"1\":{\"203\":1,\"235\":1,\"260\":1,\"413\":1,\"434\":1,\"459\":1}}],[\"suppressing\",{\"1\":{\"378\":1}}],[\"supplied\",{\"1\":{\"434\":1}}],[\"supplier\",{\"1\":{\"179\":1}}],[\"supply\",{\"1\":{\"286\":1,\"474\":1}}],[\"supplemental\",{\"1\":{\"249\":1,\"250\":1,\"259\":1}}],[\"supported\",{\"1\":{\"223\":1,\"287\":1}}],[\"supportive\",{\"1\":{\"144\":1}}],[\"supporting\",{\"0\":{\"108\":1},\"1\":{\"104\":1,\"144\":1,\"148\":1,\"163\":1,\"184\":1,\"272\":1,\"273\":1,\"291\":1,\"380\":1,\"476\":1,\"477\":1}}],[\"supports\",{\"1\":{\"111\":1,\"204\":1,\"213\":1,\"283\":1,\"316\":1,\"329\":1}}],[\"support\",{\"0\":{\"97\":1,\"156\":1,\"201\":1,\"211\":1,\"215\":1,\"228\":1,\"312\":1},\"1\":{\"97\":2,\"102\":1,\"108\":1,\"111\":1,\"117\":1,\"120\":1,\"126\":1,\"127\":1,\"133\":1,\"146\":1,\"156\":3,\"158\":1,\"163\":1,\"171\":1,\"173\":1,\"182\":1,\"186\":1,\"211\":2,\"215\":2,\"228\":2,\"229\":1,\"231\":1,\"246\":1,\"253\":1,\"258\":1,\"261\":2,\"272\":1,\"282\":1,\"292\":1,\"293\":1,\"300\":1,\"312\":2,\"331\":1,\"333\":2,\"337\":1,\"356\":1,\"398\":2,\"433\":1,\"469\":1,\"486\":1}}],[\"succeeded\",{\"1\":{\"334\":1}}],[\"succeeds\",{\"1\":{\"224\":1}}],[\"successful\",{\"1\":{\"273\":1,\"278\":1,\"290\":1,\"313\":2,\"322\":1,\"418\":1,\"471\":1}}],[\"successfully\",{\"1\":{\"194\":1,\"249\":1,\"313\":1,\"409\":1}}],[\"success\",{\"1\":{\"140\":1,\"222\":1,\"224\":1,\"237\":1,\"249\":1,\"261\":1,\"284\":1,\"289\":1,\"313\":1,\"314\":1,\"318\":2,\"322\":1,\"327\":1,\"338\":1,\"350\":1,\"386\":1,\"393\":1,\"409\":2,\"418\":1,\"419\":1,\"420\":1,\"468\":1,\"472\":1}}],[\"successes\",{\"1\":{\"136\":1,\"382\":1}}],[\"succinct\",{\"1\":{\"201\":1}}],[\"suchman\",{\"1\":{\"156\":1}}],[\"such\",{\"1\":{\"98\":1,\"105\":3,\"122\":2,\"123\":1,\"124\":1,\"128\":4,\"130\":1,\"131\":1,\"132\":1,\"135\":1,\"136\":2,\"146\":1,\"151\":1,\"160\":1,\"163\":1,\"164\":1,\"173\":1,\"182\":2,\"195\":1,\"196\":1,\"197\":1,\"201\":1,\"204\":2,\"206\":3,\"207\":1,\"210\":1,\"211\":3,\"212\":1,\"215\":1,\"216\":2,\"221\":2,\"222\":1,\"229\":2,\"231\":1,\"233\":1,\"234\":1,\"238\":2,\"242\":1,\"243\":1,\"254\":2,\"255\":1,\"256\":1,\"257\":3,\"261\":1,\"264\":1,\"269\":4,\"270\":1,\"272\":1,\"275\":1,\"278\":1,\"279\":1,\"282\":2,\"283\":1,\"288\":1,\"293\":1,\"301\":1,\"302\":1,\"304\":1,\"308\":3,\"309\":1,\"322\":2,\"327\":1,\"330\":1,\"333\":2,\"334\":1,\"337\":1,\"339\":1,\"340\":1,\"343\":1,\"345\":1,\"347\":2,\"348\":1,\"350\":2,\"356\":1,\"374\":3,\"392\":2,\"394\":2,\"402\":1,\"403\":2,\"427\":1,\"431\":1,\"432\":1,\"434\":3,\"435\":3,\"439\":1,\"441\":1,\"447\":1,\"448\":1,\"453\":1,\"455\":2,\"459\":1,\"460\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":2,\"475\":3,\"478\":1,\"482\":1,\"483\":1,\"486\":1,\"488\":2,\"490\":2,\"494\":1,\"495\":1,\"496\":1}}],[\"suh\",{\"1\":{\"132\":1,\"252\":1,\"330\":1,\"426\":1}}],[\"suffix\",{\"1\":{\"313\":2,\"409\":1}}],[\"suffixes\",{\"0\":{\"313\":1},\"1\":{\"313\":6}}],[\"sufficient\",{\"1\":{\"255\":1,\"316\":1,\"355\":1}}],[\"sufficiently\",{\"1\":{\"121\":1,\"494\":1}}],[\"suffering\",{\"1\":{\"488\":1}}],[\"suffers\",{\"1\":{\"329\":1,\"369\":1}}],[\"suffer\",{\"1\":{\"103\":1,\"190\":1,\"403\":1,\"496\":1}}],[\"suen\",{\"1\":{\"108\":1,\"142\":1}}],[\"su\",{\"1\":{\"105\":1,\"218\":1,\"445\":1,\"453\":1}}],[\"sure\",{\"1\":{\"409\":1,\"467\":1}}],[\"suresh\",{\"1\":{\"137\":1,\"265\":1}}],[\"surpass\",{\"1\":{\"344\":1,\"458\":1}}],[\"surpassing\",{\"1\":{\"313\":1}}],[\"surpassed\",{\"1\":{\"227\":1}}],[\"surpasses\",{\"1\":{\"103\":1,\"151\":1,\"348\":1,\"350\":1,\"415\":1,\"485\":1,\"496\":1}}],[\"surprisingly\",{\"1\":{\"376\":1}}],[\"surprising\",{\"1\":{\"256\":1}}],[\"surgery\",{\"1\":{\"284\":1}}],[\"surge\",{\"1\":{\"243\":1,\"460\":1}}],[\"surgeons\",{\"1\":{\"196\":2}}],[\"surveys\",{\"1\":{\"468\":1}}],[\"surveying\",{\"0\":{\"302\":1}}],[\"survey\",{\"0\":{\"261\":1,\"429\":1,\"443\":1},\"1\":{\"154\":1,\"155\":1,\"166\":2,\"169\":1,\"196\":3,\"210\":1,\"254\":2,\"261\":1,\"286\":1,\"291\":1,\"302\":1,\"429\":1,\"443\":1,\"468\":1,\"474\":1,\"476\":1}}],[\"surveyed\",{\"1\":{\"114\":1,\"252\":1,\"426\":1}}],[\"surveillance\",{\"1\":{\"137\":1}}],[\"surrounding\",{\"1\":{\"123\":1,\"302\":2,\"417\":1}}],[\"surfaces\",{\"1\":{\"207\":2,\"424\":1}}],[\"surface\",{\"1\":{\"103\":1,\"112\":1,\"207\":1,\"284\":1,\"306\":1}}],[\"suboptimal\",{\"1\":{\"488\":1}}],[\"subclass\",{\"1\":{\"442\":2}}],[\"subclasses\",{\"1\":{\"442\":2}}],[\"subhajit\",{\"1\":{\"434\":1}}],[\"subhro\",{\"1\":{\"215\":1}}],[\"subreddits\",{\"1\":{\"431\":1}}],[\"subword\",{\"1\":{\"389\":2}}],[\"subpar\",{\"1\":{\"345\":1,\"350\":1}}],[\"subigya\",{\"1\":{\"268\":1,\"452\":1}}],[\"subgraphs\",{\"1\":{\"252\":1,\"426\":1}}],[\"subdimensions\",{\"1\":{\"250\":1}}],[\"submission\",{\"1\":{\"335\":1,\"351\":1,\"352\":1}}],[\"submissions\",{\"1\":{\"217\":2}}],[\"submitting\",{\"1\":{\"210\":1}}],[\"subbaro\",{\"1\":{\"217\":1}}],[\"sub\",{\"1\":{\"151\":2,\"318\":1,\"325\":1,\"345\":1,\"348\":2,\"404\":1,\"439\":1}}],[\"subtask\",{\"1\":{\"351\":1,\"352\":1}}],[\"subtle\",{\"1\":{\"257\":1,\"413\":1,\"424\":1,\"482\":1}}],[\"subtleties\",{\"1\":{\"100\":1,\"124\":1}}],[\"subtractive\",{\"0\":{\"113\":1},\"1\":{\"113\":1}}],[\"subsets\",{\"1\":{\"337\":1}}],[\"subset\",{\"1\":{\"269\":1,\"336\":1}}],[\"subsequent\",{\"1\":{\"234\":2,\"260\":1,\"359\":1}}],[\"subsequently\",{\"1\":{\"103\":1,\"174\":1,\"314\":1,\"371\":1,\"399\":1,\"463\":1,\"479\":1}}],[\"subspace\",{\"1\":{\"188\":1}}],[\"subspaces\",{\"1\":{\"111\":1}}],[\"substitution\",{\"1\":{\"460\":2}}],[\"substitutive\",{\"1\":{\"220\":1}}],[\"substitute\",{\"1\":{\"166\":1}}],[\"substantiate\",{\"1\":{\"359\":1}}],[\"substantial\",{\"1\":{\"150\":1,\"212\":1,\"237\":1,\"239\":1,\"298\":1,\"356\":1,\"361\":1,\"362\":1,\"368\":1,\"392\":1,\"478\":2,\"489\":1}}],[\"substantially\",{\"1\":{\"146\":1,\"201\":1,\"408\":1,\"411\":1,\"415\":1,\"428\":1}}],[\"substantive\",{\"1\":{\"175\":1}}],[\"subjectivity\",{\"1\":{\"136\":1}}],[\"subjective\",{\"1\":{\"97\":1,\"152\":1,\"189\":2,\"312\":1,\"391\":1}}],[\"subjected\",{\"1\":{\"120\":1,\"374\":1}}],[\"subjects\",{\"1\":{\"103\":1,\"149\":2,\"160\":1,\"246\":1,\"263\":4,\"286\":1,\"304\":1,\"307\":2,\"333\":1,\"474\":1}}],[\"subject\",{\"0\":{\"103\":1},\"1\":{\"96\":1,\"103\":2,\"123\":1,\"349\":1,\"429\":1}}],[\"suggestions\",{\"1\":{\"124\":2,\"152\":1,\"389\":1,\"417\":1,\"466\":1}}],[\"suggesting\",{\"1\":{\"120\":1,\"135\":1,\"306\":1,\"424\":1}}],[\"suggests\",{\"1\":{\"107\":1,\"146\":1,\"201\":1,\"251\":1,\"302\":1,\"412\":1,\"423\":1,\"428\":1,\"463\":1,\"485\":1}}],[\"suggest\",{\"1\":{\"97\":1,\"121\":2,\"125\":1,\"143\":1,\"149\":1,\"158\":2,\"160\":1,\"163\":2,\"187\":1,\"209\":1,\"229\":1,\"245\":1,\"304\":1,\"312\":1,\"323\":1,\"324\":1,\"340\":1,\"362\":1,\"454\":1,\"466\":1}}],[\"seu\",{\"1\":{\"533\":1}}],[\"seung\",{\"1\":{\"274\":1}}],[\"seibold\",{\"1\":{\"379\":1}}],[\"sevenfold\",{\"1\":{\"460\":1}}],[\"seven\",{\"1\":{\"345\":1,\"350\":1}}],[\"severe\",{\"1\":{\"223\":1}}],[\"severely\",{\"1\":{\"166\":1,\"272\":1}}],[\"severity\",{\"1\":{\"121\":1,\"413\":1}}],[\"several\",{\"1\":{\"104\":1,\"111\":1,\"146\":2,\"160\":1,\"178\":1,\"196\":1,\"218\":1,\"229\":1,\"240\":2,\"261\":1,\"270\":1,\"274\":1,\"289\":1,\"302\":1,\"314\":1,\"342\":1,\"343\":1,\"355\":1,\"375\":1,\"379\":1,\"385\":1,\"417\":2,\"431\":2,\"434\":1,\"435\":1,\"450\":1,\"463\":1,\"465\":1,\"466\":2,\"467\":1,\"475\":1}}],[\"sebo\",{\"1\":{\"307\":1}}],[\"sebastian\",{\"1\":{\"203\":1,\"215\":1,\"296\":1,\"416\":1}}],[\"sehad\",{\"1\":{\"239\":1}}],[\"segment\",{\"1\":{\"430\":1}}],[\"segmented\",{\"1\":{\"389\":1}}],[\"segmentation\",{\"0\":{\"238\":1,\"442\":1},\"1\":{\"238\":3,\"407\":1,\"439\":1,\"442\":6}}],[\"segments\",{\"1\":{\"127\":1}}],[\"sexual\",{\"0\":{\"211\":1},\"1\":{\"211\":3}}],[\"session\",{\"1\":{\"203\":1}}],[\"sessions\",{\"0\":{\"265\":1},\"1\":{\"120\":1,\"133\":1,\"203\":2,\"265\":2,\"290\":1,\"331\":1}}],[\"sequential\",{\"0\":{\"197\":1},\"1\":{\"197\":3,\"417\":2,\"485\":1}}],[\"sequences\",{\"1\":{\"412\":2,\"425\":1,\"469\":1,\"479\":3}}],[\"sequence\",{\"0\":{\"104\":1},\"1\":{\"96\":1,\"104\":5,\"118\":1,\"141\":1,\"339\":2,\"366\":2,\"389\":2,\"393\":1,\"405\":1,\"412\":2,\"425\":1,\"427\":2,\"438\":1,\"450\":1,\"469\":1}}],[\"sedlmair\",{\"1\":{\"182\":1}}],[\"seyedehdelaram\",{\"1\":{\"158\":1}}],[\"seamless\",{\"1\":{\"316\":1,\"485\":1}}],[\"seamlessly\",{\"1\":{\"201\":1,\"246\":1,\"266\":1,\"284\":1,\"313\":1,\"415\":1}}],[\"seat\",{\"1\":{\"229\":1}}],[\"searching\",{\"1\":{\"392\":1}}],[\"searches\",{\"1\":{\"278\":1,\"322\":1,\"462\":1}}],[\"searched\",{\"1\":{\"174\":1,\"371\":1}}],[\"search\",{\"0\":{\"462\":1},\"1\":{\"149\":1,\"211\":1,\"269\":1,\"282\":1,\"288\":1,\"289\":1,\"322\":3,\"353\":2,\"360\":1,\"386\":2,\"394\":1,\"402\":5,\"409\":3,\"414\":1,\"462\":1,\"478\":1,\"487\":1}}],[\"sealmates\",{\"0\":{\"108\":1},\"1\":{\"108\":1}}],[\"separation\",{\"1\":{\"479\":1}}],[\"separated\",{\"1\":{\"362\":2}}],[\"separate\",{\"1\":{\"249\":1,\"340\":1,\"414\":1}}],[\"separately\",{\"1\":{\"149\":1,\"151\":1,\"348\":1}}],[\"sepideh\",{\"1\":{\"254\":1}}],[\"sepsis\",{\"0\":{\"121\":1},\"1\":{\"121\":3}}],[\"seohyun\",{\"1\":{\"356\":1}}],[\"seoyeon\",{\"1\":{\"274\":1}}],[\"seongmin\",{\"1\":{\"258\":1,\"375\":1,\"433\":1}}],[\"seo\",{\"1\":{\"143\":1,\"156\":1,\"198\":1,\"266\":1,\"356\":1,\"387\":1}}],[\"seokweon\",{\"1\":{\"143\":1}}],[\"sec\",{\"1\":{\"486\":1}}],[\"secret\",{\"0\":{\"461\":1}}],[\"secure\",{\"1\":{\"334\":1}}],[\"security\",{\"0\":{\"206\":1,\"453\":1},\"1\":{\"178\":1,\"453\":1}}],[\"sections\",{\"1\":{\"155\":2,\"335\":3}}],[\"section\",{\"0\":{\"200\":1},\"1\":{\"155\":2,\"335\":1,\"358\":1}}],[\"sectors\",{\"1\":{\"226\":1}}],[\"sector\",{\"1\":{\"138\":1,\"171\":1}}],[\"seconds\",{\"1\":{\"313\":2,\"376\":1}}],[\"secondary\",{\"1\":{\"223\":1,\"333\":1}}],[\"secondly\",{\"1\":{\"149\":1,\"260\":1,\"355\":1}}],[\"second\",{\"1\":{\"96\":1,\"123\":1,\"153\":1,\"179\":1,\"224\":1,\"238\":1,\"245\":1,\"270\":1,\"296\":1,\"335\":1,\"458\":1}}],[\"serajeh\",{\"1\":{\"492\":1}}],[\"seraphina\",{\"1\":{\"227\":1}}],[\"serena\",{\"1\":{\"207\":1,\"315\":1}}],[\"sergey\",{\"1\":{\"350\":1}}],[\"serge\",{\"1\":{\"189\":1}}],[\"sergei\",{\"1\":{\"112\":1}}],[\"sergio\",{\"1\":{\"186\":1}}],[\"serving\",{\"0\":{\"414\":1},\"1\":{\"389\":1,\"414\":2,\"435\":1,\"454\":1,\"465\":2,\"485\":1}}],[\"services\",{\"1\":{\"270\":1,\"326\":2}}],[\"service\",{\"0\":{\"447\":1},\"1\":{\"149\":1,\"164\":1,\"192\":1,\"222\":1,\"236\":1,\"316\":1,\"447\":5,\"465\":1}}],[\"served\",{\"1\":{\"274\":1}}],[\"servers\",{\"1\":{\"230\":3}}],[\"server\",{\"1\":{\"230\":1}}],[\"serve\",{\"1\":{\"218\":1,\"334\":1,\"345\":1,\"349\":1,\"412\":1,\"414\":1,\"447\":1,\"465\":2}}],[\"serves\",{\"1\":{\"119\":1,\"245\":1,\"260\":1}}],[\"serializing\",{\"1\":{\"375\":1}}],[\"serial\",{\"1\":{\"284\":1}}],[\"series\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"120\":1,\"133\":1,\"142\":1,\"183\":2,\"251\":1,\"268\":1,\"301\":1,\"331\":1,\"345\":1,\"382\":1,\"423\":1,\"452\":1,\"470\":1}}],[\"seriously\",{\"1\":{\"378\":1}}],[\"serious\",{\"1\":{\"120\":1,\"207\":1,\"298\":1}}],[\"selected\",{\"1\":{\"240\":1,\"269\":1,\"412\":1,\"432\":1}}],[\"selector\",{\"1\":{\"234\":1}}],[\"selects\",{\"1\":{\"234\":1,\"313\":1,\"350\":1,\"427\":1,\"467\":1}}],[\"select\",{\"1\":{\"195\":1,\"233\":1,\"263\":1}}],[\"selective\",{\"1\":{\"475\":1}}],[\"selection\",{\"0\":{\"350\":1},\"1\":{\"197\":1,\"234\":2,\"235\":1,\"269\":1,\"305\":1,\"323\":1,\"350\":2,\"358\":1,\"365\":1,\"408\":1,\"441\":1}}],[\"selections\",{\"1\":{\"124\":1}}],[\"selecting\",{\"1\":{\"174\":1,\"371\":1,\"441\":1}}],[\"selfhood\",{\"1\":{\"175\":2}}],[\"self\",{\"0\":{\"268\":1,\"273\":1,\"293\":1,\"385\":2,\"415\":1,\"452\":1},\"1\":{\"108\":2,\"130\":1,\"137\":1,\"144\":1,\"169\":1,\"176\":1,\"204\":1,\"205\":1,\"206\":1,\"211\":1,\"233\":1,\"254\":1,\"268\":2,\"273\":2,\"293\":10,\"302\":1,\"324\":1,\"349\":1,\"368\":1,\"376\":1,\"382\":1,\"385\":1,\"391\":1,\"415\":2,\"452\":2,\"470\":1,\"483\":1}}],[\"selvan\",{\"1\":{\"108\":1}}],[\"seegmiller\",{\"1\":{\"431\":1}}],[\"seen\",{\"1\":{\"283\":2,\"342\":1,\"400\":1,\"443\":1,\"469\":1}}],[\"seek\",{\"1\":{\"266\":1,\"390\":1,\"418\":1}}],[\"seeks\",{\"1\":{\"195\":1,\"239\":1,\"327\":1}}],[\"seeking\",{\"1\":{\"128\":1,\"398\":1}}],[\"seem\",{\"1\":{\"266\":1}}],[\"seemingly\",{\"1\":{\"171\":1,\"181\":1,\"297\":1,\"418\":1}}],[\"seems\",{\"1\":{\"107\":1,\"158\":1}}],[\"see\",{\"1\":{\"131\":1,\"173\":1,\"302\":1}}],[\"seeing\",{\"0\":{\"296\":1},\"1\":{\"108\":1,\"326\":1}}],[\"semeval\",{\"0\":{\"351\":1,\"352\":1,\"372\":1,\"388\":1,\"404\":1,\"439\":1},\"1\":{\"325\":1,\"351\":1,\"352\":1,\"372\":1,\"388\":1,\"404\":1,\"439\":1}}],[\"semester\",{\"0\":{\"308\":1},\"1\":{\"291\":1,\"308\":2,\"476\":1}}],[\"semantics\",{\"1\":{\"260\":2,\"360\":2,\"422\":1,\"430\":1,\"488\":1}}],[\"semantically\",{\"1\":{\"260\":1,\"317\":1,\"340\":1,\"359\":1,\"422\":2,\"432\":1,\"444\":1,\"485\":1}}],[\"semantic\",{\"0\":{\"260\":1,\"442\":1,\"487\":1},\"1\":{\"110\":1,\"114\":1,\"118\":1,\"205\":1,\"229\":1,\"239\":1,\"252\":1,\"257\":1,\"260\":2,\"270\":1,\"321\":1,\"350\":1,\"365\":1,\"407\":1,\"426\":1,\"430\":1,\"432\":1,\"438\":1,\"442\":3,\"487\":2,\"488\":1}}],[\"semi\",{\"0\":{\"137\":1},\"1\":{\"105\":1,\"120\":1,\"137\":2,\"212\":1,\"286\":1,\"293\":1,\"396\":1,\"474\":1}}],[\"semg\",{\"0\":{\"103\":1},\"1\":{\"103\":3}}],[\"senior\",{\"1\":{\"308\":1}}],[\"senegal\",{\"1\":{\"232\":4}}],[\"sentiment\",{\"1\":{\"227\":1,\"282\":1}}],[\"sent\",{\"1\":{\"151\":1,\"348\":1}}],[\"sentences\",{\"1\":{\"127\":1,\"365\":3,\"395\":1,\"455\":1}}],[\"sentence\",{\"1\":{\"104\":1,\"395\":1,\"404\":1,\"450\":2,\"460\":1}}],[\"sensation\",{\"0\":{\"202\":1},\"1\":{\"220\":1}}],[\"sensations\",{\"1\":{\"142\":1,\"202\":1}}],[\"sensitivity\",{\"1\":{\"178\":1,\"191\":3,\"256\":1,\"466\":1}}],[\"sensitivities\",{\"1\":{\"144\":1}}],[\"sensitive\",{\"1\":{\"146\":1,\"211\":2,\"214\":1,\"391\":1,\"409\":1,\"417\":1,\"435\":1,\"488\":1}}],[\"sensing\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"101\":1,\"237\":2,\"268\":2,\"272\":2,\"452\":2,\"475\":1}}],[\"sensor\",{\"0\":{\"295\":1,\"475\":1},\"1\":{\"194\":1,\"240\":1,\"295\":3,\"298\":1,\"305\":1,\"475\":7}}],[\"sensory\",{\"0\":{\"220\":1},\"1\":{\"144\":2,\"166\":1,\"220\":3,\"239\":2,\"362\":1,\"475\":1}}],[\"sensors\",{\"1\":{\"105\":1,\"120\":1,\"207\":2}}],[\"senses\",{\"0\":{\"239\":1},\"1\":{\"207\":1,\"239\":2,\"295\":1}}],[\"sensemaking\",{\"0\":{\"277\":1},\"1\":{\"195\":2,\"277\":3}}],[\"sense\",{\"0\":{\"123\":1},\"1\":{\"175\":1,\"191\":1,\"207\":1,\"283\":2,\"430\":1}}],[\"sets\",{\"1\":{\"301\":2,\"335\":1,\"337\":1,\"358\":1,\"365\":1,\"386\":1,\"388\":1}}],[\"setup\",{\"1\":{\"240\":2,\"304\":1,\"441\":1,\"475\":1}}],[\"setups\",{\"1\":{\"185\":1,\"435\":2}}],[\"setlur\",{\"1\":{\"226\":1}}],[\"seth\",{\"1\":{\"205\":1}}],[\"settings\",{\"1\":{\"165\":1,\"181\":1,\"236\":1,\"245\":1,\"246\":1,\"269\":1,\"275\":1,\"283\":2,\"328\":1,\"337\":2,\"372\":1,\"379\":1,\"382\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"429\":1,\"472\":2,\"481\":1}}],[\"setting\",{\"1\":{\"149\":1,\"194\":1,\"201\":1,\"263\":1,\"266\":1,\"288\":1,\"326\":1,\"362\":1,\"369\":1,\"395\":1,\"405\":2,\"413\":1,\"444\":2,\"455\":1,\"458\":1,\"485\":2}}],[\"set\",{\"1\":{\"99\":1,\"128\":1,\"146\":1,\"149\":1,\"155\":1,\"159\":1,\"164\":1,\"170\":1,\"173\":1,\"187\":1,\"202\":1,\"211\":1,\"230\":1,\"236\":1,\"257\":3,\"270\":2,\"283\":3,\"342\":1,\"350\":3,\"352\":1,\"360\":1,\"364\":1,\"365\":1,\"386\":1,\"390\":1,\"409\":1,\"411\":1,\"422\":2,\"442\":1,\"443\":1,\"482\":1,\"490\":1,\"496\":1}}],[\"soa\",{\"1\":{\"415\":5}}],[\"soap\",{\"1\":{\"335\":3}}],[\"sohl\",{\"1\":{\"389\":1}}],[\"soh\",{\"1\":{\"382\":1}}],[\"sorting\",{\"1\":{\"391\":1}}],[\"sorin\",{\"1\":{\"322\":1}}],[\"sorenson\",{\"1\":{\"48\":1}}],[\"sota\",{\"1\":{\"283\":2,\"369\":1,\"462\":1,\"490\":1}}],[\"sowon\",{\"1\":{\"274\":1}}],[\"sowmya\",{\"1\":{\"200\":1}}],[\"sophisticated\",{\"1\":{\"256\":1,\"257\":1,\"359\":1,\"361\":1,\"429\":1}}],[\"sophie\",{\"1\":{\"166\":1}}],[\"songyang\",{\"1\":{\"337\":1,\"377\":1}}],[\"song\",{\"1\":{\"293\":1,\"345\":1,\"349\":1,\"403\":1,\"429\":1,\"483\":1}}],[\"songcen\",{\"1\":{\"235\":1}}],[\"sonification\",{\"1\":{\"287\":2}}],[\"sonifications\",{\"0\":{\"287\":1}}],[\"sonatel\",{\"1\":{\"232\":1}}],[\"sontag\",{\"1\":{\"215\":1}}],[\"soden\",{\"1\":{\"217\":1}}],[\"somarathna\",{\"1\":{\"204\":1}}],[\"somin\",{\"1\":{\"201\":1}}],[\"someone\",{\"1\":{\"209\":1,\"241\":2}}],[\"something\",{\"1\":{\"209\":1}}],[\"sometimes\",{\"1\":{\"107\":1,\"226\":1,\"409\":1}}],[\"some\",{\"1\":{\"97\":1,\"98\":1,\"166\":1,\"179\":1,\"181\":2,\"211\":1,\"214\":1,\"234\":1,\"248\":1,\"256\":1,\"278\":1,\"293\":1,\"302\":1,\"312\":1,\"336\":1,\"340\":1,\"347\":1,\"350\":1,\"400\":1,\"409\":2,\"411\":1,\"413\":2,\"427\":1,\"432\":1,\"450\":2,\"461\":1,\"471\":1,\"485\":1,\"495\":1,\"498\":1}}],[\"soon\",{\"1\":{\"200\":1,\"417\":1,\"447\":1,\"470\":1}}],[\"soccer\",{\"1\":{\"182\":1}}],[\"societal\",{\"1\":{\"213\":1}}],[\"society\",{\"1\":{\"125\":1,\"302\":1}}],[\"sociotechnical\",{\"1\":{\"279\":1}}],[\"socio\",{\"1\":{\"120\":1}}],[\"socially\",{\"0\":{\"261\":1},\"1\":{\"227\":1,\"237\":1,\"261\":1}}],[\"social\",{\"0\":{\"147\":1,\"180\":1,\"196\":1,\"237\":1},\"1\":{\"107\":1,\"108\":1,\"120\":1,\"124\":1,\"144\":1,\"147\":1,\"170\":1,\"173\":5,\"180\":6,\"196\":4,\"202\":1,\"205\":2,\"227\":3,\"230\":1,\"233\":1,\"237\":3,\"245\":2,\"253\":1,\"261\":1,\"269\":1,\"270\":2,\"282\":1,\"303\":3,\"307\":1,\"400\":1,\"402\":3,\"431\":2,\"435\":1,\"460\":1}}],[\"so\",{\"1\":{\"181\":1,\"187\":1,\"215\":1,\"229\":1,\"237\":1,\"251\":1,\"257\":1,\"282\":1,\"315\":1,\"334\":1,\"423\":1,\"455\":1}}],[\"sokol2020one\",{\"1\":{\"159\":1}}],[\"soyoung\",{\"1\":{\"150\":1,\"356\":1}}],[\"sourati\",{\"1\":{\"461\":1}}],[\"sourcing\",{\"1\":{\"159\":1,\"377\":1}}],[\"sourced\",{\"1\":{\"333\":1,\"388\":1,\"447\":1,\"482\":1,\"485\":1}}],[\"sources\",{\"1\":{\"133\":1,\"249\":1,\"331\":1,\"434\":1}}],[\"source\",{\"0\":{\"315\":1,\"341\":1,\"342\":1,\"413\":1,\"496\":1},\"1\":{\"104\":1,\"139\":1,\"151\":1,\"170\":1,\"215\":1,\"231\":1,\"258\":1,\"272\":1,\"313\":3,\"315\":1,\"322\":1,\"326\":1,\"337\":3,\"341\":1,\"345\":1,\"348\":1,\"349\":2,\"350\":1,\"360\":1,\"361\":1,\"363\":1,\"365\":1,\"398\":1,\"411\":2,\"433\":1,\"443\":2,\"445\":1,\"471\":1,\"480\":1,\"490\":1,\"497\":2}}],[\"sought\",{\"1\":{\"292\":1}}],[\"soul\",{\"1\":{\"268\":1,\"452\":1}}],[\"soundscape\",{\"1\":{\"512\":2}}],[\"sounds\",{\"1\":{\"266\":1}}],[\"sound\",{\"1\":{\"142\":1,\"239\":1,\"269\":1,\"287\":1}}],[\"softmax\",{\"1\":{\"462\":1}}],[\"softpromptcomp\",{\"1\":{\"359\":2}}],[\"soft\",{\"0\":{\"115\":1,\"359\":1},\"1\":{\"115\":1,\"209\":1,\"359\":4}}],[\"software\",{\"0\":{\"122\":1,\"254\":1,\"364\":1,\"413\":1},\"1\":{\"106\":1,\"113\":1,\"122\":1,\"138\":1,\"153\":1,\"229\":1,\"230\":1,\"246\":1,\"253\":1,\"254\":3,\"257\":2,\"259\":1,\"279\":1,\"308\":5,\"341\":1,\"342\":1,\"360\":2,\"364\":3,\"413\":3,\"415\":1,\"422\":1,\"434\":1,\"437\":1,\"445\":2,\"447\":1,\"495\":2}}],[\"soloveychik\",{\"1\":{\"469\":1}}],[\"soleymani\",{\"1\":{\"333\":1}}],[\"solely\",{\"1\":{\"313\":1,\"314\":1,\"322\":1}}],[\"solen\",{\"1\":{\"249\":1,\"250\":1}}],[\"soldering\",{\"1\":{\"306\":1}}],[\"solder\",{\"1\":{\"306\":1}}],[\"solderlesspcb\",{\"0\":{\"306\":1},\"1\":{\"306\":4}}],[\"soldini\",{\"1\":{\"171\":1}}],[\"solicits\",{\"1\":{\"217\":1}}],[\"solid\",{\"1\":{\"142\":1}}],[\"solving\",{\"1\":{\"132\":1,\"228\":2,\"314\":1,\"318\":1,\"330\":1,\"338\":1,\"364\":1,\"372\":1,\"458\":1,\"494\":1}}],[\"solves\",{\"1\":{\"322\":1}}],[\"solved\",{\"1\":{\"292\":1}}],[\"solve\",{\"0\":{\"317\":1},\"1\":{\"99\":1,\"153\":1,\"235\":1,\"338\":1,\"355\":1,\"417\":1,\"490\":1}}],[\"solutions\",{\"1\":{\"127\":2,\"172\":1,\"216\":3,\"244\":1,\"251\":1,\"253\":1,\"254\":1,\"277\":1,\"284\":4,\"318\":1,\"359\":1,\"364\":1,\"370\":1,\"399\":1,\"422\":1,\"423\":1,\"425\":1,\"482\":1,\"494\":1}}],[\"solution\",{\"1\":{\"96\":2,\"143\":1,\"158\":1,\"180\":1,\"222\":1,\"240\":1,\"253\":1,\"263\":1,\"270\":1,\"282\":1,\"284\":1,\"325\":1,\"376\":1,\"405\":1,\"408\":1,\"444\":1,\"496\":1}}],[\"sadeghi\",{\"1\":{\"404\":1}}],[\"sadeh\",{\"1\":{\"135\":1}}],[\"saenko\",{\"1\":{\"376\":1}}],[\"sahebi\",{\"1\":{\"333\":1}}],[\"sahib\",{\"1\":{\"408\":1}}],[\"sahil\",{\"1\":{\"172\":1,\"370\":1,\"374\":1}}],[\"sahithya\",{\"1\":{\"133\":1,\"331\":1}}],[\"sabouni\",{\"1\":{\"304\":1}}],[\"save\",{\"1\":{\"298\":1}}],[\"savings\",{\"1\":{\"103\":1,\"408\":1,\"469\":1}}],[\"saving\",{\"1\":{\"99\":1,\"366\":1}}],[\"sasalovici\",{\"1\":{\"297\":1}}],[\"sasha\",{\"1\":{\"281\":1}}],[\"saskia\",{\"1\":{\"187\":1}}],[\"sa\",{\"1\":{\"256\":1}}],[\"sai\",{\"1\":{\"254\":1,\"265\":1}}],[\"said\",{\"1\":{\"241\":1}}],[\"sailendra\",{\"1\":{\"205\":1}}],[\"satpute\",{\"1\":{\"458\":1}}],[\"satml\",{\"1\":{\"409\":1}}],[\"satya\",{\"1\":{\"319\":1}}],[\"saturation\",{\"1\":{\"298\":2}}],[\"satisfactory\",{\"1\":{\"282\":2,\"471\":2}}],[\"satisfying\",{\"1\":{\"447\":1}}],[\"satisfy\",{\"1\":{\"233\":1}}],[\"sattigeri\",{\"1\":{\"215\":1}}],[\"satellite\",{\"0\":{\"101\":1},\"1\":{\"101\":3}}],[\"sayin\",{\"1\":{\"466\":1}}],[\"say\",{\"0\":{\"163\":1},\"1\":{\"202\":1}}],[\"saroar\",{\"1\":{\"460\":1}}],[\"sarker\",{\"1\":{\"422\":1}}],[\"sarkar\",{\"1\":{\"122\":1,\"391\":1}}],[\"sar\",{\"1\":{\"261\":5,\"288\":1}}],[\"sars\",{\"1\":{\"261\":2}}],[\"sargal\",{\"1\":{\"232\":1}}],[\"sarathi\",{\"1\":{\"391\":1}}],[\"sarasua\",{\"1\":{\"234\":1}}],[\"sarabia\",{\"1\":{\"186\":1}}],[\"sarah\",{\"1\":{\"164\":1,\"307\":1,\"431\":1}}],[\"sara\",{\"0\":{\"127\":1},\"1\":{\"127\":3,\"176\":1,\"254\":1}}],[\"saleh\",{\"1\":{\"456\":1}}],[\"salem\",{\"1\":{\"418\":1}}],[\"salika\",{\"1\":{\"391\":1}}],[\"salient\",{\"1\":{\"131\":1,\"256\":1}}],[\"salience\",{\"0\":{\"104\":1},\"1\":{\"104\":11}}],[\"saliency\",{\"1\":{\"102\":4}}],[\"sallam\",{\"1\":{\"339\":1}}],[\"salazar\",{\"1\":{\"315\":1}}],[\"saltz\",{\"1\":{\"155\":1}}],[\"salvadori\",{\"1\":{\"120\":1}}],[\"samavedhi\",{\"1\":{\"353\":1}}],[\"samuel\",{\"1\":{\"182\":1,\"263\":1}}],[\"sam\",{\"1\":{\"166\":1,\"430\":1}}],[\"sampled\",{\"1\":{\"369\":1}}],[\"sample\",{\"1\":{\"207\":1,\"308\":1,\"322\":1,\"355\":1}}],[\"samples\",{\"1\":{\"137\":1,\"151\":2,\"337\":2,\"348\":2,\"363\":1,\"378\":1,\"381\":1,\"444\":3,\"475\":1}}],[\"sampling\",{\"1\":{\"112\":1,\"214\":1,\"273\":1,\"289\":1,\"347\":1,\"455\":1,\"467\":1,\"487\":1}}],[\"same\",{\"1\":{\"106\":2,\"133\":1,\"149\":1,\"165\":1,\"188\":1,\"227\":1,\"231\":1,\"234\":1,\"241\":1,\"256\":1,\"301\":1,\"331\":1,\"335\":2,\"355\":1,\"378\":1,\"389\":2,\"419\":2,\"420\":2,\"435\":1,\"441\":1,\"443\":1,\"444\":1,\"449\":1,\"469\":1,\"486\":1,\"496\":2}}],[\"sanabria\",{\"1\":{\"419\":1,\"420\":1}}],[\"sanaei\",{\"1\":{\"304\":1}}],[\"sanvito\",{\"1\":{\"338\":1}}],[\"sandwich\",{\"0\":{\"334\":1},\"1\":{\"334\":2}}],[\"sande\",{\"1\":{\"178\":1}}],[\"sandeep\",{\"1\":{\"106\":1,\"185\":1}}],[\"sanian\",{\"1\":{\"333\":1}}],[\"sanctions\",{\"0\":{\"230\":1},\"1\":{\"230\":1}}],[\"sanker\",{\"1\":{\"226\":1}}],[\"santiago\",{\"1\":{\"205\":1}}],[\"santos\",{\"1\":{\"96\":1,\"294\":1}}],[\"safwat\",{\"1\":{\"257\":1}}],[\"safd\",{\"1\":{\"103\":1}}],[\"safely\",{\"1\":{\"261\":1,\"403\":1}}],[\"safer\",{\"1\":{\"181\":1}}],[\"safety\",{\"0\":{\"105\":1,\"206\":1,\"278\":1,\"298\":1,\"302\":1,\"344\":1,\"409\":1,\"434\":1},\"1\":{\"105\":3,\"186\":2,\"206\":4,\"211\":1,\"278\":4,\"300\":2,\"302\":2,\"313\":2,\"334\":1,\"344\":8,\"396\":2,\"403\":2,\"409\":1,\"434\":2}}],[\"safe\",{\"0\":{\"211\":1,\"372\":1},\"1\":{\"103\":3,\"211\":1,\"278\":1,\"302\":1,\"372\":1,\"434\":1}}],[\"saa荣誉奖\",{\"1\":{\"7\":1}}],[\"shtok\",{\"1\":{\"454\":1}}],[\"shwetak\",{\"1\":{\"207\":1}}],[\"shwartz\",{\"1\":{\"133\":1,\"331\":1}}],[\"shhs\",{\"1\":{\"194\":1}}],[\"shyama\",{\"1\":{\"172\":1,\"370\":1}}],[\"shervin\",{\"1\":{\"405\":1}}],[\"sheriff\",{\"1\":{\"245\":2}}],[\"she\",{\"1\":{\"395\":1}}],[\"shen\",{\"1\":{\"286\":1,\"393\":1,\"474\":1}}],[\"shenghua\",{\"1\":{\"463\":1}}],[\"shengyu\",{\"1\":{\"402\":1}}],[\"shengyun\",{\"1\":{\"258\":1,\"433\":1}}],[\"sheng\",{\"1\":{\"347\":1,\"428\":1}}],[\"shengding\",{\"1\":{\"316\":1}}],[\"shengfeng\",{\"1\":{\"260\":1}}],[\"shen2024towards\",{\"1\":{\"159\":1}}],[\"sheelagh\",{\"1\":{\"243\":1}}],[\"shed\",{\"1\":{\"214\":1,\"437\":1,\"458\":1}}],[\"shedding\",{\"1\":{\"106\":1,\"133\":1,\"256\":1,\"331\":1}}],[\"shelf\",{\"1\":{\"207\":1,\"438\":2,\"444\":1,\"490\":1}}],[\"shiwen\",{\"1\":{\"445\":1}}],[\"shibo\",{\"1\":{\"353\":1}}],[\"shibani\",{\"1\":{\"125\":1}}],[\"shishirpatil\",{\"1\":{\"326\":1}}],[\"shishir\",{\"1\":{\"326\":1}}],[\"shixuan\",{\"1\":{\"295\":1}}],[\"shivani\",{\"1\":{\"286\":1,\"474\":1}}],[\"shifting\",{\"1\":{\"308\":1}}],[\"shifts\",{\"1\":{\"303\":1,\"324\":1}}],[\"shift\",{\"0\":{\"253\":1},\"1\":{\"238\":3,\"364\":1,\"377\":1}}],[\"shi\",{\"1\":{\"222\":2,\"243\":1,\"257\":1,\"261\":1,\"369\":4,\"378\":1,\"437\":1,\"447\":1}}],[\"shijie\",{\"1\":{\"218\":1}}],[\"shiqing\",{\"1\":{\"167\":1}}],[\"shino\",{\"1\":{\"176\":1}}],[\"shin\",{\"1\":{\"143\":1,\"229\":1}}],[\"shuo\",{\"1\":{\"491\":1}}],[\"shuai\",{\"1\":{\"489\":1}}],[\"shuyuan\",{\"1\":{\"485\":1}}],[\"shuyue\",{\"1\":{\"133\":1,\"331\":1}}],[\"shulin\",{\"1\":{\"472\":1}}],[\"shu\",{\"1\":{\"453\":1,\"480\":1}}],[\"shuhua\",{\"1\":{\"353\":1}}],[\"shukla\",{\"1\":{\"319\":1}}],[\"shuwu\",{\"1\":{\"224\":1}}],[\"shum\",{\"1\":{\"125\":1}}],[\"shallow\",{\"1\":{\"479\":1}}],[\"shalit\",{\"1\":{\"309\":1}}],[\"shamma\",{\"1\":{\"468\":1}}],[\"shafiq\",{\"1\":{\"443\":1}}],[\"shafique\",{\"1\":{\"284\":1}}],[\"shaoyin\",{\"1\":{\"488\":1}}],[\"shaoguang\",{\"1\":{\"429\":1}}],[\"shaobo\",{\"1\":{\"391\":1}}],[\"shaoduo\",{\"1\":{\"366\":1}}],[\"shao\",{\"1\":{\"353\":1}}],[\"shaona\",{\"1\":{\"344\":1}}],[\"shashank\",{\"1\":{\"303\":1}}],[\"shake\",{\"1\":{\"284\":1}}],[\"shaw\",{\"1\":{\"230\":1}}],[\"shahriar\",{\"1\":{\"221\":1}}],[\"shap\",{\"1\":{\"237\":1}}],[\"shaping\",{\"0\":{\"200\":1}}],[\"shape\",{\"0\":{\"142\":1},\"1\":{\"142\":2,\"200\":1,\"286\":1,\"474\":1}}],[\"shades\",{\"0\":{\"198\":1,\"387\":1}}],[\"shaikh\",{\"1\":{\"180\":1}}],[\"shanshan\",{\"1\":{\"417\":1}}],[\"shang\",{\"1\":{\"322\":1,\"455\":1,\"470\":1,\"488\":1}}],[\"shan\",{\"1\":{\"210\":1,\"411\":1,\"445\":1}}],[\"shanning\",{\"1\":{\"173\":1}}],[\"shandler\",{\"1\":{\"106\":1}}],[\"sharif\",{\"1\":{\"431\":1}}],[\"sharing\",{\"1\":{\"302\":1,\"519\":2}}],[\"sharon\",{\"1\":{\"278\":1,\"279\":1}}],[\"sharma\",{\"1\":{\"248\":1,\"434\":1}}],[\"shares\",{\"1\":{\"409\":1}}],[\"share\",{\"1\":{\"163\":1,\"173\":1,\"198\":1,\"387\":1,\"432\":1}}],[\"shared\",{\"0\":{\"186\":1},\"1\":{\"106\":1,\"170\":1,\"186\":7,\"190\":1,\"250\":1,\"325\":2,\"462\":1,\"469\":1,\"478\":1}}],[\"sharana\",{\"1\":{\"137\":1}}],[\"shroom\",{\"0\":{\"388\":1},\"1\":{\"325\":1,\"388\":1}}],[\"shreyasi\",{\"1\":{\"372\":1}}],[\"shrestha\",{\"1\":{\"261\":1}}],[\"shree\",{\"1\":{\"104\":1}}],[\"shruti\",{\"1\":{\"154\":1}}],[\"shrier\",{\"1\":{\"81\":1}}],[\"should\",{\"0\":{\"303\":1},\"1\":{\"158\":1,\"161\":1,\"178\":1,\"198\":1,\"235\":1,\"240\":1,\"255\":1,\"256\":1,\"265\":1,\"280\":2,\"322\":1,\"350\":1,\"354\":1,\"387\":1,\"417\":1,\"422\":1,\"463\":1}}],[\"shopping\",{\"1\":{\"128\":1,\"282\":1}}],[\"shot\",{\"0\":{\"266\":1,\"386\":1,\"388\":1,\"417\":1,\"444\":1},\"1\":{\"104\":1,\"266\":1,\"317\":1,\"323\":2,\"349\":1,\"350\":1,\"355\":2,\"365\":2,\"369\":1,\"372\":1,\"376\":1,\"382\":1,\"386\":3,\"388\":3,\"405\":5,\"412\":1,\"413\":1,\"417\":1,\"442\":1,\"444\":1,\"455\":2,\"456\":1,\"470\":1,\"472\":1,\"481\":2,\"485\":2}}],[\"showcase\",{\"1\":{\"390\":1,\"434\":1,\"482\":1}}],[\"showcases\",{\"1\":{\"358\":1,\"377\":1,\"418\":1}}],[\"showcasing\",{\"1\":{\"124\":1,\"167\":1,\"300\":1,\"323\":1,\"361\":1,\"399\":1}}],[\"shows\",{\"1\":{\"159\":1,\"172\":2,\"246\":1,\"315\":1,\"325\":1,\"370\":2,\"374\":1,\"381\":1,\"386\":1,\"408\":2,\"416\":1,\"417\":1,\"444\":1,\"482\":1,\"488\":1}}],[\"shown\",{\"1\":{\"131\":1,\"172\":1,\"200\":1,\"205\":1,\"216\":1,\"221\":1,\"224\":1,\"229\":1,\"233\":1,\"258\":1,\"261\":1,\"274\":1,\"289\":1,\"294\":1,\"322\":1,\"337\":1,\"345\":1,\"347\":1,\"350\":1,\"360\":1,\"370\":1,\"379\":1,\"386\":1,\"394\":1,\"431\":1,\"432\":1,\"433\":1,\"459\":1,\"470\":1,\"486\":1,\"495\":1}}],[\"showed\",{\"1\":{\"107\":1,\"117\":1,\"132\":1,\"165\":1,\"173\":1,\"176\":1,\"191\":1,\"204\":1,\"237\":1,\"269\":1,\"290\":1,\"295\":1,\"330\":1,\"388\":1,\"428\":1,\"458\":1}}],[\"showing\",{\"1\":{\"104\":1,\"182\":2,\"221\":1,\"240\":1,\"242\":1,\"246\":1,\"304\":1,\"482\":1,\"485\":1}}],[\"show\",{\"0\":{\"240\":1},\"1\":{\"96\":1,\"102\":1,\"121\":1,\"123\":1,\"130\":1,\"148\":1,\"150\":1,\"152\":2,\"188\":1,\"189\":1,\"194\":1,\"209\":1,\"224\":1,\"233\":1,\"234\":1,\"235\":1,\"238\":1,\"240\":1,\"241\":3,\"252\":1,\"254\":1,\"256\":1,\"260\":1,\"270\":1,\"275\":1,\"279\":1,\"283\":1,\"289\":1,\"295\":1,\"296\":1,\"324\":1,\"334\":1,\"336\":1,\"344\":2,\"352\":1,\"368\":2,\"369\":1,\"376\":1,\"391\":1,\"395\":1,\"399\":1,\"402\":1,\"405\":1,\"409\":3,\"410\":1,\"413\":1,\"414\":1,\"416\":1,\"425\":1,\"426\":1,\"427\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"449\":1,\"453\":1,\"460\":1,\"465\":1,\"475\":1,\"479\":1,\"485\":2,\"490\":1}}],[\"shortage\",{\"1\":{\"469\":1}}],[\"shortcut\",{\"1\":{\"349\":1,\"372\":1}}],[\"shortcoming\",{\"1\":{\"266\":1}}],[\"shortcomings\",{\"1\":{\"121\":1,\"315\":1}}],[\"shorter\",{\"1\":{\"265\":2,\"389\":2,\"428\":1}}],[\"short\",{\"0\":{\"472\":1},\"1\":{\"74\":1,\"119\":1,\"151\":1,\"161\":1,\"203\":1,\"209\":1,\"246\":1,\"322\":1,\"348\":1,\"354\":1,\"376\":4,\"441\":1,\"467\":1,\"472\":1,\"475\":1,\"530\":1}}],[\"sichun\",{\"1\":{\"483\":1}}],[\"sicheng\",{\"1\":{\"235\":1}}],[\"sizhe\",{\"1\":{\"470\":1}}],[\"sizes\",{\"1\":{\"359\":1,\"414\":1,\"441\":1,\"494\":1}}],[\"size\",{\"1\":{\"210\":1,\"216\":1,\"272\":1,\"295\":1,\"350\":1,\"377\":1,\"378\":1,\"436\":1,\"455\":2,\"469\":1}}],[\"sized\",{\"1\":{\"152\":1,\"315\":1,\"441\":1}}],[\"sivan\",{\"1\":{\"454\":1}}],[\"sivaraman\",{\"1\":{\"121\":1}}],[\"siyu\",{\"1\":{\"363\":1,\"428\":1}}],[\"siyao\",{\"1\":{\"138\":1}}],[\"siracusano\",{\"1\":{\"338\":1}}],[\"sirui\",{\"1\":{\"329\":1}}],[\"siren\",{\"0\":{\"287\":1},\"1\":{\"287\":4}}],[\"sippy\",{\"1\":{\"302\":1}}],[\"siam\",{\"1\":{\"298\":1}}],[\"siat\",{\"1\":{\"103\":2}}],[\"silin\",{\"1\":{\"245\":1}}],[\"silvia\",{\"1\":{\"282\":1,\"297\":1}}],[\"silva\",{\"1\":{\"147\":1,\"275\":1}}],[\"silverlove\",{\"1\":{\"6\":1}}],[\"sight\",{\"1\":{\"239\":1}}],[\"signllm\",{\"1\":{\"438\":2}}],[\"sign\",{\"0\":{\"438\":1},\"1\":{\"309\":2,\"438\":10}}],[\"signs\",{\"1\":{\"309\":1,\"414\":1}}],[\"significance\",{\"1\":{\"245\":1,\"264\":1,\"350\":1,\"448\":1,\"489\":1}}],[\"significantly\",{\"0\":{\"155\":1},\"1\":{\"118\":1,\"119\":1,\"121\":1,\"123\":1,\"124\":1,\"128\":1,\"130\":1,\"155\":2,\"165\":2,\"176\":1,\"184\":1,\"198\":1,\"223\":1,\"227\":1,\"234\":1,\"235\":1,\"238\":1,\"242\":1,\"261\":1,\"263\":1,\"265\":1,\"273\":2,\"275\":1,\"290\":1,\"298\":1,\"302\":3,\"303\":1,\"305\":1,\"328\":1,\"329\":1,\"347\":1,\"350\":1,\"356\":1,\"360\":1,\"365\":1,\"374\":1,\"378\":1,\"379\":1,\"380\":1,\"382\":1,\"387\":1,\"395\":1,\"396\":1,\"398\":1,\"404\":1,\"408\":1,\"415\":1,\"416\":1,\"418\":1,\"430\":1,\"437\":1,\"441\":1,\"463\":1,\"466\":1,\"469\":1,\"470\":1,\"471\":1,\"479\":1,\"486\":2,\"489\":1,\"495\":1}}],[\"significant\",{\"1\":{\"103\":2,\"116\":1,\"120\":1,\"135\":1,\"142\":1,\"185\":1,\"190\":1,\"203\":1,\"204\":1,\"213\":1,\"237\":2,\"246\":1,\"253\":2,\"263\":1,\"265\":1,\"266\":1,\"281\":1,\"298\":1,\"304\":1,\"309\":2,\"313\":1,\"317\":1,\"318\":1,\"324\":1,\"325\":1,\"326\":1,\"338\":1,\"345\":1,\"353\":1,\"358\":1,\"360\":1,\"362\":1,\"364\":1,\"384\":1,\"408\":1,\"412\":1,\"414\":1,\"417\":1,\"434\":1,\"435\":1,\"436\":1,\"445\":2,\"453\":1,\"460\":1,\"461\":1,\"469\":1,\"472\":1,\"478\":1,\"482\":1,\"492\":1,\"498\":1}}],[\"signal\",{\"0\":{\"298\":1},\"1\":{\"120\":1,\"284\":1,\"298\":2,\"450\":1,\"462\":1}}],[\"signaling\",{\"1\":{\"119\":1}}],[\"signals\",{\"0\":{\"194\":1,\"200\":1},\"1\":{\"103\":1,\"188\":2,\"200\":1,\"204\":2,\"215\":1,\"284\":3,\"477\":1}}],[\"signatures\",{\"1\":{\"112\":1,\"295\":1,\"410\":1}}],[\"sindhu\",{\"1\":{\"424\":1}}],[\"sinno\",{\"1\":{\"317\":1}}],[\"since\",{\"1\":{\"210\":1,\"265\":2,\"271\":1,\"307\":1,\"399\":1,\"408\":1,\"435\":1,\"457\":1,\"469\":2}}],[\"singular\",{\"1\":{\"395\":1,\"400\":1}}],[\"single\",{\"1\":{\"104\":1,\"139\":1,\"197\":1,\"228\":1,\"240\":1,\"260\":1,\"263\":1,\"295\":1,\"313\":1,\"315\":1,\"333\":1,\"349\":1,\"381\":1,\"393\":1,\"400\":1,\"405\":1,\"415\":3,\"441\":1,\"444\":1,\"455\":1}}],[\"singhal\",{\"1\":{\"172\":1,\"370\":1}}],[\"singh2024rethinking\",{\"1\":{\"159\":1}}],[\"singh\",{\"1\":{\"97\":1,\"172\":1,\"211\":1,\"312\":1,\"370\":1}}],[\"siddharth\",{\"1\":{\"419\":1,\"420\":1,\"490\":1}}],[\"siddhant\",{\"1\":{\"308\":1}}],[\"siddardh\",{\"1\":{\"196\":1}}],[\"sidewalk\",{\"1\":{\"164\":3}}],[\"sita\",{\"1\":{\"185\":1}}],[\"situo\",{\"1\":{\"489\":1}}],[\"situation\",{\"1\":{\"240\":1,\"253\":1}}],[\"situational\",{\"1\":{\"214\":2,\"240\":1}}],[\"situations\",{\"0\":{\"236\":1},\"1\":{\"182\":1,\"236\":3,\"435\":1}}],[\"situated\",{\"1\":{\"156\":2,\"292\":1,\"497\":1}}],[\"situ\",{\"1\":{\"141\":1}}],[\"site\",{\"0\":{\"147\":1},\"1\":{\"117\":2,\"147\":1}}],[\"sites\",{\"0\":{\"117\":1},\"1\":{\"184\":1,\"380\":1,\"539\":1}}],[\"sixth\",{\"1\":{\"388\":1}}],[\"six\",{\"1\":{\"105\":1,\"149\":1,\"215\":1,\"251\":1,\"327\":1,\"328\":1,\"398\":1,\"412\":1,\"423\":1,\"461\":1}}],[\"sim\",{\"1\":{\"488\":2}}],[\"simamr\",{\"1\":{\"432\":1}}],[\"simret\",{\"1\":{\"271\":1,\"457\":1}}],[\"simply\",{\"1\":{\"335\":1,\"338\":1,\"362\":1}}],[\"simpler\",{\"0\":{\"327\":1}}],[\"simple\",{\"0\":{\"409\":1},\"1\":{\"216\":1,\"223\":1,\"256\":1,\"278\":1,\"304\":3,\"319\":1,\"340\":1,\"350\":1,\"369\":1,\"395\":1,\"409\":1,\"432\":1,\"454\":1}}],[\"simplification\",{\"1\":{\"327\":2}}],[\"simplified\",{\"0\":{\"327\":1},\"1\":{\"327\":2,\"422\":1}}],[\"simplifies\",{\"1\":{\"99\":1,\"113\":1,\"115\":1}}],[\"simplifying\",{\"1\":{\"282\":1,\"398\":1}}],[\"simplicity\",{\"1\":{\"203\":1,\"427\":1}}],[\"sim$1\",{\"1\":{\"159\":1}}],[\"simone\",{\"1\":{\"129\":1,\"405\":1}}],[\"simon\",{\"1\":{\"125\":2,\"161\":1,\"203\":1,\"271\":1,\"354\":1,\"457\":1,\"533\":1}}],[\"similarities\",{\"1\":{\"224\":1,\"270\":1,\"409\":1}}],[\"similarity\",{\"1\":{\"146\":1,\"384\":3,\"432\":2,\"460\":2,\"488\":1}}],[\"similarly\",{\"1\":{\"163\":1,\"252\":1,\"315\":1,\"426\":1}}],[\"similar\",{\"1\":{\"117\":1,\"171\":1,\"176\":1,\"240\":1,\"249\":1,\"264\":1,\"273\":1,\"302\":1,\"317\":2,\"322\":1,\"334\":1,\"335\":1,\"375\":1,\"448\":1,\"470\":1,\"488\":1}}],[\"simulate\",{\"1\":{\"210\":1,\"218\":1,\"403\":1}}],[\"simulated\",{\"0\":{\"132\":1,\"330\":1},\"1\":{\"132\":1,\"238\":4,\"240\":1,\"309\":1,\"330\":1,\"447\":1,\"468\":1}}],[\"simulations\",{\"1\":{\"138\":1,\"207\":1}}],[\"simulation\",{\"0\":{\"130\":1,\"218\":1,\"238\":1},\"1\":{\"130\":1,\"132\":3,\"138\":2,\"196\":1,\"218\":3,\"245\":1,\"330\":3}}],[\"simulating\",{\"1\":{\"116\":2,\"130\":1,\"132\":1,\"173\":1,\"330\":1}}],[\"simulator\",{\"1\":{\"116\":1,\"186\":1}}],[\"simultaneous\",{\"1\":{\"103\":1}}],[\"simultaneously\",{\"1\":{\"74\":1,\"191\":1,\"368\":1,\"463\":1}}],[\"stiefelhagen\",{\"1\":{\"238\":1}}],[\"stimulations\",{\"1\":{\"256\":1}}],[\"stimulating\",{\"1\":{\"227\":1,\"274\":1}}],[\"stimuli\",{\"1\":{\"220\":1,\"305\":1}}],[\"stimulus\",{\"1\":{\"185\":2,\"220\":1}}],[\"stickney\",{\"1\":{\"196\":1}}],[\"still\",{\"1\":{\"138\":1,\"161\":1,\"212\":1,\"231\":1,\"255\":1,\"261\":1,\"282\":1,\"283\":1,\"324\":1,\"341\":1,\"354\":1,\"360\":1,\"381\":1,\"395\":1,\"412\":1,\"413\":2,\"417\":1,\"431\":1,\"432\":1,\"461\":1,\"463\":1,\"478\":1,\"483\":1,\"494\":1}}],[\"stojkovic\",{\"1\":{\"465\":1}}],[\"stoll\",{\"1\":{\"400\":1}}],[\"stoica\",{\"1\":{\"326\":1,\"414\":1}}],[\"stokes\",{\"1\":{\"226\":1,\"280\":1}}],[\"stones\",{\"1\":{\"338\":1}}],[\"stone\",{\"1\":{\"176\":1,\"274\":1}}],[\"stored\",{\"1\":{\"392\":1}}],[\"stores\",{\"1\":{\"264\":1,\"448\":1,\"479\":1}}],[\"store\",{\"1\":{\"257\":1,\"318\":1,\"390\":1}}],[\"storage\",{\"1\":{\"192\":1,\"264\":1,\"448\":1,\"469\":2}}],[\"storytellers\",{\"1\":{\"243\":1}}],[\"storytelling\",{\"0\":{\"98\":1,\"243\":1},\"1\":{\"98\":4,\"217\":1,\"243\":4}}],[\"story\",{\"0\":{\"98\":1},\"1\":{\"98\":3,\"209\":2}}],[\"stories\",{\"1\":{\"74\":1,\"155\":2,\"209\":1,\"243\":1,\"530\":1}}],[\"style\",{\"1\":{\"224\":1,\"336\":1,\"485\":1}}],[\"stylus\",{\"1\":{\"126\":1}}],[\"stylistic\",{\"1\":{\"114\":1}}],[\"strated\",{\"1\":{\"414\":1}}],[\"strategic\",{\"0\":{\"429\":1},\"1\":{\"183\":1,\"245\":1,\"264\":1,\"323\":1,\"365\":1,\"377\":1,\"429\":4,\"448\":1}}],[\"strategically\",{\"1\":{\"150\":1,\"359\":1}}],[\"strategies\",{\"0\":{\"212\":1},\"1\":{\"98\":1,\"104\":1,\"112\":1,\"130\":1,\"136\":1,\"186\":2,\"195\":1,\"212\":2,\"250\":1,\"277\":1,\"279\":1,\"319\":1,\"323\":2,\"326\":1,\"339\":1,\"353\":1,\"386\":1,\"403\":1,\"411\":1,\"417\":1,\"429\":1,\"453\":2,\"462\":1,\"470\":1,\"471\":1,\"475\":1,\"487\":2,\"491\":1}}],[\"strategy\",{\"0\":{\"445\":1},\"1\":{\"139\":1,\"179\":1,\"297\":1,\"318\":1,\"355\":1,\"362\":1,\"378\":1,\"392\":1,\"405\":1,\"414\":1,\"445\":3,\"459\":1,\"462\":2,\"480\":2,\"529\":1}}],[\"stripped\",{\"0\":{\"488\":1},\"1\":{\"488\":2}}],[\"strings\",{\"1\":{\"409\":1}}],[\"strides\",{\"1\":{\"313\":1,\"412\":1}}],[\"strive\",{\"1\":{\"131\":1}}],[\"stroke\",{\"1\":{\"220\":1}}],[\"strongest\",{\"1\":{\"313\":1}}],[\"stronger\",{\"0\":{\"229\":1},\"1\":{\"318\":1}}],[\"strongly\",{\"1\":{\"252\":1,\"426\":1}}],[\"strong\",{\"0\":{\"318\":1},\"1\":{\"132\":1,\"166\":1,\"178\":1,\"189\":1,\"275\":1,\"318\":1,\"330\":1,\"344\":1,\"369\":1,\"389\":1,\"403\":1,\"411\":2,\"418\":1,\"438\":1,\"449\":1,\"463\":1,\"467\":1,\"471\":1,\"491\":1}}],[\"strengthening\",{\"1\":{\"403\":1}}],[\"strengthen\",{\"1\":{\"403\":2}}],[\"strengths\",{\"1\":{\"137\":1,\"143\":1,\"244\":1,\"327\":1}}],[\"stremmel\",{\"1\":{\"323\":1}}],[\"street\",{\"0\":{\"270\":1,\"272\":1},\"1\":{\"270\":2,\"272\":6}}],[\"stress\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"streaming\",{\"1\":{\"239\":1}}],[\"streams\",{\"0\":{\"188\":1},\"1\":{\"188\":1}}],[\"streamlined\",{\"1\":{\"359\":1,\"492\":1}}],[\"streamline\",{\"1\":{\"114\":1,\"141\":1,\"358\":1,\"396\":1}}],[\"structure\",{\"0\":{\"283\":1},\"1\":{\"141\":2,\"163\":1,\"244\":1,\"275\":1,\"283\":1,\"287\":1,\"307\":1,\"335\":1,\"384\":1,\"458\":1,\"487\":1}}],[\"structured\",{\"0\":{\"114\":1,\"402\":1,\"432\":1},\"1\":{\"114\":1,\"120\":2,\"141\":1,\"174\":1,\"212\":1,\"286\":1,\"293\":1,\"371\":1,\"398\":1,\"427\":1,\"474\":1,\"492\":2}}],[\"structures\",{\"0\":{\"259\":1},\"1\":{\"111\":1,\"112\":1,\"205\":1,\"259\":1}}],[\"structural\",{\"1\":{\"112\":2,\"235\":1,\"259\":1,\"488\":1}}],[\"struggles\",{\"0\":{\"340\":1},\"1\":{\"485\":1}}],[\"struggle\",{\"0\":{\"385\":1,\"412\":1},\"1\":{\"111\":1,\"124\":1,\"133\":1,\"324\":1,\"331\":1,\"360\":1,\"381\":1,\"454\":1,\"475\":1}}],[\"student\",{\"0\":{\"130\":1},\"1\":{\"130\":6,\"132\":3,\"153\":1,\"172\":2,\"290\":1,\"291\":1,\"307\":1,\"308\":1,\"330\":3,\"370\":2,\"476\":1,\"486\":1,\"524\":1}}],[\"students\",{\"0\":{\"125\":1,\"237\":1,\"308\":1},\"1\":{\"99\":1,\"132\":4,\"149\":1,\"156\":4,\"172\":8,\"228\":4,\"237\":2,\"268\":1,\"274\":1,\"290\":1,\"291\":1,\"292\":1,\"307\":3,\"308\":6,\"330\":4,\"370\":8,\"447\":1,\"452\":1,\"476\":1,\"486\":2}}],[\"studying\",{\"1\":{\"200\":1,\"270\":1,\"441\":1}}],[\"study\",{\"0\":{\"128\":1,\"135\":1,\"175\":1,\"185\":1,\"187\":1,\"202\":1,\"308\":1,\"323\":1,\"339\":1,\"365\":1,\"394\":1,\"431\":1,\"460\":1,\"491\":1},\"1\":{\"99\":1,\"102\":2,\"103\":1,\"105\":2,\"106\":3,\"107\":2,\"120\":2,\"128\":2,\"133\":1,\"135\":1,\"136\":2,\"140\":1,\"142\":1,\"155\":1,\"159\":1,\"160\":1,\"164\":1,\"165\":1,\"171\":1,\"172\":3,\"173\":1,\"175\":1,\"176\":2,\"184\":1,\"185\":1,\"187\":2,\"188\":2,\"189\":3,\"191\":1,\"195\":1,\"196\":1,\"200\":1,\"201\":2,\"202\":2,\"203\":2,\"212\":2,\"214\":3,\"215\":3,\"219\":1,\"228\":1,\"233\":1,\"237\":2,\"238\":2,\"242\":2,\"246\":2,\"249\":2,\"252\":1,\"254\":1,\"263\":1,\"264\":1,\"265\":1,\"268\":4,\"270\":2,\"272\":2,\"273\":1,\"274\":1,\"277\":1,\"279\":2,\"281\":1,\"290\":2,\"291\":3,\"292\":2,\"295\":1,\"300\":1,\"304\":1,\"305\":1,\"307\":1,\"308\":1,\"309\":1,\"323\":1,\"324\":1,\"326\":1,\"327\":1,\"331\":1,\"339\":2,\"341\":1,\"350\":1,\"353\":1,\"356\":2,\"358\":1,\"361\":1,\"363\":1,\"365\":1,\"370\":3,\"377\":1,\"380\":1,\"392\":1,\"394\":1,\"395\":1,\"401\":1,\"412\":2,\"425\":1,\"426\":1,\"431\":1,\"437\":1,\"445\":3,\"448\":1,\"452\":4,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"467\":1,\"470\":1,\"472\":1,\"476\":3,\"482\":1,\"486\":1,\"491\":1,\"495\":1,\"496\":1,\"497\":1}}],[\"studies\",{\"1\":{\"102\":1,\"104\":1,\"105\":1,\"110\":1,\"121\":1,\"131\":1,\"132\":1,\"142\":1,\"152\":1,\"165\":1,\"172\":1,\"174\":2,\"185\":1,\"191\":1,\"194\":1,\"195\":1,\"197\":1,\"203\":1,\"226\":2,\"228\":1,\"238\":3,\"249\":1,\"255\":1,\"288\":1,\"293\":2,\"302\":1,\"308\":1,\"317\":1,\"321\":1,\"328\":1,\"330\":1,\"370\":1,\"371\":2,\"400\":1,\"417\":3,\"425\":1,\"432\":1,\"475\":1,\"491\":1,\"498\":1}}],[\"studied\",{\"1\":{\"97\":1,\"156\":1,\"312\":1}}],[\"studio后\",{\"1\":{\"15\":1}}],[\"studio\",{\"1\":{\"15\":2,\"430\":1}}],[\"staats\",{\"1\":{\"494\":1}}],[\"starting\",{\"1\":{\"455\":1}}],[\"starts\",{\"1\":{\"399\":1,\"442\":1}}],[\"stark\",{\"1\":{\"365\":1}}],[\"staiano\",{\"1\":{\"390\":1,\"466\":1}}],[\"stacks\",{\"1\":{\"435\":1}}],[\"stack\",{\"0\":{\"458\":1},\"1\":{\"350\":1,\"458\":1}}],[\"stacked\",{\"1\":{\"305\":1}}],[\"stand\",{\"0\":{\"251\":1,\"423\":1}}],[\"standalone\",{\"1\":{\"231\":1,\"467\":1}}],[\"standardized\",{\"1\":{\"353\":1,\"379\":1,\"398\":1}}],[\"standardizability\",{\"1\":{\"133\":1,\"331\":1}}],[\"standard\",{\"1\":{\"137\":1,\"173\":2,\"188\":2,\"194\":1,\"233\":1,\"249\":1,\"271\":1,\"294\":1,\"339\":1,\"389\":1,\"396\":2,\"441\":1,\"442\":1,\"457\":1,\"470\":1,\"482\":1,\"485\":2,\"487\":1}}],[\"standards\",{\"0\":{\"396\":1},\"1\":{\"114\":1,\"298\":1,\"396\":8}}],[\"staff\",{\"1\":{\"249\":1}}],[\"stamper\",{\"1\":{\"228\":1}}],[\"staggering\",{\"1\":{\"463\":1}}],[\"staging\",{\"0\":{\"194\":1},\"1\":{\"194\":4}}],[\"stages\",{\"1\":{\"166\":1,\"174\":1,\"184\":1,\"260\":1,\"333\":1,\"371\":1,\"380\":1,\"462\":1}}],[\"stage\",{\"0\":{\"445\":1},\"1\":{\"79\":1,\"110\":1,\"123\":3,\"164\":1,\"194\":2,\"234\":1,\"321\":1,\"322\":1,\"384\":1,\"445\":1,\"458\":1}}],[\"stability\",{\"1\":{\"150\":2,\"260\":1}}],[\"stable\",{\"1\":{\"101\":1}}],[\"status\",{\"1\":{\"174\":1,\"178\":1,\"295\":1,\"371\":1,\"429\":1}}],[\"stationary\",{\"1\":{\"284\":1}}],[\"statistics\",{\"1\":{\"210\":1,\"468\":1}}],[\"statistical\",{\"1\":{\"183\":1,\"237\":1,\"246\":1,\"251\":1,\"254\":1,\"333\":1,\"350\":1,\"423\":1,\"428\":2}}],[\"statistically\",{\"1\":{\"135\":1,\"304\":1}}],[\"static\",{\"1\":{\"142\":1,\"159\":2,\"215\":3,\"434\":1}}],[\"statement\",{\"1\":{\"494\":1}}],[\"statements\",{\"0\":{\"340\":1},\"1\":{\"494\":1}}],[\"states\",{\"1\":{\"223\":2,\"322\":1,\"328\":1,\"461\":1}}],[\"state\",{\"1\":{\"101\":1,\"102\":1,\"105\":1,\"118\":1,\"123\":1,\"127\":1,\"130\":1,\"137\":1,\"139\":1,\"140\":1,\"159\":1,\"170\":2,\"183\":1,\"194\":2,\"218\":1,\"223\":1,\"275\":1,\"283\":1,\"288\":1,\"301\":2,\"309\":1,\"314\":1,\"315\":1,\"322\":1,\"334\":1,\"337\":1,\"344\":1,\"347\":2,\"350\":1,\"360\":1,\"372\":1,\"376\":2,\"386\":1,\"393\":1,\"408\":1,\"411\":1,\"413\":1,\"418\":1,\"432\":1,\"434\":1,\"438\":1,\"456\":2,\"492\":1}}],[\"stakes\",{\"1\":{\"176\":1}}],[\"stake\",{\"1\":{\"124\":1}}],[\"stakeholders\",{\"1\":{\"105\":1,\"120\":1,\"196\":2,\"217\":1,\"253\":1,\"272\":1,\"339\":1,\"490\":1,\"498\":1}}],[\"stakeholder\",{\"0\":{\"196\":1},\"1\":{\"105\":1}}],[\"stapleton\",{\"1\":{\"81\":1}}],[\"steady\",{\"1\":{\"428\":1}}],[\"steam教育的最新发展和艺术的融入已不再局限于绘画\",{\"1\":{\"78\":1}}],[\"steam教育理念传入我国\",{\"1\":{\"78\":1}}],[\"stewart\",{\"1\":{\"384\":1}}],[\"steep\",{\"1\":{\"259\":1}}],[\"stereotype\",{\"1\":{\"251\":1,\"423\":1}}],[\"stefan\",{\"1\":{\"400\":2}}],[\"stefania\",{\"1\":{\"171\":1}}],[\"steffen\",{\"1\":{\"182\":1}}],[\"steven\",{\"1\":{\"482\":1}}],[\"stevens\",{\"1\":{\"176\":1,\"302\":1}}],[\"steve\",{\"1\":{\"170\":1}}],[\"stella\",{\"1\":{\"130\":1,\"133\":1,\"331\":1}}],[\"stems\",{\"1\":{\"425\":1}}],[\"stem\",{\"0\":{\"112\":1},\"1\":{\"112\":4,\"132\":1,\"164\":1,\"330\":1,\"340\":1,\"353\":1}}],[\"stem教育里面引入中国\",{\"1\":{\"77\":1}}],[\"stephen\",{\"1\":{\"304\":1}}],[\"stepping\",{\"1\":{\"274\":1}}],[\"stepanova\",{\"1\":{\"240\":1}}],[\"steps\",{\"1\":{\"127\":1,\"138\":1,\"166\":1,\"251\":1,\"275\":1,\"313\":1,\"322\":1,\"381\":2,\"389\":1,\"405\":1,\"423\":1,\"434\":1,\"483\":1,\"494\":1}}],[\"step\",{\"0\":{\"170\":1,\"353\":2},\"1\":{\"110\":1,\"120\":1,\"125\":1,\"179\":2,\"228\":1,\"242\":1,\"257\":1,\"259\":2,\"260\":1,\"278\":1,\"321\":1,\"338\":1,\"353\":2,\"363\":1,\"379\":1,\"390\":1,\"422\":1,\"444\":1,\"458\":1,\"483\":3,\"487\":1}}],[\"skoviera\",{\"1\":{\"240\":1}}],[\"skeleton\",{\"1\":{\"246\":1,\"450\":4}}],[\"skeletons\",{\"1\":{\"235\":1}}],[\"sketched\",{\"1\":{\"181\":1}}],[\"sketches\",{\"1\":{\"181\":1}}],[\"sketchup\",{\"1\":{\"8\":1}}],[\"skewed\",{\"1\":{\"133\":2,\"331\":2}}],[\"skin\",{\"0\":{\"295\":1},\"1\":{\"220\":1,\"295\":1}}],[\"skip\",{\"1\":{\"175\":1}}],[\"skilled\",{\"1\":{\"300\":1}}],[\"skill\",{\"0\":{\"180\":1},\"1\":{\"132\":3,\"180\":3,\"330\":3}}],[\"skills\",{\"0\":{\"8\":1},\"1\":{\"120\":2,\"125\":2,\"126\":3,\"180\":2,\"217\":1,\"227\":1,\"244\":1,\"249\":1,\"274\":1,\"275\":1,\"290\":2,\"292\":3}}],[\"skiers\",{\"1\":{\"108\":1}}],[\"skrammologi\",{\"1\":{\"48\":1}}],[\"skrammel\",{\"1\":{\"48\":2}}],[\"skrammellegepladsen\",{\"1\":{\"48\":1}}],[\"广告文案等各种类型的营销和非营销内容\",{\"1\":{\"539\":1}}],[\"广告收入增长\",{\"1\":{\"528\":1}}],[\"广告赞助也是一个重要的收入来源\",{\"1\":{\"520\":1}}],[\"广泛性\",{\"1\":{\"28\":1}}],[\"广州设计周\",{\"1\":{\"9\":1}}],[\"广州市天河区立德桥下空间微改造方案\",{\"1\":{\"7\":1}}],[\"广州美术学院建筑与应用艺术学院颁发2019广州美术学院gafa\",{\"1\":{\"7\":1}}],[\"广州美术学院颁发2019届本科生优秀毕业创作奖\",{\"1\":{\"7\":1}}],[\"广东省第四届高校建筑与环境艺术设计毕业设计\",{\"1\":{\"7\":1}}],[\"广东省高等学校建筑与环境艺术设计学术委员会颁发的\",{\"1\":{\"7\":1}}],[\"羊盟奖\",{\"1\":{\"7\":1}}],[\"737\",{\"1\":{\"369\":1}}],[\"7b\",{\"1\":{\"313\":2,\"356\":3,\"369\":2,\"404\":1,\"409\":2,\"411\":1,\"471\":1,\"487\":1,\"497\":2}}],[\"74\",{\"1\":{\"234\":1,\"466\":1}}],[\"76\",{\"1\":{\"219\":1,\"401\":1}}],[\"71\",{\"1\":{\"194\":1,\"363\":1,\"372\":1}}],[\"711\",{\"1\":{\"74\":1}}],[\"78\",{\"1\":{\"194\":1,\"232\":1,\"458\":1}}],[\"77\",{\"1\":{\"194\":1}}],[\"75\",{\"1\":{\"194\":1,\"209\":1}}],[\"79\",{\"1\":{\"161\":1,\"335\":1,\"354\":1}}],[\"72\",{\"1\":{\"128\":1,\"133\":1,\"331\":1}}],[\"7岁也对应中国学龄前儿童的年龄\",{\"1\":{\"79\":1}}],[\"70b\",{\"1\":{\"365\":1,\"391\":1,\"395\":1,\"409\":1,\"411\":2,\"418\":1,\"441\":1,\"456\":1,\"492\":1,\"497\":1}}],[\"7080\",{\"1\":{\"159\":1}}],[\"70\",{\"1\":{\"49\":1,\"334\":1,\"366\":1}}],[\"7\",{\"1\":{\"7\":1,\"9\":4,\"118\":1,\"133\":1,\"210\":1,\"331\":1,\"369\":1,\"431\":1,\"460\":1,\"462\":1,\"482\":1,\"492\":2}}],[\"1️⃣\",{\"1\":{\"531\":2}}],[\"1️⃣数据驱动设计是设计师在动荡时期的生存工具\",{\"1\":{\"527\":1}}],[\"17819v1\",{\"1\":{\"498\":1}}],[\"17856v1\",{\"1\":{\"497\":1}}],[\"17860v1\",{\"1\":{\"496\":1}}],[\"17927v1\",{\"1\":{\"495\":1}}],[\"174\",{\"1\":{\"412\":1}}],[\"17\",{\"1\":{\"350\":1,\"428\":1}}],[\"1st\",{\"1\":{\"325\":1}}],[\"162\",{\"1\":{\"369\":1}}],[\"16\",{\"1\":{\"149\":2,\"286\":1,\"416\":1,\"474\":1}}],[\"102\",{\"1\":{\"419\":2,\"420\":2}}],[\"100k+\",{\"1\":{\"337\":1}}],[\"100\",{\"1\":{\"209\":1,\"313\":1,\"377\":1,\"409\":2}}],[\"10017\",{\"1\":{\"74\":1}}],[\"109\",{\"1\":{\"159\":1}}],[\"10\",{\"0\":{\"109\":1,\"320\":1},\"1\":{\"110\":1,\"172\":1,\"197\":1,\"218\":1,\"246\":1,\"304\":1,\"309\":1,\"321\":1,\"369\":1,\"370\":1,\"386\":2,\"419\":1,\"420\":1,\"458\":2}}],[\"18年大三实习的时候\",{\"1\":{\"534\":1}}],[\"18120v1\",{\"1\":{\"494\":1}}],[\"18173v1\",{\"1\":{\"492\":1}}],[\"18249v1\",{\"1\":{\"491\":1}}],[\"18327v1\",{\"1\":{\"490\":1}}],[\"18349v1\",{\"1\":{\"489\":1}}],[\"18721v1\",{\"1\":{\"486\":1}}],[\"18797v1\",{\"1\":{\"306\":1}}],[\"18647v1\",{\"1\":{\"487\":1}}],[\"18668v1\",{\"1\":{\"309\":1}}],[\"18679v1\",{\"1\":{\"308\":1}}],[\"18692v1\",{\"1\":{\"307\":1}}],[\"18403v1\",{\"1\":{\"488\":1}}],[\"184\",{\"1\":{\"159\":1}}],[\"18\",{\"1\":{\"105\":1,\"118\":1,\"126\":1,\"159\":1,\"386\":1}}],[\"1896年8月9日\",{\"1\":{\"79\":1}}],[\"157\",{\"1\":{\"350\":1}}],[\"15\",{\"1\":{\"108\":1,\"148\":1,\"164\":1,\"166\":1,\"431\":1,\"441\":1,\"460\":1}}],[\"156\",{\"1\":{\"102\":1}}],[\"150\",{\"1\":{\"49\":1}}],[\"150多场空间活动\",{\"1\":{\"26\":1}}],[\"14\",{\"1\":{\"81\":1,\"99\":1,\"148\":1,\"319\":1,\"345\":1,\"488\":1}}],[\"13b\",{\"1\":{\"369\":2,\"378\":1,\"409\":1,\"427\":1,\"441\":1,\"471\":1,\"487\":1}}],[\"139\",{\"1\":{\"345\":1}}],[\"137\",{\"1\":{\"159\":1}}],[\"134\",{\"1\":{\"114\":1}}],[\"13\",{\"1\":{\"81\":2,\"148\":1,\"185\":1,\"304\":1,\"344\":1,\"393\":2,\"395\":1,\"411\":1,\"412\":1,\"455\":1,\"480\":1,\"495\":1}}],[\"19930v1\",{\"1\":{\"472\":1}}],[\"19962v1\",{\"1\":{\"471\":1}}],[\"19992v1\",{\"1\":{\"284\":1}}],[\"19k\",{\"1\":{\"350\":1}}],[\"192\",{\"1\":{\"333\":1}}],[\"19206v1\",{\"1\":{\"295\":1}}],[\"19021v1\",{\"1\":{\"485\":1}}],[\"19027v1\",{\"1\":{\"303\":1}}],[\"19094v1\",{\"1\":{\"483\":1}}],[\"19014v1\",{\"1\":{\"305\":1}}],[\"19019v1\",{\"1\":{\"304\":1}}],[\"19037v1\",{\"1\":{\"302\":1}}],[\"19040v1\",{\"1\":{\"301\":1}}],[\"19060v1\",{\"1\":{\"300\":1}}],[\"19085v1\",{\"1\":{\"298\":1}}],[\"19114v1\",{\"1\":{\"482\":1}}],[\"19153v1\",{\"1\":{\"297\":1}}],[\"19174v1\",{\"1\":{\"296\":1}}],[\"19302v1\",{\"1\":{\"481\":1}}],[\"19318v1\",{\"1\":{\"480\":1}}],[\"1931年\",{\"1\":{\"48\":1}}],[\"19347v1\",{\"1\":{\"479\":1}}],[\"19390v1\",{\"1\":{\"478\":1}}],[\"19339v1\",{\"1\":{\"294\":1}}],[\"1943\",{\"1\":{\"530\":1}}],[\"19436v1\",{\"1\":{\"293\":1}}],[\"19475v1\",{\"1\":{\"292\":1}}],[\"1946年\",{\"1\":{\"48\":1}}],[\"19506v1\",{\"1\":{\"291\":1,\"476\":1}}],[\"19560v1\",{\"1\":{\"290\":1}}],[\"1953年\",{\"1\":{\"48\":1}}],[\"19620v1\",{\"1\":{\"289\":1}}],[\"19735v1\",{\"1\":{\"477\":1}}],[\"19760v1\",{\"1\":{\"288\":1}}],[\"19763v1\",{\"1\":{\"287\":1}}],[\"1970s\",{\"1\":{\"265\":1}}],[\"19857v1\",{\"1\":{\"475\":1}}],[\"19876v1\",{\"1\":{\"286\":1,\"474\":1}}],[\"1980年9月16日\",{\"1\":{\"79\":1}}],[\"19\",{\"1\":{\"169\":1,\"257\":1,\"290\":1,\"482\":1,\"485\":1}}],[\"1\",{\"0\":{\"507\":1,\"514\":2,\"515\":1,\"516\":1},\"1\":{\"7\":1,\"74\":1,\"103\":1,\"104\":1,\"133\":2,\"148\":1,\"156\":1,\"187\":1,\"199\":1,\"210\":1,\"222\":1,\"231\":1,\"235\":1,\"248\":1,\"261\":1,\"269\":1,\"271\":1,\"284\":1,\"304\":1,\"308\":1,\"328\":1,\"331\":2,\"335\":2,\"345\":1,\"351\":1,\"353\":2,\"355\":1,\"362\":1,\"369\":2,\"377\":1,\"391\":1,\"393\":2,\"411\":2,\"415\":1,\"419\":1,\"420\":1,\"427\":1,\"438\":1,\"447\":1,\"453\":1,\"455\":1,\"457\":1,\"460\":2,\"462\":2,\"470\":1,\"478\":1,\"486\":1,\"487\":1,\"488\":2,\"521\":1}}],[\"11m\",{\"1\":{\"395\":1}}],[\"110\",{\"1\":{\"174\":1,\"371\":1}}],[\"11\",{\"0\":{\"95\":1,\"311\":1},\"1\":{\"7\":2,\"9\":2,\"98\":1,\"128\":1,\"184\":1,\"327\":1,\"380\":1}}],[\"124\",{\"1\":{\"352\":1}}],[\"12th\",{\"1\":{\"352\":1}}],[\"128k\",{\"1\":{\"337\":1}}],[\"12k\",{\"1\":{\"126\":1}}],[\"12\",{\"1\":{\"4\":1,\"9\":2,\"81\":1,\"106\":1,\"166\":1,\"179\":1,\"190\":1,\"207\":1,\"228\":1,\"248\":1,\"257\":1,\"295\":1,\"314\":1,\"391\":1,\"411\":1,\"486\":1,\"494\":1}}],[\"6️⃣在数据驱动设计中\",{\"1\":{\"527\":1}}],[\"63\",{\"1\":{\"492\":1}}],[\"64\",{\"1\":{\"486\":1}}],[\"69\",{\"1\":{\"372\":1}}],[\"6g\",{\"0\":{\"239\":1}}],[\"66\",{\"1\":{\"222\":1}}],[\"609\",{\"1\":{\"470\":1}}],[\"60\",{\"1\":{\"214\":1,\"265\":1}}],[\"600+\",{\"1\":{\"210\":1}}],[\"600\",{\"1\":{\"126\":1}}],[\"6个月\",{\"1\":{\"92\":1}}],[\"61\",{\"1\":{\"81\":1,\"488\":1}}],[\"6\",{\"0\":{\"388\":1},\"1\":{\"7\":4,\"166\":1,\"246\":1,\"257\":1,\"304\":1,\"325\":1,\"337\":1,\"376\":1,\"388\":2,\"411\":1,\"428\":1,\"455\":1,\"460\":1,\"462\":1,\"482\":1,\"485\":1,\"486\":1}}],[\"苏州一尚门诚品书店项目装置设计\",{\"1\":{\"6\":1}}],[\"济南万科山大路创新项目景观设计\",{\"1\":{\"6\":1}}],[\"立讯空港小镇项目公共艺术及导览系统设计\",{\"1\":{\"6\":1}}],[\"公共艺术设计\",{\"1\":{\"6\":1}}],[\"公共艺术设计师\",{\"1\":{\"6\":1}}],[\"景观设计师\",{\"1\":{\"6\":1}}],[\"景观艺术设计艺术学学士\",{\"1\":{\"2\":1}}],[\"悬亮子工作室\",{\"1\":{\"6\":1}}],[\"与更结构化的设计思维方法不同\",{\"1\":{\"533\":1}}],[\"与认知心理学知识的结合使得设计实践更加倾向于将问题归结于个体心理层面\",{\"1\":{\"522\":1}}],[\"与心理咨询机构\",{\"1\":{\"520\":1}}],[\"与工业4\",{\"1\":{\"519\":1}}],[\"与物理世界的界限模糊问题\",{\"1\":{\"519\":1}}],[\"与人的沟通和处理应急问题的能力\",{\"1\":{\"93\":1}}],[\"与其他创作者\",{\"1\":{\"40\":1}}],[\"与其他玩家互动\",{\"1\":{\"40\":1}}],[\"与实际产品的桥梁\",{\"1\":{\"40\":1}}],[\"与北美最大的市场\",{\"1\":{\"34\":1}}],[\"与艺术家en\",{\"1\":{\"27\":1}}],[\"与艺术家恩利合作\",{\"1\":{\"9\":1}}],[\"与真浪数字艺术\",{\"1\":{\"9\":1}}],[\"与客户的沟通\",{\"1\":{\"92\":1}}],[\"与客户\",{\"1\":{\"6\":1}}],[\"与中国艺人李俊毅合作\",{\"1\":{\"6\":1}}],[\"节目进行杂志数字艺术创作\",{\"1\":{\"6\":1}}],[\"为消费者创造更加丰富\",{\"1\":{\"531\":1}}],[\"为了确保公正性\",{\"1\":{\"532\":1}}],[\"为了降低成本\",{\"1\":{\"531\":1}}],[\"为了应对这些挑战\",{\"1\":{\"519\":1}}],[\"为企业提供更精准的产品推荐和营销方案\",{\"1\":{\"531\":1}}],[\"为企业决策提供有力支持\",{\"1\":{\"518\":1}}],[\"为企业解决问题\",{\"1\":{\"510\":1}}],[\"为用户提供即时且相关的信息\",{\"1\":{\"517\":1}}],[\"为何我们需要转向新的设计哲学与实践\",{\"0\":{\"522\":1},\"1\":{\"502\":1}}],[\"为此\",{\"1\":{\"90\":1}}],[\"为什幺做\",{\"1\":{\"78\":1}}],[\"为什么他们不懂\",{\"1\":{\"535\":1}}],[\"为什么会选择这类行业\",{\"1\":{\"534\":1}}],[\"为什么新加坡在pisa评估中表现出色\",{\"1\":{\"524\":1}}],[\"为什么我们需要转向新的设计理念和实践\",{\"1\":{\"522\":1}}],[\"为什么做这个项目\",{\"0\":{\"77\":1}}],[\"为什么不给这个城市的孩子和国内的孩子一样的游戏机会呢\",{\"1\":{\"48\":1}}],[\"为什么选择留学\",{\"0\":{\"506\":1},\"1\":{\"30\":1,\"503\":1}}],[\"为观众展现了一个内容丰富的幻想世界\",{\"1\":{\"43\":1}}],[\"为z世代用户构建一个开放的世界\",{\"1\":{\"39\":1}}],[\"为世界创造更多充满灵魂的乐土\",{\"1\":{\"22\":1}}],[\"为后续发展奠定了坚实基础\",{\"1\":{\"14\":1}}],[\"为\",{\"1\":{\"6\":1}}],[\"为杂志担任后期艺术指导\",{\"1\":{\"6\":1}}],[\"为几十场活动和公司首场百万元婚礼活动开发创意设计方案\",{\"1\":{\"6\":1}}],[\"杂志合作\",{\"0\":{\"27\":1}}],[\"杂志\",{\"1\":{\"6\":1}}],[\"获得几十个名誉博士\",{\"1\":{\"79\":1}}],[\"获得了芒果tv\",{\"1\":{\"23\":1}}],[\"获得了众多合作机会\",{\"1\":{\"14\":1}}],[\"获得\",{\"1\":{\"6\":1}}],[\"维护潜在客户关系\",{\"1\":{\"6\":1}}],[\"拓展客户群\",{\"1\":{\"6\":1}}],[\"活动现场总是会有意外\",{\"1\":{\"92\":1}}],[\"活动策划\",{\"1\":{\"20\":1}}],[\"活动策划和设计经历思考\",{\"1\":{\"15\":1}}],[\"活动策划师\",{\"1\":{\"6\":1}}],[\"活动与展会项目经验\",{\"1\":{\"15\":1}}],[\"活动\",{\"1\":{\"5\":1,\"91\":1}}],[\"婚礼设计\",{\"1\":{\"21\":1,\"91\":1}}],[\"婚礼策划\",{\"1\":{\"5\":1}}],[\"婚礼\",{\"1\":{\"5\":1}}],[\"空间音频信号会通知他们周围的兴趣点\",{\"1\":{\"512\":1}}],[\"空间结构等等\",{\"1\":{\"509\":1}}],[\"空间结构\",{\"1\":{\"92\":1}}],[\"空间关系和空间想象力\",{\"1\":{\"81\":1}}],[\"空间布局规划和销售报价\",{\"1\":{\"6\":1}}],[\"空间\",{\"1\":{\"5\":1,\"6\":1}}],[\"空间交互等方向\",{\"1\":{\"4\":1}}],[\"hl=auto\",{\"1\":{\"539\":1}}],[\"hdong920\",{\"1\":{\"427\":1}}],[\"hsin\",{\"1\":{\"394\":1}}],[\"hsien\",{\"1\":{\"376\":1}}],[\"hsu\",{\"1\":{\"355\":1}}],[\"hsuan\",{\"1\":{\"166\":1,\"419\":1,\"420\":1}}],[\"h\",{\"1\":{\"298\":1}}],[\"hmis\",{\"1\":{\"297\":3}}],[\"hmi\",{\"0\":{\"297\":1},\"1\":{\"297\":3}}],[\"hmd\",{\"1\":{\"203\":1}}],[\"hwan\",{\"1\":{\"274\":1}}],[\"hwaryoung\",{\"1\":{\"266\":1}}],[\"hcd的核心是理解人\",{\"1\":{\"522\":1}}],[\"hcd\",{\"1\":{\"522\":3}}],[\"hcpms\",{\"1\":{\"519\":1}}],[\"hcps\",{\"1\":{\"519\":1}}],[\"hcai\",{\"1\":{\"229\":4}}],[\"hci\",{\"0\":{\"94\":1,\"161\":1,\"286\":1,\"291\":1,\"354\":1,\"474\":1,\"476\":1,\"492\":1},\"1\":{\"4\":1,\"97\":3,\"136\":1,\"142\":1,\"149\":1,\"161\":1,\"214\":1,\"227\":2,\"271\":1,\"286\":3,\"296\":1,\"312\":3,\"354\":1,\"457\":1,\"474\":3,\"492\":2}}],[\"hnam\",{\"1\":{\"183\":4}}],[\"hrc\",{\"1\":{\"201\":1}}],[\"href\",{\"1\":{\"159\":2}}],[\"hrishav\",{\"1\":{\"137\":1}}],[\"hri\",{\"0\":{\"307\":1},\"1\":{\"107\":1,\"307\":5}}],[\"huo\",{\"1\":{\"445\":1}}],[\"huimin\",{\"1\":{\"411\":1}}],[\"huijie\",{\"1\":{\"350\":1}}],[\"hurts\",{\"1\":{\"378\":1}}],[\"hurting\",{\"1\":{\"308\":1}}],[\"hundreds\",{\"1\":{\"313\":1,\"454\":1}}],[\"hung\",{\"1\":{\"192\":1,\"355\":1}}],[\"huckins\",{\"1\":{\"268\":1,\"452\":1}}],[\"hutter\",{\"1\":{\"233\":1}}],[\"huth\",{\"1\":{\"152\":1}}],[\"hugo\",{\"1\":{\"218\":1}}],[\"hussein\",{\"1\":{\"215\":1}}],[\"hue\",{\"1\":{\"197\":8}}],[\"huwa\",{\"1\":{\"178\":1}}],[\"hu\",{\"1\":{\"167\":1,\"219\":1,\"223\":1,\"316\":1,\"353\":1,\"376\":1,\"401\":1,\"453\":1}}],[\"huan\",{\"1\":{\"313\":1,\"447\":1,\"479\":1}}],[\"huang\",{\"1\":{\"154\":1,\"156\":1,\"213\":1,\"219\":1,\"222\":1,\"235\":1,\"318\":1,\"326\":1,\"401\":1,\"403\":1,\"428\":1,\"437\":1}}],[\"huaishu\",{\"1\":{\"306\":1}}],[\"hua\",{\"1\":{\"142\":1,\"485\":1}}],[\"humanize\",{\"1\":{\"266\":1}}],[\"humane\",{\"0\":{\"266\":1}}],[\"humaneval\",{\"1\":{\"215\":1,\"342\":1,\"415\":1,\"437\":1,\"482\":1}}],[\"humanoids\",{\"1\":{\"179\":2}}],[\"humanoid\",{\"0\":{\"179\":1},\"1\":{\"179\":2,\"224\":1}}],[\"humans\",{\"0\":{\"159\":1},\"1\":{\"100\":2,\"107\":2,\"118\":1,\"125\":1,\"160\":1,\"198\":2,\"199\":1,\"215\":1,\"240\":1,\"244\":2,\"253\":1,\"263\":1,\"264\":1,\"275\":1,\"288\":1,\"300\":1,\"303\":1,\"326\":2,\"335\":1,\"387\":2,\"391\":1,\"395\":1,\"404\":1,\"448\":1,\"458\":1,\"470\":1,\"496\":3}}],[\"human\",{\"0\":{\"97\":1,\"102\":1,\"112\":1,\"116\":1,\"150\":1,\"159\":1,\"174\":1,\"198\":1,\"201\":1,\"218\":1,\"229\":1,\"244\":2,\"246\":1,\"261\":1,\"263\":2,\"264\":1,\"271\":1,\"294\":1,\"300\":1,\"305\":1,\"307\":1,\"312\":1,\"324\":1,\"371\":1,\"387\":1,\"431\":1,\"439\":1,\"446\":1,\"448\":1,\"457\":1},\"1\":{\"97\":4,\"98\":1,\"100\":4,\"102\":4,\"107\":3,\"112\":1,\"116\":8,\"118\":3,\"125\":2,\"130\":1,\"131\":1,\"132\":1,\"133\":3,\"136\":1,\"137\":2,\"140\":1,\"142\":1,\"159\":2,\"174\":4,\"184\":1,\"198\":2,\"201\":1,\"207\":1,\"211\":1,\"212\":2,\"213\":1,\"215\":3,\"218\":3,\"222\":2,\"223\":3,\"229\":8,\"231\":3,\"235\":3,\"238\":1,\"240\":2,\"243\":1,\"244\":3,\"245\":2,\"246\":2,\"251\":2,\"253\":2,\"256\":1,\"261\":1,\"263\":5,\"264\":2,\"266\":5,\"269\":5,\"271\":4,\"275\":1,\"283\":1,\"286\":2,\"289\":3,\"294\":4,\"297\":1,\"300\":3,\"303\":2,\"307\":6,\"312\":4,\"315\":1,\"324\":3,\"326\":3,\"328\":1,\"330\":1,\"331\":3,\"333\":2,\"334\":1,\"335\":3,\"344\":2,\"345\":1,\"353\":1,\"361\":1,\"364\":1,\"369\":2,\"371\":4,\"378\":2,\"380\":1,\"381\":2,\"387\":2,\"388\":1,\"404\":1,\"408\":1,\"410\":2,\"416\":2,\"417\":2,\"423\":2,\"431\":5,\"432\":1,\"439\":4,\"446\":1,\"447\":1,\"448\":2,\"453\":2,\"455\":1,\"457\":4,\"459\":1,\"463\":1,\"468\":3,\"470\":1,\"474\":2,\"475\":2,\"477\":1,\"483\":2,\"485\":2,\"486\":2,\"490\":2,\"491\":3,\"492\":1,\"496\":2,\"519\":2}}],[\"hyoje\",{\"1\":{\"356\":1}}],[\"hyemin\",{\"1\":{\"381\":1}}],[\"hyeram\",{\"1\":{\"356\":1}}],[\"hyeju\",{\"1\":{\"325\":1}}],[\"hyeon\",{\"1\":{\"143\":1}}],[\"hypothetical\",{\"0\":{\"340\":1},\"1\":{\"340\":1}}],[\"hypothesis\",{\"1\":{\"273\":1,\"385\":1}}],[\"hypothesize\",{\"1\":{\"220\":1,\"454\":1}}],[\"hypothesizing\",{\"1\":{\"203\":1}}],[\"hyperparameters\",{\"1\":{\"112\":1}}],[\"hyunyoung\",{\"1\":{\"191\":1}}],[\"hybrid\",{\"0\":{\"137\":1},\"1\":{\"216\":1,\"362\":2,\"384\":1,\"459\":1}}],[\"hieu\",{\"1\":{\"435\":1}}],[\"hier\",{\"1\":{\"281\":1}}],[\"hierarchies\",{\"0\":{\"398\":1},\"1\":{\"205\":1}}],[\"hierarchical\",{\"0\":{\"103\":1,\"183\":1},\"1\":{\"103\":1,\"141\":3,\"183\":1,\"479\":2}}],[\"hierarchy\",{\"1\":{\"183\":2}}],[\"hila\",{\"1\":{\"304\":1}}],[\"hilda\",{\"1\":{\"176\":1}}],[\"history\",{\"1\":{\"264\":1,\"448\":1,\"485\":1}}],[\"histories\",{\"1\":{\"117\":1}}],[\"historically\",{\"1\":{\"394\":1}}],[\"historical\",{\"1\":{\"106\":1,\"475\":1}}],[\"hiroki\",{\"1\":{\"459\":1}}],[\"hiroshi\",{\"1\":{\"314\":1}}],[\"hiromi\",{\"1\":{\"160\":1}}],[\"hiring\",{\"1\":{\"263\":1}}],[\"hipaa\",{\"1\":{\"254\":1}}],[\"hip\",{\"1\":{\"246\":1}}],[\"hiv\",{\"0\":{\"178\":1},\"1\":{\"178\":5}}],[\"hide\",{\"1\":{\"403\":1}}],[\"hideaki\",{\"1\":{\"165\":1}}],[\"hidden\",{\"1\":{\"107\":2,\"117\":1,\"181\":2,\"213\":1,\"456\":2}}],[\"hicks\",{\"1\":{\"146\":1}}],[\"hinrich\",{\"1\":{\"497\":1}}],[\"hinder\",{\"1\":{\"261\":1}}],[\"hinders\",{\"1\":{\"242\":1,\"341\":1}}],[\"hindering\",{\"1\":{\"99\":1,\"316\":1,\"463\":1}}],[\"hintdroid\",{\"1\":{\"219\":3,\"401\":3}}],[\"hint\",{\"0\":{\"169\":1,\"219\":1,\"401\":1},\"1\":{\"169\":3,\"219\":6,\"228\":3,\"251\":1,\"401\":6,\"423\":1}}],[\"hints\",{\"0\":{\"228\":1},\"1\":{\"102\":1,\"133\":1,\"169\":1,\"179\":1,\"228\":5,\"331\":1}}],[\"highest\",{\"1\":{\"226\":1,\"325\":1,\"351\":1,\"369\":1,\"458\":1}}],[\"higher\",{\"1\":{\"172\":1,\"195\":1,\"209\":1,\"221\":1,\"273\":1,\"278\":1,\"283\":1,\"302\":1,\"303\":1,\"304\":1,\"342\":1,\"370\":1,\"389\":1,\"414\":1,\"428\":1,\"432\":2,\"456\":1,\"460\":2,\"486\":2,\"488\":1}}],[\"highlighted\",{\"1\":{\"205\":1,\"258\":1,\"328\":1,\"358\":1,\"433\":1}}],[\"highlights\",{\"1\":{\"118\":1,\"141\":1,\"152\":1,\"189\":1,\"241\":1,\"251\":1,\"259\":1,\"345\":1,\"364\":1,\"423\":1,\"424\":1,\"467\":1,\"475\":1}}],[\"highlighting\",{\"1\":{\"116\":1,\"136\":1,\"200\":1,\"210\":1,\"280\":1,\"398\":1,\"429\":1,\"498\":1}}],[\"highlight\",{\"1\":{\"115\":1,\"126\":1,\"178\":1,\"204\":1,\"254\":1,\"258\":1,\"263\":1,\"290\":1,\"384\":1,\"433\":1,\"441\":1,\"491\":1}}],[\"highly\",{\"1\":{\"97\":1,\"122\":1,\"131\":1,\"181\":1,\"289\":1,\"312\":1,\"319\":1,\"328\":1,\"389\":1,\"427\":1,\"431\":1,\"453\":1,\"471\":1}}],[\"high\",{\"0\":{\"188\":2,\"301\":1,\"475\":1,\"496\":1},\"1\":{\"101\":1,\"103\":1,\"111\":1,\"114\":1,\"120\":1,\"124\":1,\"133\":1,\"153\":1,\"165\":1,\"171\":1,\"176\":1,\"184\":2,\"188\":1,\"189\":3,\"201\":1,\"219\":1,\"223\":1,\"224\":4,\"227\":1,\"228\":1,\"237\":1,\"260\":1,\"278\":1,\"281\":2,\"284\":1,\"289\":1,\"298\":1,\"301\":1,\"306\":2,\"331\":1,\"334\":1,\"340\":1,\"344\":1,\"349\":1,\"356\":1,\"365\":1,\"369\":2,\"378\":1,\"380\":2,\"381\":2,\"382\":1,\"389\":1,\"399\":1,\"401\":1,\"407\":1,\"411\":1,\"416\":1,\"418\":1,\"432\":1,\"436\":1,\"443\":2,\"445\":1,\"447\":1,\"459\":2,\"462\":1,\"465\":1,\"475\":6,\"479\":1,\"482\":1,\"487\":1,\"496\":4}}],[\"highstreet\",{\"1\":{\"42\":1}}],[\"heinbach\",{\"1\":{\"400\":1}}],[\"heightened\",{\"0\":{\"302\":1},\"1\":{\"191\":1}}],[\"heightens\",{\"1\":{\"124\":1}}],[\"height\",{\"1\":{\"105\":1,\"246\":2}}],[\"hetailang\",{\"1\":{\"366\":1}}],[\"heterogeneity\",{\"1\":{\"233\":1}}],[\"heterogeneous\",{\"1\":{\"190\":1,\"233\":1,\"253\":1}}],[\"heejung\",{\"1\":{\"356\":1}}],[\"hepburn\",{\"1\":{\"294\":1}}],[\"he\",{\"1\":{\"235\":1,\"260\":1,\"275\":2,\"316\":1,\"364\":1,\"424\":1,\"428\":1,\"438\":1,\"449\":1,\"479\":1,\"491\":1}}],[\"hedge\",{\"1\":{\"226\":1}}],[\"hedging\",{\"1\":{\"135\":1}}],[\"helmet\",{\"1\":{\"298\":1}}],[\"helmsman\",{\"0\":{\"245\":1}}],[\"helbling\",{\"1\":{\"258\":1,\"375\":1,\"433\":1}}],[\"hellaoui\",{\"1\":{\"239\":1}}],[\"helene\",{\"1\":{\"293\":1}}],[\"helen\",{\"1\":{\"217\":1,\"302\":1}}],[\"held\",{\"1\":{\"164\":1,\"180\":1,\"478\":1}}],[\"helped\",{\"1\":{\"248\":1}}],[\"helper\",{\"1\":{\"181\":1}}],[\"helpless\",{\"1\":{\"228\":1}}],[\"helping\",{\"1\":{\"228\":1,\"284\":1,\"303\":1,\"317\":1}}],[\"helps\",{\"0\":{\"241\":1},\"1\":{\"156\":1,\"175\":1,\"291\":1,\"317\":1,\"376\":1,\"476\":1}}],[\"helpfulness\",{\"1\":{\"215\":1}}],[\"helpful\",{\"1\":{\"102\":2,\"453\":1}}],[\"help\",{\"0\":{\"181\":1,\"209\":1,\"303\":1,\"435\":1},\"1\":{\"97\":1,\"104\":1,\"111\":1,\"117\":1,\"124\":1,\"127\":1,\"131\":1,\"170\":1,\"175\":1,\"179\":1,\"181\":2,\"210\":1,\"217\":1,\"219\":2,\"228\":2,\"236\":1,\"243\":1,\"249\":1,\"254\":1,\"257\":1,\"263\":1,\"270\":1,\"303\":5,\"312\":1,\"344\":1,\"362\":1,\"386\":1,\"401\":2,\"402\":1,\"443\":1}}],[\"hensman\",{\"1\":{\"456\":1}}],[\"henny\",{\"1\":{\"184\":1,\"380\":1}}],[\"hence\",{\"1\":{\"137\":1,\"149\":1,\"194\":1,\"217\":1,\"417\":1,\"434\":1,\"470\":1,\"486\":1}}],[\"heng\",{\"1\":{\"128\":1,\"270\":1}}],[\"heavily\",{\"1\":{\"418\":1}}],[\"heavier\",{\"1\":{\"133\":1,\"331\":1}}],[\"heat\",{\"0\":{\"223\":1},\"1\":{\"223\":1}}],[\"hearst\",{\"1\":{\"280\":1}}],[\"hear\",{\"1\":{\"173\":1}}],[\"hearted\",{\"1\":{\"200\":1}}],[\"hearts\",{\"0\":{\"200\":1},\"1\":{\"200\":4}}],[\"heart\",{\"1\":{\"120\":1,\"200\":1}}],[\"headset\",{\"1\":{\"305\":1}}],[\"heading\",{\"1\":{\"158\":1}}],[\"head\",{\"1\":{\"139\":1,\"295\":1}}],[\"healthcare\",{\"0\":{\"171\":1},\"1\":{\"171\":2,\"226\":1,\"253\":3,\"356\":3,\"379\":3,\"491\":1}}],[\"healthier\",{\"1\":{\"155\":1}}],[\"health\",{\"0\":{\"178\":1,\"211\":1,\"253\":1},\"1\":{\"105\":1,\"178\":2,\"211\":3,\"237\":1,\"261\":2}}],[\"hershcovich\",{\"1\":{\"328\":1}}],[\"hernandez\",{\"1\":{\"293\":1,\"419\":1,\"420\":1}}],[\"hernández\",{\"1\":{\"147\":1}}],[\"hero\",{\"1\":{\"274\":1}}],[\"herzog\",{\"1\":{\"253\":2}}],[\"herreros\",{\"1\":{\"126\":1}}],[\"her\",{\"1\":{\"124\":1,\"395\":2}}],[\"heritage\",{\"0\":{\"117\":2},\"1\":{\"117\":5}}],[\"here\",{\"1\":{\"112\":1,\"156\":1,\"159\":1,\"176\":1,\"181\":1,\"241\":1,\"284\":1,\"393\":1}}],[\"heuristics\",{\"1\":{\"251\":1,\"257\":1,\"423\":1}}],[\"heuristic\",{\"0\":{\"251\":1,\"423\":1},\"1\":{\"96\":1,\"197\":2,\"251\":6,\"257\":1,\"423\":6,\"494\":1}}],[\"hacene\",{\"1\":{\"444\":1}}],[\"hacktivist\",{\"1\":{\"416\":1}}],[\"habib\",{\"1\":{\"369\":1}}],[\"hak\",{\"1\":{\"356\":1}}],[\"hahn\",{\"1\":{\"274\":1}}],[\"happens\",{\"1\":{\"441\":1,\"469\":1}}],[\"happening\",{\"1\":{\"302\":1}}],[\"happened\",{\"1\":{\"271\":1,\"457\":1}}],[\"haptic\",{\"0\":{\"220\":1},\"1\":{\"220\":1}}],[\"haghighi\",{\"1\":{\"254\":1}}],[\"hate\",{\"0\":{\"460\":1},\"1\":{\"241\":1,\"302\":1,\"460\":2}}],[\"hammond\",{\"1\":{\"363\":1}}],[\"hamed\",{\"1\":{\"239\":1}}],[\"hamidouche\",{\"1\":{\"239\":1}}],[\"hamza\",{\"1\":{\"176\":1}}],[\"hadoop\",{\"1\":{\"445\":1}}],[\"had\",{\"1\":{\"209\":1,\"263\":2,\"468\":1,\"497\":1}}],[\"hailin\",{\"1\":{\"443\":1}}],[\"haim\",{\"1\":{\"305\":1}}],[\"haiming\",{\"1\":{\"138\":1}}],[\"hai\",{\"1\":{\"244\":4}}],[\"haii\",{\"1\":{\"174\":1,\"371\":1}}],[\"hartmann\",{\"1\":{\"435\":1}}],[\"harteveld\",{\"1\":{\"98\":1}}],[\"harnesses\",{\"1\":{\"477\":1}}],[\"harness\",{\"1\":{\"434\":1,\"438\":1,\"475\":1}}],[\"harnessing\",{\"0\":{\"184\":1,\"375\":1,\"380\":1,\"475\":1},\"1\":{\"359\":1}}],[\"harold\",{\"1\":{\"382\":1}}],[\"harshangi\",{\"1\":{\"374\":1}}],[\"harris\",{\"1\":{\"398\":1}}],[\"harrison\",{\"1\":{\"269\":1}}],[\"harry\",{\"1\":{\"252\":1,\"398\":1,\"426\":1,\"427\":1}}],[\"haruki\",{\"1\":{\"264\":1,\"448\":1}}],[\"harland\",{\"1\":{\"221\":1}}],[\"harmless\",{\"1\":{\"453\":1}}],[\"harmbench\",{\"1\":{\"409\":1}}],[\"harmeling\",{\"1\":{\"400\":1}}],[\"harmful\",{\"1\":{\"241\":2,\"302\":1,\"313\":4,\"334\":6,\"447\":1}}],[\"harm\",{\"1\":{\"211\":1,\"302\":2,\"395\":1}}],[\"harms\",{\"0\":{\"302\":1},\"1\":{\"155\":1,\"302\":5,\"418\":1}}],[\"hardly\",{\"1\":{\"437\":1}}],[\"harder\",{\"0\":{\"229\":1,\"327\":1},\"1\":{\"443\":2}}],[\"hard\",{\"0\":{\"360\":1},\"1\":{\"190\":1,\"209\":1,\"340\":1,\"360\":3,\"377\":1,\"416\":1}}],[\"hardware\",{\"1\":{\"140\":1,\"210\":2,\"224\":1,\"295\":1,\"363\":1,\"469\":2}}],[\"haoxuan\",{\"1\":{\"450\":1}}],[\"haoming\",{\"1\":{\"417\":1}}],[\"haojie\",{\"1\":{\"414\":1}}],[\"haoran\",{\"1\":{\"391\":1}}],[\"haodi\",{\"1\":{\"353\":1}}],[\"haodong\",{\"1\":{\"337\":1}}],[\"haoyu\",{\"1\":{\"297\":1}}],[\"haoyang\",{\"1\":{\"283\":1}}],[\"haofengl\",{\"1\":{\"260\":1}}],[\"haofeng\",{\"1\":{\"260\":1}}],[\"haotian\",{\"1\":{\"151\":1,\"348\":1,\"353\":1}}],[\"hao\",{\"1\":{\"131\":1,\"326\":1,\"353\":1,\"411\":1,\"414\":1,\"472\":1}}],[\"haley\",{\"1\":{\"428\":1}}],[\"hallmark\",{\"1\":{\"497\":1}}],[\"hallucode\",{\"1\":{\"437\":2}}],[\"hallucinating\",{\"1\":{\"252\":1,\"426\":1}}],[\"hallucinations\",{\"0\":{\"198\":1,\"387\":1,\"437\":1},\"1\":{\"198\":3,\"325\":2,\"329\":1,\"343\":1,\"369\":1,\"387\":3,\"437\":9,\"463\":2,\"489\":2}}],[\"hallucination\",{\"0\":{\"325\":1,\"388\":1},\"1\":{\"114\":1,\"198\":6,\"319\":1,\"369\":3,\"387\":6,\"388\":1,\"437\":3,\"471\":1,\"483\":1,\"489\":1}}],[\"hall\",{\"1\":{\"289\":1}}],[\"half\",{\"1\":{\"74\":1,\"181\":1,\"304\":2}}],[\"haeseong\",{\"1\":{\"274\":1}}],[\"haeseung\",{\"1\":{\"198\":1,\"387\":1}}],[\"haeri\",{\"1\":{\"176\":1}}],[\"hae\",{\"0\":{\"112\":1},\"1\":{\"112\":1}}],[\"having\",{\"1\":{\"105\":1,\"112\":1,\"189\":1,\"275\":1,\"297\":1,\"369\":1}}],[\"have\",{\"0\":{\"345\":1},\"1\":{\"96\":1,\"98\":1,\"101\":2,\"105\":1,\"110\":1,\"116\":1,\"119\":1,\"123\":2,\"128\":1,\"132\":1,\"136\":1,\"138\":1,\"139\":1,\"142\":1,\"151\":1,\"153\":1,\"155\":1,\"156\":2,\"159\":1,\"163\":1,\"164\":1,\"165\":3,\"174\":1,\"179\":1,\"184\":1,\"191\":1,\"194\":1,\"195\":1,\"196\":1,\"198\":1,\"200\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"210\":1,\"211\":1,\"213\":1,\"216\":1,\"219\":1,\"220\":2,\"221\":1,\"224\":1,\"228\":1,\"243\":2,\"245\":2,\"249\":1,\"250\":2,\"251\":1,\"256\":2,\"257\":1,\"258\":2,\"261\":2,\"263\":1,\"265\":4,\"266\":1,\"269\":1,\"271\":1,\"275\":3,\"282\":2,\"284\":1,\"289\":1,\"292\":1,\"298\":1,\"307\":1,\"308\":1,\"315\":2,\"317\":2,\"318\":1,\"321\":1,\"322\":1,\"325\":1,\"328\":1,\"330\":1,\"334\":2,\"335\":1,\"338\":1,\"339\":1,\"342\":1,\"345\":1,\"347\":2,\"348\":1,\"355\":1,\"360\":1,\"363\":1,\"371\":1,\"372\":1,\"374\":2,\"375\":1,\"379\":1,\"380\":1,\"386\":1,\"387\":1,\"392\":1,\"393\":1,\"394\":2,\"400\":1,\"401\":1,\"402\":1,\"409\":1,\"410\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"417\":3,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"425\":1,\"427\":1,\"428\":1,\"431\":1,\"432\":1,\"433\":2,\"442\":1,\"443\":2,\"446\":1,\"447\":2,\"449\":1,\"450\":2,\"457\":1,\"458\":1,\"461\":1,\"463\":1,\"467\":1,\"469\":1,\"470\":2,\"471\":2,\"479\":1,\"480\":2,\"482\":2,\"483\":1,\"485\":1,\"486\":1,\"487\":2,\"491\":1,\"495\":1}}],[\"hanxu\",{\"1\":{\"483\":1}}],[\"hancheng\",{\"1\":{\"428\":1}}],[\"hancock\",{\"1\":{\"102\":1}}],[\"hangfeng\",{\"1\":{\"424\":1}}],[\"hanghao\",{\"1\":{\"316\":1}}],[\"hanbin\",{\"1\":{\"411\":1}}],[\"hanieh\",{\"1\":{\"376\":1}}],[\"hanzhong\",{\"1\":{\"283\":1}}],[\"hannah\",{\"1\":{\"273\":1}}],[\"hannock\",{\"1\":{\"178\":1}}],[\"hanjie\",{\"1\":{\"251\":1,\"423\":1}}],[\"handcraft\",{\"1\":{\"483\":1}}],[\"handcrafted\",{\"1\":{\"223\":1,\"482\":1}}],[\"handling\",{\"1\":{\"359\":1,\"389\":1,\"412\":1,\"413\":1,\"424\":1,\"425\":1,\"432\":1,\"454\":1,\"467\":1,\"475\":1,\"480\":2}}],[\"handles\",{\"1\":{\"415\":1}}],[\"handle\",{\"1\":{\"172\":1,\"306\":1,\"337\":1,\"340\":1,\"370\":1,\"394\":1,\"438\":1,\"475\":1}}],[\"hands\",{\"1\":{\"140\":1,\"220\":1,\"243\":1,\"307\":1}}],[\"hand\",{\"0\":{\"191\":1,\"203\":1},\"1\":{\"126\":1,\"156\":1,\"191\":5,\"203\":3,\"284\":1,\"318\":1,\"361\":2,\"390\":1,\"392\":1,\"418\":1,\"441\":1,\"470\":1,\"471\":2,\"490\":1}}],[\"han\",{\"1\":{\"105\":1,\"164\":1,\"176\":1,\"316\":1,\"329\":1,\"356\":1,\"399\":1,\"453\":1,\"483\":1}}],[\"hasuoshenyun\",{\"1\":{\"487\":1}}],[\"hassid\",{\"1\":{\"441\":1}}],[\"hasan\",{\"1\":{\"298\":2}}],[\"has\",{\"1\":{\"102\":2,\"103\":1,\"106\":1,\"112\":1,\"113\":1,\"119\":1,\"120\":1,\"125\":1,\"127\":1,\"138\":1,\"139\":1,\"146\":2,\"153\":1,\"167\":1,\"169\":1,\"174\":1,\"178\":1,\"181\":1,\"182\":1,\"194\":1,\"196\":1,\"200\":1,\"204\":1,\"205\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":1,\"221\":1,\"226\":1,\"229\":1,\"231\":1,\"233\":1,\"239\":1,\"242\":1,\"243\":3,\"245\":1,\"249\":1,\"253\":1,\"255\":1,\"257\":3,\"265\":1,\"270\":1,\"271\":1,\"272\":1,\"274\":2,\"279\":1,\"292\":2,\"293\":1,\"294\":1,\"298\":1,\"300\":1,\"314\":1,\"315\":1,\"317\":1,\"319\":1,\"329\":1,\"337\":2,\"341\":1,\"350\":1,\"359\":1,\"362\":1,\"366\":1,\"369\":1,\"371\":1,\"378\":1,\"379\":2,\"382\":2,\"386\":1,\"389\":1,\"391\":1,\"392\":1,\"394\":1,\"400\":2,\"412\":1,\"428\":1,\"432\":1,\"434\":1,\"435\":1,\"437\":1,\"439\":1,\"442\":1,\"450\":1,\"456\":1,\"457\":1,\"459\":1,\"460\":2,\"461\":1,\"465\":1,\"468\":1,\"469\":2,\"472\":3,\"479\":1,\"486\":1,\"489\":1,\"490\":1,\"491\":1,\"496\":1,\"497\":1}}],[\"https\",{\"1\":{\"123\":1,\"137\":1,\"159\":1,\"184\":1,\"219\":1,\"235\":1,\"249\":1,\"250\":1,\"258\":2,\"259\":1,\"260\":1,\"316\":1,\"326\":1,\"337\":1,\"338\":1,\"366\":1,\"375\":1,\"379\":1,\"380\":1,\"381\":1,\"391\":1,\"393\":1,\"398\":2,\"401\":1,\"409\":1,\"427\":1,\"432\":1,\"433\":2,\"443\":1,\"456\":1,\"458\":1,\"482\":1,\"485\":1,\"487\":1,\"494\":1,\"539\":1}}],[\"http\",{\"1\":{\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":2,\"105\":1,\"106\":1,\"107\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"136\":1,\"137\":1,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"146\":1,\"147\":1,\"148\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"158\":1,\"159\":2,\"160\":1,\"161\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"173\":1,\"174\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1,\"184\":1,\"185\":1,\"186\":1,\"187\":1,\"188\":1,\"189\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"198\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":1,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"226\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"237\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"252\":1,\"253\":1,\"254\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":1,\"259\":1,\"260\":1,\"261\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"269\":1,\"270\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"278\":1,\"279\":1,\"280\":1,\"281\":1,\"282\":1,\"283\":1,\"284\":1,\"286\":1,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":1,\"295\":1,\"296\":1,\"297\":1,\"298\":1,\"300\":1,\"301\":1,\"302\":1,\"303\":1,\"304\":1,\"305\":1,\"306\":1,\"307\":1,\"308\":1,\"309\":1,\"312\":1,\"313\":1,\"314\":1,\"315\":1,\"316\":2,\"317\":1,\"318\":1,\"319\":1,\"321\":1,\"322\":1,\"323\":1,\"324\":1,\"325\":1,\"326\":1,\"327\":1,\"328\":1,\"329\":1,\"330\":1,\"331\":1,\"333\":1,\"334\":1,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"339\":1,\"340\":1,\"341\":1,\"342\":1,\"343\":1,\"344\":1,\"345\":1,\"347\":1,\"348\":1,\"349\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":1,\"354\":1,\"355\":1,\"356\":1,\"358\":1,\"359\":1,\"360\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"365\":1,\"366\":1,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"372\":1,\"374\":1,\"375\":1,\"376\":1,\"377\":1,\"378\":1,\"379\":1,\"380\":1,\"381\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"390\":1,\"391\":1,\"392\":1,\"393\":1,\"394\":1,\"395\":1,\"396\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"408\":1,\"409\":1,\"410\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"416\":1,\"417\":1,\"418\":1,\"419\":1,\"420\":1,\"422\":1,\"423\":1,\"424\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":1,\"430\":1,\"431\":1,\"432\":1,\"433\":1,\"434\":1,\"435\":1,\"436\":1,\"437\":1,\"438\":1,\"439\":1,\"441\":1,\"442\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"447\":1,\"448\":1,\"449\":1,\"450\":1,\"452\":1,\"453\":1,\"454\":1,\"455\":1,\"456\":1,\"457\":1,\"458\":1,\"459\":1,\"460\":1,\"461\":1,\"462\":1,\"463\":1,\"465\":1,\"466\":1,\"467\":1,\"468\":1,\"469\":1,\"470\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"485\":1,\"486\":1,\"487\":1,\"488\":1,\"489\":1,\"490\":1,\"491\":1,\"492\":1,\"494\":1,\"495\":1,\"496\":1,\"497\":1,\"498\":1}}],[\"htc\",{\"1\":{\"81\":2}}],[\"hogan\",{\"1\":{\"468\":1}}],[\"hoggenmüller\",{\"1\":{\"297\":1}}],[\"hoefler\",{\"1\":{\"456\":1}}],[\"hoping\",{\"1\":{\"445\":1}}],[\"hop\",{\"1\":{\"390\":1}}],[\"hope\",{\"1\":{\"175\":1,\"243\":1,\"338\":1,\"385\":1}}],[\"hossein\",{\"1\":{\"333\":1,\"438\":1}}],[\"hosting\",{\"1\":{\"233\":1}}],[\"host\",{\"1\":{\"105\":1,\"220\":1}}],[\"hoblitzell\",{\"1\":{\"325\":1}}],[\"hot\",{\"1\":{\"315\":1}}],[\"horie\",{\"1\":{\"459\":1}}],[\"horizons\",{\"1\":{\"377\":1}}],[\"horizontal\",{\"1\":{\"151\":1,\"348\":1}}],[\"horng\",{\"1\":{\"258\":1,\"433\":1}}],[\"hoque\",{\"1\":{\"229\":1}}],[\"hofstede\",{\"1\":{\"221\":1}}],[\"holson\",{\"1\":{\"269\":1}}],[\"hold\",{\"1\":{\"376\":1,\"450\":1}}],[\"holden\",{\"1\":{\"268\":1,\"452\":1}}],[\"holds\",{\"1\":{\"215\":1,\"217\":1,\"256\":1,\"324\":1,\"392\":1}}],[\"holistic\",{\"0\":{\"297\":1},\"1\":{\"212\":1,\"216\":1,\"254\":1,\"293\":1,\"297\":3}}],[\"holiday\",{\"1\":{\"183\":1}}],[\"honest\",{\"1\":{\"453\":1}}],[\"honing\",{\"1\":{\"316\":1}}],[\"hongshen\",{\"1\":{\"489\":1}}],[\"honglei\",{\"1\":{\"430\":1}}],[\"hongyu\",{\"1\":{\"329\":1}}],[\"hongyan\",{\"1\":{\"176\":1,\"403\":1}}],[\"hongbo\",{\"1\":{\"295\":1}}],[\"hongchan\",{\"1\":{\"287\":1}}],[\"hong\",{\"1\":{\"286\":1,\"290\":1,\"474\":1}}],[\"honours\",{\"0\":{\"7\":1}}],[\"hoc\",{\"1\":{\"159\":1,\"282\":1,\"382\":1}}],[\"homomorphic\",{\"1\":{\"139\":1}}],[\"homei\",{\"1\":{\"264\":1,\"448\":1}}],[\"homeowners\",{\"1\":{\"105\":1}}],[\"homes\",{\"1\":{\"105\":2}}],[\"home\",{\"0\":{\"0\":1},\"1\":{\"74\":1,\"261\":1}}],[\"houkun\",{\"1\":{\"437\":1}}],[\"housings\",{\"0\":{\"306\":1},\"1\":{\"306\":1}}],[\"household\",{\"1\":{\"118\":1}}],[\"hou\",{\"1\":{\"228\":1,\"264\":1,\"448\":1,\"483\":1}}],[\"hour\",{\"1\":{\"133\":1,\"238\":1,\"331\":1}}],[\"hohman\",{\"1\":{\"110\":1,\"210\":1,\"321\":1}}],[\"howto100m\",{\"1\":{\"376\":1}}],[\"howell\",{\"1\":{\"248\":1}}],[\"however\",{\"1\":{\"98\":1,\"99\":1,\"100\":1,\"102\":1,\"103\":1,\"110\":1,\"111\":1,\"112\":1,\"114\":1,\"116\":1,\"119\":1,\"122\":1,\"123\":1,\"124\":1,\"128\":1,\"130\":1,\"131\":1,\"132\":1,\"133\":1,\"138\":1,\"140\":1,\"142\":1,\"146\":1,\"153\":1,\"158\":1,\"161\":1,\"163\":1,\"165\":1,\"166\":1,\"170\":1,\"174\":1,\"176\":1,\"180\":1,\"182\":1,\"187\":1,\"190\":1,\"191\":1,\"195\":1,\"196\":1,\"197\":1,\"200\":1,\"203\":1,\"204\":1,\"206\":1,\"207\":1,\"209\":1,\"210\":1,\"214\":1,\"215\":1,\"216\":1,\"221\":1,\"222\":1,\"224\":1,\"228\":1,\"231\":1,\"234\":1,\"236\":1,\"242\":1,\"244\":1,\"245\":1,\"252\":1,\"257\":1,\"261\":2,\"265\":1,\"271\":1,\"274\":1,\"282\":1,\"286\":1,\"289\":1,\"290\":1,\"291\":1,\"308\":1,\"309\":1,\"314\":1,\"316\":1,\"317\":2,\"318\":1,\"319\":1,\"321\":1,\"329\":1,\"330\":1,\"331\":1,\"338\":1,\"339\":1,\"354\":1,\"356\":1,\"362\":1,\"365\":1,\"369\":1,\"371\":1,\"374\":1,\"375\":1,\"376\":1,\"378\":1,\"379\":1,\"381\":1,\"390\":1,\"391\":1,\"394\":1,\"399\":1,\"403\":1,\"410\":1,\"412\":2,\"414\":1,\"415\":1,\"417\":1,\"418\":1,\"425\":1,\"426\":1,\"427\":1,\"428\":1,\"430\":1,\"431\":1,\"432\":1,\"434\":1,\"435\":1,\"441\":1,\"442\":1,\"445\":1,\"447\":1,\"457\":1,\"460\":2,\"461\":1,\"462\":1,\"463\":1,\"467\":1,\"471\":1,\"472\":1,\"474\":1,\"475\":1,\"476\":1,\"479\":1,\"482\":1,\"485\":1,\"486\":2,\"494\":1,\"497\":1}}],[\"howard\",{\"1\":{\"155\":1,\"164\":1}}],[\"how\",{\"0\":{\"121\":1,\"135\":1,\"156\":1,\"198\":1,\"214\":1,\"228\":1,\"261\":1,\"317\":1,\"345\":1,\"387\":1,\"443\":1},\"1\":{\"96\":2,\"97\":1,\"98\":1,\"101\":5,\"104\":1,\"106\":1,\"107\":2,\"108\":1,\"111\":1,\"122\":2,\"124\":1,\"135\":2,\"150\":1,\"152\":1,\"156\":7,\"159\":1,\"164\":2,\"171\":2,\"172\":1,\"175\":1,\"180\":1,\"182\":1,\"187\":1,\"191\":1,\"195\":1,\"197\":1,\"199\":1,\"202\":1,\"205\":1,\"207\":1,\"211\":1,\"212\":1,\"214\":1,\"217\":1,\"218\":1,\"226\":3,\"228\":1,\"229\":2,\"231\":3,\"233\":1,\"243\":2,\"244\":1,\"248\":3,\"250\":1,\"251\":1,\"252\":1,\"263\":1,\"264\":1,\"265\":1,\"269\":1,\"270\":1,\"271\":1,\"279\":1,\"281\":1,\"292\":1,\"293\":4,\"296\":2,\"308\":4,\"312\":1,\"319\":1,\"326\":1,\"328\":1,\"343\":1,\"344\":1,\"350\":1,\"355\":1,\"360\":1,\"361\":1,\"363\":1,\"368\":2,\"370\":1,\"389\":1,\"394\":1,\"399\":1,\"404\":1,\"409\":3,\"410\":1,\"423\":1,\"426\":1,\"428\":1,\"431\":1,\"434\":1,\"443\":1,\"447\":1,\"448\":1,\"454\":1,\"457\":1,\"467\":1}}],[\"i=创新\",{\"1\":{\"510\":1}}],[\"izrailevitch\",{\"1\":{\"497\":1}}],[\"ikat\",{\"1\":{\"481\":2}}],[\"ikeda\",{\"1\":{\"459\":1}}],[\"ikechukwu\",{\"1\":{\"384\":1}}],[\"iker\",{\"1\":{\"315\":2}}],[\"ie\",{\"1\":{\"455\":9}}],[\"ieee\",{\"0\":{\"217\":1}}],[\"iro\",{\"1\":{\"399\":1}}],[\"ir\",{\"1\":{\"386\":1,\"481\":1}}],[\"irrelevant\",{\"1\":{\"318\":1}}],[\"irreplaceable\",{\"1\":{\"227\":1}}],[\"icl\",{\"1\":{\"317\":5,\"405\":2}}],[\"icu\",{\"1\":{\"309\":2}}],[\"icus\",{\"1\":{\"309\":1}}],[\"icon\",{\"1\":{\"151\":1,\"348\":1}}],[\"icons\",{\"1\":{\"151\":1,\"152\":1,\"348\":1}}],[\"iitk\",{\"0\":{\"372\":1}}],[\"iii\",{\"1\":{\"333\":1}}],[\"ii\",{\"1\":{\"281\":1,\"333\":1,\"338\":1}}],[\"ilya\",{\"1\":{\"469\":1}}],[\"iliev\",{\"1\":{\"468\":1}}],[\"ilan\",{\"1\":{\"272\":1}}],[\"illegal\",{\"1\":{\"403\":1,\"418\":1}}],[\"illuminates\",{\"1\":{\"459\":1}}],[\"illuminate\",{\"1\":{\"356\":1}}],[\"illustrates\",{\"1\":{\"377\":1}}],[\"illustrated\",{\"1\":{\"257\":1}}],[\"illustrate\",{\"1\":{\"141\":1,\"281\":1,\"306\":1}}],[\"illnesses\",{\"1\":{\"181\":1}}],[\"ill\",{\"1\":{\"105\":1}}],[\"iv\",{\"1\":{\"333\":1,\"379\":1}}],[\"ivan\",{\"1\":{\"259\":1}}],[\"ively\",{\"1\":{\"389\":1}}],[\"ive\",{\"1\":{\"165\":3}}],[\"ives\",{\"1\":{\"165\":5}}],[\"idgenrec\",{\"1\":{\"485\":1}}],[\"idgen\",{\"1\":{\"485\":2}}],[\"id\",{\"0\":{\"485\":1},\"1\":{\"485\":3}}],[\"idf\",{\"1\":{\"365\":1}}],[\"idle\",{\"1\":{\"284\":1}}],[\"idiom\",{\"1\":{\"249\":1}}],[\"ideation\",{\"1\":{\"286\":1,\"474\":1}}],[\"ideators\",{\"0\":{\"233\":1},\"1\":{\"233\":8}}],[\"idealized\",{\"1\":{\"238\":1}}],[\"ideal\",{\"1\":{\"234\":1,\"260\":1,\"295\":1}}],[\"idea\",{\"1\":{\"220\":1,\"257\":1,\"378\":1,\"389\":1}}],[\"ideas\",{\"1\":{\"110\":1,\"122\":1,\"144\":2,\"148\":1,\"172\":1,\"175\":1,\"321\":1,\"370\":1,\"434\":1}}],[\"identical\",{\"1\":{\"479\":1}}],[\"identity\",{\"1\":{\"349\":1,\"431\":1}}],[\"identities\",{\"1\":{\"103\":1,\"293\":1}}],[\"identification\",{\"1\":{\"237\":1,\"242\":1,\"430\":1}}],[\"identifies\",{\"1\":{\"111\":1,\"124\":1,\"127\":1,\"180\":1,\"272\":1,\"298\":1,\"360\":1}}],[\"identified\",{\"1\":{\"99\":1,\"114\":1,\"122\":1,\"127\":1,\"148\":1,\"149\":4,\"174\":1,\"181\":3,\"250\":1,\"271\":1,\"279\":1,\"335\":1,\"371\":1,\"400\":1,\"403\":1,\"456\":1,\"457\":1,\"458\":1}}],[\"identifying\",{\"0\":{\"161\":1,\"354\":1},\"1\":{\"105\":1,\"122\":1,\"123\":1,\"161\":1,\"172\":1,\"190\":1,\"231\":1,\"237\":1,\"343\":1,\"354\":1,\"366\":1,\"370\":1,\"407\":1,\"437\":1,\"498\":1}}],[\"identify\",{\"1\":{\"97\":1,\"103\":1,\"105\":1,\"112\":1,\"120\":1,\"128\":1,\"185\":1,\"198\":1,\"204\":1,\"206\":1,\"213\":1,\"216\":1,\"223\":1,\"228\":1,\"237\":1,\"251\":1,\"252\":1,\"272\":1,\"274\":1,\"275\":1,\"279\":1,\"286\":1,\"292\":2,\"305\":1,\"308\":1,\"309\":1,\"312\":1,\"316\":1,\"338\":1,\"387\":1,\"414\":1,\"423\":1,\"426\":1,\"439\":1,\"453\":1,\"455\":1,\"467\":1,\"474\":1,\"494\":1}}],[\"ide\",{\"0\":{\"361\":1},\"1\":{\"8\":1,\"361\":2}}],[\"iyer\",{\"1\":{\"207\":1}}],[\"if\",{\"1\":{\"158\":1,\"159\":1,\"170\":1,\"199\":1,\"235\":1,\"240\":1,\"248\":1,\"263\":1,\"298\":1,\"327\":1,\"340\":1,\"363\":1,\"381\":1,\"386\":1,\"389\":1,\"422\":1,\"435\":1,\"443\":1,\"494\":1,\"497\":1}}],[\"ignore\",{\"1\":{\"324\":1}}],[\"ignoring\",{\"1\":{\"149\":1,\"318\":1}}],[\"iglesia\",{\"1\":{\"315\":1}}],[\"igo源创有术\",{\"1\":{\"4\":1}}],[\"ion\",{\"1\":{\"326\":1,\"414\":1}}],[\"io\",{\"0\":{\"462\":1},\"1\":{\"249\":1,\"250\":1,\"259\":1,\"393\":1,\"462\":4}}],[\"iot\",{\"1\":{\"239\":1}}],[\"iou\",{\"1\":{\"181\":1}}],[\"ios\",{\"1\":{\"140\":1,\"239\":2}}],[\"ioannis\",{\"1\":{\"123\":1}}],[\"ioanna\",{\"1\":{\"123\":1}}],[\"i\",{\"0\":{\"122\":1,\"163\":1,\"195\":1,\"286\":1,\"293\":1,\"303\":1,\"399\":1,\"474\":1},\"1\":{\"128\":3,\"146\":1,\"151\":1,\"190\":1,\"198\":1,\"202\":1,\"284\":1,\"307\":3,\"322\":1,\"333\":1,\"338\":1,\"339\":1,\"348\":1,\"369\":1,\"384\":2,\"387\":1,\"392\":1,\"399\":3,\"405\":1,\"450\":2,\"453\":1,\"455\":1,\"467\":1,\"477\":1,\"494\":1}}],[\"ian\",{\"1\":{\"104\":1,\"122\":1}}],[\"iman\",{\"1\":{\"492\":1}}],[\"imaginary\",{\"1\":{\"340\":2}}],[\"imagining\",{\"0\":{\"248\":1}}],[\"imaging\",{\"1\":{\"112\":1,\"246\":1,\"253\":1,\"256\":1}}],[\"imagine\",{\"1\":{\"107\":2,\"175\":1,\"248\":1}}],[\"imagenet\",{\"1\":{\"199\":1}}],[\"imagen\",{\"1\":{\"101\":1}}],[\"images\",{\"0\":{\"272\":1},\"1\":{\"101\":3,\"151\":3,\"159\":1,\"176\":1,\"213\":9,\"238\":1,\"256\":2,\"270\":1,\"272\":3,\"280\":1,\"289\":7,\"296\":1,\"348\":3,\"349\":2,\"355\":1,\"375\":1,\"393\":1,\"407\":1,\"430\":2,\"442\":1}}],[\"image\",{\"0\":{\"101\":1,\"213\":1,\"256\":1,\"349\":1,\"375\":1},\"1\":{\"101\":3,\"123\":2,\"159\":1,\"213\":4,\"248\":1,\"256\":6,\"270\":1,\"289\":1,\"319\":2,\"345\":1,\"349\":11,\"355\":4,\"375\":5,\"393\":1,\"407\":1}}],[\"imagery\",{\"0\":{\"101\":1},\"1\":{\"101\":1,\"270\":1,\"272\":1}}],[\"imitate\",{\"1\":{\"470\":1}}],[\"imitation\",{\"0\":{\"362\":1},\"1\":{\"362\":1}}],[\"immense\",{\"1\":{\"428\":1}}],[\"immersive\",{\"0\":{\"165\":1,\"195\":1,\"239\":1},\"1\":{\"165\":1,\"195\":4,\"239\":3,\"305\":1}}],[\"immature\",{\"1\":{\"282\":1}}],[\"imbalances\",{\"1\":{\"206\":1,\"459\":1}}],[\"imbalance\",{\"1\":{\"167\":1,\"459\":1}}],[\"imperative\",{\"1\":{\"313\":1}}],[\"imposing\",{\"1\":{\"446\":1}}],[\"impossible\",{\"1\":{\"443\":1}}],[\"imposes\",{\"1\":{\"434\":1}}],[\"imposed\",{\"1\":{\"294\":1}}],[\"importance\",{\"1\":{\"144\":1,\"152\":1,\"159\":1,\"164\":1,\"166\":1,\"171\":1,\"213\":1,\"237\":1,\"240\":1,\"241\":1,\"249\":1,\"251\":1,\"258\":1,\"290\":2,\"323\":1,\"328\":1,\"365\":1,\"366\":3,\"423\":1,\"429\":1,\"433\":1,\"441\":1,\"442\":1,\"463\":1,\"482\":1}}],[\"importantly\",{\"1\":{\"133\":1,\"198\":1,\"331\":1,\"350\":1,\"387\":1,\"469\":1}}],[\"important\",{\"1\":{\"116\":2,\"118\":1,\"125\":1,\"159\":1,\"179\":1,\"218\":1,\"237\":1,\"243\":1,\"270\":1,\"278\":2,\"288\":1,\"302\":1,\"308\":1,\"337\":1,\"347\":1,\"369\":2,\"395\":1,\"402\":1,\"422\":1,\"455\":3,\"461\":1}}],[\"impractical\",{\"1\":{\"410\":1,\"468\":1}}],[\"impressive\",{\"1\":{\"362\":1,\"390\":1,\"393\":1,\"449\":1}}],[\"imprecise\",{\"1\":{\"260\":1}}],[\"improving\",{\"0\":{\"402\":1},\"1\":{\"99\":1,\"133\":1,\"169\":1,\"233\":1,\"249\":1,\"317\":1,\"331\":1,\"341\":1,\"349\":1,\"386\":1,\"391\":1,\"415\":1,\"425\":1,\"427\":1,\"435\":1,\"442\":1,\"454\":1,\"466\":1,\"483\":1,\"495\":1}}],[\"improves\",{\"0\":{\"176\":1,\"319\":1,\"489\":1},\"1\":{\"158\":1,\"198\":1,\"234\":1,\"251\":1,\"275\":1,\"319\":1,\"328\":1,\"335\":1,\"347\":1,\"365\":1,\"376\":1,\"382\":1,\"386\":1,\"387\":1,\"389\":1,\"423\":1,\"430\":1,\"453\":1,\"467\":1,\"483\":1,\"494\":1}}],[\"improved\",{\"0\":{\"432\":1,\"441\":1},\"1\":{\"131\":1,\"135\":2,\"152\":1,\"155\":2,\"165\":1,\"186\":1,\"191\":1,\"259\":1,\"282\":1,\"298\":1,\"303\":1,\"319\":1,\"412\":1,\"422\":1,\"438\":1,\"460\":1,\"469\":1}}],[\"improvements\",{\"1\":{\"148\":1,\"182\":1,\"215\":1,\"257\":1,\"291\":1,\"294\":1,\"314\":1,\"318\":1,\"319\":1,\"361\":1,\"366\":1,\"368\":2,\"384\":1,\"386\":2,\"429\":1,\"441\":1,\"476\":1}}],[\"improvement\",{\"1\":{\"118\":1,\"190\":1,\"323\":1,\"358\":1,\"360\":1,\"385\":1,\"413\":1,\"419\":1,\"420\":1,\"434\":1}}],[\"improve\",{\"0\":{\"159\":1,\"338\":1},\"1\":{\"96\":2,\"119\":1,\"121\":1,\"127\":1,\"128\":1,\"148\":1,\"150\":2,\"152\":1,\"159\":2,\"170\":1,\"173\":1,\"215\":1,\"220\":1,\"222\":1,\"227\":1,\"254\":1,\"272\":2,\"274\":2,\"281\":1,\"289\":1,\"309\":1,\"327\":1,\"339\":1,\"347\":1,\"356\":1,\"382\":1,\"385\":1,\"386\":1,\"389\":1,\"391\":2,\"402\":1,\"413\":1,\"422\":1,\"459\":1,\"462\":1,\"467\":1,\"468\":1,\"479\":1,\"489\":1}}],[\"impair\",{\"1\":{\"223\":1}}],[\"impaired\",{\"1\":{\"219\":3,\"401\":3}}],[\"impairment\",{\"1\":{\"165\":1}}],[\"impairments\",{\"1\":{\"126\":1,\"184\":1,\"219\":1,\"380\":1,\"401\":1}}],[\"impactful\",{\"1\":{\"305\":1}}],[\"impacting\",{\"1\":{\"158\":1,\"460\":1}}],[\"impacted\",{\"1\":{\"128\":1}}],[\"impacts\",{\"0\":{\"135\":1},\"1\":{\"107\":1,\"135\":1,\"148\":1,\"291\":1,\"476\":1}}],[\"impact\",{\"0\":{\"128\":1,\"200\":1,\"265\":1,\"446\":1,\"461\":1},\"1\":{\"106\":1,\"128\":3,\"142\":1,\"167\":1,\"173\":1,\"185\":1,\"199\":1,\"203\":1,\"210\":1,\"213\":1,\"217\":1,\"226\":1,\"245\":1,\"246\":1,\"251\":1,\"263\":1,\"265\":3,\"302\":2,\"304\":1,\"313\":1,\"317\":1,\"329\":2,\"334\":1,\"336\":1,\"344\":1,\"374\":1,\"423\":1,\"439\":1,\"446\":2,\"461\":1,\"465\":1,\"469\":1,\"495\":1}}],[\"implicit\",{\"1\":{\"283\":2,\"424\":3,\"450\":1}}],[\"implications\",{\"0\":{\"101\":1},\"1\":{\"101\":1,\"108\":1,\"110\":1,\"121\":1,\"128\":1,\"129\":1,\"156\":1,\"180\":1,\"204\":1,\"212\":1,\"227\":1,\"244\":1,\"263\":1,\"272\":1,\"286\":1,\"293\":1,\"321\":1,\"461\":1,\"474\":1,\"498\":1}}],[\"implement\",{\"1\":{\"240\":1,\"434\":1,\"445\":1,\"481\":1}}],[\"implementing\",{\"0\":{\"343\":1},\"1\":{\"186\":1,\"206\":1,\"343\":1,\"347\":1,\"443\":1}}],[\"implements\",{\"1\":{\"153\":1,\"396\":1,\"422\":1}}],[\"implemented\",{\"1\":{\"119\":1,\"186\":1,\"231\":1,\"274\":1,\"340\":1,\"343\":1,\"475\":1}}],[\"implementations\",{\"1\":{\"488\":1}}],[\"implementation\",{\"1\":{\"115\":1,\"153\":1,\"201\":1,\"238\":1,\"274\":1,\"316\":1,\"326\":1,\"353\":2,\"396\":1,\"455\":1}}],[\"italian\",{\"1\":{\"315\":2}}],[\"itcm\",{\"1\":{\"283\":2}}],[\"itcma\",{\"0\":{\"283\":1},\"1\":{\"283\":5}}],[\"items\",{\"1\":{\"282\":1,\"479\":1,\"485\":1}}],[\"item\",{\"0\":{\"250\":1},\"1\":{\"250\":3,\"417\":1,\"479\":1,\"485\":1}}],[\"iterative\",{\"1\":{\"306\":1}}],[\"iteratively\",{\"1\":{\"184\":1,\"360\":1,\"380\":1}}],[\"iterating\",{\"1\":{\"271\":1,\"457\":1}}],[\"iterations\",{\"1\":{\"306\":1,\"403\":1}}],[\"iteration\",{\"1\":{\"104\":1,\"139\":1,\"271\":1,\"327\":1,\"457\":1}}],[\"iterate\",{\"1\":{\"110\":1,\"321\":1}}],[\"it\",{\"0\":{\"445\":1},\"1\":{\"100\":3,\"107\":1,\"108\":2,\"114\":1,\"115\":1,\"116\":1,\"124\":1,\"127\":1,\"136\":1,\"137\":1,\"146\":1,\"159\":2,\"161\":1,\"167\":1,\"179\":1,\"187\":1,\"188\":1,\"191\":2,\"195\":1,\"202\":2,\"203\":1,\"223\":1,\"232\":1,\"236\":1,\"238\":1,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"250\":1,\"251\":3,\"256\":2,\"257\":1,\"258\":1,\"263\":1,\"265\":2,\"270\":1,\"272\":1,\"274\":1,\"282\":2,\"284\":1,\"292\":1,\"295\":1,\"305\":1,\"308\":1,\"313\":2,\"322\":2,\"324\":1,\"337\":1,\"338\":1,\"340\":1,\"341\":1,\"343\":1,\"347\":1,\"350\":1,\"354\":1,\"355\":1,\"363\":1,\"368\":1,\"375\":2,\"376\":1,\"382\":3,\"389\":2,\"393\":2,\"395\":1,\"399\":3,\"405\":1,\"409\":1,\"410\":1,\"411\":1,\"416\":1,\"418\":3,\"423\":3,\"429\":1,\"430\":1,\"433\":1,\"434\":1,\"436\":2,\"437\":1,\"439\":1,\"441\":1,\"443\":1,\"444\":1,\"445\":1,\"446\":1,\"454\":2,\"458\":2,\"459\":1,\"460\":1,\"462\":1,\"467\":1,\"469\":1,\"486\":1,\"488\":2,\"494\":1,\"497\":3}}],[\"itself\",{\"1\":{\"241\":1,\"356\":1,\"435\":1}}],[\"itss\",{\"1\":{\"130\":3,\"169\":1}}],[\"its\",{\"1\":{\"99\":1,\"116\":2,\"119\":2,\"124\":1,\"143\":1,\"146\":1,\"158\":1,\"159\":1,\"165\":2,\"167\":2,\"179\":2,\"183\":1,\"188\":2,\"190\":2,\"191\":1,\"198\":1,\"201\":1,\"203\":1,\"205\":1,\"210\":2,\"213\":2,\"218\":1,\"219\":1,\"222\":1,\"227\":1,\"240\":1,\"246\":1,\"248\":1,\"250\":1,\"251\":1,\"258\":2,\"261\":1,\"263\":1,\"271\":1,\"277\":1,\"280\":1,\"281\":1,\"282\":2,\"283\":3,\"295\":2,\"306\":1,\"307\":1,\"322\":1,\"329\":2,\"333\":5,\"342\":1,\"360\":1,\"361\":2,\"366\":1,\"376\":2,\"377\":1,\"384\":1,\"386\":2,\"387\":1,\"390\":2,\"398\":4,\"399\":1,\"401\":1,\"410\":1,\"418\":1,\"422\":1,\"423\":1,\"429\":1,\"430\":1,\"433\":2,\"444\":1,\"445\":2,\"450\":1,\"454\":1,\"457\":1,\"458\":2,\"460\":1,\"467\":1,\"477\":1,\"485\":1,\"487\":2,\"489\":1,\"490\":1,\"496\":1,\"497\":1}}],[\"isd\",{\"1\":{\"510\":1}}],[\"isabelle\",{\"1\":{\"494\":2}}],[\"isac\",{\"1\":{\"153\":3}}],[\"ishibashi\",{\"1\":{\"415\":1}}],[\"ision\",{\"1\":{\"345\":1}}],[\"islam\",{\"1\":{\"298\":2}}],[\"isbister\",{\"1\":{\"236\":1}}],[\"isotta\",{\"1\":{\"189\":1}}],[\"issue\",{\"0\":{\"495\":1},\"1\":{\"113\":1,\"129\":1,\"140\":1,\"167\":1,\"190\":1,\"339\":1,\"340\":1,\"382\":1,\"425\":1,\"443\":1,\"454\":1,\"459\":1,\"462\":1,\"478\":1,\"495\":2,\"496\":1,\"498\":1}}],[\"issues\",{\"0\":{\"446\":1},\"1\":{\"101\":1,\"105\":2,\"122\":1,\"127\":1,\"148\":2,\"164\":1,\"171\":1,\"213\":1,\"219\":1,\"243\":2,\"253\":1,\"286\":2,\"333\":1,\"338\":1,\"350\":1,\"401\":1,\"403\":1,\"447\":1,\"469\":1,\"474\":2,\"483\":1,\"495\":4}}],[\"is\",{\"0\":{\"122\":1,\"185\":1,\"333\":1,\"335\":1,\"340\":1,\"463\":1},\"1\":{\"96\":2,\"97\":3,\"98\":1,\"99\":2,\"100\":4,\"101\":1,\"102\":1,\"104\":2,\"105\":1,\"106\":1,\"111\":1,\"112\":2,\"114\":1,\"116\":2,\"119\":1,\"121\":1,\"122\":1,\"128\":3,\"131\":1,\"132\":2,\"135\":1,\"137\":2,\"139\":1,\"140\":1,\"143\":2,\"144\":1,\"146\":4,\"148\":1,\"150\":1,\"151\":1,\"152\":2,\"153\":1,\"154\":2,\"155\":1,\"156\":2,\"159\":1,\"160\":1,\"161\":1,\"165\":2,\"166\":2,\"167\":1,\"169\":2,\"172\":1,\"176\":3,\"179\":4,\"181\":1,\"186\":2,\"187\":4,\"189\":2,\"190\":4,\"191\":2,\"194\":1,\"196\":2,\"199\":3,\"200\":2,\"201\":1,\"203\":1,\"204\":2,\"207\":2,\"209\":2,\"211\":1,\"212\":1,\"213\":1,\"214\":2,\"216\":1,\"220\":2,\"222\":1,\"223\":2,\"224\":2,\"226\":1,\"232\":3,\"233\":1,\"234\":2,\"235\":2,\"236\":1,\"237\":1,\"238\":1,\"239\":2,\"240\":3,\"241\":4,\"243\":2,\"244\":1,\"245\":2,\"249\":1,\"250\":2,\"251\":1,\"253\":2,\"255\":3,\"256\":2,\"257\":4,\"258\":1,\"259\":2,\"260\":1,\"261\":1,\"263\":1,\"265\":2,\"269\":2,\"271\":1,\"272\":1,\"273\":2,\"274\":1,\"278\":1,\"281\":3,\"282\":2,\"283\":1,\"284\":2,\"287\":1,\"288\":1,\"289\":1,\"290\":1,\"291\":1,\"292\":1,\"293\":2,\"294\":1,\"295\":1,\"298\":1,\"303\":1,\"304\":1,\"306\":1,\"308\":2,\"309\":1,\"312\":3,\"313\":2,\"315\":2,\"316\":4,\"318\":2,\"324\":3,\"325\":1,\"326\":5,\"328\":1,\"330\":2,\"333\":3,\"334\":2,\"335\":1,\"337\":1,\"339\":1,\"340\":2,\"343\":1,\"348\":1,\"349\":1,\"350\":3,\"352\":1,\"353\":1,\"354\":1,\"355\":7,\"356\":2,\"361\":2,\"363\":1,\"366\":1,\"369\":3,\"370\":1,\"372\":1,\"375\":3,\"376\":1,\"378\":3,\"381\":2,\"382\":5,\"384\":1,\"386\":1,\"388\":1,\"389\":3,\"391\":3,\"392\":1,\"393\":1,\"395\":3,\"396\":1,\"398\":5,\"399\":3,\"400\":2,\"402\":4,\"403\":2,\"408\":1,\"409\":5,\"412\":2,\"414\":1,\"415\":2,\"416\":7,\"417\":1,\"418\":2,\"419\":2,\"420\":2,\"422\":2,\"423\":1,\"425\":3,\"427\":1,\"428\":2,\"429\":2,\"431\":1,\"432\":1,\"433\":1,\"434\":2,\"435\":1,\"436\":2,\"438\":1,\"439\":1,\"441\":1,\"443\":3,\"444\":3,\"445\":2,\"453\":1,\"454\":2,\"455\":1,\"456\":4,\"457\":1,\"458\":1,\"459\":1,\"465\":1,\"466\":2,\"467\":2,\"469\":3,\"470\":1,\"471\":1,\"476\":1,\"478\":1,\"479\":1,\"481\":1,\"482\":2,\"483\":1,\"485\":3,\"486\":4,\"487\":1,\"488\":4,\"492\":1,\"495\":1,\"497\":4,\"498\":1}}],[\"in的感觉\",{\"1\":{\"508\":1}}],[\"inigo\",{\"1\":{\"465\":1}}],[\"initiated\",{\"1\":{\"377\":1}}],[\"initiative\",{\"0\":{\"98\":1},\"1\":{\"98\":6,\"175\":2,\"358\":1}}],[\"initialize\",{\"1\":{\"419\":1,\"420\":1}}],[\"initial\",{\"0\":{\"158\":1,\"271\":1,\"457\":1},\"1\":{\"114\":1,\"117\":1,\"159\":1,\"166\":1,\"192\":1,\"196\":1,\"218\":1,\"238\":1,\"248\":1,\"291\":1,\"365\":1,\"384\":1,\"385\":1,\"431\":1,\"476\":1}}],[\"initially\",{\"1\":{\"103\":1,\"303\":1,\"408\":1,\"409\":1}}],[\"inefficient\",{\"1\":{\"462\":1}}],[\"inefficiencies\",{\"1\":{\"356\":1}}],[\"ineffective\",{\"1\":{\"360\":1}}],[\"inevitably\",{\"1\":{\"329\":1,\"482\":1}}],[\"inquire\",{\"1\":{\"482\":1}}],[\"inquiries\",{\"1\":{\"341\":1}}],[\"inquiry\",{\"0\":{\"341\":1}}],[\"injection\",{\"0\":{\"392\":1},\"1\":{\"374\":1}}],[\"inject\",{\"1\":{\"319\":1}}],[\"injury\",{\"1\":{\"335\":1}}],[\"injuries\",{\"1\":{\"298\":1}}],[\"injured\",{\"1\":{\"246\":1}}],[\"inherited\",{\"1\":{\"196\":1}}],[\"inherently\",{\"1\":{\"159\":1,\"259\":1,\"260\":1,\"355\":1,\"485\":1}}],[\"inherent\",{\"1\":{\"116\":1,\"135\":1,\"235\":1,\"244\":1,\"281\":1,\"293\":1,\"314\":1,\"333\":1,\"378\":1,\"386\":1,\"404\":1,\"424\":1,\"425\":1,\"459\":1,\"489\":1}}],[\"inma\",{\"1\":{\"176\":1}}],[\"inner\",{\"1\":{\"148\":1}}],[\"innovation\",{\"1\":{\"180\":1,\"232\":1,\"259\":1,\"356\":1,\"377\":1,\"519\":2}}],[\"innovations\",{\"1\":{\"132\":1,\"192\":1,\"217\":1,\"330\":1,\"358\":1}}],[\"innovatively\",{\"1\":{\"314\":1}}],[\"innovative\",{\"1\":{\"118\":1,\"127\":1,\"167\":1,\"213\":1,\"246\":1,\"266\":1,\"329\":1,\"359\":1,\"396\":1,\"404\":1,\"510\":1}}],[\"ingrain\",{\"1\":{\"131\":1}}],[\"inaturalist\",{\"1\":{\"444\":1}}],[\"inaugurated\",{\"1\":{\"359\":1}}],[\"inadequate\",{\"1\":{\"298\":1,\"345\":1,\"425\":1,\"479\":1}}],[\"inadvertently\",{\"1\":{\"124\":1,\"167\":1,\"170\":1,\"460\":1}}],[\"inappropriate\",{\"1\":{\"211\":1}}],[\"inaccuracies\",{\"1\":{\"198\":1,\"356\":1,\"387\":1}}],[\"inaccurate\",{\"1\":{\"131\":1,\"198\":1,\"283\":1,\"387\":1,\"416\":1}}],[\"inaccessible\",{\"1\":{\"105\":1}}],[\"incurs\",{\"1\":{\"318\":1}}],[\"incentives\",{\"1\":{\"233\":10}}],[\"incentive\",{\"0\":{\"233\":1},\"1\":{\"233\":5}}],[\"incentivizing\",{\"1\":{\"200\":1}}],[\"incentivize\",{\"1\":{\"200\":1}}],[\"inclined\",{\"1\":{\"203\":1}}],[\"inclusion\",{\"0\":{\"282\":1},\"1\":{\"282\":1,\"333\":1,\"350\":1,\"408\":1,\"475\":1}}],[\"inclusivity\",{\"1\":{\"277\":1}}],[\"inclusive\",{\"0\":{\"144\":1},\"1\":{\"144\":1,\"153\":1,\"164\":1,\"181\":1,\"377\":1}}],[\"included\",{\"1\":{\"237\":1,\"350\":1,\"382\":1,\"410\":1,\"492\":1}}],[\"includes\",{\"1\":{\"129\":1,\"172\":1,\"213\":1,\"218\":1,\"239\":1,\"240\":1,\"250\":1,\"254\":1,\"259\":1,\"272\":1,\"337\":1,\"370\":1,\"379\":1,\"398\":1,\"411\":1,\"422\":1,\"447\":1}}],[\"include\",{\"1\":{\"104\":1,\"144\":1,\"209\":1,\"275\":1,\"277\":1,\"307\":1,\"334\":1,\"337\":1,\"350\":1,\"422\":1}}],[\"including\",{\"1\":{\"99\":1,\"101\":1,\"104\":1,\"112\":1,\"144\":1,\"146\":1,\"151\":1,\"155\":1,\"169\":1,\"178\":1,\"181\":1,\"204\":1,\"211\":1,\"213\":1,\"214\":1,\"215\":2,\"220\":1,\"221\":1,\"228\":1,\"229\":1,\"230\":1,\"237\":1,\"243\":1,\"246\":1,\"254\":1,\"274\":1,\"286\":1,\"298\":1,\"302\":1,\"313\":1,\"327\":1,\"333\":1,\"336\":1,\"345\":1,\"348\":1,\"353\":1,\"355\":1,\"356\":1,\"361\":1,\"364\":1,\"365\":1,\"377\":2,\"379\":1,\"384\":1,\"391\":1,\"404\":1,\"417\":1,\"418\":1,\"439\":1,\"444\":1,\"454\":1,\"456\":1,\"460\":1,\"466\":1,\"472\":1,\"474\":1,\"475\":1,\"477\":1,\"481\":1,\"482\":1,\"491\":1,\"495\":1}}],[\"incomplete\",{\"1\":{\"234\":1}}],[\"income\",{\"1\":{\"178\":2}}],[\"inconsistency\",{\"1\":{\"339\":1}}],[\"inconsistencies\",{\"1\":{\"327\":1,\"356\":1,\"372\":1,\"437\":1}}],[\"inconsistent\",{\"1\":{\"199\":1,\"216\":1,\"494\":1}}],[\"incongruent\",{\"1\":{\"230\":1}}],[\"incorporates\",{\"1\":{\"196\":1,\"222\":1,\"259\":1,\"298\":1,\"314\":1,\"393\":1}}],[\"incorporated\",{\"1\":{\"163\":1,\"301\":1,\"479\":1}}],[\"incorporate\",{\"1\":{\"151\":1,\"184\":1,\"245\":1,\"296\":1,\"318\":1,\"322\":1,\"348\":1,\"366\":1,\"380\":1,\"386\":1,\"432\":1,\"450\":1}}],[\"incorporation\",{\"1\":{\"123\":1,\"190\":1,\"316\":1,\"388\":1,\"439\":1,\"495\":1}}],[\"incorporating\",{\"0\":{\"432\":1},\"1\":{\"118\":1,\"121\":1,\"139\":1,\"155\":1,\"184\":1,\"215\":1,\"238\":1,\"243\":1,\"244\":1,\"261\":1,\"328\":1,\"341\":1,\"350\":1,\"377\":1,\"380\":1,\"400\":1,\"487\":1}}],[\"incorrectly\",{\"1\":{\"251\":1,\"423\":1}}],[\"incorrect\",{\"1\":{\"124\":1,\"466\":1,\"470\":1}}],[\"incrementally\",{\"1\":{\"131\":1}}],[\"incremental\",{\"0\":{\"131\":2},\"1\":{\"131\":3}}],[\"increasing\",{\"0\":{\"428\":1},\"1\":{\"135\":1,\"138\":1,\"140\":1,\"161\":1,\"181\":1,\"200\":1,\"237\":1,\"274\":2,\"337\":1,\"350\":1,\"354\":1,\"432\":1,\"439\":1,\"461\":1,\"498\":1}}],[\"increasingly\",{\"1\":{\"116\":1,\"135\":1,\"164\":1,\"211\":1,\"215\":1,\"221\":1,\"269\":1,\"275\":1,\"281\":1,\"286\":1,\"313\":1,\"334\":1,\"337\":1,\"391\":1,\"418\":1,\"435\":1,\"447\":1,\"474\":1,\"494\":1}}],[\"increase\",{\"1\":{\"158\":1,\"172\":1,\"184\":1,\"254\":1,\"272\":1,\"287\":1,\"298\":1,\"323\":1,\"338\":1,\"344\":1,\"370\":1,\"380\":1,\"428\":1,\"460\":1,\"482\":1,\"495\":1}}],[\"increased\",{\"0\":{\"374\":1},\"1\":{\"133\":1,\"155\":1,\"176\":2,\"200\":2,\"203\":1,\"209\":1,\"215\":1,\"261\":1,\"265\":1,\"303\":1,\"331\":1,\"374\":1,\"415\":1,\"490\":1}}],[\"increases\",{\"0\":{\"155\":1},\"1\":{\"113\":1,\"281\":1,\"358\":1,\"395\":1,\"487\":1}}],[\"inputs\",{\"0\":{\"219\":1,\"401\":1},\"1\":{\"104\":1,\"201\":2,\"219\":1,\"259\":1,\"260\":1,\"327\":2,\"345\":1,\"401\":1,\"425\":1,\"465\":1}}],[\"input\",{\"0\":{\"219\":1,\"401\":1},\"1\":{\"104\":1,\"116\":1,\"118\":1,\"123\":1,\"144\":1,\"153\":1,\"159\":3,\"163\":4,\"219\":3,\"224\":3,\"232\":1,\"255\":2,\"257\":1,\"362\":2,\"382\":2,\"384\":1,\"399\":2,\"401\":3,\"402\":1,\"405\":1,\"410\":1,\"412\":2,\"422\":2,\"425\":3,\"450\":1,\"453\":1,\"490\":1}}],[\"infrequent\",{\"1\":{\"461\":1}}],[\"infrared\",{\"0\":{\"194\":1},\"1\":{\"223\":1}}],[\"inflexible\",{\"1\":{\"427\":1}}],[\"influencing\",{\"1\":{\"308\":1}}],[\"influenced\",{\"1\":{\"166\":1,\"191\":1,\"269\":1,\"303\":1}}],[\"influence\",{\"0\":{\"203\":1,\"221\":1},\"1\":{\"128\":1,\"152\":1,\"172\":1,\"185\":1,\"221\":1,\"227\":2,\"245\":1,\"255\":1,\"263\":1,\"292\":1,\"304\":1,\"339\":1,\"370\":1,\"417\":1,\"439\":1,\"472\":1}}],[\"influences\",{\"1\":{\"107\":1,\"108\":1,\"195\":1,\"466\":1}}],[\"influential\",{\"1\":{\"122\":1}}],[\"info\",{\"1\":{\"389\":1}}],[\"informative\",{\"1\":{\"234\":1,\"404\":2}}],[\"informativeness\",{\"1\":{\"155\":1}}],[\"information\",{\"0\":{\"167\":2,\"255\":1,\"272\":1,\"432\":1,\"435\":1,\"455\":1,\"492\":1},\"1\":{\"118\":1,\"121\":1,\"123\":4,\"128\":1,\"131\":1,\"137\":1,\"139\":1,\"167\":6,\"178\":1,\"201\":1,\"205\":1,\"207\":6,\"211\":1,\"213\":1,\"219\":1,\"222\":1,\"223\":1,\"226\":1,\"232\":1,\"235\":3,\"240\":1,\"255\":12,\"256\":1,\"272\":1,\"277\":2,\"282\":2,\"293\":1,\"294\":1,\"298\":1,\"308\":1,\"318\":2,\"322\":1,\"326\":1,\"329\":3,\"333\":1,\"335\":1,\"340\":1,\"349\":1,\"350\":1,\"355\":1,\"356\":1,\"359\":1,\"382\":1,\"390\":2,\"392\":1,\"394\":1,\"400\":1,\"401\":1,\"402\":1,\"403\":2,\"413\":1,\"417\":1,\"425\":1,\"432\":2,\"435\":2,\"439\":1,\"443\":1,\"447\":1,\"453\":1,\"455\":4,\"461\":1,\"481\":4,\"491\":1,\"492\":3}}],[\"informal\",{\"1\":{\"122\":1,\"494\":1}}],[\"inform\",{\"1\":{\"105\":1,\"152\":1,\"236\":1,\"385\":1}}],[\"informer\",{\"1\":{\"103\":2}}],[\"informed\",{\"1\":{\"96\":1,\"150\":1,\"160\":1,\"169\":1,\"226\":1,\"356\":1,\"527\":1}}],[\"infused\",{\"1\":{\"229\":1}}],[\"inferior\",{\"1\":{\"471\":1}}],[\"inferred\",{\"1\":{\"148\":1}}],[\"inferring\",{\"1\":{\"124\":1,\"434\":1}}],[\"infers\",{\"1\":{\"108\":1}}],[\"infer\",{\"0\":{\"107\":1}}],[\"inferences\",{\"1\":{\"283\":1,\"475\":1}}],[\"inference\",{\"0\":{\"210\":1,\"366\":1,\"372\":1,\"456\":1,\"462\":1,\"465\":1,\"487\":1},\"1\":{\"103\":2,\"151\":1,\"210\":1,\"316\":1,\"322\":1,\"348\":1,\"366\":2,\"372\":2,\"389\":1,\"390\":1,\"428\":1,\"435\":1,\"441\":1,\"447\":1,\"462\":3,\"465\":2,\"469\":2,\"497\":2}}],[\"invisible\",{\"1\":{\"293\":1}}],[\"invite\",{\"1\":{\"243\":1}}],[\"invites\",{\"1\":{\"217\":1}}],[\"inviting\",{\"1\":{\"180\":1}}],[\"invoking\",{\"1\":{\"220\":1}}],[\"involving\",{\"1\":{\"133\":1,\"149\":1,\"176\":2,\"184\":1,\"186\":1,\"238\":2,\"244\":1,\"269\":1,\"271\":1,\"297\":1,\"314\":1,\"319\":1,\"331\":1,\"340\":1,\"380\":1,\"393\":1,\"457\":1}}],[\"involvement\",{\"1\":{\"186\":1,\"326\":1}}],[\"involves\",{\"1\":{\"123\":1,\"127\":1,\"165\":1,\"186\":2,\"204\":1,\"275\":1,\"289\":1,\"292\":1,\"307\":1,\"318\":1,\"339\":1,\"365\":1,\"376\":1,\"416\":1,\"470\":1,\"495\":1}}],[\"involved\",{\"1\":{\"106\":1,\"123\":1,\"149\":1,\"195\":1,\"196\":1,\"254\":1,\"271\":1,\"340\":1,\"457\":1,\"461\":1,\"469\":2}}],[\"involve\",{\"1\":{\"100\":1,\"139\":1,\"301\":1,\"307\":1,\"404\":1,\"425\":1,\"471\":1,\"487\":1}}],[\"investig\",{\"1\":{\"458\":1}}],[\"investigating\",{\"0\":{\"200\":1,\"458\":1,\"466\":1,\"470\":1},\"1\":{\"212\":1,\"279\":1,\"458\":1}}],[\"investigations\",{\"0\":{\"486\":1},\"1\":{\"255\":1}}],[\"investigation\",{\"0\":{\"120\":1,\"472\":1},\"1\":{\"156\":1,\"256\":1,\"291\":1,\"297\":1,\"304\":2,\"359\":1,\"390\":1,\"411\":1,\"422\":1,\"472\":1,\"476\":1}}],[\"investigated\",{\"1\":{\"190\":1,\"317\":1,\"417\":1}}],[\"investigates\",{\"1\":{\"172\":1,\"203\":1,\"251\":1,\"263\":1,\"288\":1,\"323\":1,\"327\":1,\"339\":1,\"370\":1,\"372\":1,\"423\":1,\"470\":1}}],[\"investigate\",{\"1\":{\"101\":2,\"121\":1,\"122\":1,\"135\":1,\"156\":1,\"185\":1,\"195\":1,\"199\":1,\"202\":1,\"215\":1,\"220\":1,\"228\":1,\"230\":1,\"233\":1,\"248\":1,\"274\":1,\"278\":1,\"308\":1,\"328\":1,\"363\":1,\"386\":1,\"404\":1,\"422\":1,\"425\":1,\"431\":1,\"439\":1,\"450\":1,\"461\":1,\"467\":1,\"468\":1}}],[\"investing\",{\"1\":{\"437\":1}}],[\"invertible\",{\"1\":{\"224\":1}}],[\"inversion\",{\"0\":{\"224\":1},\"1\":{\"224\":2}}],[\"inventory\",{\"1\":{\"183\":1}}],[\"invariance\",{\"1\":{\"456\":1}}],[\"invariant\",{\"1\":{\"199\":1}}],[\"invaluable\",{\"1\":{\"206\":1}}],[\"invalid\",{\"1\":{\"161\":1,\"354\":1}}],[\"invade\",{\"1\":{\"206\":1}}],[\"invasive\",{\"0\":{\"284\":1},\"1\":{\"120\":1,\"166\":2,\"284\":3,\"403\":1}}],[\"insensitive\",{\"1\":{\"488\":1}}],[\"inside\",{\"1\":{\"203\":1,\"269\":1}}],[\"insightlens\",{\"0\":{\"242\":1},\"1\":{\"242\":2}}],[\"insightful\",{\"1\":{\"174\":1,\"324\":1,\"371\":1}}],[\"insight\",{\"1\":{\"102\":1,\"199\":1,\"242\":1,\"360\":1,\"369\":1,\"414\":1}}],[\"insights\",{\"0\":{\"164\":1,\"174\":1,\"227\":1,\"242\":1,\"297\":1,\"371\":1},\"1\":{\"97\":1,\"99\":1,\"107\":1,\"111\":1,\"117\":1,\"122\":1,\"136\":1,\"144\":1,\"159\":1,\"160\":1,\"163\":1,\"172\":1,\"173\":1,\"176\":1,\"185\":1,\"198\":1,\"237\":1,\"242\":4,\"246\":1,\"260\":1,\"282\":1,\"290\":2,\"291\":1,\"297\":1,\"303\":1,\"312\":1,\"339\":1,\"350\":1,\"359\":1,\"365\":1,\"370\":1,\"379\":1,\"387\":1,\"413\":1,\"429\":1,\"439\":1,\"445\":1,\"465\":1,\"475\":1,\"476\":1,\"482\":1,\"498\":1,\"529\":1}}],[\"inscrutability\",{\"1\":{\"171\":1}}],[\"inspect\",{\"1\":{\"258\":1,\"433\":1}}],[\"inspection\",{\"1\":{\"219\":1,\"401\":1,\"488\":1}}],[\"inspecting\",{\"1\":{\"140\":1}}],[\"inspired\",{\"1\":{\"207\":1,\"223\":1,\"323\":1,\"404\":1,\"410\":1,\"411\":1,\"438\":1}}],[\"inspire\",{\"1\":{\"181\":1,\"218\":1}}],[\"inspires\",{\"1\":{\"175\":1}}],[\"inspirational\",{\"1\":{\"218\":1}}],[\"inspiration\",{\"1\":{\"132\":1,\"240\":1,\"330\":1,\"386\":1}}],[\"insufficient\",{\"1\":{\"111\":1,\"164\":1,\"355\":1}}],[\"institutional\",{\"1\":{\"176\":1,\"205\":1,\"477\":1}}],[\"instigate\",{\"1\":{\"150\":1}}],[\"instance\",{\"1\":{\"350\":1,\"454\":1}}],[\"instances\",{\"0\":{\"156\":1},\"1\":{\"131\":1,\"156\":3,\"161\":1,\"344\":1,\"345\":1,\"354\":1,\"395\":1,\"428\":1,\"458\":1}}],[\"installation\",{\"1\":{\"201\":1}}],[\"install\",{\"1\":{\"128\":1}}],[\"instagram\",{\"1\":{\"122\":1}}],[\"instead\",{\"1\":{\"108\":1,\"165\":1,\"282\":1,\"362\":2,\"441\":1,\"454\":1}}],[\"instrumental\",{\"1\":{\"167\":1,\"216\":1}}],[\"instruments\",{\"1\":{\"105\":1,\"359\":1}}],[\"instructing\",{\"1\":{\"317\":1}}],[\"instructions\",{\"1\":{\"151\":1,\"163\":2,\"274\":1,\"283\":2,\"317\":1,\"348\":1,\"375\":2,\"407\":2}}],[\"instruction\",{\"0\":{\"407\":1},\"1\":{\"151\":1,\"319\":2,\"344\":1,\"348\":1,\"375\":1,\"386\":1,\"391\":1,\"411\":1,\"482\":1}}],[\"instructors\",{\"1\":{\"99\":3,\"156\":6,\"182\":1}}],[\"intuitionistic\",{\"0\":{\"322\":1},\"1\":{\"322\":1}}],[\"intuition\",{\"1\":{\"187\":1,\"197\":1}}],[\"intuitively\",{\"1\":{\"317\":1,\"322\":1,\"418\":1}}],[\"intuitive\",{\"1\":{\"100\":1,\"131\":1,\"140\":1,\"149\":1,\"183\":1,\"201\":3,\"266\":1,\"287\":1,\"326\":1,\"361\":1,\"454\":1}}],[\"intra\",{\"1\":{\"384\":1}}],[\"introspected\",{\"1\":{\"188\":1}}],[\"introspection\",{\"1\":{\"188\":2}}],[\"introductory\",{\"0\":{\"307\":1},\"1\":{\"172\":1,\"370\":1}}],[\"introduction\",{\"1\":{\"153\":1,\"307\":1,\"469\":1}}],[\"introducing\",{\"1\":{\"123\":1,\"157\":1,\"170\":1,\"174\":1,\"182\":1,\"183\":1,\"371\":1,\"395\":1,\"430\":1,\"470\":1,\"489\":1}}],[\"introduces\",{\"1\":{\"113\":1,\"167\":1,\"170\":2,\"201\":1,\"213\":1,\"234\":1,\"261\":1,\"266\":1,\"283\":1,\"300\":1,\"306\":1,\"316\":1,\"318\":1,\"347\":1,\"355\":1,\"376\":2,\"391\":1,\"393\":1,\"412\":1,\"442\":1,\"459\":2,\"477\":1,\"492\":1}}],[\"introduce\",{\"1\":{\"105\":1,\"110\":1,\"111\":1,\"113\":1,\"116\":1,\"117\":1,\"118\":1,\"131\":1,\"133\":1,\"137\":1,\"138\":1,\"139\":2,\"140\":1,\"141\":1,\"143\":1,\"173\":1,\"182\":1,\"183\":1,\"188\":1,\"194\":1,\"199\":2,\"215\":1,\"218\":1,\"235\":1,\"238\":1,\"241\":1,\"242\":1,\"257\":1,\"259\":1,\"309\":1,\"314\":1,\"321\":1,\"328\":1,\"331\":1,\"333\":1,\"334\":1,\"337\":1,\"345\":1,\"349\":1,\"353\":1,\"361\":1,\"364\":1,\"369\":1,\"377\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"391\":1,\"395\":1,\"396\":1,\"400\":1,\"411\":1,\"418\":2,\"424\":1,\"427\":1,\"430\":1,\"435\":1,\"447\":1,\"456\":1,\"467\":1,\"480\":1,\"482\":1,\"483\":1}}],[\"introduced\",{\"1\":{\"103\":1,\"126\":1,\"164\":1,\"181\":1,\"318\":1,\"329\":1,\"336\":1,\"435\":1}}],[\"intrinsically\",{\"1\":{\"362\":1}}],[\"intrinsic\",{\"1\":{\"359\":1,\"424\":1,\"449\":1,\"483\":1}}],[\"intricacies\",{\"1\":{\"266\":1,\"491\":1}}],[\"intricacy\",{\"1\":{\"133\":1,\"331\":1}}],[\"intricate\",{\"1\":{\"123\":1,\"242\":1,\"337\":1,\"338\":1,\"360\":1,\"424\":2,\"479\":1}}],[\"intriguingly\",{\"1\":{\"212\":1}}],[\"intriguing\",{\"1\":{\"100\":1}}],[\"into\",{\"0\":{\"141\":1,\"174\":1,\"249\":1,\"371\":1,\"419\":1,\"420\":1,\"432\":1},\"1\":{\"97\":1,\"100\":1,\"102\":1,\"103\":2,\"110\":1,\"114\":1,\"117\":1,\"118\":1,\"119\":1,\"120\":1,\"122\":1,\"123\":1,\"128\":2,\"132\":1,\"136\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"149\":1,\"151\":1,\"155\":1,\"159\":1,\"160\":2,\"163\":1,\"172\":2,\"182\":2,\"187\":1,\"188\":1,\"190\":1,\"194\":1,\"199\":2,\"205\":1,\"207\":3,\"213\":1,\"228\":1,\"231\":2,\"237\":1,\"239\":1,\"243\":1,\"246\":1,\"250\":1,\"252\":1,\"253\":1,\"258\":1,\"259\":1,\"266\":2,\"269\":1,\"270\":2,\"277\":1,\"282\":1,\"284\":2,\"290\":1,\"291\":1,\"296\":2,\"297\":3,\"298\":1,\"301\":2,\"303\":1,\"312\":1,\"313\":1,\"314\":1,\"316\":1,\"317\":1,\"319\":2,\"321\":1,\"326\":1,\"328\":2,\"330\":1,\"333\":1,\"334\":1,\"341\":1,\"348\":1,\"350\":1,\"356\":1,\"359\":1,\"360\":1,\"362\":3,\"364\":1,\"365\":2,\"369\":2,\"370\":2,\"375\":1,\"379\":1,\"389\":1,\"393\":1,\"394\":1,\"398\":1,\"399\":1,\"400\":1,\"407\":1,\"408\":1,\"416\":1,\"419\":1,\"420\":1,\"424\":1,\"426\":1,\"429\":1,\"432\":1,\"433\":1,\"434\":1,\"438\":4,\"439\":1,\"443\":1,\"450\":1,\"463\":1,\"465\":1,\"469\":1,\"472\":1,\"476\":1,\"479\":1,\"482\":1,\"485\":2,\"494\":1}}],[\"intel\",{\"1\":{\"363\":1}}],[\"intellectually\",{\"1\":{\"175\":1}}],[\"intelligibility\",{\"1\":{\"156\":1}}],[\"intelligible\",{\"1\":{\"156\":1}}],[\"intelligence\",{\"0\":{\"211\":1,\"398\":1},\"1\":{\"102\":2,\"125\":1,\"135\":1,\"148\":1,\"171\":1,\"176\":1,\"211\":1,\"212\":1,\"216\":1,\"239\":1,\"244\":1,\"248\":1,\"261\":1,\"268\":1,\"282\":2,\"288\":1,\"303\":1,\"333\":1,\"356\":1,\"363\":1,\"398\":3,\"416\":1,\"446\":1,\"452\":1}}],[\"intelligentization\",{\"1\":{\"138\":1}}],[\"intelligent\",{\"0\":{\"99\":1,\"129\":1,\"130\":1,\"284\":1},\"1\":{\"99\":1,\"100\":1,\"129\":1,\"130\":1,\"138\":1,\"144\":1,\"169\":1,\"196\":1,\"210\":1,\"221\":1,\"222\":1,\"282\":1,\"283\":1,\"284\":1,\"289\":1,\"318\":1,\"388\":1,\"471\":1,\"529\":1}}],[\"intense\",{\"1\":{\"478\":1}}],[\"intensity\",{\"1\":{\"220\":1}}],[\"intensive\",{\"1\":{\"121\":1,\"309\":1,\"384\":1}}],[\"intents\",{\"1\":{\"242\":1}}],[\"intent\",{\"1\":{\"124\":1,\"140\":1,\"222\":1,\"241\":1,\"252\":1,\"275\":1,\"386\":1,\"403\":2,\"426\":1,\"437\":1}}],[\"intentionally\",{\"1\":{\"334\":1}}],[\"intention\",{\"0\":{\"118\":1},\"1\":{\"118\":3}}],[\"intentions\",{\"0\":{\"107\":1},\"1\":{\"107\":1,\"110\":1,\"118\":1,\"124\":1,\"321\":1}}],[\"intended\",{\"1\":{\"114\":1,\"124\":1,\"135\":1,\"161\":3,\"284\":1,\"354\":3,\"460\":1}}],[\"integral\",{\"1\":{\"169\":1}}],[\"integrations\",{\"1\":{\"447\":1}}],[\"integration\",{\"1\":{\"115\":1,\"120\":1,\"138\":1,\"140\":1,\"172\":1,\"201\":2,\"246\":1,\"277\":1,\"291\":2,\"300\":1,\"314\":1,\"319\":1,\"325\":1,\"326\":1,\"358\":1,\"362\":1,\"370\":1,\"384\":1,\"394\":1,\"398\":2,\"447\":1,\"475\":1,\"476\":2,\"477\":1,\"485\":1}}],[\"integrating\",{\"0\":{\"156\":1,\"201\":1,\"264\":1,\"268\":1,\"314\":1,\"448\":1,\"452\":1},\"1\":{\"114\":1,\"119\":1,\"130\":1,\"132\":1,\"136\":1,\"137\":1,\"138\":1,\"156\":1,\"164\":1,\"184\":1,\"201\":2,\"229\":1,\"245\":1,\"246\":1,\"268\":2,\"280\":1,\"305\":1,\"323\":1,\"328\":1,\"330\":1,\"356\":1,\"364\":1,\"372\":1,\"380\":1,\"399\":1,\"452\":2}}],[\"integrates\",{\"1\":{\"127\":1,\"205\":1,\"244\":1,\"329\":1,\"360\":1,\"361\":1}}],[\"integrate\",{\"1\":{\"114\":1,\"142\":1,\"163\":1,\"167\":1,\"231\":1,\"258\":1,\"270\":1,\"314\":2,\"343\":1,\"433\":1}}],[\"integrated\",{\"0\":{\"231\":1},\"1\":{\"102\":1,\"228\":1,\"248\":1,\"266\":1,\"270\":1,\"296\":1,\"298\":1,\"308\":1,\"313\":1,\"361\":1,\"408\":1,\"510\":2,\"519\":1}}],[\"integrity\",{\"1\":{\"167\":1,\"361\":1,\"443\":1}}],[\"integer\",{\"1\":{\"96\":1}}],[\"interior\",{\"0\":{\"399\":1},\"1\":{\"399\":3}}],[\"interrelations\",{\"1\":{\"398\":1}}],[\"interrelationships\",{\"1\":{\"143\":1}}],[\"interleaved\",{\"0\":{\"393\":1}}],[\"interlocutor\",{\"1\":{\"175\":1}}],[\"interlocutors\",{\"1\":{\"108\":1}}],[\"intersecting\",{\"1\":{\"368\":1}}],[\"intersection\",{\"1\":{\"181\":1}}],[\"inter\",{\"1\":{\"238\":1,\"335\":1,\"360\":2,\"384\":1,\"408\":1,\"453\":1,\"479\":2}}],[\"interconnected\",{\"1\":{\"204\":1,\"230\":1}}],[\"international\",{\"1\":{\"396\":2,\"524\":1}}],[\"internal\",{\"1\":{\"199\":1,\"210\":1,\"255\":1,\"283\":1,\"297\":1,\"340\":3,\"390\":1,\"437\":1,\"494\":1}}],[\"internet\",{\"0\":{\"239\":1},\"1\":{\"74\":1,\"133\":1,\"239\":2,\"331\":1,\"519\":1}}],[\"interdisciplinary\",{\"1\":{\"180\":1,\"429\":1}}],[\"interference\",{\"1\":{\"340\":1}}],[\"interfere\",{\"1\":{\"170\":1}}],[\"interface\",{\"0\":{\"294\":1},\"1\":{\"99\":1,\"111\":1,\"119\":1,\"151\":1,\"154\":1,\"158\":3,\"159\":3,\"163\":1,\"166\":1,\"183\":1,\"201\":2,\"209\":1,\"215\":1,\"222\":1,\"270\":1,\"271\":1,\"284\":1,\"287\":1,\"294\":1,\"297\":2,\"348\":1,\"361\":1,\"375\":1,\"457\":1}}],[\"interfaces\",{\"0\":{\"166\":1,\"184\":1,\"201\":1,\"380\":1},\"1\":{\"99\":1,\"138\":1,\"140\":1,\"149\":1,\"154\":3,\"158\":1,\"172\":1,\"184\":5,\"201\":1,\"207\":2,\"231\":2,\"242\":2,\"259\":1,\"282\":1,\"297\":2,\"370\":1,\"380\":5}}],[\"interoperable\",{\"1\":{\"157\":1}}],[\"interpersonal\",{\"1\":{\"278\":1}}],[\"interplays\",{\"1\":{\"467\":1}}],[\"interplay\",{\"1\":{\"152\":1,\"273\":1,\"328\":1}}],[\"interpret\",{\"1\":{\"284\":1}}],[\"interpreter\",{\"1\":{\"259\":1}}],[\"interpretations\",{\"1\":{\"333\":1}}],[\"interpretation\",{\"1\":{\"188\":1,\"195\":1,\"242\":1}}],[\"interpretable\",{\"0\":{\"183\":1,\"213\":1},\"1\":{\"368\":1}}],[\"interpretability\",{\"1\":{\"104\":1,\"131\":1,\"183\":1,\"199\":2,\"281\":2,\"353\":1,\"390\":3}}],[\"interpreting\",{\"0\":{\"490\":1},\"1\":{\"111\":2,\"118\":1,\"199\":1,\"222\":1,\"281\":1,\"416\":1,\"477\":1}}],[\"intervenes\",{\"1\":{\"390\":1}}],[\"interventions\",{\"0\":{\"120\":1},\"1\":{\"120\":1,\"150\":2,\"158\":1,\"222\":1,\"237\":2,\"261\":3,\"274\":1,\"279\":3,\"468\":2}}],[\"intervention\",{\"0\":{\"150\":1},\"1\":{\"112\":1,\"120\":2,\"150\":3,\"161\":1,\"237\":1,\"274\":1,\"354\":1,\"477\":1,\"491\":1}}],[\"interval\",{\"1\":{\"143\":1}}],[\"interviewed\",{\"1\":{\"164\":2,\"252\":1,\"308\":1,\"426\":1}}],[\"interviews\",{\"1\":{\"106\":1,\"120\":1,\"122\":1,\"135\":1,\"148\":1,\"154\":1,\"156\":1,\"172\":2,\"212\":1,\"252\":1,\"286\":1,\"290\":1,\"291\":1,\"293\":1,\"309\":1,\"370\":2,\"426\":1,\"474\":1,\"476\":1}}],[\"interview\",{\"1\":{\"98\":1,\"164\":1,\"175\":1,\"210\":1,\"272\":1,\"277\":1,\"279\":1}}],[\"intermediate\",{\"1\":{\"110\":1,\"190\":1,\"224\":1,\"236\":1,\"275\":1,\"313\":1,\"321\":1}}],[\"interacts\",{\"1\":{\"418\":1}}],[\"interacted\",{\"1\":{\"135\":1,\"215\":1}}],[\"interactivity\",{\"1\":{\"159\":1,\"303\":1}}],[\"interactively\",{\"0\":{\"159\":1,\"210\":1},\"1\":{\"99\":1,\"153\":1,\"210\":1,\"213\":1,\"258\":1,\"270\":1,\"433\":1}}],[\"interactive\",{\"0\":{\"104\":1,\"111\":1,\"112\":1,\"133\":1,\"146\":1,\"153\":1,\"158\":1,\"204\":1,\"207\":1,\"218\":1,\"229\":1,\"231\":1,\"238\":1,\"258\":1,\"260\":1,\"269\":1,\"289\":1,\"294\":1,\"331\":1,\"375\":1,\"433\":1,\"486\":1},\"1\":{\"97\":2,\"99\":1,\"104\":1,\"110\":1,\"119\":1,\"133\":1,\"141\":1,\"146\":2,\"159\":3,\"182\":1,\"204\":1,\"205\":1,\"207\":5,\"211\":1,\"213\":2,\"217\":3,\"218\":1,\"224\":1,\"227\":1,\"229\":3,\"231\":3,\"238\":3,\"242\":1,\"243\":1,\"249\":1,\"258\":2,\"259\":2,\"260\":2,\"270\":1,\"289\":3,\"291\":1,\"296\":1,\"300\":1,\"305\":1,\"312\":2,\"318\":3,\"321\":1,\"331\":1,\"413\":1,\"433\":2,\"476\":1,\"481\":1,\"486\":2}}],[\"interacting\",{\"1\":{\"100\":1,\"107\":3,\"125\":1,\"183\":1,\"186\":1,\"211\":1,\"269\":1,\"277\":1,\"326\":1,\"447\":1}}],[\"interactions\",{\"0\":{\"106\":1,\"115\":1},\"1\":{\"106\":1,\"108\":1,\"111\":1,\"115\":2,\"149\":1,\"163\":1,\"172\":1,\"174\":1,\"183\":1,\"190\":1,\"191\":2,\"205\":1,\"207\":1,\"211\":1,\"222\":1,\"244\":1,\"252\":1,\"253\":1,\"261\":1,\"266\":3,\"282\":1,\"297\":2,\"303\":1,\"305\":1,\"307\":1,\"370\":1,\"371\":1,\"426\":1,\"429\":1,\"479\":2}}],[\"interaction\",{\"0\":{\"125\":1,\"126\":1,\"149\":1,\"150\":1,\"174\":1,\"191\":1,\"261\":1,\"271\":1,\"307\":1,\"371\":1,\"457\":1,\"466\":1},\"1\":{\"99\":1,\"100\":1,\"115\":1,\"119\":1,\"125\":2,\"126\":2,\"130\":1,\"137\":1,\"142\":1,\"148\":1,\"149\":2,\"151\":1,\"158\":4,\"159\":1,\"163\":1,\"174\":7,\"183\":2,\"191\":1,\"195\":1,\"198\":1,\"201\":3,\"203\":1,\"204\":1,\"207\":1,\"229\":1,\"235\":1,\"240\":2,\"244\":1,\"245\":1,\"253\":1,\"264\":1,\"266\":1,\"269\":2,\"271\":5,\"273\":1,\"283\":1,\"286\":1,\"294\":1,\"297\":1,\"307\":4,\"315\":1,\"335\":1,\"344\":1,\"347\":1,\"348\":1,\"371\":7,\"387\":1,\"411\":1,\"430\":1,\"448\":1,\"457\":5,\"466\":1,\"474\":1,\"480\":1,\"492\":1}}],[\"interact\",{\"1\":{\"98\":1,\"111\":2,\"125\":1,\"148\":1,\"151\":1,\"167\":1,\"207\":1,\"218\":1,\"252\":1,\"263\":1,\"275\":1,\"307\":1,\"326\":1,\"328\":1,\"348\":1,\"426\":1,\"466\":1}}],[\"interestingly\",{\"1\":{\"251\":1,\"313\":1,\"423\":1}}],[\"interesting\",{\"1\":{\"159\":1,\"252\":1,\"301\":1,\"353\":1,\"426\":1}}],[\"interestingness\",{\"1\":{\"155\":1}}],[\"interest\",{\"0\":{\"155\":1},\"1\":{\"112\":2,\"119\":1,\"151\":1,\"161\":1,\"164\":1,\"166\":3,\"181\":1,\"205\":1,\"269\":1,\"282\":1,\"292\":1,\"337\":1,\"348\":1,\"354\":1,\"417\":1,\"436\":1,\"460\":1}}],[\"interests\",{\"1\":{\"97\":1,\"312\":1,\"498\":1}}],[\"interested\",{\"1\":{\"74\":1,\"382\":1}}],[\"inthe\",{\"1\":{\"74\":1}}],[\"in\",{\"0\":{\"105\":1,\"108\":1,\"110\":1,\"111\":1,\"112\":2,\"115\":1,\"119\":1,\"120\":1,\"122\":1,\"125\":1,\"135\":1,\"137\":1,\"150\":1,\"158\":1,\"160\":1,\"161\":2,\"163\":1,\"170\":2,\"171\":1,\"172\":2,\"176\":1,\"178\":1,\"182\":1,\"196\":1,\"197\":1,\"200\":1,\"203\":1,\"204\":1,\"212\":1,\"219\":1,\"231\":1,\"233\":1,\"236\":1,\"237\":1,\"239\":1,\"242\":1,\"243\":1,\"244\":1,\"245\":1,\"251\":1,\"252\":1,\"253\":1,\"255\":1,\"264\":1,\"265\":1,\"273\":1,\"274\":1,\"275\":1,\"277\":1,\"286\":1,\"289\":1,\"290\":1,\"292\":1,\"297\":1,\"302\":1,\"304\":1,\"306\":1,\"308\":1,\"317\":1,\"319\":1,\"321\":1,\"323\":1,\"324\":1,\"345\":1,\"350\":1,\"351\":1,\"354\":2,\"366\":1,\"368\":1,\"370\":2,\"381\":1,\"384\":1,\"385\":1,\"390\":1,\"391\":1,\"400\":1,\"401\":1,\"403\":1,\"408\":2,\"412\":1,\"416\":1,\"423\":1,\"425\":1,\"426\":1,\"428\":1,\"430\":1,\"437\":1,\"448\":1,\"456\":1,\"466\":1,\"469\":1,\"474\":1,\"477\":1,\"478\":1,\"479\":1,\"480\":1,\"488\":1,\"490\":1,\"492\":1,\"497\":1,\"519\":1},\"1\":{\"74\":2,\"96\":4,\"97\":4,\"98\":3,\"100\":2,\"101\":5,\"102\":6,\"103\":4,\"105\":1,\"106\":4,\"107\":1,\"108\":5,\"110\":4,\"111\":5,\"112\":9,\"113\":3,\"114\":1,\"115\":1,\"116\":2,\"117\":2,\"118\":8,\"119\":8,\"120\":5,\"121\":3,\"122\":1,\"123\":7,\"124\":5,\"125\":4,\"126\":6,\"127\":2,\"128\":2,\"129\":1,\"130\":5,\"131\":2,\"132\":5,\"133\":4,\"135\":5,\"136\":2,\"137\":6,\"138\":3,\"140\":3,\"141\":1,\"142\":5,\"143\":1,\"144\":1,\"146\":3,\"148\":4,\"149\":6,\"150\":3,\"151\":3,\"152\":2,\"153\":4,\"154\":2,\"155\":4,\"156\":7,\"157\":2,\"158\":2,\"159\":3,\"160\":4,\"161\":5,\"163\":5,\"164\":3,\"165\":8,\"166\":4,\"167\":4,\"169\":4,\"170\":3,\"171\":5,\"172\":6,\"173\":2,\"174\":4,\"175\":4,\"176\":7,\"178\":4,\"179\":2,\"180\":1,\"181\":3,\"182\":4,\"183\":2,\"184\":2,\"185\":1,\"186\":5,\"187\":1,\"188\":3,\"189\":7,\"190\":5,\"191\":2,\"192\":6,\"194\":5,\"195\":6,\"196\":7,\"197\":5,\"198\":2,\"199\":4,\"200\":2,\"201\":2,\"202\":2,\"203\":1,\"204\":4,\"205\":4,\"206\":2,\"207\":4,\"211\":1,\"212\":4,\"213\":5,\"215\":7,\"216\":6,\"217\":2,\"218\":4,\"219\":4,\"220\":2,\"221\":1,\"222\":4,\"223\":3,\"224\":2,\"226\":3,\"227\":4,\"228\":2,\"229\":4,\"230\":3,\"231\":3,\"232\":5,\"233\":4,\"234\":3,\"235\":6,\"236\":5,\"237\":6,\"238\":7,\"239\":3,\"240\":2,\"241\":1,\"242\":2,\"243\":6,\"244\":8,\"245\":4,\"246\":3,\"248\":5,\"249\":5,\"250\":1,\"251\":7,\"252\":4,\"253\":6,\"254\":3,\"255\":5,\"256\":4,\"257\":6,\"259\":2,\"260\":9,\"261\":4,\"263\":2,\"264\":4,\"265\":6,\"266\":1,\"268\":5,\"269\":4,\"270\":2,\"271\":1,\"272\":7,\"273\":5,\"274\":5,\"275\":4,\"277\":3,\"278\":3,\"279\":4,\"280\":3,\"281\":2,\"282\":4,\"283\":9,\"284\":1,\"286\":7,\"287\":1,\"288\":2,\"289\":3,\"290\":6,\"291\":2,\"292\":7,\"293\":3,\"294\":4,\"295\":1,\"296\":1,\"297\":2,\"298\":5,\"300\":6,\"301\":5,\"302\":3,\"303\":1,\"304\":3,\"305\":4,\"307\":5,\"308\":6,\"309\":4,\"312\":4,\"313\":4,\"314\":3,\"315\":6,\"317\":6,\"318\":4,\"319\":4,\"321\":4,\"322\":4,\"323\":6,\"324\":4,\"325\":4,\"326\":3,\"328\":7,\"329\":2,\"330\":5,\"331\":4,\"333\":3,\"334\":6,\"335\":4,\"336\":5,\"337\":3,\"339\":4,\"340\":4,\"343\":1,\"344\":2,\"345\":4,\"347\":2,\"348\":3,\"349\":4,\"350\":5,\"351\":3,\"352\":3,\"353\":3,\"354\":5,\"355\":3,\"356\":5,\"358\":4,\"359\":2,\"360\":7,\"362\":4,\"363\":8,\"364\":3,\"365\":6,\"366\":3,\"368\":1,\"369\":3,\"370\":6,\"371\":4,\"372\":2,\"374\":2,\"375\":1,\"376\":1,\"377\":6,\"378\":2,\"379\":2,\"380\":2,\"381\":8,\"382\":8,\"384\":2,\"385\":2,\"386\":2,\"387\":2,\"388\":4,\"389\":3,\"390\":4,\"391\":5,\"392\":4,\"393\":1,\"394\":6,\"395\":6,\"396\":3,\"398\":3,\"399\":3,\"400\":5,\"401\":4,\"402\":3,\"403\":4,\"404\":2,\"405\":5,\"407\":1,\"408\":4,\"409\":6,\"410\":3,\"411\":5,\"412\":5,\"413\":8,\"414\":1,\"415\":6,\"417\":4,\"418\":4,\"419\":5,\"420\":5,\"422\":5,\"423\":7,\"424\":8,\"425\":5,\"426\":4,\"427\":5,\"428\":6,\"429\":3,\"430\":5,\"431\":3,\"432\":7,\"434\":4,\"435\":6,\"436\":2,\"437\":12,\"438\":1,\"439\":5,\"441\":2,\"442\":3,\"443\":4,\"444\":3,\"445\":3,\"446\":1,\"447\":3,\"448\":4,\"449\":3,\"450\":4,\"452\":5,\"453\":3,\"454\":2,\"455\":4,\"456\":4,\"457\":1,\"458\":7,\"459\":6,\"460\":6,\"461\":4,\"462\":8,\"463\":4,\"465\":2,\"466\":2,\"467\":7,\"468\":2,\"469\":8,\"470\":4,\"471\":3,\"472\":4,\"474\":7,\"475\":2,\"476\":2,\"477\":5,\"478\":3,\"479\":4,\"480\":1,\"481\":6,\"482\":5,\"483\":2,\"485\":4,\"486\":1,\"488\":10,\"489\":4,\"490\":5,\"491\":3,\"492\":8,\"494\":4,\"495\":6,\"496\":3,\"497\":4,\"498\":4,\"502\":1,\"510\":1,\"533\":1}}],[\"inductive\",{\"0\":{\"424\":1},\"1\":{\"424\":1}}],[\"induce\",{\"1\":{\"403\":1}}],[\"industries\",{\"1\":{\"212\":2,\"465\":1}}],[\"industrial\",{\"1\":{\"179\":1,\"246\":2,\"253\":1,\"436\":1,\"479\":1}}],[\"industry\",{\"0\":{\"6\":1,\"519\":1},\"1\":{\"106\":1,\"114\":1,\"201\":2,\"223\":1,\"243\":1,\"252\":1,\"291\":1,\"300\":1,\"363\":1,\"377\":1,\"426\":1,\"472\":1,\"476\":1,\"482\":1,\"502\":1,\"510\":1,\"519\":4}}],[\"indeed\",{\"1\":{\"470\":1}}],[\"indefinitely\",{\"1\":{\"415\":1}}],[\"indelab\",{\"0\":{\"388\":1},\"1\":{\"388\":1}}],[\"index\",{\"1\":{\"369\":1,\"477\":1}}],[\"indexing\",{\"0\":{\"156\":1},\"1\":{\"156\":2,\"384\":1}}],[\"independent\",{\"1\":{\"184\":1,\"230\":1,\"275\":1,\"291\":1,\"324\":1,\"340\":1,\"378\":1,\"380\":1,\"395\":1,\"476\":1,\"479\":1}}],[\"independently\",{\"1\":{\"183\":1,\"335\":1,\"415\":1}}],[\"independence\",{\"1\":{\"166\":2,\"184\":1,\"380\":1}}],[\"indesign\",{\"1\":{\"8\":1}}],[\"indices\",{\"1\":{\"400\":3}}],[\"indicator\",{\"1\":{\"273\":2}}],[\"indicators\",{\"1\":{\"146\":2,\"335\":1,\"400\":2}}],[\"indicative\",{\"1\":{\"194\":1}}],[\"indicating\",{\"1\":{\"120\":1,\"121\":1,\"186\":1,\"213\":1,\"283\":1,\"290\":1}}],[\"indicated\",{\"1\":{\"164\":1,\"166\":1,\"222\":1,\"227\":1,\"273\":1,\"408\":1}}],[\"indicates\",{\"1\":{\"154\":1,\"216\":1,\"458\":1,\"463\":1,\"492\":1}}],[\"indicate\",{\"1\":{\"127\":1,\"135\":1,\"152\":1,\"155\":1,\"185\":1,\"190\":1,\"195\":1,\"197\":1,\"198\":1,\"226\":1,\"234\":1,\"257\":1,\"278\":1,\"322\":1,\"336\":1,\"339\":1,\"384\":1,\"387\":1,\"404\":1,\"422\":1,\"425\":1,\"461\":1}}],[\"indistinguishable\",{\"1\":{\"266\":1,\"410\":1}}],[\"indispensable\",{\"1\":{\"219\":1,\"401\":1}}],[\"indian\",{\"1\":{\"172\":1,\"370\":1}}],[\"india\",{\"0\":{\"172\":1,\"370\":1}}],[\"individuals\",{\"1\":{\"120\":2,\"144\":1,\"160\":1,\"184\":2,\"187\":1,\"203\":1,\"219\":2,\"233\":1,\"245\":1,\"261\":1,\"380\":2,\"392\":1,\"395\":1,\"401\":2,\"461\":1}}],[\"individual\",{\"1\":{\"106\":1,\"123\":2,\"130\":1,\"135\":1,\"206\":1,\"237\":1,\"269\":1,\"400\":1,\"428\":1,\"468\":1}}],[\"indoor\",{\"1\":{\"105\":2}}],[\"ipek\",{\"1\":{\"336\":1}}],[\"ipedia\",{\"1\":{\"9\":2}}],[\"ip价值评估\",{\"1\":{\"36\":1}}],[\"ip的版权与大部分的收益往往归属于头部的中心化机构\",{\"1\":{\"35\":1}}],[\"ip产业链条长而复杂\",{\"1\":{\"35\":1}}],[\"ip传统商业模型和痛点\",{\"0\":{\"35\":1}}],[\"ip市场现状\",{\"0\":{\"34\":1}}],[\"ip孵化的传统逻辑\",{\"1\":{\"33\":1}}],[\"ip雪橇熊参展上海web3\",{\"1\":{\"4\":1}}],[\"ip\",{\"1\":{\"4\":1,\"9\":1,\"36\":2,\"96\":3}}],[\"ip合作\",{\"1\":{\"4\":1}}],[\"ip运营\",{\"1\":{\"4\":1}}],[\"汽车涂鸦绘画\",{\"1\":{\"4\":1}}],[\"x4qb7\",{\"1\":{\"259\":1}}],[\"xgboost\",{\"1\":{\"237\":1}}],[\"xueqi\",{\"1\":{\"463\":1}}],[\"xun\",{\"1\":{\"429\":1}}],[\"xunliang\",{\"1\":{\"329\":1}}],[\"xuandong\",{\"1\":{\"428\":1}}],[\"xuyang\",{\"1\":{\"394\":1}}],[\"xu\",{\"1\":{\"138\":2,\"223\":1,\"235\":2,\"260\":1,\"268\":1,\"316\":1,\"318\":1,\"361\":1,\"362\":1,\"403\":1,\"449\":2,\"452\":1,\"472\":1,\"480\":1,\"485\":1,\"489\":1}}],[\"xenos\",{\"1\":{\"123\":1}}],[\"xiuwei\",{\"1\":{\"488\":1}}],[\"xiuhong\",{\"1\":{\"414\":1}}],[\"xize\",{\"1\":{\"378\":1}}],[\"ximeng\",{\"1\":{\"376\":1}}],[\"xilinx\",{\"1\":{\"363\":1}}],[\"xie\",{\"1\":{\"353\":1,\"411\":1}}],[\"xieyang\",{\"1\":{\"114\":1}}],[\"xiyan\",{\"1\":{\"353\":1}}],[\"xiong\",{\"1\":{\"198\":1,\"387\":1,\"443\":1}}],[\"xi\",{\"1\":{\"116\":1,\"118\":1,\"399\":1}}],[\"xianpei\",{\"1\":{\"329\":1}}],[\"xiangfeng\",{\"1\":{\"439\":1}}],[\"xiangmin\",{\"1\":{\"223\":1}}],[\"xiang\",{\"1\":{\"176\":1,\"283\":1,\"345\":1,\"412\":1,\"471\":2}}],[\"xiaojin\",{\"1\":{\"483\":1}}],[\"xiaojiang\",{\"1\":{\"103\":1}}],[\"xiaokang\",{\"1\":{\"480\":1}}],[\"xiaolu\",{\"1\":{\"479\":1}}],[\"xiaolong\",{\"1\":{\"318\":1}}],[\"xiaoming\",{\"1\":{\"486\":1}}],[\"xiaomin\",{\"1\":{\"475\":1}}],[\"xiaoqian\",{\"1\":{\"393\":1}}],[\"xiaoxin\",{\"1\":{\"391\":1}}],[\"xiaozhong\",{\"1\":{\"324\":1,\"392\":1}}],[\"xiaowei\",{\"1\":{\"245\":1}}],[\"xiaofei\",{\"1\":{\"235\":1}}],[\"xiaoyuan\",{\"1\":{\"261\":1}}],[\"xiaoyi\",{\"1\":{\"210\":1}}],[\"xiaoyan\",{\"1\":{\"195\":1,\"439\":1}}],[\"xiao\",{\"1\":{\"192\":1,\"228\":1,\"251\":1,\"349\":1,\"423\":1,\"497\":1}}],[\"xiaoru\",{\"1\":{\"141\":1}}],[\"xia\",{\"1\":{\"105\":1,\"429\":1,\"482\":1}}],[\"xinchen\",{\"1\":{\"377\":1}}],[\"xinrun\",{\"1\":{\"377\":1}}],[\"xinyuan\",{\"1\":{\"353\":1}}],[\"xinying\",{\"1\":{\"228\":1}}],[\"xinning\",{\"1\":{\"293\":1}}],[\"xingxuan\",{\"1\":{\"443\":1}}],[\"xingcheng\",{\"1\":{\"414\":1}}],[\"xingyao\",{\"1\":{\"411\":1}}],[\"xingyu\",{\"1\":{\"243\":1}}],[\"xingwei\",{\"1\":{\"377\":1}}],[\"xingmei\",{\"1\":{\"318\":1}}],[\"xingbo\",{\"1\":{\"242\":1}}],[\"xinhai\",{\"1\":{\"176\":1}}],[\"xinzhi\",{\"1\":{\"150\":1}}],[\"xin\",{\"1\":{\"103\":1,\"130\":1,\"295\":1,\"361\":1,\"413\":1}}],[\"xai\",{\"0\":{\"131\":1,\"171\":1,\"231\":1},\"1\":{\"102\":2,\"131\":3,\"159\":2,\"231\":5,\"288\":1}}],[\"xr\",{\"1\":{\"26\":1}}],[\"x\",{\"1\":{\"4\":1,\"24\":4,\"175\":1,\"281\":3,\"335\":1,\"519\":1,\"539\":4}}],[\"月刊的电子封面展示\",{\"1\":{\"9\":1}}],[\"月刊收录\",{\"1\":{\"9\":1}}],[\"月杂志合作艺术家\",{\"1\":{\"9\":1}}],[\"月由羊城设计联盟\",{\"1\":{\"7\":1}}],[\"月由全国高等艺术院校建筑与环境艺术设计教学年会组委会颁发的第十六届全国高等院校艺术类建筑与设计专业教学年会银奖\",{\"1\":{\"7\":1}}],[\"月由亚洲设计学院奖组委会和亚洲建筑与城市联盟颁发的第17届亚洲设计学院奖文化建筑与空间组优秀奖\",{\"1\":{\"7\":1}}],[\"月大湾区首届元宇宙数字艺术节加密艺术类优秀奖\",{\"1\":{\"7\":1}}],[\"月大湾区首届元宇宙数字艺术节元宇宙空间创意类铜奖\",{\"1\":{\"7\":1}}],[\"月\",{\"1\":{\"4\":1,\"7\":3,\"9\":8}}],[\"年垃圾游乐场的诞生和发展历史\",{\"1\":{\"530\":1}}],[\"年虚构的垃圾城市故事\",{\"1\":{\"530\":1}}],[\"年最热门的技能\",{\"0\":{\"518\":1},\"1\":{\"502\":1}}],[\"年代\",{\"1\":{\"49\":1}}],[\"年元宇宙时装周上\",{\"1\":{\"42\":1}}],[\"年以来\",{\"1\":{\"36\":1}}],[\"年重庆\",{\"1\":{\"9\":1}}],[\"年\",{\"1\":{\"4\":1,\"7\":6,\"9\":11,\"36\":1,\"42\":1}}],[\"芒果tv等媒体活动的合作机会\",{\"1\":{\"507\":1}}],[\"芒果tv\",{\"1\":{\"4\":1}}],[\"雪橇熊\",{\"1\":{\"4\":1,\"23\":1}}],[\"雪橇熊x大艺博特别版\",{\"1\":{\"4\":1}}],[\"雪橇熊nftcn\",{\"1\":{\"4\":1}}],[\"制作\",{\"1\":{\"4\":1}}],[\"b测试\",{\"1\":{\"539\":1}}],[\"b2b服务模式为企业定制解决方案\",{\"1\":{\"520\":1}}],[\"b2c\",{\"1\":{\"519\":1}}],[\"b+i\",{\"1\":{\"510\":1}}],[\"b=商业\",{\"1\":{\"510\":1}}],[\"bfp\",{\"1\":{\"469\":2}}],[\"bühler\",{\"1\":{\"273\":1}}],[\"bhati\",{\"1\":{\"270\":1}}],[\"bhatia\",{\"1\":{\"133\":1,\"331\":1}}],[\"bharucha\",{\"1\":{\"220\":1}}],[\"bcis\",{\"1\":{\"166\":1}}],[\"bci\",{\"1\":{\"166\":6,\"188\":1}}],[\"bryan\",{\"1\":{\"213\":1,\"376\":2}}],[\"breed\",{\"1\":{\"375\":1}}],[\"breast\",{\"1\":{\"372\":1}}],[\"breakthroughs\",{\"1\":{\"469\":1}}],[\"break\",{\"1\":{\"344\":1,\"434\":1,\"479\":1}}],[\"breakdowns\",{\"1\":{\"290\":1}}],[\"breaking\",{\"0\":{\"479\":1},\"1\":{\"268\":1,\"452\":1}}],[\"breadth\",{\"1\":{\"287\":1,\"353\":1}}],[\"breathing\",{\"0\":{\"141\":1}}],[\"breda\",{\"1\":{\"207\":1}}],[\"bramblett\",{\"1\":{\"490\":1}}],[\"bradley\",{\"1\":{\"388\":1}}],[\"brake\",{\"1\":{\"335\":1}}],[\"brain\",{\"0\":{\"166\":1},\"1\":{\"166\":1,\"188\":1,\"189\":1,\"223\":1,\"284\":5}}],[\"branch\",{\"0\":{\"471\":1},\"1\":{\"360\":3}}],[\"branches\",{\"0\":{\"360\":1},\"1\":{\"119\":1,\"360\":4,\"398\":1}}],[\"branching\",{\"1\":{\"119\":2}}],[\"bruno\",{\"1\":{\"158\":1,\"384\":1,\"390\":1}}],[\"brumar\",{\"1\":{\"111\":1}}],[\"bronzini\",{\"1\":{\"390\":1}}],[\"browsing\",{\"1\":{\"296\":1,\"361\":1}}],[\"browsers\",{\"1\":{\"128\":3}}],[\"browser\",{\"0\":{\"128\":1},\"1\":{\"128\":3,\"146\":1}}],[\"brooks\",{\"1\":{\"170\":1}}],[\"brought\",{\"1\":{\"164\":1,\"243\":1,\"409\":1,\"415\":1,\"465\":1}}],[\"broadening\",{\"1\":{\"377\":1}}],[\"broadens\",{\"1\":{\"269\":1}}],[\"broader\",{\"1\":{\"180\":1,\"192\":1,\"216\":1,\"222\":1,\"236\":1,\"251\":1,\"291\":1,\"295\":1,\"408\":1,\"423\":1,\"476\":1,\"477\":1}}],[\"broadly\",{\"1\":{\"156\":1,\"428\":1}}],[\"broad\",{\"1\":{\"146\":1,\"178\":1,\"258\":1,\"344\":1,\"433\":1}}],[\"bright\",{\"1\":{\"302\":1}}],[\"bridging\",{\"0\":{\"216\":1,\"492\":1},\"1\":{\"297\":1,\"408\":1,\"430\":1}}],[\"bridges\",{\"1\":{\"438\":1}}],[\"bridget\",{\"1\":{\"226\":1}}],[\"bridge\",{\"0\":{\"297\":1},\"1\":{\"42\":1,\"169\":1,\"212\":1,\"323\":1,\"328\":1,\"395\":1,\"437\":1}}],[\"brittleness\",{\"1\":{\"482\":1}}],[\"brittle\",{\"1\":{\"395\":1}}],[\"britain\",{\"1\":{\"156\":1}}],[\"british\",{\"1\":{\"74\":1}}],[\"briefly\",{\"1\":{\"293\":1}}],[\"brief\",{\"0\":{\"261\":1},\"1\":{\"153\":1,\"261\":1,\"467\":1}}],[\"bringing\",{\"0\":{\"465\":1}}],[\"bring\",{\"1\":{\"119\":1}}],[\"brian\",{\"1\":{\"111\":1,\"131\":1,\"389\":1,\"533\":1}}],[\"bloom\",{\"1\":{\"486\":1}}],[\"blocks\",{\"0\":{\"275\":1},\"1\":{\"275\":1,\"389\":1,\"427\":1}}],[\"blocking\",{\"1\":{\"230\":2}}],[\"blocked\",{\"1\":{\"230\":2}}],[\"block\",{\"0\":{\"469\":1},\"1\":{\"213\":1,\"230\":1,\"355\":1,\"469\":4}}],[\"blockchain\",{\"0\":{\"192\":1},\"1\":{\"192\":6}}],[\"blip\",{\"1\":{\"319\":1}}],[\"blind\",{\"1\":{\"277\":1}}],[\"blaise\",{\"1\":{\"435\":1}}],[\"blaž\",{\"1\":{\"301\":1}}],[\"black\",{\"1\":{\"106\":1,\"171\":1,\"253\":1,\"325\":1,\"334\":1,\"352\":1,\"403\":1}}],[\"blv\",{\"1\":{\"277\":3}}],[\"ble\",{\"1\":{\"414\":1}}],[\"bleu\",{\"1\":{\"219\":1,\"365\":1,\"401\":1}}],[\"blend\",{\"1\":{\"248\":1}}],[\"blends\",{\"1\":{\"216\":1}}],[\"blender\",{\"1\":{\"8\":1,\"509\":1}}],[\"blánaid\",{\"1\":{\"146\":1}}],[\"bultan\",{\"1\":{\"422\":1}}],[\"bulea\",{\"1\":{\"220\":1}}],[\"budget\",{\"0\":{\"366\":1,\"441\":1},\"1\":{\"366\":2,\"441\":1,\"459\":1}}],[\"buggy\",{\"1\":{\"363\":1}}],[\"bugsinpy\",{\"1\":{\"350\":1}}],[\"bugs\",{\"1\":{\"350\":5}}],[\"bug\",{\"1\":{\"350\":5}}],[\"burcu\",{\"1\":{\"466\":1}}],[\"burgeoning\",{\"1\":{\"429\":1}}],[\"burns\",{\"1\":{\"281\":1}}],[\"burdens\",{\"1\":{\"359\":1}}],[\"burden\",{\"1\":{\"143\":1,\"185\":1,\"434\":1}}],[\"bumbacher\",{\"1\":{\"292\":1}}],[\"bum\",{\"1\":{\"213\":1}}],[\"buck\",{\"0\":{\"192\":1}}],[\"buckingham\",{\"1\":{\"125\":1}}],[\"businesses\",{\"1\":{\"447\":1}}],[\"business\",{\"1\":{\"183\":1,\"443\":1,\"510\":1}}],[\"bubbles\",{\"1\":{\"167\":3}}],[\"built\",{\"1\":{\"104\":1,\"232\":1,\"252\":1,\"270\":1,\"340\":1,\"426\":1,\"428\":1,\"480\":1,\"486\":1}}],[\"building\",{\"0\":{\"112\":1,\"361\":1},\"1\":{\"115\":1,\"142\":1,\"164\":1,\"169\":1,\"171\":1,\"178\":1,\"201\":1,\"227\":1,\"294\":1,\"361\":1,\"393\":1,\"404\":1,\"417\":1,\"437\":1}}],[\"builds\",{\"1\":{\"104\":1,\"388\":1}}],[\"build\",{\"1\":{\"99\":1,\"133\":1,\"135\":1,\"159\":1,\"172\":1,\"218\":1,\"231\":1,\"248\":2,\"275\":1,\"331\":1,\"361\":1,\"370\":1,\"381\":2,\"388\":1,\"450\":1,\"455\":1,\"486\":1,\"488\":2}}],[\"builders\",{\"1\":{\"248\":1}}],[\"builder\",{\"0\":{\"99\":1},\"1\":{\"99\":2}}],[\"butler\",{\"1\":{\"379\":1}}],[\"buttons\",{\"1\":{\"207\":1}}],[\"but\",{\"0\":{\"209\":1,\"241\":1},\"1\":{\"99\":1,\"100\":1,\"102\":1,\"106\":2,\"107\":3,\"114\":1,\"124\":1,\"125\":1,\"128\":1,\"151\":1,\"153\":1,\"159\":1,\"160\":1,\"175\":1,\"181\":2,\"184\":1,\"188\":1,\"204\":1,\"209\":1,\"219\":1,\"220\":1,\"230\":1,\"236\":1,\"241\":1,\"243\":1,\"248\":1,\"253\":1,\"255\":1,\"266\":1,\"279\":1,\"318\":1,\"323\":1,\"324\":1,\"334\":1,\"335\":2,\"344\":1,\"345\":1,\"348\":1,\"355\":1,\"356\":1,\"358\":1,\"368\":1,\"369\":1,\"376\":1,\"380\":1,\"381\":1,\"391\":1,\"393\":1,\"395\":1,\"398\":1,\"401\":1,\"405\":1,\"413\":1,\"422\":1,\"424\":1,\"427\":1,\"434\":1,\"443\":1,\"459\":1,\"460\":2,\"477\":1,\"482\":1,\"488\":1,\"490\":1,\"495\":2,\"497\":1}}],[\"bujak\",{\"1\":{\"81\":1}}],[\"bahe\",{\"1\":{\"479\":5}}],[\"baolong\",{\"1\":{\"463\":1}}],[\"battery\",{\"1\":{\"468\":1}}],[\"batch\",{\"1\":{\"414\":1}}],[\"batmaz\",{\"1\":{\"195\":1}}],[\"bayesian\",{\"0\":{\"478\":1},\"1\":{\"392\":1,\"478\":1}}],[\"bauer\",{\"1\":{\"379\":1}}],[\"bag\",{\"1\":{\"347\":1}}],[\"baghshah\",{\"1\":{\"333\":1}}],[\"bazilinskyy\",{\"1\":{\"297\":1}}],[\"bae\",{\"1\":{\"274\":1}}],[\"babette\",{\"1\":{\"273\":1}}],[\"baba\",{\"1\":{\"160\":1}}],[\"bandwidth\",{\"1\":{\"239\":1}}],[\"bantry\",{\"1\":{\"237\":1}}],[\"bansal2020sam\",{\"1\":{\"159\":1}}],[\"ba\",{\"1\":{\"232\":1,\"525\":1}}],[\"bai\",{\"1\":{\"167\":1}}],[\"balance\",{\"1\":{\"210\":1,\"214\":1}}],[\"balanced\",{\"0\":{\"214\":1},\"1\":{\"106\":1}}],[\"balancing\",{\"0\":{\"167\":1}}],[\"bachmann\",{\"1\":{\"234\":1}}],[\"bach\",{\"1\":{\"217\":1}}],[\"backbones\",{\"0\":{\"472\":1},\"1\":{\"450\":1}}],[\"backbone\",{\"1\":{\"223\":2}}],[\"background\",{\"1\":{\"221\":1,\"237\":1,\"240\":1,\"274\":1,\"282\":2,\"308\":1}}],[\"backgrounds\",{\"1\":{\"133\":1,\"176\":1,\"284\":1,\"331\":1}}],[\"back\",{\"0\":{\"279\":1},\"1\":{\"213\":1,\"361\":1,\"460\":2}}],[\"backyard\",{\"1\":{\"207\":1}}],[\"baca\",{\"1\":{\"164\":1}}],[\"basaldella\",{\"1\":{\"467\":1}}],[\"basak\",{\"1\":{\"431\":1}}],[\"bastien\",{\"1\":{\"444\":1}}],[\"basmov\",{\"1\":{\"340\":1}}],[\"basit\",{\"1\":{\"284\":1}}],[\"basic\",{\"1\":{\"227\":1,\"350\":1,\"362\":1}}],[\"basis\",{\"0\":{\"157\":1},\"1\":{\"183\":1}}],[\"baselines\",{\"1\":{\"313\":1,\"389\":1,\"432\":1,\"449\":1,\"495\":1}}],[\"baseline\",{\"1\":{\"131\":1,\"187\":1,\"190\":1,\"222\":1,\"256\":1,\"415\":1,\"434\":1,\"435\":1,\"445\":1,\"467\":1,\"497\":1}}],[\"base\",{\"1\":{\"131\":2,\"146\":1,\"215\":1,\"323\":1,\"352\":1,\"378\":1,\"384\":1}}],[\"based\",{\"0\":{\"103\":1,\"115\":1,\"116\":1,\"120\":1,\"138\":1,\"149\":1,\"165\":1,\"167\":1,\"186\":1,\"203\":1,\"211\":1,\"249\":1,\"259\":1,\"260\":1,\"264\":1,\"283\":1,\"295\":1,\"304\":1,\"324\":1,\"350\":1,\"360\":1,\"361\":1,\"364\":1,\"382\":1,\"384\":1,\"388\":1,\"402\":1,\"410\":1,\"413\":1,\"422\":1,\"439\":1,\"445\":1,\"448\":1,\"462\":1,\"477\":1,\"495\":1},\"1\":{\"97\":1,\"98\":1,\"100\":1,\"101\":1,\"102\":1,\"103\":1,\"105\":1,\"108\":1,\"110\":4,\"112\":1,\"115\":2,\"116\":2,\"118\":1,\"120\":2,\"123\":1,\"124\":1,\"126\":1,\"128\":1,\"133\":1,\"135\":2,\"139\":1,\"140\":1,\"149\":3,\"150\":2,\"151\":1,\"154\":1,\"155\":1,\"156\":2,\"157\":1,\"158\":2,\"163\":4,\"165\":3,\"167\":3,\"171\":3,\"178\":1,\"179\":2,\"187\":1,\"189\":1,\"191\":1,\"194\":4,\"195\":2,\"196\":2,\"197\":1,\"199\":1,\"209\":1,\"211\":3,\"213\":2,\"216\":1,\"219\":3,\"222\":1,\"226\":1,\"227\":1,\"228\":1,\"232\":1,\"234\":2,\"235\":1,\"237\":2,\"238\":2,\"240\":1,\"242\":4,\"245\":2,\"246\":1,\"251\":2,\"252\":1,\"256\":2,\"260\":1,\"261\":1,\"264\":1,\"266\":2,\"268\":1,\"271\":1,\"272\":1,\"273\":1,\"274\":1,\"283\":1,\"287\":1,\"289\":2,\"291\":1,\"301\":1,\"308\":2,\"312\":1,\"314\":1,\"318\":1,\"319\":1,\"321\":4,\"325\":1,\"329\":2,\"331\":1,\"334\":1,\"335\":1,\"337\":1,\"340\":2,\"344\":2,\"347\":2,\"348\":1,\"350\":1,\"351\":1,\"360\":3,\"361\":2,\"362\":1,\"364\":1,\"366\":1,\"369\":1,\"378\":4,\"384\":1,\"386\":1,\"390\":1,\"391\":4,\"401\":3,\"402\":1,\"407\":1,\"409\":2,\"410\":3,\"413\":1,\"415\":1,\"419\":2,\"420\":2,\"422\":3,\"423\":2,\"425\":1,\"426\":1,\"427\":1,\"431\":1,\"437\":1,\"441\":1,\"445\":4,\"447\":1,\"448\":1,\"450\":2,\"452\":1,\"453\":1,\"456\":1,\"457\":1,\"459\":1,\"460\":3,\"461\":1,\"462\":1,\"467\":1,\"475\":2,\"476\":1,\"477\":1,\"479\":1,\"481\":1,\"483\":2,\"485\":4,\"486\":1,\"491\":1,\"495\":2,\"519\":1}}],[\"bakul\",{\"1\":{\"137\":1}}],[\"barney\",{\"1\":{\"369\":1}}],[\"barrier\",{\"0\":{\"479\":1},\"1\":{\"479\":1}}],[\"barriers\",{\"1\":{\"113\":2,\"122\":1,\"164\":1,\"180\":1,\"355\":1,\"408\":1}}],[\"barr\",{\"1\":{\"350\":1}}],[\"bard\",{\"1\":{\"334\":1}}],[\"bar\",{\"1\":{\"309\":1}}],[\"barker\",{\"1\":{\"305\":1}}],[\"baris\",{\"1\":{\"336\":1}}],[\"bariah\",{\"1\":{\"239\":1}}],[\"barik\",{\"1\":{\"110\":1,\"321\":1}}],[\"barbero\",{\"1\":{\"171\":1}}],[\"barua\",{\"1\":{\"137\":1}}],[\"bartneck\",{\"1\":{\"107\":1}}],[\"bad\",{\"0\":{\"107\":1},\"1\":{\"334\":1}}],[\"bamba\",{\"1\":{\"232\":1}}],[\"bam\",{\"0\":{\"4\":1,\"18\":1},\"1\":{\"14\":2,\"22\":1}}],[\"bohan\",{\"1\":{\"480\":1}}],[\"bohao\",{\"1\":{\"432\":1}}],[\"bosheng\",{\"1\":{\"443\":1}}],[\"boji\",{\"1\":{\"411\":1}}],[\"boukli\",{\"1\":{\"444\":1}}],[\"bouchard\",{\"1\":{\"398\":1}}],[\"bounded\",{\"1\":{\"326\":1}}],[\"bound\",{\"1\":{\"279\":1}}],[\"boundary\",{\"0\":{\"439\":1},\"1\":{\"233\":1,\"439\":1,\"489\":1}}],[\"boundaries\",{\"1\":{\"205\":1,\"304\":1,\"439\":2,\"489\":1}}],[\"bounding\",{\"1\":{\"182\":1}}],[\"bounds\",{\"1\":{\"96\":1}}],[\"bowen\",{\"1\":{\"382\":1,\"411\":1}}],[\"boasts\",{\"1\":{\"349\":1}}],[\"boards\",{\"1\":{\"113\":2,\"306\":1}}],[\"bomb\",{\"1\":{\"334\":1}}],[\"bochen\",{\"1\":{\"246\":1}}],[\"body\",{\"0\":{\"238\":1},\"1\":{\"238\":1,\"246\":2,\"261\":1,\"274\":1,\"335\":1}}],[\"bodily\",{\"0\":{\"202\":1},\"1\":{\"202\":1}}],[\"bottlenecks\",{\"1\":{\"329\":1}}],[\"bottleneck\",{\"1\":{\"255\":2,\"260\":1}}],[\"bot\",{\"0\":{\"232\":1}}],[\"both\",{\"0\":{\"313\":1},\"1\":{\"96\":2,\"98\":3,\"103\":1,\"107\":1,\"119\":1,\"125\":1,\"130\":2,\"132\":1,\"136\":1,\"137\":3,\"143\":1,\"144\":1,\"151\":1,\"154\":1,\"158\":1,\"160\":1,\"163\":1,\"164\":1,\"170\":1,\"176\":1,\"180\":1,\"183\":1,\"184\":1,\"186\":1,\"189\":2,\"195\":1,\"196\":1,\"199\":1,\"207\":1,\"215\":1,\"222\":1,\"227\":2,\"234\":1,\"235\":1,\"238\":1,\"243\":2,\"244\":1,\"257\":1,\"259\":2,\"266\":1,\"273\":1,\"275\":1,\"301\":1,\"305\":1,\"307\":1,\"308\":1,\"315\":1,\"318\":1,\"327\":1,\"328\":1,\"330\":1,\"334\":1,\"335\":1,\"338\":1,\"348\":1,\"349\":1,\"356\":1,\"361\":1,\"366\":1,\"368\":1,\"375\":2,\"377\":1,\"380\":1,\"393\":2,\"394\":1,\"398\":1,\"403\":1,\"408\":1,\"411\":1,\"424\":3,\"434\":1,\"439\":1,\"441\":1,\"445\":1,\"450\":1,\"460\":1,\"469\":1,\"470\":4,\"471\":1,\"472\":1,\"480\":2,\"482\":2,\"486\":1,\"487\":1}}],[\"bonxai\",{\"1\":{\"231\":1}}],[\"bonnemann\",{\"1\":{\"220\":1}}],[\"bonagiri\",{\"1\":{\"205\":1}}],[\"boyu\",{\"1\":{\"219\":1,\"350\":1,\"401\":1}}],[\"boilerplate\",{\"1\":{\"172\":1,\"370\":1}}],[\"boxi\",{\"1\":{\"329\":1}}],[\"box\",{\"1\":{\"171\":1,\"182\":1,\"253\":2,\"325\":1,\"334\":1,\"352\":1,\"403\":1,\"404\":2}}],[\"book\",{\"1\":{\"369\":1}}],[\"books\",{\"1\":{\"369\":3}}],[\"boost\",{\"0\":{\"470\":1},\"1\":{\"323\":1,\"450\":1}}],[\"boosting\",{\"1\":{\"214\":1,\"294\":1,\"305\":1}}],[\"boosts\",{\"1\":{\"159\":1,\"317\":1,\"427\":1}}],[\"boomboombam\",{\"0\":{\"4\":1},\"1\":{\"22\":1}}],[\"bozkir\",{\"1\":{\"139\":1,\"273\":1}}],[\"bo\",{\"1\":{\"131\":1,\"456\":1,\"478\":1}}],[\"bold\",{\"1\":{\"123\":1}}],[\"biyan\",{\"1\":{\"483\":1}}],[\"bit\",{\"0\":{\"456\":1},\"1\":{\"389\":1}}],[\"bits\",{\"1\":{\"207\":1,\"456\":2}}],[\"bilstm\",{\"1\":{\"351\":1}}],[\"billions\",{\"1\":{\"479\":1}}],[\"billion\",{\"1\":{\"377\":4,\"480\":1}}],[\"billinghurst\",{\"1\":{\"81\":1}}],[\"bill\",{\"1\":{\"345\":1}}],[\"bifulco\",{\"1\":{\"338\":1}}],[\"bibek\",{\"1\":{\"334\":1}}],[\"bihe\",{\"1\":{\"303\":1}}],[\"bihui\",{\"1\":{\"128\":1}}],[\"biggest\",{\"1\":{\"465\":1}}],[\"big\",{\"1\":{\"253\":1,\"413\":1}}],[\"bigham\",{\"1\":{\"210\":1}}],[\"bimodal\",{\"1\":{\"240\":1}}],[\"bim\",{\"1\":{\"201\":1}}],[\"bi\",{\"1\":{\"163\":1,\"231\":1,\"347\":1,\"419\":1,\"420\":1,\"463\":1}}],[\"bias\",{\"0\":{\"263\":1,\"425\":1},\"1\":{\"160\":1,\"328\":1,\"336\":1,\"362\":1,\"395\":1,\"398\":1,\"424\":2,\"425\":5}}],[\"biased\",{\"0\":{\"160\":1,\"395\":1},\"1\":{\"160\":1,\"416\":1}}],[\"biases\",{\"0\":{\"424\":1},\"1\":{\"133\":1,\"136\":1,\"148\":1,\"160\":3,\"196\":2,\"251\":3,\"263\":2,\"328\":2,\"331\":1,\"394\":1,\"423\":3,\"424\":1}}],[\"bird\",{\"1\":{\"159\":1}}],[\"biscuit\",{\"0\":{\"110\":1,\"321\":1},\"1\":{\"110\":3,\"321\":3}}],[\"binllm\",{\"1\":{\"488\":3}}],[\"binaries\",{\"0\":{\"488\":1},\"1\":{\"488\":2}}],[\"binary\",{\"1\":{\"294\":2,\"394\":1,\"466\":1,\"488\":4}}],[\"binzong\",{\"1\":{\"479\":1}}],[\"bingning\",{\"1\":{\"478\":1}}],[\"bingert\",{\"1\":{\"395\":1}}],[\"bingchen\",{\"1\":{\"349\":1}}],[\"binhang\",{\"1\":{\"377\":1,\"462\":1}}],[\"bin\",{\"1\":{\"104\":1,\"257\":1,\"342\":1,\"360\":1}}],[\"biorxiv\",{\"1\":{\"428\":1}}],[\"bioontology\",{\"1\":{\"398\":1}}],[\"bioportal\",{\"1\":{\"398\":3}}],[\"biologists\",{\"1\":{\"259\":1}}],[\"biological\",{\"0\":{\"259\":1},\"1\":{\"103\":2,\"207\":1,\"259\":2}}],[\"biomedicine\",{\"0\":{\"323\":1},\"1\":{\"323\":2}}],[\"biomedical\",{\"0\":{\"372\":1},\"1\":{\"281\":1,\"372\":1,\"379\":3}}],[\"biomech\",{\"1\":{\"246\":1}}],[\"biomechanical\",{\"0\":{\"246\":1},\"1\":{\"246\":2}}],[\"biomarkers\",{\"0\":{\"237\":1},\"1\":{\"237\":3}}],[\"biographically\",{\"1\":{\"227\":1}}],[\"biography\",{\"0\":{\"9\":1}}],[\"biodiversity\",{\"1\":{\"217\":1}}],[\"biodegradable\",{\"0\":{\"207\":1},\"1\":{\"207\":1}}],[\"bionic\",{\"1\":{\"166\":2}}],[\"bio\",{\"1\":{\"120\":1,\"295\":1}}],[\"biosensors\",{\"1\":{\"120\":1}}],[\"biessmann\",{\"1\":{\"101\":1}}],[\"bickham\",{\"1\":{\"81\":1}}],[\"byte\",{\"1\":{\"389\":1}}],[\"byeolhee\",{\"1\":{\"356\":1}}],[\"byers\",{\"1\":{\"176\":1}}],[\"byung\",{\"1\":{\"274\":1}}],[\"byrne\",{\"1\":{\"164\":1}}],[\"by\",{\"0\":{\"142\":1,\"155\":1,\"165\":1,\"223\":1,\"340\":1,\"342\":1,\"353\":1,\"402\":1,\"480\":1},\"1\":{\"74\":1,\"96\":1,\"98\":1,\"104\":1,\"106\":1,\"108\":1,\"110\":2,\"113\":2,\"116\":2,\"117\":1,\"118\":1,\"120\":1,\"122\":3,\"124\":2,\"125\":1,\"127\":3,\"128\":4,\"130\":1,\"131\":5,\"133\":1,\"135\":1,\"136\":1,\"137\":2,\"139\":1,\"140\":2,\"142\":1,\"144\":1,\"146\":1,\"150\":1,\"152\":1,\"155\":3,\"156\":1,\"157\":1,\"158\":3,\"159\":3,\"161\":1,\"163\":1,\"165\":2,\"166\":1,\"167\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":3,\"175\":1,\"176\":4,\"179\":2,\"181\":1,\"182\":1,\"183\":1,\"186\":1,\"187\":5,\"188\":2,\"190\":4,\"191\":1,\"192\":1,\"196\":1,\"198\":2,\"200\":2,\"201\":3,\"203\":1,\"204\":1,\"205\":1,\"207\":1,\"209\":1,\"212\":1,\"213\":1,\"214\":1,\"216\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":1,\"222\":2,\"223\":4,\"224\":1,\"227\":1,\"231\":1,\"232\":2,\"233\":1,\"234\":1,\"235\":1,\"238\":2,\"239\":2,\"240\":1,\"244\":1,\"245\":1,\"246\":1,\"249\":2,\"250\":3,\"253\":4,\"255\":3,\"257\":1,\"259\":3,\"260\":1,\"261\":1,\"265\":1,\"266\":1,\"269\":2,\"272\":1,\"277\":2,\"281\":1,\"282\":2,\"283\":2,\"287\":1,\"288\":1,\"289\":3,\"290\":1,\"291\":1,\"292\":1,\"294\":3,\"296\":1,\"297\":1,\"301\":1,\"302\":3,\"303\":1,\"304\":1,\"305\":2,\"306\":1,\"313\":1,\"315\":1,\"316\":1,\"317\":1,\"319\":1,\"321\":2,\"323\":2,\"324\":2,\"327\":1,\"328\":2,\"329\":1,\"331\":1,\"334\":3,\"335\":3,\"338\":1,\"339\":1,\"340\":1,\"342\":3,\"347\":1,\"353\":1,\"354\":1,\"356\":3,\"358\":2,\"359\":3,\"361\":1,\"362\":2,\"364\":2,\"365\":2,\"366\":3,\"368\":3,\"369\":3,\"370\":3,\"375\":2,\"376\":1,\"377\":2,\"378\":2,\"382\":1,\"384\":2,\"386\":1,\"387\":2,\"389\":3,\"390\":2,\"391\":4,\"395\":2,\"396\":1,\"398\":2,\"399\":1,\"400\":1,\"401\":1,\"403\":1,\"404\":2,\"407\":1,\"408\":2,\"410\":1,\"411\":2,\"415\":2,\"416\":1,\"418\":1,\"419\":1,\"420\":1,\"428\":2,\"429\":1,\"430\":3,\"434\":2,\"435\":3,\"438\":2,\"439\":2,\"442\":3,\"443\":1,\"444\":1,\"445\":2,\"450\":1,\"453\":2,\"454\":4,\"455\":1,\"458\":1,\"459\":2,\"460\":5,\"462\":2,\"463\":1,\"465\":2,\"469\":4,\"470\":2,\"472\":1,\"476\":1,\"477\":1,\"479\":1,\"482\":1,\"485\":1,\"487\":2,\"488\":1,\"490\":1,\"492\":2,\"494\":1,\"495\":1,\"496\":1,\"497\":1}}],[\"bevs\",{\"1\":{\"468\":1}}],[\"begin\",{\"1\":{\"470\":1}}],[\"begins\",{\"1\":{\"418\":1}}],[\"begs\",{\"1\":{\"441\":1}}],[\"beats\",{\"1\":{\"411\":1}}],[\"bear\",{\"1\":{\"164\":1}}],[\"beidi\",{\"1\":{\"427\":1}}],[\"beir\",{\"1\":{\"347\":1}}],[\"being\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"117\":1,\"120\":1,\"151\":1,\"171\":1,\"176\":1,\"181\":1,\"184\":1,\"204\":1,\"211\":3,\"221\":1,\"254\":1,\"259\":1,\"268\":2,\"286\":1,\"288\":1,\"302\":5,\"308\":1,\"315\":1,\"334\":1,\"339\":1,\"348\":1,\"376\":1,\"380\":1,\"391\":1,\"398\":1,\"418\":1,\"428\":1,\"432\":1,\"452\":2,\"453\":2,\"463\":1,\"465\":2,\"466\":1,\"469\":1,\"474\":1,\"486\":1,\"496\":1}}],[\"behave\",{\"1\":{\"497\":1}}],[\"behaviours\",{\"1\":{\"302\":1}}],[\"behaviour\",{\"1\":{\"202\":1,\"249\":1}}],[\"behaviors\",{\"0\":{\"479\":1},\"1\":{\"120\":1,\"132\":1,\"198\":1,\"200\":1,\"245\":2,\"258\":1,\"303\":2,\"330\":1,\"387\":1,\"417\":1,\"433\":1,\"468\":1,\"479\":4}}],[\"behavioral\",{\"0\":{\"268\":1,\"452\":1},\"1\":{\"107\":1,\"120\":2,\"227\":1,\"237\":3,\"268\":4,\"452\":4,\"468\":1}}],[\"behavior\",{\"0\":{\"100\":1,\"108\":1,\"116\":1,\"200\":1,\"254\":1,\"327\":1},\"1\":{\"96\":1,\"100\":5,\"107\":1,\"108\":3,\"116\":4,\"152\":2,\"158\":1,\"200\":2,\"218\":5,\"238\":1,\"269\":2,\"283\":2,\"288\":1,\"303\":1,\"307\":2,\"361\":1,\"422\":1,\"429\":1,\"467\":1,\"468\":1,\"475\":1,\"479\":4,\"488\":1,\"497\":1}}],[\"behrendt\",{\"1\":{\"400\":1}}],[\"behzadan\",{\"1\":{\"334\":1}}],[\"behind\",{\"1\":{\"258\":1,\"326\":1,\"333\":1,\"409\":1,\"414\":1,\"433\":1,\"470\":1}}],[\"beddia\",{\"1\":{\"460\":1}}],[\"bed\",{\"1\":{\"245\":1}}],[\"bela\",{\"1\":{\"458\":1}}],[\"below\",{\"1\":{\"234\":1}}],[\"belonging\",{\"1\":{\"209\":1}}],[\"believable\",{\"1\":{\"218\":3}}],[\"believe\",{\"1\":{\"125\":1,\"176\":1,\"217\":1,\"243\":1,\"298\":1,\"326\":1,\"345\":1,\"412\":1,\"437\":1}}],[\"beliefs\",{\"0\":{\"468\":1},\"1\":{\"175\":1,\"245\":1,\"303\":2,\"461\":1,\"468\":1}}],[\"belief\",{\"1\":{\"159\":1,\"278\":1,\"441\":1}}],[\"bestanswer\",{\"1\":{\"337\":1}}],[\"best\",{\"1\":{\"206\":1,\"231\":1,\"315\":1,\"325\":1,\"350\":2,\"378\":1,\"388\":2,\"445\":1,\"458\":2,\"462\":1,\"467\":1,\"478\":1,\"486\":1,\"488\":1,\"494\":1,\"497\":1}}],[\"bespoke\",{\"1\":{\"189\":1}}],[\"beyond\",{\"0\":{\"171\":1,\"231\":1},\"1\":{\"135\":1,\"142\":1,\"151\":1,\"159\":1,\"214\":1,\"216\":1,\"239\":1,\"324\":1,\"326\":1,\"348\":1,\"419\":1,\"420\":1,\"489\":1}}],[\"before\",{\"1\":{\"123\":1,\"151\":1,\"160\":1,\"179\":1,\"326\":1,\"348\":1,\"424\":1,\"454\":2,\"475\":1}}],[\"beta\",{\"1\":{\"404\":1}}],[\"better\",{\"0\":{\"123\":1,\"185\":1,\"229\":1,\"261\":1,\"264\":1,\"416\":1,\"441\":1,\"448\":1},\"1\":{\"125\":1,\"126\":1,\"130\":1,\"131\":1,\"137\":2,\"148\":1,\"155\":1,\"176\":1,\"204\":1,\"209\":1,\"215\":1,\"216\":1,\"223\":1,\"226\":1,\"228\":1,\"231\":1,\"235\":1,\"236\":1,\"240\":1,\"243\":1,\"256\":1,\"266\":1,\"268\":1,\"319\":1,\"324\":1,\"349\":1,\"356\":1,\"374\":1,\"385\":3,\"386\":2,\"388\":1,\"394\":1,\"405\":2,\"416\":1,\"431\":1,\"441\":1,\"452\":1,\"455\":1,\"485\":2,\"491\":1,\"494\":1}}],[\"between\",{\"0\":{\"189\":1,\"273\":1,\"492\":1},\"1\":{\"97\":1,\"100\":1,\"106\":1,\"110\":1,\"120\":1,\"143\":1,\"149\":1,\"152\":1,\"159\":1,\"160\":1,\"165\":2,\"169\":1,\"172\":2,\"173\":1,\"186\":1,\"189\":1,\"191\":1,\"195\":1,\"199\":2,\"201\":1,\"202\":2,\"204\":1,\"209\":1,\"212\":1,\"216\":1,\"217\":1,\"221\":1,\"229\":1,\"230\":1,\"232\":1,\"233\":1,\"235\":1,\"237\":2,\"238\":1,\"239\":1,\"244\":1,\"250\":1,\"253\":1,\"255\":5,\"263\":1,\"273\":1,\"279\":1,\"290\":1,\"291\":1,\"292\":1,\"295\":1,\"297\":1,\"302\":1,\"304\":5,\"312\":1,\"321\":1,\"323\":1,\"324\":1,\"327\":1,\"328\":1,\"335\":1,\"370\":2,\"378\":2,\"379\":1,\"384\":1,\"400\":1,\"402\":3,\"404\":1,\"408\":1,\"410\":1,\"418\":1,\"424\":1,\"428\":1,\"430\":1,\"438\":1,\"439\":1,\"444\":2,\"461\":1,\"462\":1,\"468\":1,\"476\":1,\"490\":1,\"496\":1,\"497\":1}}],[\"berkeleybop\",{\"1\":{\"398\":1}}],[\"bertran\",{\"1\":{\"368\":1}}],[\"berti\",{\"1\":{\"343\":1}}],[\"bert\",{\"0\":{\"460\":1},\"1\":{\"324\":1,\"327\":1,\"460\":4}}],[\"bertelsen\",{\"1\":{\"48\":1}}],[\"berger\",{\"1\":{\"287\":1}}],[\"bernard\",{\"1\":{\"432\":1}}],[\"bernardo\",{\"1\":{\"315\":1}}],[\"bernstein\",{\"1\":{\"180\":1,\"234\":1}}],[\"bernegger\",{\"1\":{\"171\":1}}],[\"berners\",{\"1\":{\"157\":1}}],[\"berat\",{\"1\":{\"116\":1,\"118\":1}}],[\"beck\",{\"1\":{\"182\":1}}],[\"becoming\",{\"1\":{\"135\":1,\"240\":1,\"265\":1,\"381\":1,\"443\":2,\"494\":1}}],[\"becomes\",{\"0\":{\"327\":1},\"1\":{\"139\":1,\"266\":1,\"443\":1,\"453\":1}}],[\"become\",{\"1\":{\"105\":1,\"112\":1,\"116\":1,\"125\":1,\"148\":1,\"179\":1,\"219\":1,\"269\":1,\"271\":1,\"313\":1,\"325\":1,\"337\":1,\"344\":1,\"374\":1,\"400\":1,\"401\":1,\"457\":1,\"482\":1}}],[\"because\",{\"0\":{\"107\":1},\"1\":{\"97\":1,\"107\":1,\"161\":1,\"175\":1,\"220\":1,\"302\":1,\"312\":1,\"354\":1}}],[\"been\",{\"1\":{\"101\":1,\"110\":1,\"128\":1,\"151\":1,\"153\":1,\"159\":1,\"165\":2,\"181\":1,\"184\":1,\"191\":1,\"196\":2,\"200\":1,\"204\":1,\"211\":2,\"221\":1,\"229\":1,\"231\":1,\"239\":1,\"243\":2,\"245\":1,\"257\":1,\"261\":1,\"270\":1,\"272\":1,\"279\":1,\"282\":1,\"294\":1,\"308\":1,\"315\":3,\"317\":1,\"321\":1,\"334\":1,\"339\":1,\"341\":1,\"347\":1,\"348\":1,\"366\":1,\"379\":2,\"380\":1,\"386\":1,\"391\":1,\"393\":1,\"412\":1,\"417\":1,\"427\":1,\"428\":1,\"431\":1,\"434\":1,\"450\":2,\"459\":1,\"460\":1,\"463\":1,\"469\":1,\"471\":1,\"472\":1,\"479\":1,\"489\":1,\"491\":1,\"497\":1}}],[\"be\",{\"0\":{\"107\":1,\"404\":1},\"1\":{\"96\":1,\"101\":2,\"102\":1,\"107\":1,\"108\":1,\"112\":3,\"121\":2,\"123\":1,\"125\":1,\"126\":1,\"128\":1,\"132\":2,\"133\":1,\"136\":1,\"149\":1,\"154\":1,\"155\":1,\"158\":1,\"161\":2,\"163\":1,\"172\":1,\"173\":1,\"175\":1,\"176\":1,\"178\":1,\"179\":2,\"181\":1,\"183\":1,\"194\":2,\"198\":1,\"199\":2,\"202\":1,\"203\":1,\"215\":1,\"219\":2,\"224\":1,\"226\":1,\"228\":1,\"229\":2,\"231\":2,\"233\":1,\"235\":1,\"237\":1,\"238\":1,\"240\":1,\"243\":1,\"248\":1,\"250\":1,\"251\":1,\"253\":1,\"255\":1,\"256\":3,\"257\":2,\"258\":1,\"261\":2,\"269\":1,\"278\":2,\"279\":1,\"280\":2,\"281\":1,\"292\":3,\"296\":2,\"301\":1,\"302\":1,\"303\":1,\"306\":1,\"308\":2,\"315\":1,\"326\":2,\"327\":1,\"330\":2,\"331\":1,\"334\":1,\"335\":3,\"340\":1,\"350\":2,\"354\":2,\"355\":2,\"363\":2,\"369\":2,\"370\":1,\"381\":5,\"382\":2,\"385\":2,\"386\":1,\"387\":1,\"399\":1,\"400\":2,\"401\":2,\"403\":1,\"408\":1,\"410\":1,\"411\":3,\"412\":1,\"415\":1,\"417\":1,\"422\":3,\"423\":1,\"427\":2,\"431\":1,\"433\":1,\"434\":1,\"435\":2,\"436\":1,\"443\":2,\"447\":1,\"449\":1,\"453\":2,\"454\":1,\"455\":2,\"468\":1,\"470\":1,\"475\":1,\"479\":1,\"482\":2,\"490\":1,\"494\":3}}],[\"bendou\",{\"1\":{\"444\":1}}],[\"benign\",{\"1\":{\"418\":1}}],[\"beneficial\",{\"1\":{\"350\":1,\"384\":1}}],[\"benefiting\",{\"1\":{\"324\":1}}],[\"benefited\",{\"1\":{\"250\":1,\"386\":1}}],[\"benefit\",{\"1\":{\"132\":1,\"322\":1,\"330\":1,\"389\":1,\"410\":2,\"470\":1}}],[\"benefits\",{\"1\":{\"97\":1,\"119\":1,\"136\":1,\"148\":1,\"207\":1,\"209\":1,\"213\":1,\"233\":1,\"268\":1,\"291\":1,\"312\":1,\"358\":1,\"364\":1,\"392\":1,\"412\":1,\"452\":1,\"470\":2,\"476\":1,\"478\":1}}],[\"bench\",{\"1\":{\"344\":1,\"345\":4,\"377\":2,\"495\":1}}],[\"benchmarked\",{\"1\":{\"350\":1,\"460\":1}}],[\"benchmarking\",{\"1\":{\"338\":1,\"368\":1,\"411\":1}}],[\"benchmark\",{\"0\":{\"338\":1,\"463\":1},\"1\":{\"151\":1,\"183\":1,\"188\":1,\"213\":1,\"215\":2,\"323\":1,\"329\":1,\"333\":1,\"337\":1,\"344\":1,\"345\":1,\"348\":1,\"350\":1,\"377\":1,\"379\":1,\"391\":2,\"394\":1,\"404\":2,\"412\":2,\"415\":1,\"437\":1,\"444\":1,\"454\":1,\"463\":1,\"468\":1,\"470\":1,\"480\":1,\"481\":1,\"482\":1,\"495\":1}}],[\"benchmarks\",{\"0\":{\"337\":1,\"482\":1},\"1\":{\"133\":2,\"215\":4,\"315\":3,\"318\":1,\"331\":2,\"337\":2,\"338\":2,\"342\":1,\"344\":1,\"345\":2,\"359\":1,\"366\":1,\"376\":1,\"382\":1,\"386\":1,\"389\":1,\"393\":2,\"411\":2,\"412\":1,\"434\":1,\"438\":1,\"442\":1,\"443\":1,\"444\":1,\"458\":1,\"480\":1,\"482\":9}}],[\"bennice\",{\"1\":{\"269\":1}}],[\"benjie\",{\"1\":{\"269\":1}}],[\"benjamin\",{\"1\":{\"107\":1,\"217\":1,\"288\":1,\"315\":1,\"363\":1,\"385\":1}}],[\"ben\",{\"1\":{\"42\":1,\"308\":1}}],[\"b\",{\"0\":{\"4\":2,\"18\":2},\"1\":{\"14\":4,\"22\":2,\"148\":1,\"160\":1,\"166\":1,\"236\":1,\"281\":1,\"304\":1,\"334\":1,\"402\":1,\"539\":2}}],[\"创意系统画布强调文化在创新中的核心作用\",{\"1\":{\"524\":1}}],[\"创意系统画布如何帮助组织或社会层面推动创新\",{\"1\":{\"524\":1}}],[\"创意系统画布\",{\"1\":{\"524\":2}}],[\"创意帽\",{\"1\":{\"523\":1}}],[\"创意十分感兴趣\",{\"1\":{\"509\":1}}],[\"创意设计工作室\",{\"0\":{\"19\":1}}],[\"创建增强型玩具环境并不仅仅是在传统玩具中加入一些技术\",{\"1\":{\"81\":1}}],[\"创新技术与工程设计相结合\",{\"1\":{\"510\":1}}],[\"创新跨越\",{\"0\":{\"524\":1},\"1\":{\"502\":1}}],[\"创新教育面临着广泛的挑战\",{\"1\":{\"80\":1}}],[\"创新\",{\"1\":{\"80\":1}}],[\"创新多媒体娱乐理学硕士\",{\"1\":{\"2\":1}}],[\"创业者涌入\",{\"1\":{\"78\":1}}],[\"创业经历\",{\"0\":{\"3\":1}}],[\"创客款有\",{\"1\":{\"78\":1}}],[\"创造新的现实\",{\"1\":{\"533\":1}}],[\"创造不仅美观而且具有高附加值的设计\",{\"1\":{\"525\":1}}],[\"创造力自信\",{\"1\":{\"524\":1}}],[\"创造出更具有吸引力的产品\",{\"1\":{\"533\":1}}],[\"创造出具有独特魅力的产品\",{\"1\":{\"531\":1}}],[\"创造出能够满足整个社会群体需求的设计作品\",{\"1\":{\"522\":1}}],[\"创造出艺术人物和数字概念场景\",{\"1\":{\"22\":1}}],[\"创造满足人们需求\",{\"1\":{\"522\":1}}],[\"创造了一种引人入胜的共生关系\",{\"1\":{\"81\":1}}],[\"创造可以是物质性的\",{\"1\":{\"74\":1,\"530\":1}}],[\"创造和发展\",{\"1\":{\"40\":1}}],[\"创造来赚钱\",{\"1\":{\"39\":1}}],[\"创作背景来源于三年前\",{\"1\":{\"23\":1}}],[\"创立o\",{\"1\":{\"15\":1}}],[\"创立与发展\",{\"1\":{\"15\":1}}],[\"e=工程\",{\"1\":{\"510\":1}}],[\"epfl\",{\"1\":{\"409\":1}}],[\"epoch\",{\"1\":{\"359\":1}}],[\"ephemeral\",{\"0\":{\"110\":1,\"321\":1},\"1\":{\"110\":2,\"321\":2}}],[\"ekaterina\",{\"1\":{\"351\":1,\"352\":1,\"365\":1}}],[\"ehsan\",{\"1\":{\"486\":1}}],[\"ehsaneddin\",{\"1\":{\"333\":1}}],[\"ehealth\",{\"1\":{\"178\":2}}],[\"eytan\",{\"1\":{\"309\":1}}],[\"eye\",{\"0\":{\"139\":1,\"152\":1,\"185\":1,\"290\":1},\"1\":{\"116\":3,\"126\":1,\"127\":2,\"139\":3,\"152\":2,\"185\":1,\"273\":1,\"290\":3}}],[\"eber\",{\"1\":{\"282\":2}}],[\"egeonu\",{\"1\":{\"246\":1}}],[\"ecosystem\",{\"1\":{\"519\":2}}],[\"economic\",{\"1\":{\"270\":1,\"498\":1}}],[\"economy\",{\"1\":{\"217\":1,\"293\":1,\"519\":1}}],[\"ecological\",{\"1\":{\"223\":1}}],[\"ecg\",{\"0\":{\"223\":1},\"1\":{\"223\":2}}],[\"ethics\",{\"0\":{\"286\":1,\"474\":1},\"1\":{\"286\":1,\"307\":1,\"474\":1}}],[\"ethical\",{\"0\":{\"446\":1},\"1\":{\"169\":1,\"171\":1,\"213\":1,\"261\":1,\"286\":3,\"291\":1,\"398\":2,\"403\":1,\"446\":2,\"474\":3,\"476\":1,\"492\":1}}],[\"ethically\",{\"1\":{\"136\":1}}],[\"etc\",{\"1\":{\"217\":3,\"333\":1,\"347\":1,\"353\":1,\"453\":1,\"495\":1}}],[\"et\",{\"1\":{\"215\":1,\"404\":1,\"462\":1}}],[\"eurus\",{\"1\":{\"411\":4}}],[\"european\",{\"1\":{\"117\":1}}],[\"eugene\",{\"1\":{\"386\":1}}],[\"eunsol\",{\"1\":{\"268\":1,\"452\":1}}],[\"eunchae\",{\"1\":{\"212\":1}}],[\"eun\",{\"1\":{\"198\":1,\"387\":1}}],[\"eer\",{\"1\":{\"190\":1}}],[\"eeg\",{\"1\":{\"189\":1,\"256\":7,\"284\":1}}],[\"eels\",{\"0\":{\"112\":1},\"1\":{\"112\":4}}],[\"eileen\",{\"1\":{\"395\":1}}],[\"eicu\",{\"1\":{\"309\":1}}],[\"eini\",{\"1\":{\"309\":1}}],[\"eight\",{\"1\":{\"164\":1,\"179\":1,\"242\":1,\"250\":1,\"495\":1}}],[\"either\",{\"1\":{\"98\":1,\"107\":1,\"124\":1,\"131\":1,\"209\":1,\"215\":1,\"238\":1,\"252\":1,\"263\":2,\"304\":1,\"326\":1,\"345\":1,\"409\":1,\"426\":1,\"439\":1,\"496\":1}}],[\"efe\",{\"1\":{\"139\":1,\"273\":1}}],[\"efforts\",{\"1\":{\"215\":1,\"253\":1,\"463\":1,\"489\":1}}],[\"effort\",{\"1\":{\"154\":1,\"233\":1,\"242\":1,\"297\":1,\"325\":1,\"333\":1,\"339\":1,\"396\":1,\"459\":1}}],[\"effects\",{\"0\":{\"191\":1,\"230\":1},\"1\":{\"121\":1,\"142\":1,\"155\":1,\"183\":1,\"198\":1,\"200\":1,\"230\":2,\"235\":1,\"263\":1,\"387\":1}}],[\"effect\",{\"0\":{\"106\":1,\"279\":1},\"1\":{\"107\":1,\"165\":1,\"183\":1,\"185\":1,\"191\":1,\"214\":1,\"230\":1,\"233\":1,\"263\":1,\"326\":1,\"428\":1}}],[\"effectiveness~\",{\"1\":{\"159\":1}}],[\"effectiveness\",{\"0\":{\"102\":1},\"1\":{\"99\":1,\"116\":1,\"132\":1,\"136\":1,\"141\":1,\"150\":1,\"186\":1,\"189\":1,\"190\":1,\"204\":1,\"216\":1,\"218\":1,\"233\":1,\"234\":1,\"242\":1,\"246\":1,\"268\":1,\"271\":1,\"274\":1,\"289\":1,\"301\":1,\"322\":1,\"330\":1,\"336\":1,\"344\":1,\"347\":1,\"350\":1,\"351\":1,\"355\":1,\"360\":1,\"376\":1,\"394\":1,\"411\":1,\"418\":1,\"425\":1,\"445\":2,\"452\":1,\"453\":1,\"454\":1,\"457\":1,\"463\":2,\"467\":1,\"470\":1,\"477\":1,\"481\":1,\"483\":1,\"496\":2}}],[\"effective\",{\"0\":{\"466\":1,\"472\":1},\"1\":{\"97\":1,\"99\":1,\"125\":1,\"132\":1,\"137\":1,\"138\":1,\"143\":1,\"152\":1,\"158\":2,\"159\":1,\"169\":2,\"197\":2,\"200\":1,\"203\":1,\"226\":1,\"233\":2,\"237\":1,\"261\":1,\"290\":1,\"312\":1,\"318\":1,\"326\":1,\"330\":1,\"339\":1,\"356\":1,\"389\":1,\"399\":1,\"403\":1,\"432\":1,\"437\":1,\"447\":1,\"458\":1,\"459\":2,\"465\":1,\"470\":1,\"475\":1}}],[\"effectively\",{\"1\":{\"97\":1,\"116\":1,\"125\":1,\"133\":1,\"137\":1,\"151\":1,\"166\":1,\"167\":1,\"180\":1,\"184\":1,\"213\":1,\"257\":2,\"264\":1,\"270\":1,\"301\":1,\"312\":1,\"329\":2,\"331\":1,\"348\":1,\"349\":1,\"355\":1,\"359\":1,\"378\":1,\"380\":1,\"393\":1,\"402\":1,\"417\":1,\"425\":1,\"432\":1,\"434\":1,\"436\":1,\"448\":1,\"453\":1,\"455\":1,\"466\":1,\"471\":2,\"472\":1,\"480\":1,\"485\":1,\"491\":1,\"498\":1}}],[\"efficiencies\",{\"1\":{\"244\":1}}],[\"efficiency\",{\"0\":{\"342\":1,\"356\":1,\"465\":1},\"1\":{\"118\":2,\"130\":1,\"136\":1,\"139\":1,\"158\":3,\"170\":1,\"182\":1,\"189\":1,\"260\":1,\"269\":1,\"316\":1,\"318\":2,\"342\":3,\"356\":1,\"358\":2,\"359\":1,\"360\":1,\"361\":1,\"374\":1,\"389\":1,\"405\":1,\"445\":1,\"465\":1,\"477\":1,\"479\":4}}],[\"efficiently\",{\"0\":{\"436\":1},\"1\":{\"96\":2,\"166\":1,\"222\":1,\"260\":1,\"284\":1,\"326\":1,\"349\":1,\"414\":1,\"443\":1,\"447\":1,\"455\":1}}],[\"efficient\",{\"0\":{\"103\":1,\"210\":1,\"359\":1,\"414\":1,\"425\":1,\"427\":1,\"462\":1,\"483\":1},\"1\":{\"96\":1,\"100\":1,\"210\":1,\"238\":1,\"242\":2,\"264\":1,\"295\":1,\"308\":1,\"319\":1,\"338\":1,\"342\":2,\"361\":1,\"392\":1,\"405\":3,\"414\":2,\"415\":1,\"425\":2,\"427\":1,\"436\":1,\"448\":1,\"462\":1,\"465\":1,\"469\":3,\"492\":1}}],[\"efficacy\",{\"1\":{\"102\":1,\"120\":2,\"143\":1,\"167\":1,\"261\":1,\"280\":1,\"298\":1,\"317\":1,\"328\":1,\"356\":1,\"359\":1,\"365\":1,\"396\":1,\"418\":1,\"432\":1,\"450\":1,\"460\":2,\"489\":1}}],[\"equation\",{\"1\":{\"422\":2}}],[\"equality\",{\"1\":{\"180\":1}}],[\"equal\",{\"0\":{\"280\":1,\"329\":1},\"1\":{\"108\":1,\"112\":1,\"132\":1,\"190\":1,\"275\":1,\"280\":1,\"330\":1,\"369\":1,\"389\":1}}],[\"equally\",{\"1\":{\"107\":1,\"108\":2}}],[\"equipping\",{\"1\":{\"435\":1}}],[\"equipped\",{\"1\":{\"124\":1,\"125\":1,\"151\":1,\"181\":1,\"348\":1}}],[\"equip\",{\"1\":{\"329\":1}}],[\"equivalence\",{\"1\":{\"304\":2}}],[\"equivalent\",{\"1\":{\"207\":1,\"422\":2,\"462\":1}}],[\"edbi这几个字母经常被重新组合出现\",{\"1\":{\"510\":1}}],[\"edc\",{\"1\":{\"382\":3}}],[\"eda\",{\"0\":{\"363\":1},\"1\":{\"363\":1}}],[\"edoardo\",{\"1\":{\"327\":1}}],[\"edges\",{\"1\":{\"301\":2}}],[\"edge\",{\"0\":{\"436\":1},\"1\":{\"213\":1,\"266\":1,\"301\":1,\"307\":2,\"436\":1,\"475\":2}}],[\"edged\",{\"0\":{\"106\":1}}],[\"edu\",{\"1\":{\"184\":1,\"380\":1}}],[\"educating\",{\"1\":{\"160\":1}}],[\"educational\",{\"0\":{\"216\":1},\"1\":{\"132\":1,\"165\":1,\"216\":2,\"221\":1,\"228\":1,\"254\":1,\"290\":1,\"291\":1,\"292\":1,\"330\":1,\"333\":1,\"476\":1,\"486\":1}}],[\"education\",{\"0\":{\"2\":1,\"132\":1,\"156\":1,\"290\":1,\"292\":1,\"330\":1},\"1\":{\"156\":3,\"169\":2,\"172\":1,\"216\":4,\"217\":1,\"221\":2,\"273\":1,\"286\":1,\"290\":1,\"292\":4,\"370\":1,\"474\":1,\"486\":2}}],[\"educators\",{\"0\":{\"129\":1},\"1\":{\"126\":1,\"129\":1,\"292\":2,\"308\":1,\"398\":2}}],[\"educate\",{\"1\":{\"117\":1}}],[\"ed\",{\"1\":{\"155\":1}}],[\"editors\",{\"1\":{\"336\":1}}],[\"editor\",{\"1\":{\"260\":1,\"361\":1}}],[\"edits\",{\"1\":{\"170\":1,\"260\":1,\"324\":2}}],[\"editing\",{\"0\":{\"170\":1,\"224\":1,\"260\":1,\"375\":1,\"463\":1},\"1\":{\"139\":1,\"170\":3,\"224\":3,\"260\":5,\"336\":1,\"453\":1,\"463\":3}}],[\"edit\",{\"1\":{\"139\":1,\"159\":2,\"170\":1,\"324\":1}}],[\"erman\",{\"1\":{\"410\":1}}],[\"erroneous\",{\"1\":{\"340\":1,\"489\":1}}],[\"error\",{\"0\":{\"322\":1},\"1\":{\"152\":1,\"190\":1,\"246\":1,\"322\":2,\"325\":1,\"340\":1,\"363\":2,\"369\":1,\"391\":9,\"447\":1,\"470\":1,\"492\":2}}],[\"errors\",{\"0\":{\"102\":1,\"363\":1,\"391\":1,\"445\":1,\"470\":1},\"1\":{\"102\":1,\"170\":1,\"172\":1,\"181\":1,\"241\":1,\"275\":1,\"333\":1,\"356\":1,\"370\":1,\"390\":2,\"391\":5,\"445\":5,\"470\":2,\"471\":1,\"483\":1,\"494\":1,\"496\":2}}],[\"ersonalization\",{\"1\":{\"392\":1}}],[\"erskine\",{\"1\":{\"294\":1}}],[\"ersilia\",{\"1\":{\"120\":1}}],[\"era\",{\"0\":{\"243\":1},\"1\":{\"533\":1}}],[\"eric\",{\"1\":{\"268\":1,\"452\":1}}],[\"erickson\",{\"1\":{\"184\":1,\"380\":1}}],[\"erick\",{\"1\":{\"119\":1,\"344\":1}}],[\"erica\",{\"1\":{\"98\":1,\"147\":1}}],[\"earth\",{\"0\":{\"249\":1}}],[\"earl\",{\"1\":{\"350\":1}}],[\"earlier\",{\"1\":{\"244\":1,\"306\":1}}],[\"early\",{\"1\":{\"97\":1,\"126\":1,\"260\":1,\"291\":1,\"309\":1,\"312\":1,\"476\":1}}],[\"ea\",{\"1\":{\"150\":2}}],[\"each\",{\"1\":{\"126\":1,\"132\":1,\"137\":1,\"139\":1,\"146\":1,\"149\":3,\"151\":1,\"170\":1,\"174\":1,\"183\":1,\"199\":1,\"204\":1,\"213\":1,\"219\":1,\"222\":1,\"226\":2,\"231\":1,\"243\":1,\"244\":1,\"249\":1,\"260\":1,\"264\":1,\"266\":1,\"271\":1,\"275\":1,\"279\":1,\"281\":1,\"307\":1,\"322\":1,\"326\":1,\"330\":1,\"335\":1,\"348\":1,\"350\":2,\"353\":1,\"366\":1,\"369\":3,\"371\":1,\"389\":1,\"398\":1,\"399\":2,\"400\":1,\"401\":1,\"410\":1,\"411\":1,\"415\":2,\"435\":1,\"442\":1,\"448\":1,\"450\":2,\"454\":1,\"457\":1,\"462\":2,\"470\":1,\"477\":1,\"483\":1,\"485\":1}}],[\"easier\",{\"1\":{\"258\":1,\"326\":1,\"389\":1,\"433\":1,\"456\":1}}],[\"easily\",{\"1\":{\"101\":1,\"146\":1,\"258\":1,\"301\":1,\"315\":1,\"327\":1,\"337\":1,\"338\":1,\"382\":1,\"395\":1,\"400\":1,\"403\":1,\"433\":1,\"445\":1}}],[\"eason\",{\"1\":{\"192\":1}}],[\"easy\",{\"0\":{\"192\":1},\"1\":{\"316\":2,\"324\":1,\"338\":1}}],[\"ease\",{\"1\":{\"99\":1}}],[\"elhoseiny\",{\"1\":{\"393\":1}}],[\"elgammal\",{\"1\":{\"349\":1}}],[\"elaborate\",{\"1\":{\"319\":1}}],[\"elapsed\",{\"1\":{\"264\":1,\"448\":1}}],[\"eldan\",{\"1\":{\"418\":1}}],[\"elderly\",{\"0\":{\"282\":1},\"1\":{\"282\":4}}],[\"eldon\",{\"1\":{\"151\":1,\"348\":1}}],[\"elucidates\",{\"1\":{\"269\":1}}],[\"elucidate\",{\"1\":{\"256\":1}}],[\"elbow\",{\"1\":{\"246\":1}}],[\"ell\",{\"1\":{\"301\":1}}],[\"ella\",{\"1\":{\"236\":1}}],[\"ellen\",{\"1\":{\"113\":1,\"115\":1,\"261\":1}}],[\"elodie\",{\"1\":{\"232\":1}}],[\"elon\",{\"1\":{\"179\":1}}],[\"elongated\",{\"1\":{\"151\":1,\"348\":1}}],[\"elmqvist\",{\"1\":{\"229\":1}}],[\"eli\",{\"1\":{\"454\":1}}],[\"eliminating\",{\"1\":{\"246\":1}}],[\"eliminate\",{\"1\":{\"192\":1}}],[\"eliminates\",{\"1\":{\"113\":1,\"165\":1,\"259\":1,\"483\":1,\"491\":1}}],[\"elisa\",{\"1\":{\"171\":1,\"369\":1}}],[\"elicit\",{\"1\":{\"334\":1}}],[\"eliciting\",{\"1\":{\"158\":1,\"391\":1}}],[\"elicitation\",{\"0\":{\"158\":1}}],[\"elio\",{\"1\":{\"120\":1}}],[\"elina\",{\"1\":{\"98\":1}}],[\"elena\",{\"1\":{\"315\":1}}],[\"eleanor\",{\"1\":{\"237\":1}}],[\"elections\",{\"1\":{\"234\":1}}],[\"electric\",{\"1\":{\"468\":1}}],[\"electrical\",{\"1\":{\"207\":1,\"306\":1}}],[\"electrodes\",{\"1\":{\"284\":1}}],[\"electroencephalogram\",{\"1\":{\"284\":1}}],[\"electroencephalography\",{\"0\":{\"256\":1},\"1\":{\"256\":1}}],[\"electrocardiogram\",{\"1\":{\"223\":1}}],[\"electronic\",{\"0\":{\"306\":1},\"1\":{\"207\":1,\"306\":2,\"363\":1}}],[\"electron\",{\"1\":{\"112\":2}}],[\"electromyography\",{\"1\":{\"103\":1}}],[\"eleftheria\",{\"1\":{\"207\":1}}],[\"eley\",{\"1\":{\"207\":1}}],[\"elevator\",{\"0\":{\"202\":1}}],[\"elevated\",{\"1\":{\"106\":1,\"280\":1}}],[\"elementary\",{\"1\":{\"151\":2,\"348\":2}}],[\"elements\",{\"1\":{\"141\":1,\"152\":1,\"163\":1,\"199\":2,\"216\":1,\"266\":1,\"278\":1,\"297\":1,\"353\":1,\"382\":1,\"492\":1}}],[\"element\",{\"1\":{\"98\":1,\"163\":2}}],[\"evident\",{\"1\":{\"253\":1,\"469\":1}}],[\"evidence\",{\"0\":{\"230\":1},\"1\":{\"100\":1,\"148\":1,\"150\":1,\"197\":1,\"204\":1,\"227\":1,\"230\":1,\"249\":1,\"251\":1,\"302\":1,\"384\":2,\"423\":1,\"453\":3}}],[\"evi\",{\"1\":{\"237\":1}}],[\"evo\",{\"1\":{\"482\":1}}],[\"evoeval\",{\"0\":{\"482\":1},\"1\":{\"482\":4}}],[\"evoke\",{\"1\":{\"220\":1}}],[\"evolve\",{\"1\":{\"349\":1,\"482\":1}}],[\"evolved\",{\"0\":{\"345\":1}}],[\"evolves\",{\"1\":{\"105\":1,\"244\":1}}],[\"evolving\",{\"0\":{\"218\":1,\"482\":1},\"1\":{\"143\":1,\"218\":3,\"221\":1,\"326\":1,\"396\":1,\"398\":1,\"482\":1}}],[\"evolutionary\",{\"1\":{\"289\":1}}],[\"evolution\",{\"0\":{\"211\":1,\"289\":1,\"390\":1},\"1\":{\"135\":1,\"146\":1,\"150\":1,\"174\":1,\"211\":1,\"218\":4,\"253\":1,\"289\":2,\"364\":1,\"371\":1,\"390\":2,\"424\":1,\"446\":1,\"495\":2}}],[\"eval\",{\"1\":{\"337\":1,\"482\":1}}],[\"evaluator\",{\"0\":{\"335\":1},\"1\":{\"336\":1}}],[\"evaluating\",{\"0\":{\"119\":1,\"121\":1,\"215\":1,\"223\":1,\"254\":1,\"275\":1,\"337\":1,\"342\":1,\"391\":1,\"437\":1},\"1\":{\"130\":1,\"199\":1,\"203\":1,\"223\":2,\"236\":1,\"292\":1,\"324\":1,\"328\":1,\"333\":1,\"336\":1,\"337\":1,\"345\":1,\"353\":1,\"379\":1,\"394\":2,\"413\":1,\"424\":1,\"437\":1,\"492\":1}}],[\"evaluations\",{\"1\":{\"117\":1,\"133\":1,\"235\":1,\"238\":1,\"246\":1,\"283\":1,\"324\":1,\"331\":1,\"333\":1,\"386\":1,\"408\":2,\"417\":1,\"480\":1,\"498\":1}}],[\"evaluation\",{\"0\":{\"149\":1,\"161\":1,\"238\":1,\"316\":1,\"324\":1,\"353\":1,\"354\":1,\"379\":1,\"413\":1,\"432\":1,\"463\":1,\"497\":1},\"1\":{\"96\":1,\"105\":1,\"121\":1,\"126\":1,\"133\":3,\"141\":1,\"149\":1,\"151\":1,\"161\":3,\"167\":1,\"184\":1,\"189\":1,\"213\":1,\"215\":2,\"218\":2,\"227\":1,\"238\":1,\"245\":1,\"259\":1,\"271\":1,\"275\":1,\"289\":3,\"296\":1,\"309\":1,\"315\":2,\"316\":6,\"324\":3,\"325\":1,\"331\":3,\"333\":4,\"335\":2,\"336\":1,\"337\":4,\"338\":2,\"339\":1,\"342\":1,\"348\":1,\"353\":4,\"354\":3,\"356\":1,\"362\":1,\"369\":2,\"372\":1,\"379\":4,\"380\":1,\"384\":1,\"388\":1,\"396\":1,\"399\":1,\"403\":2,\"412\":2,\"413\":2,\"414\":1,\"418\":1,\"422\":1,\"429\":1,\"432\":7,\"437\":1,\"457\":1,\"458\":1,\"463\":1,\"480\":1,\"482\":1,\"488\":2,\"490\":1,\"496\":1}}],[\"evaluates\",{\"1\":{\"336\":1,\"356\":1}}],[\"evaluated\",{\"1\":{\"131\":1,\"142\":1,\"160\":1,\"182\":1,\"190\":1,\"201\":1,\"210\":1,\"256\":1,\"257\":1,\"315\":1,\"336\":1,\"418\":1,\"490\":1}}],[\"evaluate\",{\"0\":{\"245\":1},\"1\":{\"99\":1,\"101\":1,\"123\":1,\"148\":1,\"166\":1,\"174\":1,\"176\":1,\"183\":1,\"189\":1,\"196\":1,\"227\":1,\"238\":1,\"240\":2,\"245\":2,\"254\":1,\"272\":2,\"275\":1,\"289\":1,\"306\":1,\"308\":1,\"317\":1,\"329\":1,\"333\":1,\"337\":1,\"340\":1,\"342\":3,\"345\":1,\"369\":1,\"371\":1,\"379\":1,\"391\":1,\"395\":2,\"402\":1,\"412\":1,\"413\":1,\"415\":1,\"418\":1,\"432\":1,\"434\":1,\"445\":1,\"455\":1,\"466\":1,\"468\":4,\"471\":1,\"480\":1,\"481\":1,\"482\":1,\"486\":1,\"490\":1,\"492\":1,\"494\":1}}],[\"evade\",{\"1\":{\"334\":1}}],[\"evan\",{\"1\":{\"227\":1}}],[\"evasive\",{\"1\":{\"186\":1}}],[\"evelyn\",{\"1\":{\"178\":1}}],[\"eve\",{\"0\":{\"140\":1},\"1\":{\"140\":3}}],[\"event\",{\"1\":{\"243\":1,\"251\":1,\"298\":1,\"423\":1}}],[\"eventually\",{\"1\":{\"223\":1}}],[\"events\",{\"1\":{\"105\":1,\"230\":1,\"249\":1,\"309\":2,\"424\":4,\"475\":1}}],[\"even\",{\"1\":{\"128\":1,\"135\":1,\"141\":1,\"175\":1,\"176\":1,\"205\":1,\"228\":1,\"233\":2,\"235\":1,\"241\":1,\"251\":1,\"265\":1,\"282\":1,\"283\":1,\"334\":1,\"359\":1,\"381\":1,\"395\":1,\"409\":1,\"423\":1,\"453\":1,\"455\":1,\"459\":1,\"485\":1}}],[\"ever\",{\"1\":{\"119\":1,\"232\":1,\"350\":1,\"465\":1,\"482\":1}}],[\"every\",{\"1\":{\"214\":1}}],[\"everyday\",{\"1\":{\"140\":2,\"192\":1,\"207\":1,\"269\":1,\"461\":1}}],[\"everyone\",{\"1\":{\"108\":1,\"214\":1}}],[\"everywhere\",{\"1\":{\"74\":1}}],[\"exemplify\",{\"1\":{\"338\":1}}],[\"exemplars\",{\"1\":{\"199\":1}}],[\"exert\",{\"1\":{\"233\":1}}],[\"executing\",{\"1\":{\"222\":1,\"275\":1,\"326\":1}}],[\"execution\",{\"1\":{\"146\":1,\"269\":1,\"275\":1,\"326\":2,\"341\":1,\"342\":1}}],[\"executed\",{\"1\":{\"240\":1}}],[\"execute\",{\"1\":{\"151\":1,\"348\":1}}],[\"exhibited\",{\"1\":{\"245\":2,\"251\":1,\"392\":1,\"423\":1,\"469\":1}}],[\"exhibition\",{\"1\":{\"173\":1}}],[\"exhibitions\",{\"1\":{\"173\":1}}],[\"exhibiting\",{\"1\":{\"142\":1}}],[\"exhibits\",{\"1\":{\"151\":1,\"246\":1,\"249\":2,\"317\":1,\"348\":1,\"424\":1,\"478\":1}}],[\"exhibit\",{\"0\":{\"249\":1,\"327\":1},\"1\":{\"151\":1,\"195\":1,\"196\":1,\"233\":1,\"235\":1,\"249\":1,\"251\":1,\"273\":1,\"279\":1,\"303\":1,\"318\":1,\"329\":1,\"339\":1,\"344\":1,\"348\":1,\"394\":1,\"423\":1,\"437\":1,\"466\":1,\"471\":1,\"483\":1,\"488\":1}}],[\"exacerbates\",{\"1\":{\"453\":1}}],[\"exacerbate\",{\"1\":{\"259\":1,\"408\":1}}],[\"exacerbated\",{\"1\":{\"124\":1}}],[\"example\",{\"1\":{\"183\":1,\"199\":1,\"226\":1,\"238\":1,\"251\":1,\"302\":1,\"339\":1,\"375\":1,\"423\":1,\"444\":1,\"466\":1,\"482\":1,\"498\":1}}],[\"examples\",{\"1\":{\"141\":1,\"228\":1,\"229\":1,\"250\":4,\"294\":1,\"306\":1,\"317\":2,\"323\":1,\"360\":3,\"388\":2,\"405\":1,\"432\":1,\"455\":1,\"470\":2,\"494\":1}}],[\"examinations\",{\"1\":{\"333\":1}}],[\"examination\",{\"1\":{\"271\":1,\"364\":1,\"457\":1,\"489\":1}}],[\"examines\",{\"1\":{\"136\":1,\"273\":1,\"343\":1}}],[\"examine\",{\"1\":{\"122\":1,\"202\":1,\"216\":1,\"297\":1,\"302\":1,\"335\":1,\"339\":1,\"363\":1,\"365\":1,\"372\":1,\"374\":1,\"384\":1}}],[\"examined\",{\"1\":{\"106\":1,\"178\":1,\"185\":1,\"191\":1,\"212\":1,\"293\":1,\"304\":1,\"336\":1}}],[\"examining\",{\"0\":{\"100\":1,\"251\":1,\"273\":1,\"286\":1,\"423\":1,\"474\":1},\"1\":{\"198\":1,\"226\":1,\"236\":1,\"291\":1,\"364\":1,\"387\":1,\"461\":1,\"476\":1}}],[\"extra\",{\"1\":{\"294\":1,\"355\":1,\"439\":1,\"442\":1}}],[\"extractor\",{\"1\":{\"349\":1}}],[\"extracting\",{\"1\":{\"305\":1,\"351\":1,\"360\":2,\"455\":1}}],[\"extraction\",{\"0\":{\"351\":1,\"455\":1},\"1\":{\"127\":1,\"205\":1,\"382\":2,\"455\":2,\"492\":1}}],[\"extracts\",{\"1\":{\"213\":1,\"407\":1}}],[\"extracted\",{\"1\":{\"182\":1,\"194\":1,\"223\":1,\"252\":1,\"282\":2,\"333\":1,\"335\":1,\"359\":1,\"410\":2,\"417\":1,\"426\":1}}],[\"extract\",{\"0\":{\"382\":1},\"1\":{\"111\":1,\"141\":1,\"223\":1,\"242\":1,\"318\":1,\"382\":2,\"430\":1,\"455\":1,\"479\":1,\"492\":1}}],[\"extremely\",{\"1\":{\"249\":1,\"337\":1,\"469\":2}}],[\"extreme\",{\"1\":{\"223\":1,\"249\":1,\"412\":1}}],[\"externally\",{\"1\":{\"425\":1}}],[\"external\",{\"0\":{\"297\":1},\"1\":{\"123\":1,\"148\":1,\"149\":1,\"222\":1,\"293\":1,\"297\":2,\"323\":1,\"329\":1,\"374\":1,\"390\":1,\"425\":1,\"450\":1,\"453\":4,\"471\":1,\"483\":2}}],[\"extent\",{\"1\":{\"209\":1,\"272\":1,\"302\":1,\"428\":1,\"437\":1}}],[\"extending\",{\"0\":{\"291\":1,\"476\":1},\"1\":{\"200\":1,\"246\":1}}],[\"extend\",{\"1\":{\"128\":1,\"142\":1,\"200\":1,\"207\":1,\"279\":1,\"338\":1,\"496\":1}}],[\"extended\",{\"0\":{\"265\":1},\"1\":{\"112\":1,\"157\":1,\"176\":1,\"265\":1}}],[\"extends\",{\"1\":{\"104\":1,\"216\":1,\"239\":1,\"388\":1,\"393\":1}}],[\"extensible\",{\"1\":{\"287\":1,\"338\":1}}],[\"extensibility\",{\"1\":{\"105\":1,\"258\":1,\"361\":1,\"433\":1}}],[\"extensions\",{\"0\":{\"128\":1},\"1\":{\"128\":11,\"269\":1,\"294\":1}}],[\"extension\",{\"1\":{\"110\":1,\"128\":3,\"321\":1,\"361\":1,\"480\":1}}],[\"extensively\",{\"1\":{\"171\":1,\"183\":1,\"293\":1,\"450\":1}}],[\"extensive\",{\"1\":{\"96\":1,\"117\":1,\"130\":1,\"151\":1,\"224\":1,\"235\":1,\"245\":1,\"246\":1,\"286\":1,\"298\":1,\"314\":1,\"340\":1,\"348\":1,\"353\":1,\"359\":1,\"377\":1,\"389\":1,\"399\":1,\"402\":1,\"413\":1,\"417\":2,\"425\":1,\"430\":1,\"438\":1,\"447\":1,\"450\":1,\"455\":1,\"460\":1,\"463\":1,\"474\":1,\"478\":1,\"479\":2,\"488\":1,\"496\":1}}],[\"exchanging\",{\"1\":{\"435\":1}}],[\"exchange\",{\"0\":{\"458\":1},\"1\":{\"106\":1,\"458\":1}}],[\"except\",{\"1\":{\"412\":1}}],[\"exceptional\",{\"1\":{\"319\":1,\"377\":1,\"458\":1}}],[\"exceeds\",{\"1\":{\"412\":1}}],[\"exceeding\",{\"1\":{\"412\":1,\"489\":1}}],[\"exceed\",{\"1\":{\"382\":1}}],[\"excels\",{\"1\":{\"137\":1,\"151\":1,\"348\":1,\"377\":1}}],[\"excel\",{\"1\":{\"122\":1,\"137\":1,\"172\":1,\"309\":1,\"370\":1}}],[\"excelled\",{\"1\":{\"106\":1,\"393\":1}}],[\"excluded\",{\"1\":{\"410\":1}}],[\"excludes\",{\"1\":{\"161\":1,\"354\":1}}],[\"exclusively\",{\"1\":{\"227\":1}}],[\"excitingly\",{\"1\":{\"132\":1,\"330\":1}}],[\"exoskeleton\",{\"1\":{\"103\":1,\"246\":1}}],[\"exists\",{\"1\":{\"125\":1,\"182\":1}}],[\"existing\",{\"0\":{\"141\":1},\"1\":{\"103\":2,\"133\":1,\"137\":1,\"140\":1,\"141\":1,\"143\":1,\"163\":1,\"165\":1,\"167\":2,\"172\":1,\"176\":1,\"194\":1,\"197\":1,\"215\":1,\"222\":1,\"228\":1,\"231\":1,\"235\":1,\"238\":1,\"239\":1,\"246\":1,\"248\":1,\"260\":1,\"292\":2,\"300\":1,\"301\":2,\"304\":1,\"314\":1,\"316\":1,\"318\":1,\"319\":1,\"322\":1,\"329\":1,\"331\":1,\"333\":2,\"337\":1,\"338\":1,\"342\":1,\"345\":1,\"349\":1,\"353\":2,\"360\":1,\"362\":1,\"366\":1,\"368\":1,\"370\":1,\"378\":2,\"379\":1,\"392\":1,\"393\":1,\"399\":2,\"402\":1,\"411\":1,\"412\":1,\"413\":1,\"414\":1,\"415\":1,\"417\":1,\"418\":1,\"430\":1,\"432\":1,\"437\":2,\"447\":4,\"449\":1,\"453\":1,\"458\":1,\"463\":1,\"467\":2,\"468\":1,\"475\":1,\"480\":1,\"481\":1,\"482\":3,\"485\":1,\"488\":1,\"490\":1,\"495\":1,\"496\":1}}],[\"exist\",{\"1\":{\"100\":1,\"257\":1,\"403\":1}}],[\"existed\",{\"1\":{\"98\":1}}],[\"exponential\",{\"1\":{\"253\":1,\"314\":1,\"482\":1}}],[\"expose\",{\"1\":{\"409\":1}}],[\"exposes\",{\"1\":{\"390\":1}}],[\"exposed\",{\"1\":{\"211\":1,\"223\":1,\"304\":1}}],[\"exposure\",{\"1\":{\"244\":1,\"302\":1,\"443\":1}}],[\"exposing\",{\"1\":{\"135\":1,\"213\":1,\"460\":1}}],[\"express\",{\"1\":{\"226\":1,\"265\":1,\"399\":1}}],[\"expressed\",{\"1\":{\"202\":1,\"226\":1,\"252\":1,\"426\":1,\"485\":1}}],[\"expression\",{\"1\":{\"204\":1}}],[\"expressions\",{\"1\":{\"108\":1,\"124\":2}}],[\"expressing\",{\"0\":{\"302\":1},\"1\":{\"204\":1,\"241\":1,\"302\":2}}],[\"expressive\",{\"1\":{\"108\":2,\"269\":1}}],[\"expansion\",{\"1\":{\"465\":1}}],[\"expansive\",{\"1\":{\"181\":1}}],[\"expanded\",{\"1\":{\"297\":1}}],[\"expanding\",{\"1\":{\"253\":1,\"465\":1}}],[\"expand\",{\"1\":{\"167\":1,\"191\":1,\"261\":1}}],[\"expands\",{\"1\":{\"104\":1,\"183\":1}}],[\"expedite\",{\"1\":{\"498\":1}}],[\"expectation\",{\"1\":{\"294\":1,\"422\":1}}],[\"expectations\",{\"1\":{\"135\":1,\"163\":1,\"189\":1,\"263\":1,\"288\":1,\"369\":1,\"399\":1}}],[\"expected\",{\"1\":{\"257\":1,\"378\":1,\"422\":1}}],[\"expensive\",{\"1\":{\"133\":1,\"137\":1,\"140\":1,\"188\":1,\"194\":1,\"238\":1,\"284\":2,\"331\":1,\"353\":1,\"355\":1,\"416\":1,\"468\":1}}],[\"experiencing\",{\"1\":{\"296\":1}}],[\"experienced\",{\"1\":{\"106\":1,\"114\":1,\"156\":1,\"218\":1,\"242\":1,\"272\":1,\"445\":1}}],[\"experiences\",{\"0\":{\"302\":1},\"1\":{\"98\":1,\"117\":2,\"133\":1,\"135\":1,\"142\":1,\"144\":1,\"156\":2,\"175\":1,\"191\":1,\"210\":1,\"217\":1,\"227\":2,\"239\":1,\"264\":1,\"302\":3,\"305\":1,\"308\":1,\"331\":1,\"448\":1}}],[\"experience\",{\"0\":{\"3\":1,\"122\":1,\"154\":1,\"254\":1},\"1\":{\"105\":1,\"108\":1,\"114\":1,\"117\":1,\"122\":1,\"127\":2,\"128\":1,\"130\":1,\"142\":4,\"173\":1,\"189\":1,\"191\":2,\"192\":1,\"202\":1,\"203\":4,\"210\":1,\"214\":1,\"218\":1,\"220\":1,\"223\":1,\"229\":1,\"242\":1,\"243\":1,\"269\":1,\"273\":1,\"274\":1,\"282\":1,\"304\":1,\"324\":1,\"361\":1,\"386\":1}}],[\"experiential\",{\"1\":{\"142\":1,\"180\":1}}],[\"experimentation\",{\"1\":{\"136\":1,\"298\":1}}],[\"experimental\",{\"0\":{\"185\":1,\"230\":1,\"472\":1},\"1\":{\"116\":1,\"123\":1,\"130\":1,\"137\":1,\"141\":1,\"150\":1,\"160\":1,\"185\":2,\"190\":1,\"191\":1,\"222\":1,\"235\":1,\"255\":1,\"284\":1,\"291\":1,\"304\":1,\"329\":1,\"360\":1,\"385\":1,\"405\":1,\"422\":2,\"432\":1,\"472\":1,\"476\":1,\"479\":1,\"483\":1,\"485\":1,\"489\":1,\"492\":1}}],[\"experiments\",{\"1\":{\"121\":1,\"132\":1,\"136\":1,\"138\":1,\"142\":1,\"150\":1,\"152\":1,\"196\":1,\"199\":1,\"207\":1,\"209\":1,\"218\":1,\"219\":1,\"224\":1,\"226\":1,\"240\":1,\"245\":1,\"251\":1,\"260\":1,\"266\":1,\"273\":1,\"275\":1,\"282\":1,\"306\":1,\"314\":1,\"319\":1,\"327\":1,\"330\":1,\"334\":1,\"341\":1,\"362\":1,\"378\":1,\"399\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"417\":1,\"423\":1,\"425\":2,\"437\":1,\"442\":1,\"447\":1,\"449\":1,\"450\":1,\"453\":1,\"470\":1,\"478\":1,\"481\":1,\"485\":1,\"487\":1,\"490\":1,\"491\":1,\"494\":1,\"495\":1,\"498\":1}}],[\"experiment\",{\"0\":{\"112\":1},\"1\":{\"108\":1,\"112\":5,\"119\":1,\"152\":1,\"155\":1,\"158\":1,\"165\":1,\"173\":1,\"197\":1,\"218\":1,\"265\":2,\"269\":2,\"270\":1,\"274\":1,\"289\":1,\"303\":1,\"325\":1,\"355\":1}}],[\"expertise\",{\"1\":{\"136\":1,\"196\":1,\"212\":1,\"222\":1,\"238\":1,\"408\":1}}],[\"experts\",{\"0\":{\"344\":1,\"400\":2,\"427\":1},\"1\":{\"105\":1,\"120\":1,\"126\":1,\"189\":2,\"216\":1,\"290\":1,\"336\":1,\"344\":1,\"391\":1,\"400\":4,\"427\":2}}],[\"expert\",{\"0\":{\"196\":1},\"1\":{\"99\":1,\"133\":1,\"196\":1,\"217\":1,\"272\":1,\"290\":1,\"331\":1,\"356\":1,\"384\":1,\"477\":1,\"486\":3,\"490\":1}}],[\"explicitly\",{\"1\":{\"419\":1,\"420\":1,\"432\":1}}],[\"explicit\",{\"1\":{\"123\":1,\"424\":3}}],[\"exploding\",{\"1\":{\"469\":1}}],[\"exploded\",{\"1\":{\"102\":1}}],[\"exploit\",{\"1\":{\"334\":1,\"386\":1,\"419\":1,\"420\":1,\"469\":1}}],[\"exploited\",{\"1\":{\"327\":1}}],[\"exploitation\",{\"0\":{\"318\":1},\"1\":{\"318\":4}}],[\"exploiting\",{\"0\":{\"158\":1,\"407\":1},\"1\":{\"390\":1,\"427\":1}}],[\"explosion\",{\"1\":{\"314\":1}}],[\"exploratory\",{\"0\":{\"308\":1,\"339\":1},\"1\":{\"158\":3,\"197\":1,\"291\":1,\"301\":1,\"476\":1}}],[\"exploration\",{\"0\":{\"146\":1,\"158\":1,\"189\":1,\"271\":1,\"296\":1,\"318\":1,\"457\":1},\"1\":{\"100\":1,\"120\":1,\"142\":1,\"189\":1,\"213\":1,\"217\":1,\"226\":2,\"239\":1,\"242\":1,\"252\":2,\"257\":1,\"290\":1,\"296\":3,\"297\":1,\"301\":1,\"318\":4,\"377\":1,\"411\":1,\"426\":2,\"478\":1}}],[\"explored\",{\"1\":{\"155\":1,\"178\":1,\"181\":1,\"204\":1,\"226\":1,\"274\":1,\"392\":1,\"431\":1,\"491\":1}}],[\"explores\",{\"1\":{\"115\":1,\"205\":1,\"239\":1,\"277\":1,\"287\":1,\"296\":1,\"362\":1,\"365\":1,\"407\":1,\"424\":1,\"439\":1,\"458\":1,\"460\":1}}],[\"explore\",{\"1\":{\"98\":2,\"101\":1,\"107\":1,\"110\":2,\"112\":1,\"120\":1,\"144\":1,\"174\":1,\"179\":1,\"181\":1,\"213\":1,\"216\":1,\"217\":1,\"226\":1,\"233\":1,\"252\":1,\"255\":1,\"257\":1,\"270\":1,\"277\":1,\"278\":2,\"293\":1,\"296\":1,\"308\":1,\"319\":1,\"321\":2,\"342\":1,\"355\":1,\"371\":1,\"385\":1,\"389\":1,\"404\":1,\"417\":1,\"426\":1,\"429\":1,\"466\":1,\"470\":1,\"471\":1,\"472\":1,\"485\":1,\"496\":1,\"498\":1}}],[\"exploring\",{\"0\":{\"98\":1,\"102\":1,\"115\":1,\"120\":1,\"163\":1,\"174\":1,\"204\":1,\"226\":1,\"228\":1,\"242\":1,\"252\":1,\"290\":1,\"297\":1,\"371\":1,\"372\":1,\"426\":1,\"437\":1,\"491\":1,\"496\":1},\"1\":{\"112\":1,\"138\":1,\"211\":1,\"323\":1,\"437\":1,\"465\":1}}],[\"explanatory\",{\"1\":{\"187\":1}}],[\"explanation\",{\"0\":{\"111\":1},\"1\":{\"102\":1,\"131\":2,\"159\":1,\"187\":1,\"288\":1}}],[\"explanations\",{\"0\":{\"102\":1,\"131\":1,\"288\":1},\"1\":{\"102\":4,\"131\":6,\"158\":1,\"159\":2,\"171\":2,\"183\":1,\"199\":2,\"242\":1,\"288\":2,\"363\":3,\"391\":1}}],[\"explained\",{\"1\":{\"286\":1,\"324\":1,\"474\":1}}],[\"explain\",{\"1\":{\"187\":1}}],[\"explainability\",{\"0\":{\"129\":1,\"199\":1,\"231\":1},\"1\":{\"217\":1}}],[\"explainable\",{\"0\":{\"124\":1},\"1\":{\"102\":1,\"124\":1,\"131\":1,\"159\":1,\"229\":1,\"231\":1,\"253\":1,\"288\":1}}],[\"explaining\",{\"0\":{\"199\":1,\"363\":1},\"1\":{\"102\":1,\"111\":1,\"159\":1,\"187\":1,\"390\":1}}],[\"esha\",{\"1\":{\"465\":1}}],[\"eshwar\",{\"1\":{\"200\":1}}],[\"essam\",{\"1\":{\"393\":1}}],[\"essential\",{\"1\":{\"114\":1,\"118\":1,\"141\":1,\"143\":1,\"160\":1,\"184\":1,\"191\":1,\"211\":1,\"216\":1,\"235\":1,\"244\":1,\"260\":1,\"353\":1,\"363\":1,\"378\":1,\"380\":1,\"396\":1,\"417\":1,\"447\":1,\"488\":1}}],[\"eslam\",{\"1\":{\"393\":1}}],[\"escalates\",{\"1\":{\"418\":1}}],[\"escalated\",{\"1\":{\"139\":1}}],[\"escalation\",{\"1\":{\"297\":1}}],[\"esin\",{\"1\":{\"241\":1}}],[\"esfahani\",{\"1\":{\"158\":1}}],[\"establishing\",{\"1\":{\"217\":1,\"236\":1,\"326\":1,\"492\":1}}],[\"established\",{\"1\":{\"153\":1,\"159\":1,\"244\":2,\"260\":1,\"324\":1,\"411\":1,\"437\":1,\"460\":1,\"461\":2,\"477\":1}}],[\"establishes\",{\"1\":{\"118\":1}}],[\"establish\",{\"1\":{\"151\":1,\"166\":1,\"319\":1,\"348\":1,\"497\":1}}],[\"estimating\",{\"1\":{\"272\":1}}],[\"estimations\",{\"1\":{\"246\":3}}],[\"estimation\",{\"0\":{\"103\":1},\"1\":{\"246\":1,\"428\":1}}],[\"estimate\",{\"1\":{\"230\":1}}],[\"estimated\",{\"1\":{\"183\":1,\"196\":1,\"197\":1,\"408\":1}}],[\"estimates\",{\"1\":{\"96\":1,\"135\":2}}],[\"especially\",{\"1\":{\"96\":1,\"152\":1,\"169\":1,\"184\":1,\"213\":1,\"219\":1,\"228\":1,\"290\":1,\"337\":1,\"369\":1,\"380\":1,\"381\":1,\"401\":1,\"402\":1,\"408\":1,\"432\":1,\"446\":1,\"468\":1,\"470\":1,\"479\":1}}],[\"espacio\",{\"1\":{\"9\":1}}],[\"enrich\",{\"1\":{\"450\":1}}],[\"enriches\",{\"1\":{\"359\":1}}],[\"enriched\",{\"1\":{\"152\":1,\"213\":1,\"271\":1,\"457\":1}}],[\"enforces\",{\"1\":{\"384\":1}}],[\"enforced\",{\"1\":{\"209\":1,\"253\":1}}],[\"enhong\",{\"1\":{\"318\":1}}],[\"enhancing\",{\"0\":{\"115\":1,\"124\":1,\"274\":1,\"281\":1,\"347\":1,\"356\":1,\"360\":1,\"405\":1,\"471\":1,\"477\":1},\"1\":{\"120\":1,\"146\":1,\"159\":1,\"173\":3,\"195\":1,\"203\":1,\"216\":1,\"264\":1,\"290\":1,\"291\":1,\"318\":2,\"337\":1,\"400\":1,\"404\":1,\"413\":1,\"425\":1,\"429\":1,\"430\":1,\"438\":1,\"448\":1,\"459\":1,\"463\":1,\"467\":1,\"472\":1,\"476\":1,\"477\":1,\"489\":2}}],[\"enhances\",{\"1\":{\"204\":1,\"246\":1,\"283\":1,\"314\":1,\"350\":1,\"359\":1,\"384\":1,\"398\":1,\"404\":2,\"407\":1,\"454\":1}}],[\"enhancements\",{\"0\":{\"152\":1},\"1\":{\"316\":1}}],[\"enhancement\",{\"1\":{\"124\":1}}],[\"enhanced\",{\"0\":{\"454\":1,\"479\":1},\"1\":{\"119\":1,\"133\":1,\"151\":2,\"187\":1,\"331\":1,\"348\":2,\"349\":1,\"364\":1,\"377\":1,\"403\":1,\"419\":1,\"420\":1,\"425\":1,\"432\":2,\"529\":1}}],[\"enhance\",{\"0\":{\"160\":1,\"298\":1,\"362\":1},\"1\":{\"114\":1,\"115\":1,\"118\":1,\"120\":2,\"123\":1,\"124\":2,\"127\":1,\"130\":2,\"142\":1,\"166\":1,\"167\":1,\"186\":1,\"201\":1,\"206\":1,\"218\":1,\"223\":1,\"229\":1,\"233\":1,\"234\":1,\"240\":2,\"245\":1,\"246\":1,\"252\":1,\"258\":1,\"274\":1,\"277\":1,\"300\":1,\"305\":1,\"318\":1,\"323\":1,\"328\":1,\"353\":1,\"355\":1,\"356\":1,\"358\":1,\"360\":1,\"365\":1,\"384\":1,\"404\":2,\"426\":1,\"433\":1,\"447\":1,\"462\":1,\"470\":1,\"471\":2,\"475\":1,\"477\":1,\"479\":1,\"481\":1,\"487\":1}}],[\"ensembling\",{\"1\":{\"467\":1}}],[\"ensembles\",{\"1\":{\"467\":1}}],[\"ensemble\",{\"0\":{\"344\":1,\"386\":1},\"1\":{\"305\":1,\"344\":1,\"386\":2,\"439\":1,\"467\":1}}],[\"ensure\",{\"1\":{\"176\":1,\"219\":1,\"234\":1,\"266\":1,\"302\":1,\"396\":1,\"401\":1,\"442\":1}}],[\"ensures\",{\"1\":{\"114\":1,\"139\":1,\"378\":1,\"398\":1}}],[\"ensuring\",{\"1\":{\"125\":1,\"135\":1,\"167\":1,\"206\":1,\"211\":1,\"260\":1,\"288\":1,\"313\":1,\"333\":1,\"334\":1,\"356\":1,\"359\":1,\"361\":1,\"396\":2,\"428\":1,\"453\":1,\"466\":1,\"480\":1}}],[\"en\",{\"0\":{\"232\":1}}],[\"enock\",{\"1\":{\"302\":1}}],[\"enormous\",{\"1\":{\"217\":1,\"469\":1}}],[\"enough\",{\"1\":{\"173\":1,\"240\":1,\"253\":1,\"278\":1,\"318\":1,\"432\":1,\"482\":1}}],[\"entailment\",{\"1\":{\"424\":1}}],[\"entailing\",{\"1\":{\"360\":1}}],[\"entailed\",{\"1\":{\"359\":1}}],[\"entangle\",{\"1\":{\"242\":1}}],[\"entangled\",{\"1\":{\"199\":1,\"337\":1}}],[\"enthrall\",{\"1\":{\"269\":1}}],[\"entity\",{\"0\":{\"323\":1},\"1\":{\"323\":1,\"395\":1,\"455\":1,\"459\":1}}],[\"entities\",{\"1\":{\"143\":1,\"293\":1,\"340\":1}}],[\"entirely\",{\"1\":{\"340\":1,\"439\":1}}],[\"entire\",{\"1\":{\"259\":1,\"286\":1,\"412\":1,\"443\":1,\"454\":1,\"474\":1}}],[\"entries\",{\"1\":{\"365\":2}}],[\"entrepreneurial\",{\"1\":{\"293\":1}}],[\"entrepreneurship\",{\"0\":{\"3\":1}}],[\"entropy\",{\"0\":{\"255\":1},\"1\":{\"240\":1,\"255\":1}}],[\"entry\",{\"1\":{\"209\":1,\"388\":1}}],[\"enterprise\",{\"1\":{\"279\":1}}],[\"entertaining\",{\"1\":{\"265\":1}}],[\"entertain\",{\"1\":{\"265\":1}}],[\"entertainment\",{\"0\":{\"282\":1},\"1\":{\"265\":1,\"269\":1}}],[\"enter\",{\"1\":{\"180\":1}}],[\"enumerating\",{\"1\":{\"140\":1}}],[\"enjoyable\",{\"1\":{\"243\":1}}],[\"enjoyment\",{\"1\":{\"140\":1}}],[\"enjoyed\",{\"1\":{\"99\":1}}],[\"encapsulated\",{\"1\":{\"314\":1}}],[\"encapsulates\",{\"1\":{\"264\":1,\"448\":1}}],[\"encrypted\",{\"1\":{\"139\":1}}],[\"encryption\",{\"1\":{\"139\":1}}],[\"encompassed\",{\"1\":{\"358\":1}}],[\"encompasses\",{\"1\":{\"96\":1,\"239\":1,\"398\":1,\"460\":1}}],[\"encompassing\",{\"1\":{\"151\":1,\"254\":1,\"333\":1,\"348\":1,\"356\":1,\"437\":1}}],[\"encode\",{\"1\":{\"207\":1,\"485\":1}}],[\"encodes\",{\"1\":{\"207\":1}}],[\"encoded\",{\"1\":{\"151\":1,\"207\":2,\"348\":1,\"390\":1}}],[\"encoders\",{\"1\":{\"190\":1,\"315\":1,\"347\":1}}],[\"encoder\",{\"1\":{\"103\":1,\"190\":1,\"213\":1,\"234\":2,\"395\":3,\"419\":1,\"420\":1,\"436\":1,\"460\":1}}],[\"encoding\",{\"0\":{\"165\":1,\"454\":1},\"1\":{\"123\":1,\"189\":1,\"197\":1,\"256\":2,\"269\":1,\"479\":4}}],[\"encouraging\",{\"1\":{\"200\":1,\"232\":1,\"255\":1,\"297\":1,\"428\":1}}],[\"encourages\",{\"1\":{\"301\":1}}],[\"encourage\",{\"1\":{\"132\":1,\"154\":1,\"226\":1,\"268\":1,\"303\":1,\"330\":1,\"395\":1,\"452\":1,\"489\":1}}],[\"encouraged\",{\"1\":{\"108\":1,\"209\":1,\"274\":1}}],[\"encountering\",{\"1\":{\"296\":1,\"482\":1}}],[\"encounter\",{\"1\":{\"110\":1,\"122\":1,\"176\":1,\"321\":1,\"323\":1,\"432\":1}}],[\"encountered\",{\"1\":{\"106\":1,\"122\":1,\"261\":1,\"358\":1,\"459\":1}}],[\"enkelejda\",{\"1\":{\"127\":1,\"139\":1,\"273\":1,\"290\":1}}],[\"enkeleda\",{\"1\":{\"127\":1}}],[\"enabling\",{\"0\":{\"140\":1,\"434\":1,\"480\":1},\"1\":{\"126\":1,\"141\":1,\"159\":1,\"171\":1,\"176\":1,\"194\":1,\"195\":1,\"210\":1,\"218\":1,\"229\":1,\"257\":1,\"274\":1,\"326\":1,\"340\":1,\"390\":1,\"393\":1,\"430\":1,\"469\":1,\"485\":1}}],[\"enabled\",{\"1\":{\"174\":1,\"194\":1,\"319\":1,\"371\":1,\"491\":1}}],[\"enables\",{\"1\":{\"108\":1,\"127\":1,\"130\":1,\"137\":1,\"140\":1,\"194\":1,\"201\":1,\"210\":1,\"213\":1,\"218\":1,\"224\":1,\"264\":1,\"294\":1,\"296\":1,\"313\":1,\"317\":1,\"361\":1,\"415\":2,\"448\":1,\"463\":1}}],[\"enable\",{\"0\":{\"261\":1},\"1\":{\"105\":1,\"159\":1,\"184\":1,\"195\":1,\"201\":1,\"215\":1,\"229\":1,\"244\":1,\"334\":1,\"337\":1,\"342\":1,\"378\":1,\"380\":1,\"408\":1,\"414\":1,\"469\":1}}],[\"envisions\",{\"1\":{\"364\":1}}],[\"envision\",{\"1\":{\"248\":1}}],[\"envisioned\",{\"1\":{\"216\":1}}],[\"envisioning\",{\"1\":{\"100\":1,\"169\":1,\"227\":1}}],[\"environmentalism\",{\"1\":{\"217\":1}}],[\"environmental\",{\"1\":{\"165\":4,\"173\":1,\"207\":2,\"217\":2,\"244\":1,\"475\":1,\"478\":1}}],[\"environments\",{\"0\":{\"165\":1},\"1\":{\"119\":1,\"135\":1,\"157\":1,\"165\":1,\"179\":1,\"180\":1,\"195\":2,\"201\":1,\"204\":2,\"219\":1,\"231\":1,\"269\":1,\"274\":1,\"283\":1,\"300\":1,\"304\":3,\"305\":1,\"318\":2,\"340\":1,\"345\":1,\"381\":1,\"401\":1,\"407\":1,\"430\":1,\"465\":1}}],[\"environment\",{\"1\":{\"118\":1,\"120\":2,\"140\":1,\"146\":2,\"156\":2,\"173\":3,\"186\":1,\"195\":2,\"218\":1,\"244\":1,\"253\":1,\"270\":1,\"272\":1,\"275\":2,\"283\":2,\"318\":1,\"361\":1,\"411\":1,\"417\":1,\"429\":1,\"494\":1}}],[\"energy\",{\"0\":{\"465\":1},\"1\":{\"112\":1,\"128\":1,\"217\":1,\"295\":1,\"465\":5}}],[\"engel\",{\"1\":{\"296\":1}}],[\"engin\",{\"1\":{\"292\":1}}],[\"engine\",{\"1\":{\"201\":1,\"284\":1,\"326\":1}}],[\"engineer\",{\"1\":{\"175\":1,\"495\":1}}],[\"engineers\",{\"0\":{\"153\":1},\"1\":{\"257\":1,\"308\":1,\"363\":1}}],[\"engineering\",{\"0\":{\"364\":1,\"519\":1},\"1\":{\"110\":1,\"153\":1,\"252\":1,\"257\":1,\"305\":1,\"308\":5,\"318\":1,\"321\":1,\"358\":1,\"364\":2,\"388\":1,\"403\":1,\"404\":1,\"426\":1,\"437\":1,\"442\":1,\"453\":1,\"502\":1,\"510\":2}}],[\"english\",{\"0\":{\"124\":1,\"395\":1,\"497\":1},\"1\":{\"124\":1,\"155\":1,\"315\":3,\"333\":2,\"352\":1,\"365\":1,\"377\":3,\"395\":1,\"416\":1,\"497\":2}}],[\"engaging\",{\"0\":{\"148\":1},\"1\":{\"117\":1,\"120\":1,\"172\":1,\"191\":1,\"192\":1,\"212\":1,\"269\":1,\"286\":2,\"326\":1,\"370\":1,\"418\":1,\"474\":2}}],[\"engaged\",{\"1\":{\"108\":1,\"179\":1,\"198\":1,\"304\":1,\"387\":1}}],[\"engagement\",{\"0\":{\"198\":1,\"387\":1},\"1\":{\"108\":1,\"120\":1,\"130\":1,\"131\":1,\"167\":1,\"198\":1,\"200\":2,\"216\":1,\"248\":1,\"268\":1,\"269\":2,\"286\":1,\"291\":1,\"387\":1,\"452\":1,\"474\":1,\"476\":1}}],[\"engage\",{\"1\":{\"98\":1,\"108\":1,\"110\":1,\"132\":1,\"155\":1,\"175\":1,\"186\":1,\"195\":1,\"243\":1,\"293\":1,\"321\":1,\"330\":1,\"399\":1,\"498\":1}}],[\"endpoints\",{\"1\":{\"414\":1}}],[\"endowing\",{\"1\":{\"329\":1}}],[\"endert\",{\"1\":{\"231\":1}}],[\"ended\",{\"1\":{\"151\":1,\"296\":1,\"348\":1,\"378\":1}}],[\"ends\",{\"1\":{\"153\":1}}],[\"end\",{\"0\":{\"293\":1,\"362\":2},\"1\":{\"99\":1,\"107\":1,\"213\":1,\"231\":1,\"298\":1,\"317\":1,\"329\":1,\"345\":2,\"361\":2,\"362\":4,\"390\":2,\"412\":1,\"417\":1,\"445\":5,\"456\":2,\"462\":2}}],[\"e9t8\",{\"1\":{\"74\":1}}],[\"emre\",{\"1\":{\"269\":1}}],[\"eminent\",{\"1\":{\"355\":1}}],[\"emission\",{\"1\":{\"238\":1}}],[\"emilie\",{\"1\":{\"232\":1}}],[\"emiliano\",{\"1\":{\"171\":1}}],[\"emily\",{\"1\":{\"155\":1,\"322\":1}}],[\"emulating\",{\"1\":{\"222\":1}}],[\"email\",{\"1\":{\"539\":2}}],[\"emails\",{\"1\":{\"124\":1}}],[\"emami\",{\"1\":{\"323\":1}}],[\"emani\",{\"1\":{\"144\":1}}],[\"emory\",{\"1\":{\"392\":1}}],[\"emocommonsense\",{\"1\":{\"123\":1}}],[\"emotive\",{\"1\":{\"266\":1}}],[\"emotic\",{\"1\":{\"123\":1}}],[\"emotions\",{\"0\":{\"204\":1,\"265\":1,\"305\":1},\"1\":{\"123\":2,\"202\":1,\"204\":7,\"265\":3}}],[\"emotion\",{\"0\":{\"123\":1,\"266\":1,\"351\":2},\"1\":{\"123\":2,\"202\":1,\"204\":5,\"218\":1,\"265\":1,\"266\":1,\"305\":1,\"351\":3}}],[\"emotionally\",{\"1\":{\"237\":1,\"305\":1}}],[\"emotional\",{\"0\":{\"237\":1,\"265\":1},\"1\":{\"120\":2,\"203\":1,\"237\":3,\"265\":2,\"266\":1,\"268\":1,\"452\":1,\"461\":1}}],[\"emerge\",{\"1\":{\"337\":1,\"410\":1}}],[\"emerges\",{\"1\":{\"167\":1,\"461\":1}}],[\"emergency\",{\"0\":{\"298\":1},\"1\":{\"298\":2}}],[\"emergence\",{\"1\":{\"130\":1,\"244\":1}}],[\"emergent\",{\"1\":{\"163\":1,\"252\":1,\"376\":1,\"426\":1,\"495\":1}}],[\"emerged\",{\"1\":{\"120\":1,\"136\":1,\"213\":1,\"304\":1,\"317\":1}}],[\"emerging\",{\"1\":{\"119\":1,\"200\":1,\"201\":1,\"239\":1,\"272\":1,\"286\":1,\"364\":1,\"379\":1,\"443\":1,\"474\":1}}],[\"embarks\",{\"1\":{\"358\":1}}],[\"embarked\",{\"1\":{\"106\":1,\"266\":1}}],[\"embeds\",{\"1\":{\"268\":1,\"452\":1}}],[\"embedding\",{\"0\":{\"347\":1},\"1\":{\"190\":1,\"231\":1,\"301\":2,\"347\":2,\"368\":1,\"410\":1,\"419\":1,\"420\":1,\"460\":1}}],[\"embeddings\",{\"1\":{\"124\":1,\"301\":2,\"352\":1,\"365\":1,\"384\":1,\"442\":1,\"479\":4}}],[\"embedded\",{\"0\":{\"115\":1},\"1\":{\"115\":1,\"207\":2,\"223\":1,\"390\":1,\"480\":1}}],[\"embodying\",{\"1\":{\"274\":1}}],[\"embody\",{\"1\":{\"227\":1,\"438\":1,\"453\":1}}],[\"embodiment\",{\"1\":{\"107\":2,\"227\":1}}],[\"embodied\",{\"0\":{\"227\":1,\"274\":1},\"1\":{\"107\":3,\"117\":1,\"195\":1,\"227\":3}}],[\"embrace\",{\"1\":{\"216\":1}}],[\"emph\",{\"1\":{\"334\":1}}],[\"emphasis\",{\"1\":{\"172\":1,\"370\":1,\"424\":1}}],[\"emphasise\",{\"1\":{\"166\":1}}],[\"emphasize\",{\"1\":{\"152\":1,\"231\":1,\"280\":1,\"290\":1,\"301\":1,\"447\":1}}],[\"emphasizes\",{\"1\":{\"106\":1,\"143\":1,\"300\":1,\"328\":1}}],[\"emphasizing\",{\"1\":{\"144\":1,\"254\":1,\"290\":1,\"466\":1,\"492\":1}}],[\"empathic\",{\"1\":{\"274\":1}}],[\"empathy\",{\"0\":{\"274\":1},\"1\":{\"274\":4,\"461\":1}}],[\"empathetic\",{\"1\":{\"266\":1}}],[\"empowered\",{\"1\":{\"239\":1}}],[\"empower\",{\"1\":{\"213\":1,\"229\":1}}],[\"empowers\",{\"1\":{\"133\":1,\"331\":1}}],[\"empowering\",{\"1\":{\"74\":1,\"217\":1}}],[\"employing\",{\"1\":{\"120\":1,\"174\":1,\"187\":1,\"234\":1,\"238\":1,\"318\":1,\"350\":1,\"356\":1,\"371\":1,\"404\":1,\"408\":1,\"424\":1,\"425\":1,\"459\":1,\"487\":1}}],[\"employs\",{\"1\":{\"120\":1,\"141\":1,\"176\":1,\"257\":1,\"284\":1,\"306\":1,\"360\":1,\"479\":1}}],[\"employed\",{\"1\":{\"116\":1,\"277\":1,\"459\":1}}],[\"employ\",{\"1\":{\"96\":1,\"142\":1,\"195\":1,\"245\":1,\"289\":2,\"309\":1,\"430\":1,\"442\":1,\"458\":1,\"495\":1}}],[\"empirical\",{\"0\":{\"128\":1,\"149\":1,\"394\":1},\"1\":{\"96\":1,\"125\":1,\"128\":2,\"139\":1,\"148\":1,\"149\":1,\"150\":1,\"167\":1,\"174\":1,\"197\":1,\"214\":1,\"309\":1,\"371\":1,\"394\":2,\"408\":1,\"417\":2,\"490\":1,\"495\":1}}],[\"empirically\",{\"1\":{\"96\":1,\"102\":1,\"317\":1,\"369\":1,\"376\":1,\"486\":1,\"495\":1}}],[\"emdrup\",{\"1\":{\"49\":1}}],[\"emdrup的第一位主持人\",{\"1\":{\"48\":1}}],[\"emdrup还使用了\",{\"1\":{\"48\":1}}],[\"e\",{\"1\":{\"22\":2,\"101\":1,\"102\":1,\"124\":1,\"126\":1,\"128\":6,\"133\":1,\"151\":2,\"164\":1,\"166\":1,\"190\":1,\"192\":1,\"198\":2,\"202\":1,\"211\":1,\"222\":1,\"224\":1,\"240\":1,\"241\":1,\"252\":1,\"268\":1,\"270\":1,\"284\":1,\"302\":1,\"305\":1,\"319\":1,\"322\":1,\"326\":2,\"331\":1,\"339\":1,\"348\":2,\"353\":1,\"369\":1,\"379\":1,\"382\":1,\"384\":2,\"387\":2,\"390\":1,\"391\":2,\"394\":1,\"398\":1,\"405\":1,\"409\":4,\"417\":1,\"426\":1,\"427\":1,\"441\":1,\"444\":1,\"450\":2,\"452\":1,\"453\":1,\"454\":1,\"455\":2,\"467\":1,\"483\":1,\"494\":2}}],[\"ezvr\",{\"1\":{\"9\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n,id:o}})=>{const u=bt[s];e==="suggest"?self.postMessage([e,o,tt(t,u,n)]):e==="search"?self.postMessage([e,o,Z(t,u,n)]):self.postMessage({suggestions:[e,o,tt(t,u,n)],results:[e,o,Z(t,u,n)]})};
//# sourceMappingURL=index.js.map
